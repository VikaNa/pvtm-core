,Index,date,text,title
9538,3628169,2018-02-20 17:34:22,"Sonos One is the speaker to beat for those who want great sound and smarts0The connected speaker wars are upon us, and one day they will be detailed in history books for all to remember. But here, now, it can be hard to cut through the various narratives surrounding the options out there and pick a winner. Now that the cards are on the table in terms of offerings from the major players, it’s pretty clear that Sonos has the best option available for most people.Sonos One, the connected speaker the company released last year, is a terrific sounding Wi-Fi-enabled speaker that also has built-in support for Amazon’s Alexa, which is if not the best smart assistant out there, then at least tied for first with Google’s Assistant.On the sound front, Sonos has the most experience of any of the top three companies making smart speakers worth your consideration, too. The Sonos One is, in many ways, just an updated version of the Sonos Play:1 that’s acoustically very similar — but that’s actually a really good thing. The Sonos One, like the Play:1, is a terrific-sounding audio device, especially given its size and physical footprint.I’ve been using a pair of Sonos Ones for the past couple of weeks, and it’s clear they do a great job of filling a room with sound, thanks in part to Sonos’ sound-shaping tech that uses a two-minute setup process involving waving your phone around to properly model the audio they put out for your space.Individually, a Sonos One is already a strong contender, even against the Google Home Max and HomePod, for sound quality for most people (who don’t need the additional power or won’t notice the auditory improvements afforded by the larger speakers) — but the Sonos One has a another neat trick up its sleeve, because it can form a stereo pair with a second Sonos One. This provides true sound separation, meaning left and right channels reproduced as they were actually meant to be, instead of via some simulated stereo separation effect (which can be pretty cool, as HomePod reviews show, but which ultimately can’t match true stereo separation).Another huge benefit of Sonos versus the competition: the Sonos One integrates out of the box with the rest of your Sonos setup, should you have one. You can control all speakers via voice, and group them together for whole home/room-by-room playback. Google’s Home Max can work together with Chromecast-enabled speakers for similar multi-room streaming setups, and HomePod is set to get an update that will add multi-room and stereo syncing, but Sonos One offers both of these now, and using a method that’s proven to work.There’s also pricing to consider. Sonos One, in a bundle with two, is available for $349 right now, which is the same price as a single HomePod. It’s an unbeatable deal, given the other advantages listed above, especially because it means you can see if you like it alone, or equip multiple rooms with Alexa smarts and quality connected sound in one go.There are reasons to consider other options, to be sure, especially if you’re 100 percent committed to the Apple ecosystem of device and services, but in general, for most people, for most use cases, Sonos One is the far better choice.",Sonos One is the speaker to beat for those who want great sound and smarts
9539,3628170,2018-02-20 16:00:03,"Stride, Atlassian’s Slack competitor, opens its API to all developers0The arrival of Stride, Atlassian’s Slack competitor, was probably the company’s biggest launch of 2017. While the company generally allows developers to easily integrate with its products, Stride’s API remained in closed beta for significantly longer than the product itself, which exited beta last September. Today, however, Atlassian is opening the Stride API to all developers.As the company notes, this is the first API that sits on top of the new Atlassian API platform. Thanks to this, Stride developers will get access to a new app management console that makes it easier for them to manage their app’s credentials, for example. In addition, Atlassian also is making a new documentation interface available for Stride developers.The Stride team stresses that third-party apps in Stride are “first class citizens.” Unsurprisingly, developers can create new Bots and other experiences that center around sending and receiving messages. Apps, however, also can display their own user interface for showing contextual information in the Stride sidebar with the help of a JavaScript API.Developers also can include app cards inside conversations and create action buttons (or even build a pop-up dialog for when they need to show multiple button actions, for example). These buttons can both act on Stride itself (to open the app sidebar or a dialog, for example) or call on a backend service (which could be any REST endpoint). The team also notes that Stride apps can upload files (think presentations, videos and pictures) into conversations.Atlassian says about 1,000 developers signed up for early access to the API.What’s maybe more important, though, is that the company also says that “tens of thousands of teams” now use Stride. That’s not exactly at the same level of Slack, which has more than 6 million active users, or Microsoft Teams, which is now in use by more than 125,000 teams, but it shows there’s some momentum behind the platform. The modern workplace, after all, seems to have a need for an ever-increasing number of tools that provide constant interruptions.","Stride, Atlassian’s Slack competitor, opens its API to all developers"
9540,3628171,2018-02-20 14:00:52,"Say goodbye to Android Pay and hello to Google Pay0As we reported last month, Google is uniting all of its different payment tools under the Google Pay brand. On Android, however, the Android Pay app stuck with its existing brand. That’s changing today, though, with the launch of Google Pay for Android. With this, Google is rolling out an update to Android Pay and introducing some new functionality that the company hopes will make its payment service ubiquitous — both in stores and on the internet.In addition, Google is also launching a redesign of the Google Wallet app for sending and requesting money, and it’s now called Google Pay Send. Users in the U.S. and U.K., though, will also soon be able to use the Google Pay app for sending and requesting money. New users can download the Google Pay app today and existing Android Pay users will get updated over the course of the next few days.At first glance, the new Google Pay app is basically a redesign of Android Pay, with a look and feel that adheres closer to Google’s own Material Design guidelines than the original. In terms of functionality, there isn’t all that much here that’s new. One notable change, though, is that the Google Pay home screen now shows you relevant stores around you where you can pay with Google Pay. That list is personalized, based on previous stores where you used the service, as well as your location. In addition, the home screen shows you all of your recent purchases and you can still add all of your loyalty cards to the app.As Google’s VP of Product Management for Payments, Pali Bhat, told me, the team really wanted to make it extremely easy to get started with Google Pay and use the service to pay for goods online and in the real world — and to do so with as little friction as possible. That means that users who bank with Bank of America in the U.S. or a Google partner like Mbank in Poland can set up Google Pay right from their bank’s app without having to install Google Pay. Once that’s set up, you can simply pay with Google Pay online and out in the real world.Similarly, if an online app or website wants to support this, developers can call a Google API to see if a given user has Google Pay enabled and can then accept payments through Google Pay (which still get routed through the developers’ regular payment processors like Stripe or Braintree).“We give developers a very simple API to implement Google Pay,” Bhat noted. “The API is simple because we are not processing that payment. We just securely pass the credentials to whoever is doing it.” Apps like DoorDash, Airbnb, Hotel Tonight and others already support this feature today.",Say goodbye to Android Pay and hello to Google Pay
9541,3628172,2018-02-19 11:35:25,"The official Galaxy S9 launch will be on February 25Update: Someone claims to have used the Galaxy S9, revealing information about the design, camera, speakers and its rumored Animoji-like feature. Plus, Samsung has teased the camera skills of the Galaxy S9 in four short videos.The Samsung Galaxy S9 launch is February 25 and will be the biggest of 2018 so far, with the eyes of the world firmly placed on the South Korean firm as it introduces its retort to Apple's iPhone X, which arrived towards the end of 2017.The invites for the event have gone out, and we know we'll see the S9 at Samsung's official launch event at MWC 2018 at the end of February.There's no question that this is when we'll see the refined sequel to last year's Samsung Galaxy S8, with the invite clearly teasing the number nine, and we already have a good idea of what this Android smartphone looks like.The thing is, the S9 isn't looking like a phone to redefine the space. All the rumors, leaks and speculation to date point towards an iterative update. For those hoping for a game-changing device in the Galaxy portfolio, you may have to wait until 2019.It won't just be the S9 though, with rumors heavily hinting at the larger Samsung Galaxy S9 Plus set to arrive alongside - following the trend of launching dual flagships phones that the firm started back in 2015 with the Galaxy S6.What's the new Samsung Galaxy S9 news?A Reddit user (who has since deleted their account) claims to have used the Galaxy S9 and has revealed several new details, plus, the Samsung Galaxy S9 has been officially teased in several videos pointing to new camera abilities. We're also hearing that the phone could have an Animoji-like feature and two speakers.What will the Samsung Galaxy S9 price be?We were expecting £639 / $725 / AU$1200 - the same as the Galaxy S8 - to be the price of the new phone, but an industry insider has now told TechRadar the price will be upped to £789 in the UK. That may mean price hikes for other markets too.What's the Samsung Galaxy S9 release date?The Samsung Galaxy S9 launch date is February 25, according to Samsung's Unpacked 2018 invite. It's unclear when it'll arrive to its first customers, but pre-orders are likely to start on March 1 and the phone may ship in mid-March.Samsung Galaxy S9 release dateThe Samsung Galaxy S9 launch date is locked down for Sunday, February 25 in Barcelona.We know that, because Samsung has sent us an invite to its Unpacked event on that date, teasing the number nine in the process.The Samsung Galaxy S9 invite teases the number nine and an improved cameraThis makes the Galaxy S9 launch earlier than 2017's S8 announcement, which happened after MWC, in March.The pre-order date is rumored to be March 1, while the Samsung Galaxy S9 release date is tipped to be mid-March according to our sources close to the S9.TechRadar's take: The Galaxy S9 launch date is now locked in, and while the pre-order and release dates aren't confirmed, the rumored timings make sense, but are certainly subject to change.Samsung Galaxy S9 priceWhat we can safely predict is that the Samsung Galaxy S9 price is sure to be steep, as the Galaxy S8 launched at $720, £689, AU$1,199.The Galaxy S9 price will, at the very least, mirror that launch price of the S8, but there's always a chance it could cost even more.One industry insider has told TechRadar the price for the S9 will be starting at £789 in the UK. There's no word on if the price will be higher than the S8 in other markets, but it would be a bit unfair to charge more just in the UK.TechRadar's take: We expect the Galaxy S9 to inherit the same launch price as its predecessor or it'll cost quite a bit more.Samsung Galaxy S9 screenHottest leaks:An in-screen scannerA water-repellent coatingA 5.8-inch QHD+ screenThe Bell reports unnamed industry sources saying that Samsung has been hard at work on the display panels for the Galaxy S9 since late March 2017. If true, that’s apparently about six months earlier than usual.As for the form the screen will take, it's rumored (and likely) to retain the Galaxy S8's 18.5:9 aspect ratio.Samsung Galaxy S9 specsBased on the rumors so far, this is what the Galaxy S9's spec sheet looks like.There's also a good chance the display will be the same size as the Galaxy S8, coming in at 5.8 inches, despite some rumors to the contrary. It's also sure to be curved and continue Samsung's trend of using Super AMOLED, which delivers vibrant visuals and good contrast.Rumors of an in-screen fingerprint scanner abound, like the new Qualcomm Fingerprint Sensor can sit below quite thick displays, but we don't anticipate this coming to the S9.Synaptics hasannounced an in-screen scanner and mentioned not just OLED but also ""infinity display"" (which is what Samsung calls the S8's screen) and that it's being used on a phone made by a top five manufacturer - but we don't see it being Samsung.Aside from that, in late 2016 Samsung licensed a new glass coating technology that makes water bounce off your smartphone screen. Samsung plans to include this tech in an upcoming phone, so it may mean the Galaxy S9 is much easier to use in the rain.Watch the video below to see how the new glass coating technology worksTechRadar's take: The screen is likely to be a similar size to the S8's and will probably retain that phone's 18.5:9 aspect ratio, since that's the new popular form. Don't count on an in-screen scanner though.Samsung Galaxy S9 designHottest leaks:Even smaller bezelsA repositioned fingerprint scanner below the rear cameraThe headphone jack remainsThe Samsung Galaxy S9 is becoming clearer almost by the day, with more and more high quality leaks hitter the web giving us a convincing look at what the ninth generation 'S' flagship will offer us.Twitter leaker Roland Quandt posts a series of official looking renders of the Samsung Galaxy S9 and S9 Plus, revealing three color options, a rear facing fingerprint scanner beneath the camera and even more slender bezels.Case manufacturer dbrand posts a product page for its Samsung Galaxy S9 cases, and appears to leak the design of the phone in the process.Jan 26, 2018A short video seems to show us the Samsung Galaxy S9 from the front and back, albeit with the screen off.Jan 26, 2018Trusted leaker @evleaks shares renders of the Galaxy S9, with the design consistent with with the other leaks we've already witnessed. It does look a little bezel heavy compared to some other leaks though.French leaker @OnLeaks posts a video showing a render of the Samsung Galaxy S9, complete with single rear camera, headphone jack and shifted fingerprint scanner.Dec 13, 2017A leaked image complete with dimensions backs up the 90% screen-to-body ratio, as it suggests that the Galaxy S9 will be marginally shorter than the S8, but will otherwise look similar, albeit with the scanner moved beneath the camera lens.Samsung Galaxy S9 cameraThe Galaxy S9 camera is going to be a key feature of Samsung's phone this year, and not because of a dual-lens camera or higher megapixels.Instead, we're expecting to see a variable aperture, super slow motion video and the possibility of the world's first phone with HDR video capture.The MWC 2018 launch event invite heavily teases the Samsung Galaxy S9 camera by including the words ""The Camera. Reimagined.""This could mean that the Galaxy S9 camera may beat the top-of-the-line Google Pixel 2 camera if it includes a variable aperture or becomes adds the world's first camera that can capture Mobile HDR video.Samsung Galaxy S9 camera rumors point to a 12MP Dual Pixel lens with optical image stabilization and a variable f/1.5-2.4 aperture. That means it would be able to switch between f/1.5 (great for low light shots) and f/2.4 (ideal when the lighting is better and you want more of the photo to be in focus).This rumor comes from a leaked image of the Galaxy S9's box, which also lists it as being 'super speed', having an 8MP front-facing camera and supporting 'super slow-mo'.This could be the retail box for the Samsung Galaxy S9. Credit: RedditThat last point is something we've heard rumored before, as industry sources claim Samsung is working on a rear camera that can shoot at 1,000 frames per second, which would be better than anything on the market in a phone right now - though a source has now said the S9 won't feature this.More recently we've heard additional details of what exactly 'super slow-mo' might mean, with sources claiming that the S9's camera will be able to tell when motion starts and automatically begin recording slow motion footage when that happens.There would also apparently be a mode where you can record at normal speed and then tap a button to switch to slow-mo at specific points.Samsung has also unleashed four teaser videos highlighting the S9's camera. While they don't confirm anything, the first three hint that it might have clever slow motion skills as detailed above, work well in low light and support an Animoji-like feature.The fourth video shows far more, hinting at many of the same things, as well as an improved flash, live photo abilities, improved selfies - perhaps thanks to a bokeh effect or a wide-angle lens, and even social components.We've also heard more generally that the Galaxy S9 will have an upgraded camera system. Phone screens can display HDR video, but capturing it on a phone is currently impossible.Qualcomm's new Snapdragon 845 mobile chipset, however, is supposed to change all of that, and we expect this chip to be inside the Galaxy S9 in the US.One thing we're pretty sure of: the cameras on the S9 and Galaxy S9 Plus will be different. The sketch above echos that sentiment, as do some case renders.This cut-out could either house a dual-lens camera or a single-lens and scanner. Credit: Techtastic/WeiboEither way, Samsung might offer a new way for you to unlock your phone, with leaker @UniverseIce claiming that the Galaxy S9 will have a '3D sensor front camera'.They don't explain what they mean by that, but it sounds a lot like the iPhone X's Face ID system which allows you to use facial recognition to unlock the phone. And the 3D part suggests that like Apple's solution it won't be fooled by a picture.We wouldn't count on this feature as the information has been limited, but we wouldn't rule it out either.And Samsung might also borrow Apple's Animoji feature, with one report saying you'll be able to map your face onto 3D emoji.Another source echos these claims, adding that you can create your own avatar and then control it like an Animoji.TechRadar's take: New camera modes and features are likely and a 3D face scanner is possible, as is a variable aperture camera, but most evidence suggests the Galaxy S9 won't have a dual-lens camera.Samsung Galaxy S9 batteryHottest leaks:A 3,000mAh batteryMore power efficiencyThe latest battery rumor comes in the form of a picture of a replacement unit on sale already for the phone, and it has a 3,000mAh capacity - the same as the Galaxy S8.This lines up with a certificate supposedly originating from Brazil's telecom regulator Anatel, which also shows the Galaxy S9's battery as being 3,000mAh.That's disappointing news, especially since an earlier report suggests Samsung will now use substrate-like PCB tech that will allow the Exynos chipset manufacturer to include a bigger battery without increasing the size of the processor.However, whatever the size, efficiency improvements in the new chipsets should help the battery last longer.Wireless charging is all but a given too, since the Galaxy S8 already sports it, and a photo supposedly showing the retail box (above) lists wireless charging.TechRadar's take: Samsung is probably likely to be cautious about packing too big a battery into the phone given what happened with the Note 7, but a slight size increase is possible.Samsung Galaxy S9 OS and powerHottest leaks:Snapdragon 845 or Exynos 9810 chipsetJust 4GB of RAMQualcomm has announced the Snapdragon 845, which will likely be powering US versions of the Samsung Galaxy S9.It's an octa-core chip with four cores running at 2.8GHz and four at 1.8GHz, with the fastest cores delivering up to 30% better performance than the fastest cores in the Snapdragon 835. AI processing and graphics performance have also been improved, while power use has been reduced.The chipset also allows cameras to record 4K Ultra HD video at 60fps.Outside of the US, buyers are likely to get Samsung's own Exynos 9810.It includes an LTE modem which supports theoretical download speeds of 1.2Gbps - faster than any other phone, meaning you could potentially download an HD movie within just 10 seconds.That's a claim that's been echoed by a recent benchmark for the Samsung Galaxy S9 Plus, but the benchmark is questionable.While it lists the Exynos 9810 chipset, which is likely to be used, along with Android Oreo, the actual scores achieved by the phone in the benchmark are far lower than we'd expect from a Samsung flagship, so the listing could be fake.More recently though we've see a benchmark for the standard S9, again packing the Exynos 9810, and this has far more believable scores - along with 4GB of RAM.We've also heard talk of 4GB of RAM yet again, with another source saying that while the S9 Plus might have 6GB, the standard S9 will have just 4GB, alongside 64GB or 128GB of storage. And that 4GB claim has also appeared on a leaked box photo, above.TechRadar's take: As unlikely as just 4GB of RAM might seem most of the evidence seems to be pointing in that direction. What we can be more sure of is that you'll get either a Snapdragon 845 or Exynos 9810 chipset (depending on where you are) and that the phone will run Android Oreo.Check out our review of the Samsung Galaxy S8 below.Samsung Galaxy S9 other featuresHottest leaks:An improved iris scannerStereo speakersSamsung will likely improve the iris scanner ifor the Galaxy S9, with rumors suggesting it will be boosted to 3MP (from 2MP on the S8) and better able to recognize your eyes, even if you wear glasses or the lighting is poor. It will also apparently be faster than on the S8.We've also heard that it could leverage 'Intelligent Scan Biometrics' to combine both iris scanning and facial recognition, along with the ability to work better in poor lighting.A recent patent has detailed a similar system, with an iris camera that would recognize both of your eyes and part of your face.The Galaxy S9's iris scanner could get an upgrade. Credit: LetsGoDigital/WIPOThe Samsung Galaxy S9 might also have good sound, as there are rumors of it both having AKG stereo speakers and a free set of Bluetooth AKG headphones. Both of those things have now also been listed on an image seemingly showing the phone's box.And talk of stereo speakers has popped up again more recently, with rumors that the Galaxy S9 will have both a bottom-firing speaker and one built into the earpiece.The Galaxy S9 has also now passed through the FCC (Federal Communications Commission) and in the process we've learned that it will support all major LTE bands and that it will be manufactured in Vietnam - that latter point is interesting because the leaked box image pictured above was first shared in Vietnam, so this further suggests it might be accurate.We've also seen a Samsung patent for a sensor which would analyze atmospheric conditions and alert you to how much pollution there was in the air.Plus, one source has also claimed the Dex docking station we saw debut alongside the Galaxy S8 and Galaxy S8 Plus will get an update that will make it work more like a charging pad and allow you to type on the screen too. It means you won't need to use a keyboard and mouse when connecting your phone up to a monitor.And there's evidence that the Galaxy S9 could have a dual-SIM slot, as there's mention of one at the Chinese Ministry of Industry and Information Technology (MIIT), though that model might be limited to parts of Asia.TechRadar's take: With the exception of a pollution monitor all of these features are believable, and the iris scanner upgrades seem especially likely.","Samsung Galaxy S9 release date, price, news and rumors"
9542,3628173,2018-02-20 07:52:29,"11 must-see toys at the New York Toy Fair 2018For the young and young-at-heartSharesTech + Toys = Tons of FunThis year’s International Toy Fair at the Javits Center in New York City was full of nostalgia, with toy lines celebrating LEGO’s 60th anniversary, Mattel Hot Wheels’ 50th anniversary, and Marvel Studios’ 10-year anniversary.But nostalgia aside, the tech toys on display were anything but retro. Some companies use cutting-edge augmented reality (AR) tech into their games; many others are teaching kids (and adults) how to code with awesome bots, without making it feel at all like learning.We’re highlighting all of the connected toys to keep an eye on for this upcoming year for your kids, nieces and nephews — and we won’t tell anyone if you play around with them yourself first.PrevPage 1 of 12Next PrevPage 1 of 12NextHot Wheels Rocket League RC RivalsPrice: $180 (about £130 / AU$230)Release date: Fall 2018""Can they do flips?!"" I immediately asked. A Mattel representative responded sadly that they had done some programming tests, but legal stepped in over the possibility of RC cars being launched into players' faces.This remote control tie-in to the wildly popular Rocket League video game is a blast to watch and play. You use your iOS or Android smartphone to control your car, which is themed off of the Hot Wheels DLC cars found in-game (the Mattel representative said there's no tie-in between RC Hot Wheels and the video game).While you can't do flips, you do have a boost button, and can drive along the sloped plastic walls without immediately flipping over. The infrared goal sensors can tell the difference between cars and the ball for tracking scoring. And you can adjust max speeds for younger players to get used to the controls.Considering how fun it was to try, the only downsides seem to be the price and the cars' short battery life: you'll need to recharge after just a quarter-hour.PrevPage 2 of 12Next PrevPage 2 of 12NextHot Wheels Augmoto AR RacingPrice: $120 (about £85 / AU$150)Release date: Fall 2018Hot Wheels racing has always been about luck and imagination. You and your friends place cars on tracks, and when they collide, physics takes over. There isn't always much interactivity to it.But with Augmoto, prepare for a rapid 180-degree skid in the other direction. Two racers start their engines, choose when to take pit stops to increase speed, risk loops to get items, and launch missiles and lightning at each other.We'll be going more in depth with the experience soon, but suffice to say that it's a frantic, exhilarating experience, and we're hoping to see more branded AR tie-ins (Mario Kart's koopa shells and bananas, maybe?)PrevPage 3 of 12Next PrevPage 3 of 12NextLEGO BOOST: Ninjago StormbringerPrice: $40 (about £30 / AU$50)Release date: August 1, 2018I watched as a LEGO rep placed an evil ninja on Stormbringer's back. After a few seconds, it started roaring, shook side to side and finally launched aside the poor villain, which gave a Wilhelm scream.This grumpy dragon is an add-on to the LEGO BOOST set, a robot coding kit for kids ages 7–12. It comes equipped with various capabilities and sensors, which includes sensing the difference between good and evil ninjas, and responding to voice and tactile feedback.The image-only block coding interface seen above helps kids to learn coding principals and logic at any age or reading level. And because kids are working with LEGOs, they can incorporate any and all LEGO sets into their robots, which rely on Bluetooth functionality, a Move Hub and a motor to function.The rest of the LEGO BOOST family. Courtesy of LEGOStormbringer was a blast to play around with. But it does require the base BOOST set to work, which retails for $160 (about £115 / $AU200), so it's an expensive bundle to buy into.PrevPage 4 of 12Next PrevPage 4 of 12NextlittleBits Droid Inventor Kit + CodingPrice: $99.95/£99.99Release date: Available now, coding updates available in AprilWe've already gushed about how fun it is to build your own droid from the inside out, and then guide it on missions. And though the kit is on our Best Toys of 2018 list, our staffers wrote that once Artoo is built, the app activities for the droid can be a little limited.But littleBits is planning on rectifying this with a major update this April, which will incorporate Google and MIT's Scratch Books coding structure to give owners so much more variety of actions for their droid, as well as teach kids how to code.I had a chance to dive into the code ahead of time on the show floor, and rest assured that this new sandbox of code will give you plenty to do. Artoo can be programmed with custom coding based on input / output, timing, loops, logic and math, based on your needs. I barely scratched the surface, and had a lot of fun imagining the possibilities of the tech.We recommend you get a head start on Artoo's base functions and buy it now before the daunting code language hits in a couple months, since the update is totally free via the app.PrevPage 5 of 12Next PrevPage 5 of 12NextMerge 6DoF BlasterPrice: N/ARelease date: Summer 2018Merge Labs was last on our radar for Merge Cube, our Most Unique Product award winner at CES 2017. Now it's back with this not-quite-VR, not-quite-AR gizmo that channels everything fun about Superhot VR, without needing a headset at all.The company attached an iPhone X to the purple plastic gun seen above: the 6DoF Blaster. It uses a combination of positional tracking tech, including ARKit and ARCore, to register your real-life movements.So, as I played a Unity-based cover shooter, I had to duck under imaginary pillars to dodge lasers — except I wasn't wearing a headset, so I could watch passersby as they watched me.It's not exactly AR, since the game wasn't based on my surroundings. But the lack of headset lets you enjoy an experience without feeling cut off. And since Merge Labs has released a public SDK to code for the 6DoF Blaster, other developers could make AR games for the device if they wanted. It's an exciting accessory to watch, especially if it's as affordable as Merge's other products.PrevPage 6 of 12Next PrevPage 6 of 12NextJurassic World Kamigami RobotsPrice: $60 (about £40 / AU$75) per dinosaurRelease date: Spring 2018It's only fitting that the toys for Jurassic Park: Fallen Kingdom, a franchise famous for its heroic archaeologists, promote STEM learning.These rock'em sock'em dinobots are based off of Blue the Velociraptor and the upcoming film's evil raptor. Kids will take about an hour to build them from a tech base with foldable plastic pieces. Then they can make them fight, dance or speak using another visual code language. The free app is gamified, so teaching the raptor to perform certain tasks will unlock others.A representative said the raptors can't do enough damage to one another to make any pieces fall off, but that the app is trained to help kids improve their raptor design if it falls over a lot to its rival.Still, it doesn't seem to have quite the same amount of replayability and rebuilding potential as LEGO BOOST, littleBits' R2-D2, or Ubtech's Jimu TankBot Kit.PrevPage 7 of 12Next PrevPage 7 of 12NextMarvel Hero Vision Iron Man ARPrice: $50 (about £35 / AU$65)Release date: Spring 2018On the Hasbro demo floor, I donned Tony Stark's mask and gauntlet, complete with infinity stone, and took on Thanos' army.You start by placing AR markers around a room to spread out your targets, then use a couple of hand motions to attack enemies or defend yourself: fist forward to fire, palm towards you for a shield. The app will have three modes and 10 levels, culminating in a fight against Thanos himself.Like most AR experiences right now, it's not graphically intensive, so you don't exactly feel like you're in the film. And my virtual ""arm"" glitched a few times and pointed in odd directions, blocking my vision. But, on the whole, it was quite fun, and definitely something Marvel fans will love.PrevPage 8 of 12Next PrevPage 8 of 12NextHonorable Mention: Star Wars Ultimate Co-Pilot ChewbaccaPrice: $130 (about £90 / AU$165)Release date: Fall 2018The previous slides featured the most promising connected toys at the NYC Toy Fair that will most likely show up under lots of Christmas trees this year. But we couldn't help but include some of our other favorites that may not exactly be ""smart,"" but still offer tons of fun.For instance, Co-Pilot Chewie, a 16-inch animatronic doll that will roar at you in Shyriiwook (the Wookie language), with over 100 different movements and phrases. He responds to specific voice and touch commands, too. For example, tickle his toes and he'll get grumpy with you, or make him fly and he'll get excited.He's adorably perfect, and apparently my immediate love for little Chewie showed, because when I left Hasbro's show floor, a representative joked that she wanted to check my backpack and see if I had tucked him inside. If only.PrevPage 9 of 12Next PrevPage 9 of 12NextHonorable Mention: Escape Room in a Box: The Werewolf ExperimentPrice: $30 (about £20 / AU$40)Release date: Available nowThis escape room game is currently on sale, but it's worth calling out for its Alexa companion functionality: through Mattel's app, you can ask Alexa on your Amazon Echo for hints and time updates as you try to escape the virtual room. We're excited to see if other board games incorporate voice commands or virtual assistants in some way in the future.PrevPage 10 of 12Next PrevPage 10 of 12NextHonorable Mention: Jurassic World Pterano-DronePrice: $120 (about £85 / AU$150)Release date: Spring 2018This Fallen Kingdom tie-in drone had a little trouble getting off the ground during my demo, but it has some cool safety features making it kid-friendly: it has auto-flight and auto-landing features, and when kids reach for the rotor blades, they automatically shut down to prevent injuries.PrevPage 11 of 12Next PrevPage 11 of 12NextHonorable Mention: LEGO Duplo Steam TrainPrice: $60 (about £40 / AU$75)Release date: Late 2018Like LEGO BOOST but with bigger blocks, this Duplo set teaches little ones using colored blocks placed on the tracks that trigger certain actions — essentially, coding commands. It comes with a Bluetooth-enabled app that gives remote control, too. If toddlers show enough self-control to keep the train on the tracks, it could prove a nifty teaching tool.",11 must-see toys at the New York Toy Fair 2018
9543,3628175,2018-02-19 05:45:24,"Xbox One X and One S getting 1440p support in the near futureSuper excited about supersamplingSharesOften regarded as one of the Xbox One X's most impressive features, supersampling uses the console's additional horsepower to provide improved graphical quality to your games when played on Full HD (1080p) displays.At present, the Xbox One X downscales 4K signals exclusively to 1080p, meaning those with QHD (1440p) monitors are left playing at a much lower resolution than their screens are capable of – but not for long!Though support for 1440p monitor was first announced late last year, Kevin Gammill, program manager for Microsoft's Xbox Platform Partner Group has now revealed via Twitter that it's likely to arrive in the very near future.Many questions recently around the timing of our 1440p support. Those of you in our early preview ring should be pleasantly surprised very soon if you have an Xbox One S or Xbox One X.February 16, 2018Though Gammill did not explicitly announce a timeframe, his tweet does mention the ""many questions recently around the timing of our 1440p support"" before strongly suggesting that members of Xbox's early preview ring will be ""pleasantly surprised very soon if [they] have an Xbox One S or Xbox One X.""We've expected 1440p support to come to the Xbox One X for a while now, however the inclusion of Xbox One S in the tweet is more of a surprise.While the console isn't capable of rendering games at a native 4K resolution, perhaps it will soon be able to upscale to 1440p in the near future. We'll keep you posted as we find out more over the coming days.",Xbox One X and One S getting 1440p support in the near future
9544,3628177,2018-02-15 21:10:00,"HP Spectre x360 reviewThis flagship 2-in-1 laptop gets the job doneOur VerdictEven with less battery life than promised, there’s not a lot to hate about the new Spectre x360. In fact, with a stylus in the box, excellent speakers and a gorgeous touchscreen, there’s a lot to love here.ForStylus includedStunning speakersBeautiful displayDay-long battery lifeAgainstWeak hingesAnnoying keyboard layoutHP’s Spectre line has always been one to offer looks and decent performance at a reasonable cost. After recently taking a step back and reevaluating its Spectre line, more specifically the 13-inch Spectre x360 2-in-1 laptop, a redesigned chassis, improved internals and a fancy stylus came to fruition.With the x360, HP has made some significant improvements to the design and layout of various buttons and has even added microSD card support. For that, in spite of battery life that falls short of HP’s promise and an unusual keyboard layout, it’s easy to recommend this laptop.Spec sheetHere is the HP Spectre x360 configuration sent to TechRadar for review:Price and availabilityIn the US, the HP Spectre x360 we evaluated is listed at $1,249 with what you see to the right. Through its website, HP also offers more affordable and far more expensive builds. The entry-level model of the x360 is currently priced at $979 for an Intel Core i5, 8GB of RAM, and a 256GB SSD.In the UK, HP offers a few more options, however for the same model we tested (save for, again, double the storage) you’re looking at £1,499. A slightly less expensive model with an Intel Core i5 and a 4K display is also available in the UK.These prices are all competitive with the likes of the Lenovo Yoga 920 ($1,549, £1,349, AU$1,954) or Microsoft’s Surface Book 2 ($1,499, £1,499, AU$2,600), if not a bit more affordable to either model.DesignA smooth, silver aluminum housing wraps the entire body, broken up only by shiny buttons or speaker grilles. The material reminds us of previous Spectre’s, which unfortunately means it will eventually show some wear and tear.On the left side of the laptop is a microSD card slot, a power button with embedded indicator light, a 3.5mm headphone jack, and a USB 3.1 port. On the right, you’ll find a volume rocker, fingerprint sensor, and two USB-C ports with Thunderbolt 3.There aren’t a lot of ports, but having both USB Type-A and USB Type-C is a welcomed approach to eliminating some of the pain that switching USB standards can cause users. Either of the Thunderbolt 3 ports can be used to charge the laptop.Upon opening the laptop, you’re immediately greeted with a rather large touchpad that’s smooth to the touch and has a reassuring click to it.Above the keyboard is a rather interesting speaker grille. The design is fun to look at and allows more than enough sound to come through.Just above that speaker grille are two hinges that allow the Spectre x360 to rotate 360 degrees to go from laptop to tent to tablet mode. The hinges are smooth when moving the screen, but do a bad job of holding it in place when tapping on the screen or typing away on the keyboard.Instead of reaching up with one hand to scroll through some text, we find ourselves using another hand to hold the display still while interacting with its touch interface.As part of reevaluating the Spectre x360 line, HP was able to shrink the bezels surrounding the 13.3-inch FHD (1,920 x 1,080) display. HP offers a privacy screen feature for an extra $60 in the US, but the unit we tested lacks this feature. The feature is supposed to eliminate coworkers or strangers from glancing at your screen and obtaining confidential information.A full-sized keyboard sits just above the touchpad, only with an extra column of keys to the far right. The added keys serve as the Page Up/Down, Home/End, and a Delete keys.This row continues to give us problems, as our muscle memory when doesn’t expect there to be anything to the right of the Enter and Backspace keys. Compounding the confusion is the fact the right arrow key lines up with the added row, instead of with the Shift/Return/Backspace keys as is normally the case.Other than the added column of keys, HP has a winning keyboard. It’s smooth and the keys require little force for touch-typists.A welcome characteristic of the touchpad is just how wide it is. It’s longer than the spacebar, making it easy to interact with when needed. It has a reassuring click to it, and is smooth when using gestures to navigate Windows 10.The HP PenIncluded in the box is an HP Pen. The stylus works with Windows 10 Ink for drawing stick figures or jotting down notes.In our testing, the stylus and Spectre x360’s interaction is seamless, with digital ink flowing from the pen with ease.The lid of the pen lifts up to reveal a USB-C port used to charge the HP Pen. Having a hidden charging port that doesn’t require sticking the end of a stylus into a port like the Apple Pencil, or removing the lid and replacing a battery as required by the Surface Pen, is convenient and not nearly as awkward.Adding to the convenience is the fact that you can use the same cable you use to charge the x360.",HP Spectre x360 review
9545,3628248,2018-02-21 08:03:32,"TNW SitesGoogle’s Reply app is shaping up to be the messaging assistant of my dreamsLast week, we heard that Google was working on Reply, an AI-powered app that lets you reply to texts from various messaging apps by tapping on contextually meaningful replies right in your notifications. I just sideloaded a beta version of it earlier today, and it already looks pretty promising as an assistant that helps you text faster.Once you’ve installed Reply on your Android device and granted it the necessary permissions, the app will take over notifications from services like WhatsApp, Twitter DM, Hangouts, and Slack, and add buttons labeled with responses; tap one and it’ll send it out instantly.In my brief testing, I found it to work like a charm with day-to-day conversations. Here, have a look at some replies it suggested below:As Android Police noted, some features that we heard about, like responding with your exact travel time to a location, don’t yet work; however, this is still a beta build and it’s safe to assume that such niggles will be sorted before Reply is ready for prime time.Besides saving me the effort of a few swipes on my keyboard, it also seems like it could kill my texting paralysis – that occasional inertia I experience when trying to think of what to say, or whether I’m saying it the right way.Whether Reply will kill off the art of textual conversation is a matter for another piece – but for making plans, confirming your presence at an event, and answering basic questions from your contacts, it’s a real time-saver.Google’s getting fairly good at this ahead of the competition, so it’ll be interesting to see if rivals build their own AI-powered tools for the job, or if Reply will be around for a while and continue to evolve.",Google's Reply app is shaping up to be the messaging assistant of my dreams
9546,3628249,2018-02-21 06:57:13,"A $42 million ‘10,000 year clock’ is being installed on Jeff Bezos’ propertyAmazon founder Jeff Bezos took to Twitter to announce that construction of the 10,000 year clock has begun on his property in Texas, with a view to encourage thinking about humanity in the long term.Conceived back in 1986 by American computer scientist Daniel Hills, the clock’s mechanism is designed to advance far more slowly than your average timekeeping device: it’ll tick once a year, its century hand will move once every 100 years, and its cuckoo will emerge every millenium.Installation has begun—500 ft tall, all mechanical, powered by day/night thermal cycles, synchronized at solar noon, a symbol for long-term thinking—the #10000YearClock is coming together thx to the genius of Danny Hillis, Zander Rose & the whole Clock team! Enjoy the video. pic.twitter.com/FYIyaUIbdJThe project was fleshed out at the Long Now Foundation, a San Francisco-based nonprofit that aims to foster thinking in longer terms than months and years about our time on earth, and our responsibilities towards society and the planet.Following the completion of a few prototypes, the Foundation has now begun building a 500-foot mechanical version on Bezos’ land in the Sierra Diablo range of West Texas. The construction is presently drilling into the mountains there to house its components, which are being machined and assembled in California.There’s no schedule for completion of the project, but you can expect it to take a while. Once it’s ready, you should be able to visit and see it with your own eyes.The Long Now Foundation expands on the purpose of the clock on its site:Ideally, it would do for thinking about time what the photographs of Earth from space have done for thinking about the environment. Such icons reframe the way people think.It’s an interesting time for this project to break ground, given the rapid pace of advancements in technology and the global scaling of major corporations in the past few years. It seems only natural now, that this clock is being installed on property owned by the richest man on the planet.","A $42 million '10,000 year clock' is being installed on Jeff Bezos' property"
9547,3628250,2018-02-21 05:00:53,"TNW SitesWhen open-world games were becoming all the rage in the 2000s, Burnout Paradise stitched the concept beautifully into a city-wide racing title that featured a wide range of tracks, game modes, and vehicles, and delivered more fun per mile than competing arcade driving games at the time.10 years on, it’s making a comeback in the form of a 4K 60fps remaster for PS 4 Pro and Xbox One X – along with 1080p revamps for older versions. The multiplayer action, original soundtrack, eight DLC packs, and all 150 vehicles are along for the ride.From the comparison images showing the refreshed textures, it seems like Burnout Paradise Remastered will be worth a go for fans of the original, as well as newcomers alike. Its pedal-to-the-metal gameplay, damage system and sprawling environment made for a ton of fun in its heyday, and a new coat of paint should help bring back the magic.Burnout Paradise Remastered arrives on March 16 for $40; you can pre-order it right now, or start playing on March 9 with an EA Access membership.",Burnout Paradise celebrates its 10th anniversary with 4K console remaster
9548,3628251,2018-02-21 01:24:14,"TNW SitesDeepfakes algorithm nails Donald Trump in most convincing fake yetA YouTuber this week created a convincing fake video of Donald Trump using the deepfakes algorithm.We’ve written previously about the algorithm, specifically about its use in creating fake pornographic videos featuring the face of your favorite celebrities and the body of a passable porn actress. Now that popular platforms like Reddit, Imgur, and Pornhub are cracking down on the practice, AI enthusiasts are turning their attention to newer, SFW applications.This is good. But also not.Take this example, which features Alec Baldwin on Saturday Night Live doing his famous impersonation of President Trump. YouTuber ‘derpfakes‘ trained the AI image swap tool to create a composite of Trump’s face, over Baldwin’s speech and mannerisms. The result is a convincing, if not quite ready for primetime representation of the Commander in Chief.While the algorithm isn’t quite there yet, the progress made in the past several weeks has been staggering. What went from a pixelated semi-recognizable representation of your favorite actresses in compromising positions, is now showing real potential for upending our understanding of truth.Trump, in the above video, looks a little too perfect to be lifelike, and he’s backed by the speech and mannerisms of Baldwin, which are ever-so-slightly off since it’s not actually Trump doing the talking, or moving.But how long do you think it takes before the same users smooth out the kinks?Users are already trying. Take this fake, a mashup of Bruno Ganz playing Adolf Hitler in the 2004 film ‘Downfall’ and current president of Argentina, Mauricio Macri.Or there’s this, a fake of Inés Arrimadas, a member of Catalonia’s parliament overlaid on a pornographic video from adult content producer Twisty’s.The latter two examples are from weeks earlier, where subtle facial cues gave away the illusion.Now, though, the fakes are more convincing than ever and downright passable for lesser-known celebrities or public figures. In a year or two, as the algorithms continue improving, it’s unclear whether the average person will even be able to discern authentic vides from fakes.At that point, even video evidence becomes questionable, and perhaps even unbelievable. In a world that already can’t agree on simple facts, the future looks pretty terrifying.",Deepfakes algorithm nails Donald Trump in most convincing fake yet
9549,3628252,2018-02-21 00:01:26,"About TNWTNW SitesSpotify ‘on its way’ to creating first hardware productA trio of job listings on Spotify’s website are again getting the rumor mill swirling about new hardware.First spotted by MusicAlly, one of the jobs details the company is “on its way to creating its first physical products,” and that it was seeking someone to work in its Stockholm office to handle “manufacturing, supply chain, sales, and marketing.”Products, plural, is perhaps what’s most interesting. While it’s easy to envision an iPod-like device that steals fractional marketshare from competitors like Apple, it’s difficult to wrap your head around what appears to be Spotify’s big picture idea for the hardware division.While details are scant, we can glean some information from a previous job listing. In it, the company noted a desire to create a “category defining product akin to Pebble Watch, Amazon Echo, and Snap Spectacles,” suggesting the inclusion of smart assistants, augmented reality capability, and voice control.So perhaps iPods and portable music players aren’t the end game after all. Instead, it seems Spotify is exploring multiple fronts, perhaps including a smart speaker, and a wearable device.",Spotify 'on its way' to creating first hardware product
9550,3628253,2018-02-20 22:52:31,"About TNWTNW SitesSamsung announces a whopping 30TB SSDSamsung today announced it has begun mass production of the world’s largest solid state drive (SSD). While unassuming in its 2.5 inch form factor, this drive packs a whopping 30.72 terabytes and leverages Samsung’s latest V-NAND technology.Called the PM1643, the drive combines 32 new 1TB NAND flash sticks, each comprised of 16 stacked layers of 512Gb V-NAND chips. Nerd speak aside, these 1TB packages allow for super speedy transfer — about three to four times that of an average SSD — and incredible capacity to boot.Each drive is reportedly capable of storing 5,700 full HD movies.With our launch of the 30.72TB SSD, we are once again shattering the enterprise storage capacity barrier, and in the process, opening up new horizons for ultra-high capacity storage systems worldwide. Samsung will continue to move aggressively in meeting the shifting demand toward SSDs over 10TB and at the same time, accelerating adoption of our trail-blazing storage solutions in a new age of enterprise systems.All told, the device delivers twice the capacity and performance of it’s previous record-setting SSD, the 15.36TB version introduced in March 2016.You probably won’t be seeing these bad boys in your next laptop (or desktop, even) any time soon, but the race to the top benefits all of us as it forces prices down on the drives you are likely to see in consumer devices.",Samsung unveils a whopping 30TB SSD
9551,3628254,2018-02-20 22:20:41,"TNW SitesHistorians battle publishers for the right to resurrect dead MMOsFans of abandoned online games are close to bringing back certain “dead” titles — if the US Copyright Office doesn’t side with the studios, who this week began pushing back against their efforts.The US Copyright Office, which oversees the Digital Millennium Copyright Act (DMCA), renews its provisions every three years. These provisions include the specific rules which criminalize any attempts to circumvent copyright protection, and the exemptions to those rules. During the renewal, it takes public comment and petitions on whether any new exemptions should be made.A new comment, submitted last October by the Museum of Art and Digital Entertainment (MADE), asked the Copyright Office to make an exemption to the DMCA for “abandoned” online games. If granted, it would mean that museums and archives would be able to run their own servers for these games, essentially resurrecting them for small-but-devoted fanbases.The MADE emphasized how important these games would be for understanding the history of the medium:For future historians, video games like Minecraft and Second Life will say as much about 21st century America as Dicken’s Oliver Twist does about 19th-century Britain. That is, if these games actually survive into the future. Unfortunately, video game preservation lags far behind other media and is impaired by technological challenges and legal limitations. … So, despite their ever-growing cultural importance, online video games continue to turn into digital dust when their copyright owners cease to provide access to an external server necessary for the game to function—i.e., when these games are “abandoned.”To be clear, there’s already a DMCA exemption in place for abandoned games that are stored within the gamer’s own console or computer. Granted in 2015, it allows users to modify their games to eliminate the need for an authentication server from the company. Museums and libraries would even be allowed to jailbreak consoles to get games working under this exemption.However, the Librarian of Congress, who oversees the Copyright Office, wouldn’t extend the protection to online multiplayer games at that time. According to the Electronic Frontier Foundation, who rallied for the exemption, the Librarian stated that doing so would violate rules that prevent anyone from selling circumvention software. In theory, you’d have to sell or traffic in some kind of software which would allow multiple online users to play abandoned games.Game companies are pushing back against the idea. The Entertainment Software Association (ESA), which represents multiple game publishers, filed an opposition comment, saying that publishers do preserve old games under the current exemption via the retro games market. Specifically, it cited the remastered Crash Bandicoot game and the rerelease of Call of Duty: Modern Warfare. It also points out that MADE, a non-profit, charges a $10 admission fee to play its games, meaning any servers it hosts will fall under commercial use.A separate comment, filed by the ESA, the Motion Picture Association, the Recording Industry Association, and the Association of American Publishers, complained that the MADE isn’t interested in preservation, just in allowing users to keep playing:The petition and comments from proponents in the current record do not establish that these efforts are insufficient with respect to preserving video games. In fact, the petitioner’s proposed class appears to enable recreational video game play, including by numerous museum “affiliates,” rather than preservation strictly for academic purposes.Personally, while I understand the ESA’s reservations, I think it’s important to allow access to older games, and game companies haven’t proven to me that their interest in the retro market goes beyond remastering and rereleasing. And online games can’t really be preserved or understood in their context if you aren’t allowed to, you know, play them online. So having a preservation society host online servers would give hobbyists and interested amateurs a chance to experience the game in its original state.",Gamers and game-makers fight over abandoned online games
9552,3628255,2018-02-20 20:01:44,"TNW SitesThe Stanford gaydar AI is hogwashLast year a couple of Stanford researchers created an algorithm to determine whether Caucasians are gay or not with better accuracy than humans. The algorithm, trained with human bias, is better at determining whether or not other humans will think a face is gay. Unfortunately the researchers don’t present their work that way.The Journal of Personality and Social Psychology recently published the final version of the white paper that details the work, essentially giving it full credence as a peer-reviewed piece of science.That makes sense; there’s nothing wrong with the paper and all the science (that can actually be reviewed) obviously checks out. The problem is this: humans cannot predict sexuality by looking at a picture. Whatever measure of accuracy our species can achieve is attributable to luck and intuition, neither of which can be quantified. So who gives a damn if a computer is better at making random guesses than a person is?The first major problem with the paper is its title: “Deep Neural Networks Can Detect Sexual Orientation From Faces.”Yet, the entire paper goes on to point out that the researchers designed an algorithm that imitates human bias. At no point is this AI predicting gayness: it predicts whether humans will think a face looks gay or not based on data from humans who tried to predict whether a human’s face was gay or not.It’s basically using second-hand knowledge of people, from humans who got it right a little-better than half the time, to determine what made those people think the faces they were looking at were gay or straight. It’s predicting patterns. It does not predict gayness.According to the study:Given a single facial image, a classifier could correctly distinguish between gay and heterosexual men in 81% of cases, and in 71% of cases for women. Human judges achieved much lower accuracy: 61% for men and 54% for women.Not only is there absolutely no scientific basis for guessing whether a face is gay or not, but using images to do so presents a myriad of problems, not the least of which is the fact that the images were sourced from people who readily identified as either gay or heterosexual. This indicates the researchers believe human sexuality is binary.And there’s a definite danger in giving those who would discriminate tools with which to do so that they can claim are backed by science.Companies, like Faception, are already selling AI products that supposedly use “advanced algorithms” to predict whether a person will commit a crime.This is problematic for obvious reasons, as Princeton psychology professor Alexander Todorov told The Washington Post: “The evidence that there is accuracy in these judgments is extremely weak. Just when we thought that physiognomy ended 100 years ago. Oh, well.”Simply put, as far as verifiable science goes, you can’t predict if someone is going to molest a child, have same-sex relationships, or commit an act of terrorism just by looking at their face. Claims from Faception CEO Shai Gilboa that “Our personality is determined by our DNA and reflected in our face. It’s a kind of signal,” are loose, at best.Yet, somehow, according to The Washington Post, “Faception said it’s already signed a contract with a homeland security agency to help identify terrorists.” And as we’ve said before: that’s a terrible and racist idea.What we’re seeing here is an extension of human bias. Creating an algorithm that has more psychic ability than humans is nothing more than automating ignorance.Furthermore the study itself points out that there’s not enough data on minorities for the researchers to use the algorithm on anything other than well-lit Caucasian faces, which makes it virtually useless for anything other than academic discourse.It’s also worth mentioning that the entire basis of the hypothesis is built on one controversial idea called “the hormonal theory of sexuality.” A theory which basically says homosexuality is decided in the womb, but there’s no actual scientific consensus whether this is true.Still, this didn’t stop the Stanford team from building a hypothesis based on this theory, the white paper for the study says:…the widely accepted prenatal hormone theory (PHT) of sexual orientation predicts the existence of links between facial appearance and sexual orientation. According to the PHT, same-gender sexual orientation stems from the underexposure of male fetuses or overexposure of female fetuses to androgens that are responsible for sexual differentiation.To be fair, the white paper also states the researchers are following a lead that gay people are better at determining whether a face is gay or not because, “Recent evidence shows that gay men and lesbians who arguably have more experience and motivation to detect the sexual orientation of others, are marginally more accurate than heterosexuals.” This sounds like something that could be true, but the math only adds up if the following statement is true: if x = gay people then all gay people = x. Not all gay people are the same, and it’s offensive to conduct work based on a theory that being gay makes you better or worse at accurately completing a task.Kosinksi, according to The New York Times, actually thought about creating an algorithm to determine whether a person was an atheist or not first, but decided instead to go with the gay detector. It’s evident this wasn’t created out of the desire to solve a problem.He claims the purpose of the study was simply to prove a point, but that doesn’t explain why the team chose to title it “Deep Neural Networks Can Detect Sexual Orientation From Faces,” which indicates AI can actually detect sexual orientation… from faces. This is simply not true.To date there is no scientifically verifiable way to determine a person’s sexuality without asking them. And even then things get problematic. What makes a person gay or straight? Society at large seems to struggle with the concepts of bisexuality, transsexualism, and fluid-sexuality. It’s unfathomable that any scientist would believe they’ve solved the “gay” puzzle with a crappy algorithm. And that’s not editorializing, Kosinski himself told Quartz:This is the lamest algorithm you can use, trained on a small sample with small resolution with off-the-shelf tools that are actually not made for what we are asking them to do.It seems reckless for science to be conducted in an ambiguous manner, based on supposedly altruistic motivations, with what appears to be deceptive terminology. Kosinski’s claims that this algorithm is supposed to point out the dangers of facial recognition AI conflict with the title of the paper and the assumptions it proposes.There’s no such thing as gaydar, and it takes an immense amount of arrogance for anyone to claim they’ve unlocked the secrets of human sexuality with a couple of theories and a lame algorithm. This is nothing more than an AI that points out what human bias looks like extrapolated.Can it predict gayness? Absolutely not. It conflates the idea of what we think gay people look like with what gay people actually look. Even at 100 percent accuracy it still wouldn’t actually be predicting what a gay face looks like because a computer doesn’t understand the concept of gay. It finds correlations between disparate data groups, because it has no other choice.Not everyone identifies as gay or straight, but an even bigger problem is the fact that many people who have sex with people of the same gender consider themselves heterosexuals. Is being gay a state or an action? Are you gay because you want to have sex with someone who is the same gender as you, or because you do have sex with them?This doesn’t even address the fact that, statistically, one in four men have homosexual thoughts. If a man is in love with another man, but doesn’t have sex with him, is he gay? If a man has casual sex with other men, but doesn’t love them or even find himself attracted to them outside of coitus, is he straight?I can self-identify as a three toed-sloth, and you may even see my picture in my author’s profile and say “yep, he looks like the type”.The problem with this paper is that it takes a small dataset, that only contains self-reporting Caucasians, and claims that an AI can “detect” homosexuality. The AI that “detects” cancer doesn’t “predict” what people will say is and isn’t cancer, it looks for the actual obvious signs of cancer with no regard for what 51 percent or 65 percent of non-experts think.We are always guessing when we try to tell something about a person by the way their faces look, even if we’re correct. Computers don’t have a magic power that allows them to bypass the arguments against physiognomy, no matter how accurate their predictions are. Saying your AI detects homosexuality is arming bigots with science.",Opinon: The Stanford gaydar AI is hogwash
9553,3628256,2018-02-20 17:30:37,"About TNWTNW SitesSnapchat copies Instagram for once, adds gif stickersSnapchat today announced it was bringing a few new features to its app, including gif stickers courtesy of GIPHY. Because social media is a tawdry pile of copied homework, it shouldn’t come as a surprise that Snapchat is finally doing unto others what has been done unto it — Instagram just introduced this exact feature last month.In order to use the feature, select the sticker icon from the Snap editing screen — check our guide if you need a primer on how that works. Then tap search to bring up a list of stickers from GIPHY. They function just like stationary stickers, down to the ability to stick them to moving objects in videos — a feature which you might recognize from the GIPHY World ARKit app from last year. Like I said: copied homework.Another update coming to the app is a tab feature which groups Friend stories, Discover, and Group Chats into separate tabs. We’ve contacted Snapchat for clarity about whether this is something users can customize for their own convenience or just a new sorting system.Presumably this is part of Snapchat’s attempt to simplify its notoriously user-unfriendly app, as CEO Evan Spiegel promised to do last year. The redesign is already off to a rocky start, with users across the board decrying the confusing update and pleading for a chance to roll back to a previous version. The app’s ratings have tanked on the App Store, with recent negative reviews both there and on Google Play mentioning the update specifically.GIPHY stickers are available now on Snapchat, while tabs will be rolling out to iOS and Android in the next few weeks.","Snapchat copies Instagram for once, adds Giphy stickers"
9554,3628257,2018-02-20 16:45:48,"About TNWTNW SitesCryptocurrency News 2/20/2018Editor’s note: The following column is written as intentional satire and may or may not have deliberate or undeliberate errors.Hey everyone, looks like you get two newsletters today, not because the last one was published late but because I’m the most generous, beautiful person to walk this horrible earth.The coins are doing even better than usualAfter sacrificing a goat on top of a volcano, Lord Satoshi has made the Bitcoin return to $11,560 as of writing this, which is quite incredible considering it hasn’t been there since mid-January. I really hope that this means I can finally make money on Bitcoin and start paying rent again versus hiding in the vents whenever the landlord knocks on the door. Litecoin also is screaming toward $250, meaning that if you bought LTC about 2 and a half weeks ago, you’d be making a lot of money but also have invested in Litecoin, a coin that I cannot for the life of me tell you the value of.Reports from the Litecoin Cash communitySo this weekend I spent a lot of time in the Litecoin Cash discord channel, and I can personally tell you that it was not enlightening. The fork happened sometime on Sunday, and everyone went nuts, except actually they didn’t get the wallet online until yesterday. If you don’t remember from when I possibly posted but likely didn’t, if you had Litecoin in a Litecoin wallet during the fork block’s forking, you were able to claim 10 Litecoin Cash. If you had it on an exchange (or basically anywhere you can’t access your wallet’s private keys), you are screwed. If you get some sort of error, you are screwed. The Litecoin Cash wallet requires a full sync of the blockchain (like Bitcoin Core or Litecoin Core wallets). Anyway, the good news is I got my Litecoin cash, and a lot of people didn’t which is a shame for them.Ed’s philosophy cornerThis whole thing got me thinking about how messed up and unfair cryptocurrency really is. It is not remotely equal. I had – and was able to afford at one point, as I purchased them a while ago – about 42 Litecoin. So that means I sit on about 421 (decimal points!) Litecoin Cash. I have a natural head-start because of a financial advantage. If anything cryptocurrency has truly become the libertarian nightmare – you can truly take over a new cryptocurrency if you have expensive mining hardware, or have the funds to buy in early (or, indeed, get free coins from forking). To quote Litecoin Cash’s website:SHA256 Mining – After the fork block, Litecoin Cash switched to SHA256 proof-of-work hashes. This enables a new use for previously obsolete Bitcoin mining hardware.Except that doesn’t totally make sense. Why? Because it’s not like all the people with 13.5 TH/s machines are going to say “let’s let people with a Voodoo 5500 mine it first so that we’re all mates.” Bullshit! They absolutely turned their guns and pools on the currency immediately. And there’s no way that the difficulty adjustment can say “okay, this computer is a tiny baby, we shall treat it fairly.” No, the “smart difficulty adjustment” isn’t built to do that. It’s built to make it so that blocks are still mined to actually move the currency. It’s not so much their fault as it is disingenuous for anyone to claim there’s a use for legacy hardware anymore. We’re past the point where things can be easy for normal people.Congrats Bitfinex on adding the fast protocol for transferring Bitcoin – and immediately getting responded to with one of those Ethereum scam artists.Bitmex also released a report about Tether yesterday, which suggested that there may be real dollars attached to US Dollar Tethers, based on uh, a correlation in funds increasing in Puerto Rico that can’t be tied to Tether. The report ends with a really reassuring comment:Continue as is and risk being be shut down by the authorities at some point.“It’s a perfect storm of extraordinary factors coming together: fire, ice and criminal negligence,” he argues in the documentary, “Titanic: The New Evidence,” which will air in the United States on the Smithsonian Channel on January 21. “The fire was known about, but it was played down. She should never have been put to sea.”They also said “Bitfinex is on fire.”John McAfee update: tired but strongLeaving the Blockchain SuperConference. A moving experience. We spoke of the war waged against us by banks, governments and the establishment, and we vowed to stand firm against it. Alarmingly, some of our youth have been oblivious. But please, young ones, wake up! We need you! pic.twitter.com/YaqQMs3LRn",Cryptocurrency News 2/20/2018
9555,3630928,2018-02-21 11:47:00,"The Morning After: Spotify plots its own smart speakerHi there! Watching the PyeongChang Winter Olympics? Been watching them in VR? We have. We also have news of a Burnout comeback (The Best Driving Game) and Amazon brings its Prime rewards to Whole Foods shoppers.Are you looking for the exact moment robots decide to turn on their human creators? You might want to mark this day on your calendar. Boston Dynamics has posted video of a SpotMini test where it gauged the bot's ability to adjust to interference -- in this case, from a pesky human.Spotify might be working a smart speaker, according to new job listings. ""Spotify is on its way to creating its first physical products and set up an operational organization for manufacturing, supply chain, sales and marketing,"" one ad states. So far, it has relied on other products like Google's Assistant, Amazon Echo and Sonos One to stream its service. However, Apple recently launched its HomePod speaker with only native Apple Music support, showing Spotify's need to take action on its own hardware.The ads show the new operations manager, senior product manager: hardware production and project manager: hardware production and engineering would be handling manufacturing and supply for the new product. It all sounds like it's closer to being made, whatever form it takes.Burnout Paradise was one of those rare racers that transcended its genre and was just so good. If its recent 10th birthday had you feeling nostalgic, then we've got good news: Come March 16th, you'll be able to hit the streets of Paradise City once again, with the complete original soundtrack, and all 150 cars and eight main expansion packs -- including the Big Surf Island premium DLC. Burnout Paradise Remastered will run in native 1080p on PlayStation 4 and Xbox hardware, and EA promises 4K resolution and 60 frames per-second on PlayStation 4 Pro and Xbox One X.Amazon says its Rewards Visa will now offer users the same level of reward when they shop at Whole Foods as they receive at Amazon itself. Eligible Prime members will now receive a flat five-percent bonus on all purchases at Whole Foods, just as they do online. By comparison, shopping beyond Amazon's universe will net you two percent back at restaurants, gas stations and drugstores and a single percent elsewhere.For all of its Nintendo customizations, the Switch is ultimately a tablet running a garden variety NVIDIA Tegra processor -- and that means it can potentially handle the same software as other mobile devices. To prove that point, the fail0verflow team has shown a Switch running honest-to-goodness Linux. The touchscreen, networking and accelerated 3D graphics are all functioning, but you're not about to run Steam games on it (many Linux apps aren't built for ARM-based chips), but you could theoretically use the Switch as a basic computer.",The Morning After: Spotify plots its own smart speaker
9556,3631266,2018-02-21 11:26:16,"How a French 16-year-old is teaching the world to build VR headsets for $100Palmer Luckey, the founder of Oculus and the inventor of arguably the most successful commercial virtual reality headset, was only 17 when he completed his first prototype for the device. Fast-forward to the present day, and Luckey is now worth roughly $750 million at the age of 25, having sold his company to Facebook.16-year-old Maxime Coutté’s story has something in common with Luckey’s beginnings, in that he set out at an early age to build the hardware needed to enjoy VR experiences, and appears to have the same sort of tenacity and passion for technology as the inventor whose footsteps he’s following in.16-year-old Maxime Coutté began programming at 13 and created the Relativ VR headset system three years laterBut the similarities end there; Coutté took up the project because his interest in VR was piqued by the virtual reality-focused anime series Sword Art Online. Finding himself unable to afford an Oculus Rift of his own at 13, he sought out the help of a couple of friends and his math teacher in the village of Savins in northern France, where he grew up.Now, Coutté has not only crafted a working VR headset that allows him to play SteamVR games, but also open sourced the tech behind it so anyone can make their own and improve upon it as they see fit. I found his story fascinating, and was delighted to learn more about his journey from halfway around the world.The result is a 3D printed headset equipped with a 5.5-inch 2,560 x 1,440 pixel LCD screen that allows for a 90-degree field of view. Coutté has paired it with Leap Motion’s hand tracking device to detect and mirror arm movements for aiming and other functions in games, and said that the headset will work with SteamVR-compatible content.Relativ headset with Leap Motion hand trackingCoutté recalls getting started with VR at school, as part of a special interest group at school:I started programming when I was 13, thanks to Sensei (Coutté’s math teacher at the Collège Lelorgne de Savigny middle school in Provins, Jerome Dieudonne), who created a robotics club with our school’s backing. On the first day there, it hosted 12 students; the next week we were down to just three of us.At the time, I was hugely into this anime called Sword Art Online, in which the protagonist, Kirito, using a VR headset, is plunged into a role playing video game. I was fascinated by the idea of a second reality in VR and I wanted to experience it. I spoke with one of my fellow club members, Gabriel Combe, about how I couldn’t afford a VR headset, and we both agreed that we should build our own.That got us into the math and physics behind VR (quaternions, proper acceleration, and antiderivatives). Then, we bought the cheapest components we could and we reinvented VR. Now, using our FastVR and Relativ tools, you can build your VR headset too.Coutté wasn’t deterred by the lack of easy access to resources in his village, or by his lack of experience in building hardware:Every piece of knowledge one would need for such a project is online. From math courses, to electronics tutorials and cheap components, all the knowledge and prerequisites are free and at our fingertips. And by building it from scratch, with all the fresh know-how available to us in 2017, we avoided common assumptions and challenges associated with VR technology back in 2013.Together with Combe, Sensei, and their friend Jonas Ceccon – who sourced parts including lenses and the screen from suppliers in China – the foursome formed a team to participate in a robotics competition, and began work on their mammoth project:When we started working on the VR headset we had no idea of how to build it. I started with the most basics things, like how we would track player movements, and those questions led us to the math behind components such as the accelerometer. As we progressed in answering our questions, our next steps became clear and we kept at it.The most difficult part was to make it compatible with every existent SteamVR game. We still struggle with this. The easiest part of our project was to release a first prototype of the headset core that handles the fundamental processes of displaying VR content, after having mastered the key concepts behind VR – the first one was coded in just six nights.In the process, Coutté and his team realized that simply building the hardware and getting it to work wouldn’t be enough to ensure smooth performance in VR experiences. So Coutté came up with a solution that he called WRMHL, to enable quick communication between the Arduino hardware platform and the Unity3D engine. He explained that this made it possible to reduce latency by a great deal by reducing computations on the headset and pushing them to the connected PC instead, and it worked well given that the team was using low-cost components.Instead of trying to beat million-dollar companies at their own game, the team decided to focus on working within a budget of just $100. Coutté explained:Every choice has been made in the perspective of this constraint, and it led us to build more and more things by ourselves to see how we can stretch our budget.For instance, we needed to solve latency issues that we faced when using low-powered components – and so I came up with WRHML. This mindset of building from the ground up help us build an easily hackable VR headset and development kit for building games.The latter came about when Coutté met a team of game developers who had one person on their team dedicated solely to making the title compatible with VR headsets. That sparked the idea of creating FastVR with Combe; the tool lets devs make their standard Unity-based PC games VR-ready, simply by dragging and dropping the FastVR package into their build and setting it up through a quick configuration process.Coutté is all too aware of the fact that his creation may not match up to the design, build quality, or performance of the latest high-end VR headsets on the market today – but that hasn’t deterred him from sticking with the project. In fact, it’s led to an interesting development:Of course, our headset is ugly, and hard to setup. But I’m confident that the community we are building will help improve this.The community he speaks of is a small army of followers who interact with his team on GitHub and Discord, as the entire Relativ project is open-source. “Building a company felt like overkill, considering our age and the pain of starting a firm,” he explained. “Our goal is simply to have fun and to help democratize VR. The best way to do it was to open-source it.”The idea to open-source the entire project came from Oculus’ chief architect, Atman Binstock, who Coutté met in San Francisco through Oussama Ammar, co-founder of the European startup accelerator The Family.Ammar recounts hearing from Coutté a year ago through a cold email. “It was just a short email claiming that he could build a better VR headset than Oculus,” he told TNW, and mentioned that when he realized Coutté’s intention was to build a company, he felt bad for him.We come from the same background (coming from a small town, for instance), and I created my first company as a teenager. I know how painful it is at that age to manage “company problems,” and I don’t think you should start too early, because it decreases your chances of just learning, right at the age when your learning can be the most intense. So I told Max that if he’d agree not to build a company yet, but to open source his project, I’d be happy to mentor him. Plus, since I go to Silicon Valley every summer, he could come with me and I’d show him around. I think being in Silicon Valley does two big things: it gives you a sense of purpose and it shows you that it’s not unique to be really talented. Those are two very important, humbling experiences.Ammar explained that the trip to Silicon Valley was an eye-opener for Coutté:At that moment, I don’t think Maxime totally understood why I was against helping him to build a company, but the corruption of that first trip to the Valley did the trick. When we arrived in SV, David Marcus (Vice President of Messaging Products at Facebook) introduced me to Atman, and during that meeting with Atman something funny happened.He told Max that what he is doing is great, but that managing hardware, supply chains, etc., is all a huge pain. And he said that what would be cool is for Max to build a reputation and a bit of a brand by offering all of this to the rest of the world. It will give him a taste of management, and a certain kind of entrepreneurial experience that Max craves, but without the usual business headaches of running a company.Maxime was super enthusiastic to have someone who really understood the tech he was working on giving him the same advice as I had. And as the trip went on, probably 90 percent of the entrepreneurs he met, and especially those who had started out really young, gave him the same advice. Max came back to France super inspired, and open-sourced the project.After he returned to France, he rewrote the entire codebase for the project over the course of five months, convinced his team and Sensei to go open-source with it, and then published it on GitHub for others to dissect, download, and learn from.The Discord channel is rife with some 200 people sharing links to tutorials, projects, and ideas about DIY VR gear, frameworks for development and plans for improving on the original Relativ build. Coutté is also working with a team from among the community on a companion app for the headset. “By having lots of people from different backgrounds and with different visions, we could push the project far beyond what our four-member team has achieved so far,” he explained.The internet has certainly made it easier than ever to access knowledge and learn how complex systems like virtual reality gear work. But with Relativ, Coutte has turned his passion for technology – and Sword Art Online – into something much bigger, that others from around the world can participate in.",How a French 16-year-old taught the world to build VR headsets for $100
9557,3631267,2018-02-21 11:21:17,"TNW SitesBaking mischief and delight into your products will make people love themOne boozy night in 2010, two developers and an illustrator were sitting in a bar discussing how utterly annoying they found the game FarmVille. As you probably remember, FarmVille was that horribly addictive social network game that let you grow crops and raise livestock, polluting your Facebook newsfeed with endless notifications about how to tend to your farm.After a few more drinks, the group decided to figure out a way to mess with the game. They sketched up an app which let you post updates identical to those of Farmville’s, but instead, the messages were much more… savage. From giving your neighbors cows Mad Cow Disease to putting a meth lab or a slaughterhouse in the middle of your friend’s farm, the app, which they called Farmvillain, was designed to let you have fun at your friends’ expense – you know, the best kind of fun.Feeling accomplished, the group sent their creation to a few close friends for a laugh. Within a few months, the game, created as a complete joke, had gone viral, with millions of people playing it worldwide. Undoubtedly, there were many people who absolutely loathed Farmvillain, but there were also millions of people who loved it. After two years, the friends ended up selling their game for quite a decent sum, forever shocked at how a stupid idea turned into something so many people loved.Farmvillain cow with Mad Cow DiseaseAs it turns out, one of the developers at the bar was Eamonn Carey. Now the Managing Director of Techstars London, Eamonn visited TQ in Amsterdam last week to give a talk about the simple things businesses can do to acquire and engage more users. He uses his story about Farmvillian to illustrate the power of mischief and delight, and believes making users happy — even just for a moment — can do wonders in terms of engagement.Interested in how you can make people happier with your products? Here are four of Eamonn’s most practical tips to get you started.1. Stand out from the crowdWhile it sounds pretty obvious, setting your product apart from a competitor’s is incredibly important for one main reason, according to Eamonn, and that’s because our attention spans are declining. With human attention spans now down to about six to eight seconds, delays in the time it takes a product or platform to load or a payment to go through can lead to severe frustration, resulting in a lack of engagement.Eamonn says it’s important for those who are building app-based products to be aware of an unfortunate industry statistic: “roughly 50 percent of apps that are downloaded are used once, and then never opened again,” therefore finding a way to break through to get people’s attention is crucial.“The problem you have with building a product these days is that attention is a unit of scarcity,” Eamonn reiterated. With a limited amount of time in each day, it’s incredibly important to figure out your point of difference and make sure it’s relevant for users.2. Produce moments of delight“The best thing you can do in the world is to put a smile on someone’s face,” Eamonn explained.Many successful companies invest in manipulative ways to make people feel good, such as fake notifications or withholding ‘likes,’ but Eamonn recommends doing things that “genuinely animate and excite people and make them happy,” as these are especially powerful.Eamonn’s suggests starting withmomentsof delight. “These moments don’t need to be fireworks exploding on the screen, it can literally be something as simple as a piece of UI,” he explained. They can just be subtle “moments of levity baked into products” that engage with people and make them happy for just a second.Mailchimp’s sweaty handMailChimp, for example,uses an animation of a chimp’s hand hovering over a button sweating just as you’re about to send a bulk email (because we all know what can go wrong whensending a bulk email). Similarly, on Halloween, Uber changed its icons on its map to broomsticks and pumpkins.While these features aren’t overt or huge, Eamonn says that “all of these little moments engage with something the user is feeling at that moment and are incredibly powerful as a result.”3. Introduce small changes incrementallyAround five years ago, gamification was declared ‘the future of work,’ promising to transform user behavior, create motivation, and keep people engaged. While there were a number of great tools released due to this transition, Eamonn’s experience was that “most companies just shoved a whole lot in at once,” instead of building things up incrementally.“You can’t suddenly go from having nothing to everything. You don’t want to fundamentally change the tone of your product in order to just tick a box,” he continued. Instead of “shoehorning something in where it doesn’t belong,” you need to think systematically to work out how to build little pockets of happiness into your product and scale these out over time.4. Content is the best path inFor Eamonn, there’s no question about what’s the most productive way to reach users: “Content is absolutely the best path in for people.”Since content is the best way to reach people, it’s a good idea to begin to incorporate the ‘little moments of delight’ mentioned earlier into the content you’re already producing: your existing social media feeds, mailouts, and website copy. “Obviously this requires you to have an amazing copywriter, but I think there are enough of those people out there that you can hire that resource,” Eamonn said reassuringly.Eamonn wants people to know that testing things out within existing content is much easier and less scary than they might think. The only thing you need to do is make sure that you’re human in how you design and pitch products. Have a sense of humor, show your personality, and remember to tell stories that you yourself would also enjoy.So while the creation of Farmvillain might not be the best example, Eamonn’s main tip when it comes to using mischief and delight is to be nice to people, to not be annoying, and most of all, to be human.As Eamonn concluded, “the world can be a pretty depressing place, so making people happy can be incredibly powerful.”",Baking mischief and delight into your products will make people love them
9558,3633571,2018-02-21 13:00:00,"Watch SpaceX's Falcon 9 launch its internet satellite payloadA SpaceX Falcon 9 is blasting into the skies today carrying Hisdesat's PAZ satellite, which will spend five and a half years carrying out radar and imaging work for the Spanish government and businesses. The launch, which uses a first stage booster previously used last August during the FORMOSAT-5 mission, will also have on board SpaceX's first demonstration satellites for its proposed satellite broadband service, which will be tested out before a full constellation launch over the next five years. Weather permitting, you'll be able to watch the launch live on PAZ's YouTube channel at 9AM EST (6AM PST), although a back-up window has been scheduled for Thursday 22 February, should things go awry -- since today's launch was initially slated for February 17, that's not entirely unfeasible.",Watch SpaceX's Falcon 9 launch its internet satellite payload
9559,3633572,2018-02-21 13:31:00,"US AG creates a new 'Cybersecurity Task Force'Days after the Mueller investigation revealed indictments against 13 Russian nationals for election tampering, US Attorney General Jeff Sessions announced a new Cybersecurity Task Force. Its marching orders are a bit vague -- ""canvass the many ways that the Department is combatting the global cyber threat"" -- but the AG's first ask is for it to investigate efforts to interfere with US elections and infrastructure.It's difficult to guess what this group may achieve or identify as problems, but Sessions said in a statement that ""The Internet has given us amazing new tools that help us work, communicate, and participate in our economy, but these tools can also be exploited by criminals, terrorists, and enemy governments...At the Department of Justice, we take these threats seriously. That is why today I am ordering the creation of a Cyber-Digital Task Force to advise me on the most effective ways that this Department can confront these threats and keep the American people safe.""",US AG creates a new 'Cybersecurity Task Force'
9560,3633573,2018-02-21 02:19:00,"Netflix: 'Stranger Things' directors aren't leaving the showWe haven't heard much about the next season of Stranger Things, but a rumor on Moviefone.com indicated that its directors, Ross and Matt Duffer, will be less involved after season three wraps up. This evening, Netflix tweeted ""Rumors that the Duffer Bros are leaving Stranger Things after season 3 are false,"" which doesn't exactly refute the report, but hopefully means our Eggos and walkie-talkies are safe. The reported cited a source claiming the two would step back to work on two other projects for Netflix, so this could just mean they're more spread around. Still, we'll be watching carefully to make sure the quality holds up.Don't drop your Eggos. Rumors that the Duffer Bros are leaving Stranger Things after season 3 are false. pic.twitter.com/x4kbL9990e",Netflix: 'Stranger Things' directors aren't leaving the show
9561,3633574,2018-02-21 04:59:00,"AT&T will launch mobile 5G in Atlanta, Dallas and WacoAT&T is finally willing to say exactly where you'll see mobile 5G in 2018. The carrier has confirmed that ""parts"" of Atlanta, Waco and its home turf of Dallas will adopt the standards-based service when it goes live before the end of the year. It'll name the remaining nine cities ""in the coming months."" There's no mention of the first devices (many of those will have to wait until 2019), but it's clear that this will be a cautious first step into the future rather than a full-on leap.The initial coverage will use millimeter wave (very high frequency) spectrum, which isn't great for range and may serve as more of a stopgap. The kind of coverage you're used to will have to wait until later, when AT&T can justify moving 5G to more commonly used bands. That shouldn't be too hard when much of the LTE equipment it's installing now should help with the migration, but that still entails a wait.This will still make AT&T the first big carrier to launch 5G. Sprint's rollout is expected in 2019, T-Mobile only anticipates national coverage by 2020, and Verizon has so far talked more about its fixed 5G than something you can carry in your pocket. However, AT&T can't quite claim a resounding victory. It's making some sacrifices to shout ""first,"" and mature 5G won't arrive for a long while.","AT&T will launch mobile 5G in Atlanta, Dallas and Waco"
9562,3633673,2018-02-21 10:30:00,"We use cookies and browser capability checks to help us deliver our online services, including to learn if you enabled Flash for video or ad blocking. By using our website or by closing this message box, you agree to our use of browser capability checks, and to our use of cookies as described in our Cookie Policy.Schools that struggle to prepare students for success losing ground; ‘The shake-out is coming’Concord University in West Virginia and Clemson University in South Carolina were both founded shortly after the Civil War. During the 20th century, each grew rapidly. Now, the two public universities that sit just 300 miles apart face very different circumstances.Clemson, a large research university, enrolled its largest-ever freshman class in 2017 and in December broke ground on an $87 million building for the college of business.","After Decades of Growth, Colleges Find It’s Survival of the Fittest"
9563,3633830,2018-02-21 08:01:42,"0UPDATE: Today’s attempt has been scrubbed due to high upper atmosphere winds. The next attempt window is Thursday February 22 at the same time.SpaceX is launching a Falcon 9 with client Hisdesat’s PAZ satellite on board today, provided weather remains favorable and everything else goes according to plan. The satellite, an imaging and radar instrument with a planned lifespan of five and a half years, will serve Spanish government and commercial needs, and will also work as part of a constellation together with TerraSAR-X and TanDEM-X to be used jointly between Hisdesat and Airbus.The launch will make use of a first stage booster for the Falcon 9 rocket first used last August during the FORMOSAT-5 mission, and today’s launch will take place at 6:17 AM PST (9:17 AM EST) during an instantaneous launch window. A backup window is scheduled for Thursday, February 22 at the same time should the launch be scrubbed for Wednesday. It’s taking off from Vandenberg Air Force Base in California.This launch will also carry SpaceX’s first demonstration satellites for its satellite broadband internet service, to be tested ahead of a full-scale constellation launch. It’s also said to be the first launch of the second generation of SpaceX’s fairing, which is designed to be be better able to survive launch for re-use on future missions.",Watch SpaceX launch a Falcon 9 carrying its first internet demo satellites live here
9564,3633910,2018-02-21 13:52:50,"TNW SitesThe Vatican (yes, the Holy City) is organizing its first ever hackathon“No, I did not have to pitch the idea to the Pope,” laughs Jakub Florkiewicz, the co-initiator of the first Vatican Hackathon in history. “But the participants of the event hopefully will,” he adds.So how did this unlikely event come to be? “It’s a story about a student arriving in Rome with an idea, and finding people who got excited about that idea,” Florkiewicz tells me.“Passionate myself about hackathons, I had the luck of meeting people – Vatican insiders – who actually were thinking of launching one at the Holy City and inviting youth to help organize and participate in it. We clicked quickly and decided to work together. Our initial group spearheading the idea comprised of fr. Eric Salobir, founder of OPTIC – the first Vatican-affiliated think tank on technology, the Vatican’s Secretariat for Communication and a passionate priest – fr. Philip Larrey.” Other Vatican institutions joined their team later.“The Vactican operates different dicasteries and congregations, and each have different speeds of how they go into digital innovation,” he tells me. Some needed more explanation than others, but after flying a few times to Rome, Florkiewicz and Father Eric succeeded in getting the ball rolling.“Hackathon is a broad concept – we had to spend some time presenting our idea to various people in Rome, to ensure everyone understand that this had nothing to do with actually hacking. We wanted to ensure everyone understood that we are trying to address non-religious, global socio-economic problems. A hackathon might seem unexpected to be organized by a religious organization. But not until one learns about the problems ‘to be hacked.’ It focusses on three themes: Social Inclusion, Interfaith Dialogue, and Migrants and Refugees.”Any student, from any religious background, was invited to apply. This led to 120 participants, who will be working on projects from March 8 to 11 in the Vatican.The project is part of a larger mission, spearheaded by Father Eric, to “foster dialogue between the tech world and the humanities,” he tells me.Father Eric is one of the founders of OPTIC, a Dominican “research network” aimed at precisely that. Just last month, they organized a summit in Paris, bringing together tech bigwigs like LinkedIn co-founder Reid Hoffman and MIT Media Lab director Joi Ito to discuss societal topics with people from other backgrounds – like Grandmasters of the Dominican order.“I wouldn’t say it’s a religious organization,” says Father Eric, “it’s a value based organization.” Surprisingly, to me at least, in our 40-minute conversation, the topic of Christianity or religion in general wasn’t even broached until I asked about it.“At the end of the day, everything is spiritual, but the Catholic social teaching is just aimed to improve the dignity of the human being in the society and the economy,” he tells me. “This means helping people to find their way in the economy and society.”Father Eric likens the outreach through technology with the outreach of Christian hospitals. “The way technology shapes the society we live in, is also important. Being positively impactful thorugh technology is also part of our mission,” he said.“We can be a common place for people who never meet. People from humanities and tech never meet each other, even in universities. We created this place to foster that dialogue,” Father Eric says.Florkiewicz concurred with that, “We want to promote collaboration across boundaries, divisions, and backgrounds, that’s we invite people from all kinds of different backgrounds. We’re inviting Muslims, Jews, speakers from West Bank, we want to promote discussion and collaboration. We mix invitees into teams that are extremely diverse, that work on problems that we care about.”“The second thing is that we want to inspire people around the world to use technology to solve these problems, so clerics everywhere know that they can use this model to access skills of young people, and to address issues in their immediate neighborhood.”I asked Father Eric if this is how he plans to bring the Vatican into the future. “If I had a dry sense of humor, I’d say it will bring the Vatican into the present instead of the future,” he jokes.“Taking the Vatican into the future can only be step by step,” he continues. “We’re building the next step.”“The Vatican has a long tradition with science and technology – sometimes a difficult one, like with Galilleo. But the Church reconsidered that later. Anyway, this hackathon is one of the key points to build the next step of this tradition. It’s helping to accelerate the pace of innovation in Rome.”Now a more cynical person might see this hackathon as a somewhat desperate attempt at marketing the church towards young people, but Father Eric laughs me in the face when I bring this up. “We have no time or resource for marketing, that’s not the right way to frame it. The point is to be useful, that’s why we chose those issues, to be impactful.”",The Vatican is organizing its first ever hackathon
9565,3633911,2018-02-21 13:45:05,"About TNWTNW SitesIOTA is vulnerable to replay attacks but has no intention of fixing the flawFledgling blockchain startup IOTA has ran into yet another technical issue. Researcher Joseph Rebstock has detailed a vulnerability in its network which makes users susceptible to replay attacks – a common exploit vector in which valid data is erroneously repeated in order to steal cryptocurrency from users.The issues stems from a function related to IOTA’s choice to use one-time signatures when processing transactions on the Tangle – the company’s self-proclaimed “next-generation” blockchain technology which promises more efficient transactions and scalability.“Reattaching is often required to get a transaction through and bundles can only be safely signed a single time,” the research explains. “Therefore the user is allowed to simply reattach any bundle of transactions they want without any proof of ownership. This should not be a problem because every bundle has a unique hash.”But as it turns out, the function does not work as intended.“The expected behaviour should be that only one use of the same bundle hash should be allowed inside a consistent transaction history (subtangle),” Rebstock writes. But instead, “[t]he coordinator will repeatedly approve the same bundle hash over and over.”“This means that while you may have signed a transaction to send 500 Miota it can be attached to the network 10 times draining the account of 5,000 Miota,” he insists.The researcher has provided several examples to prove the validity of the attack vector:In all fairness, the attack vector described in the report hinges on reusing wallet addresses – a malpractice the company has repeatedly warned against.Still, it is worth noting that, while the vulnerability is similar to the signature issue previously disclosed by Neha Narula from the MIT Digital Currency Initiative (DCI), this is a newly discovered flaw.“Fortunately, since IOTA discourages the reuse of addresses it is uncommon for there to be any funds left on the address,” the researcher clarifies. “The replay attack is only applicable where addresses has [sic] been reused.”“However it should not be confused with the signature reuse issue, which is only a theoretical concern for a single reuse,” Rebstock continues. “The replay attack applies with only one reuse and is easy to implement.”The good thing, the author highlights, is that the glitch is relatively easy to eliminate.IOTA developer Lewis Freiberg has since confirmed the issue is indeed authentic in a statement on Reddit. Still, the developer downplayed the severity of the vulnerability, adding that the company has no intention of tweaking the core architecture of the network to “accommodate this edge case.”“If the user in the example scenario above had [refrained from reusing their wallet address,] then all of the IOTA from that address would have been sent else where,” Freiberg says. “Thus the attack would’ve never worked.”In any case, the decision not to patch the exploit is odd – especially because both the Rebstock and Freiberg agree it is a pretty “simple fix.”One important outtake the researcher emphasizes is that missing to provide a solution to “such an obvious problem should give everyone involved with IOTA [a] pause and hopefully a bit more humility.”Rebstock also remarks that the current setup of IOTA requires every transaction approval to go through the network’s coordinator – an implementation that many have argued ultimately renders the Tangle a centralized system.While the IOTA team has conceded this is currently the case, it has promised to get rid of the coordinator in the future. However, the company has yet to lay out its plan for phasing out the coordinator.Meanwhile, the cryptocurrency startup has remained focused on raising awareness of its technology and expanding its network of collaborators.The company recently announced its Ecosystem platform which aims to offer developers, startups, and hobbyists with a powerful set of tools to build for IOTA. In addition to this, IOTA signed a memorandum of understanding with Taipei to make the city smarter and secured a substantiative investment from Robert Bosch Venture Capital.These developments follow the launch of the IOTA Data Marketplace in collaboration with high-profile brands like Accenture, Fujitsu, and Bosch. The announcement attracted fair amount of controversy when it later became clear that Microsoft – which was purportedly in “partnership” with IOTA – is not an official partner, but merely a technology provider.IOTA later argued the miscommunication came about as a result of an inaccurate statement (provided by Microsoft) in its Data Marketplace announcement.The vulnerability marks another time IOTA has received criticism over the design of its network architecture. The company previously came under scrutiny for “rolling its own” hash function – a cardinal offence in the world of cryptography.While some other cryptocurrencies like ZCash have engage in similar practices, what made IOTA’s hash function problematic is that it purportedly did not undergo rigorous testing.IOTA eventually addressed these issues in a four-segment response published on its blog – though many known figures in the blockchain and cryptocurrency space ultimately disagreed with IOTA’s line of defence.In the meantime, those interested can peruse Rebstock’s full report on GitHub here.We reached out to IOTA co-founders Dominik Schiener and David Sønstebø for a comment on the vulnerability. Schiener has since responded to TNW with the following statement:“There is no vulnerability. Make that the headline.”Update: Moments after this piece went up, Sønstebø contacted TNW to second Schiener’s words.“Indeed, do not just link to Lewis [Freiberg’s] post, read it and understand it,” he commented. “Otherwise, this article will be completely inaccurate, this is not a vulnerability at all.”",IOTA is vulnerable to replay attacks but has no intention of fixing the flaw
9566,3636595,2018-02-21 14:22:00,"AI is making more realistic CG animal furCreating realistic animal fur has always been a vexing problem for 3D animators because of the complex way the fibers interact with light. Now, thanks to our ubiquitous friend artificial intelligence, University of California researchers have found a way to do it better. ""Our model generates much more accurate simulations and is 10 times faster than the state of the art,"" said lead author Ravi Ramamoorthi. The result could be that very soon, you'll see more believable (and no doubt cuter) furry critters in movies, TV and video games.A lot of fur rendering systems were designed for human hair, and that's a problem. Fur fibers have a larger central section -- aka, the medulla -- that scatters light differently, giving a soft, yet glossy appearance. Current renderers don't look at the medulla, but merely consider how light bounces from one fur fiber to the next. As a result, they have to do a lot of number-crunching and tend to be slow.The UC researchers instead used a principal called subsurface scattering to see how light richocets around and through translucent fur medullas. To understand the principal, you can shine a smartphone's flashlight through your finger in a dark room. ""You will see a ring of light, because the light has entered through your finger, scattered inside and then gone back out,"" the UC team explained.Applying subsurface scattering to fur is a thorny mathematical problem, though, so the UC team turned to a neural network. After being trained on just a single scene, the AI was able to apply subsurface scattering to a variety of other scenes, including wolf, raccoon and hamster models.The results are a clear improvement, and the technique works equally well for hair. The team is now shooting for real-time fur rendering, which could be extremely useful for game designers who want to introduce more realistic animals. Fur-covered Sonic or Pikachu, anyone?",AI is making more realistic CG animal fur
9567,3636596,2018-02-21 15:00:00,"Renault's 'smart island' runs on wind power and recycled batteriesRenault has launched a ""smart island"" in Portugal that uses its Zoe electric vehicle, home batteries, smart charging and vehicle-to-grid (V2E) energy storage to run without fossil fuels. The idea is to make the Madeira island of Porto Santo energy independent and stimulate renewable energy production. ""[We want] to build a model that can be carried over to other islands and cities,"" Renault Electric Vehicle Director Eric Feunteun told Engadget.Unlike Tesla's massive Powerpack installation, the Renault project is more of a community endeavor on the small (16 square mile) and sparsely populated (5,483 inhabitants), tourism-oriented island. It will unroll in three phases: In the first, 20 fortunate Porto Santo volunteers will get 14 Zoes and six Kango Z.E. utility vans to use every day. They'll benefit from 40 new connected public and private charging stations set up by Renault and local utility Empresa de Electricitade da Madeira (EEM).""Let's say you come home from work at 7 PM with a decent charge left, and only need two to three hours of charging,"" said Feunteun. ""The smart charging system we're testing will decide when the best time to do that is, based on usage, energy availability and other factors. Then, it can charge up to eight times a day in chunks as small as 15 minutes.""During phase 2, the EV will become part of the grid by feeding electricity back into it during peak hours. That way, the EV owners will not just benefit from the smart charging network but also pay it back by supplying what are essentially mobile home storage batteries. ""Electric vehicles have moved from being a constraint, as seen by the energy companies, to an opportunity in the last few months and years,"" Feunteun said. ""In particular, they provide a way to store electricity, which is one of the key problems on their table with renewable energy.""In the final phase, Renault will introduce ""second-life"" stationary ""Powervault"" and other batteries that have been recycled from its Zoe and other EVs. Those could come from cars that have been in accidents, or from Renault's recent program that allowed owners to swap the old 22 kWh hour batteries for the new 41 kWh ones. The idea is that even though an old battery isn't up to the heavy demands of an EV, it's still plenty useful for supplying homes and power grids.Those will attach to local solar and wind systems, storing the unpredictable levels of electricity they produce. The energy can then be recovered by the grid as needed, much as Tesla's Australian Powerpacks can supply energy during peak usage.Renault teased the idea of combining electric vehicles and homes to form a grid when it launched the Symbioz concept car, complete with its own home. You could pull the vehicle right into your living room (or garage, in the real world), plug it in, and either charge it or give energy back to your home.RenaultUsing EVs in a smart charging network makes a lot of sense, as they can absorb energy when supply exceeds demand (at night, or when the sun is shining) and give it back when everyone needs power. ""So if there is really a peak at 7 PM, when everyone is cooking, then the battery continues to empty on the grid,"" explained Feunteun. ""Then, it will recharge at midnight when it is windy and no one is consuming energy.""Renault is one of the first companies to test smart charging in a solar and wind energy-powered region, using EVs and home batteries to smooth peak power. The data it gathers will be invaluable when it expands the program to larger eco-districts or cities.It will even recycle the batteries used on the Porto Santo Zoe and Kangoo vehicles to smooth the power grid, creating a true circular upgrade. ""The Madeiro project is one of the most complete electric vehicle green initiatives ever,"" said Feunteul. ""It brings together all the different ecosystems to create a zero emissions, carbon-free island.""Steve should have known that civil engineering was not for him when he spent most of his time at university monkeying with his 8086 clone PC. Although he graduated, a lifelong obsession of wanting the Solitaire win animation to go faster had begun. Always seeking a gadget fix, he dabbles in photography, video, 3D animation and is a licensed private pilot. He followed l'amour de sa vie from Vancouver, BC, to France and now lives in Paris.",Renault's 'smart island' runs on wind power and recycled batteries
9568,3636598,2018-02-21 06:01:00,"Bigelow forms command center for its expandable space stationsBigelow Aerospace has big plans for its future expandable space stations, so it has formed a whole new company for them. Called Bigelow Space Operations (BSO), the new private space company will oversee the marketing and customer service, as well as become the operations center for the space habitats its parent corporation is developing. If you'll recall, Bigelow's inflatable module BEAM is currently being tested aboard the ISS. It's been doing so well NASA has decided to keep it around longer to be used as an on-orbit storage facility.Company chief Robert Bigelow told reporters in a call that forming BSO is necessary as it segues ""into becoming a production company"" after existing as ""a laboratory for 17 years."" Bigelow has far more ambitious plans for its creation: It's already building two versions of the B330, an expandable module that's much bigger than BEAM and can stay in space on its own. ""These single structures that house humans on a permanent basis will be the largest, most complex structures ever known as stations for human use in space,"" it said in its announcement. The company has long believed that its B330s could become standalone space stations in Low Earth Orbit that it even teamed up with ULA for its launch. More recently, it announced its plans to send a B330 space station to the lunar orbit by 2022.Bigelow said it's spending millions of dollars to study the space market and hiring 3 to 4 dozen employees for its new company. It's also looking to build a new manufacturing facility in Florida, Alabama or another suitable place. If things go as planned and its space stations become operational, Bigelow expects to hire 400 to 500 more people to serve as BSO's employees, providing support for its stations as they orbit the Earth and the moon.",Bigelow forms command center for its expandable space stations
9569,3636694,2018-02-21 00:00:00,"CRISPR In China: Cancer Treatment With Gene Editing Underway : Shots - Health NewsMore than a third of patients with cancer of the esophagus responded to experimental treatment in China with the gene-editing technique CRISPR. Several CRISPR studies are underway there.Shaorong Deng gets an experimental treatment for cancer of the esophagus that uses his own immune system cells. They have been genetically modified with the gene-editing technique known as CRISPR. Yuhan Xu/NPR hide captiontoggle captionYuhan Xu/NPRShaorong Deng gets an experimental treatment for cancer of the esophagus that uses his own immune system cells. They have been genetically modified with the gene-editing technique known as CRISPR.Yuhan Xu/NPRShaorong Deng is sitting up in bed at the Hangzhou Cancer Hospital waiting for his doctor. Thin and frail, the 53-year-old construction worker's coat drapes around his shoulders to protect against the chilly air.So he's back at the hospital to get an experimental treatment. It involves using cells from his own immune system, known as T cells, after they have been taken out of his body and genetically altered in a lab by the gene-editing tool called CRISPR.""I consider myself very lucky,"" Deng says through an interpreter as a nurse finishes taking his blood pressure.Just then, the door swings open and the nurse rushes back in. She's cradling a clear plastic pouch filled with a yellowish fluid. She hangs the pouch above Deng's bed, attaches one end of an intravenous tube to the bottom, and slides a long needle at the other end into Deng's arm.""This is the T-cell infusion,"" says Dr. Shixiu Wu, who's president of the cancer hospital in Hangzhou, a little over a hundred miles southwest of Shanghai. ""Now it begins — getting the immune cell therapy.""Deng stares at the IV as millions of genetically modified immune system cells slowly drip into his body. The infusion will take at least an hour.""I can only hope it will completely — completely — get rid of the cancer,"" Deng says.Deng is participating in what Wu says is the most advanced study in China testing CRISPR in sick people. But at least eight other Chinese studies of CRISPR for various forms of cancer are listed on a U.S. government website that serves as a clearinghouse for biomedical research worldwide. The list includes studies of CRISPR as a treatment for cancers of the lung, bladder, cervix and prostate.In contrast, only one CRISPR cancer study has been approved in the United States, and it's only just now starting to look for the first patient to treat.The Hangzhou Cancer Hospital in Hangzhou, China, is forging ahead with CRISPR treatment for cancer.Rob Stein/NPR""China is starting to pull ahead of other parts of the world — maybe for the time — in regards to biomedicine,"" says Hallam Stevens, an anthropologist at the Nanyang Technological University in Singapore who studies Chinese bioscience. ""They've been really investing heavily in it over the last couple of decades and it's starting to pay off in a big way.""The treatment Wu is testing involves taking a sample of blood from each patient. A lab at a biotech company two hours away by bullet train extracts T cells from the blood. Scientists then use CRISPR to knock out a gene in the T cells known as PD-1. This engineering feat modifies the T cells so that they zero in on and attack the cancer cells, once they're infused back into each patient.Deng was returning to the hospital to receive his second infusion of gene-edited T cells. When he first arrived at the hospital about a month ago, he was so weak he needed a wheelchair. But Deng says he started feeling better soon after his first infusion.""I feel very stable,"" Deng says. ""I was weak in the limbs before and now I am not weak anymore.""It's still too early to draw firm conclusions about how effective the treatment will be. Or what the full extent of side effects from it. Deng is one of just 21 patients with advanced, incurable cancer of the esophagus that Wu has treated so far with CRISPR-edited T cells.But Wu says about 40 percent of the patients appear to have responded. One patient is still alive almost a year later.There's no randomized comparison group in this study. But usually, such patients would have no hope, Wu says. ""If they have not received this treatment they will die — most of them will die in three to six months,"" he says.Wu says he's writing up the results for a scientific publication. But these results have not yet been peer-reviewed or published.So far, Wu says the only side effects have been mostly minor — an occasional fever or rash. Nine patients in the study have died, but Wu says that was from their cancer, not the treatment. One patient discontinued treatment because of a high fever. The rest appear to be stable or in ""partial remission,"" Wu says. They are undergoing monthly treatments.""As a cancer doctor, you see many deaths,"" Wu says ""So it's good to be part of this.""""China sees biomedicine as one way it can compete with the West and show off its technological prowess and scientific chops in the 21st Century,"" Stevens says. ""It's also something that's going to be critical for keeping China's population healthy in the 21st century.""Less skeptical about scienceIn general, medical research in China isn't as stringently regulated as in the West, Stevens says. ""It begins at a different starting point.""In the West, there is a history of awareness about the dangers of medical experimentation run wild, he says, going back to the Nazi era and also the notorious syphilis study at Tuskegee University in Alabama, where doctors withheld treatment from black men for decades.""Rather than having the starting point being: 'This could be dangerous or this could be risky for people and we need to take that concern uppermost,' "" Stevens says. ""They start with the premise: 'This is going to be beneficial for China.' ""Wu says his study was allowed to begin after being approved by a single nine-member hospital committee in just two months. He says the committee included a lawyer, a bioethicist, nurses, doctors, a journalist, a representative of cancer patients and a representative of the public.""If they have not received this treatment they will die — most of them will die in three to six months,"" says Dr. Shixiu Wu. Rob Stein/NPR hide captiontoggle captionRob Stein/NPR""If they have not received this treatment they will die — most of them will die in three to six months,"" says Dr. Shixiu Wu.Rob Stein/NPR""Chinese patients want to be cured very much,"" Wu says. ""There's a Chinese saying: A living dog is better than a dead lion. So patients are willing to try new cures. That's why the ethics committee and the lab are very positive about this.""But the less stringent oversight makes doctors and bioethicists worry that some Chinese doctors may be rushing ahead too quickly, possibly putting patients at risk.""My concern is: Are we really ready? There so much about CRISPR that we don't understand,"" says Lainie Ross, a bioethicist the University of Chicago. ""We could be doing more harm than benefit. We need to very, very cautious. This an incredibly powerful tool.""Ross noted that there are already drugs available that work that harness the immune system in the same way as Wu is trying to use CRISPR.""One concern is: If you could do this with a medication rather than CRISPR, are you actually telling the participants about the alternative?"" Ross says. ""We have safe and effective FDA-approved drugs that could do the same thing. Are they being informed properly?""Others express similar concern.""China's like the Wild West,"" says Dr. Carl June, a University of Pennsylvania scientist involved in the U.S. study of CRISPR to treat cancer that's finally starting after nearly two years of much more intense review than Wu's study in China.""China has made this a very high priority — a national priority to develop this,"" June says. ""There's some very high-quality research in China, and then there are others that are not high quality.""June doesn't have any specific concerns about Wu's study and doesn't think the U.S. should relax its safeguards to protect patients. But he worries the U.S. is falling behind like it did at the beginning of the space race.""It's kind of like Sputnik 2.0,"" June says, referring to the Soviet satellite that shocked the United States when it became the first artificial satellite to orbit the Earth.""CRISPR technologies have created a Sputnik moment where vast new improvements can occur if we focus on them and make it a priority,"" June says. ""I want it to be done safely. But I want it to be a high priority.""And June would like to see a level playing field where researchers around the world follow the same rules.For his part, Wu stresses doctors explained all the possible risks to the patients very carefully before proceeding. And he's already started treating patients with another kind of cancer — cancer of the pancreas.""We [are] just beginning. We should improve it to get more benefits for the patients,"" Wu says. ""If you don't try it, you'll never know.""Shots is the online channel for health stories from the NPR Science Desk. We report on news that can make a difference for your health and show how policy shapes our health choices. Look to Shots for the latest on research and medical treatments, as well as the business side of health. Your hosts are Scott Hensley and Nancy Shute. You can reach the Shots team via our contact form.",Doctors In China Lead Race To Treat Cancer By Editing Genes
9570,3636695,2018-02-20 23:32:06,"Shivaun Moeran and Adam Raff met, married and started a company — thereby sparking a chain of events that might, ultimately, take down this age of internet giants as we know it — because they were both huge nerds. In the late 1980s, Adam was studying programming at the University of Edinburgh, while Shivaun was focused on physics and computer science at King’s College London. They had mutual friends who kept insisting they were perfect for each other. So one weekend, they went on a date and discovered other similarities: They both loved stand-up comedy. Each had a science-minded father. They shared a weakness for puns.In the years that followed, those overlapping enthusiasms led to cohabitation, a raucous wedding and parallel careers at big technology firms. The thing is, though, when you’re young and geeky and fall in love with someone else young and geeky, all your nerdy friends want you to set them up on dates as well. So Adam and Shivaun, who took Adam’s last name after marriage, approached the problem like two good programmers: They designed a dating app.The app was known as MatchMate, and the idea was simple: Rather than just pairing people with similar interests, their software would put together potential mates according to an array of parameters, such as which pub they were currently standing in, and whether they had friends in common, and what movies they liked or candidates they voted for, and dozens of other factors that might be important in finding a life partner (or at least a tonight partner). The magic of MatchMate was that it could allow a user to mix variables and search for pairings within a specific group, a trick that computer scientists call parameterization. “It was like asking your best friend to set you up,” Shivaun told me. “Someone who says, ‘Well, you probably think you’d like this guy because he’s handsome, but actually you’d like this other guy because he’s not as good-looking, but he’s really funny.’ ”Within computer science, this kind of algorithmic alchemy is sometimes known as vertical search, and it’s notoriously hard to master. Even Google, with its thousands of Ph.D.s, gets spooked by vertical-search problems. “Google’s built around horizontal search, which means if you type in ‘What’s the population of Myanmar,’ then Google finds websites that include the words ‘Myanmar’ and ‘population,’ and figures out which ones are most likely to answer your question,” says Neha Narula, who was a software engineer at Google before joining the M.I.T. Media Lab. You don’t really care if Google sends you to Wikipedia or a news article or some other site, as long as its results are accurate and trustworthy. But, Narula says, “when you start asking questions with only one correct answer, like, Which site has the cheapest vacuum cleaner? — that’s much, much harder.”For search engines like Google, finding that one correct answer becomes particularly difficult when people have numerous parameters they want satisfied: Which vacuum cleaner is cheapest but also energy-efficient and good on thick carpets and won’t scare the dog? To balance those competing preferences, you need a great vertical-search engine, which was something Adam and Shivaun had thought a lot about.PhotoAdam and Shivaun Raff in London earlier this month, more than a decade after their battle with Google began.Credit Muir Vidler for The New York TimesSoon the Raffs began daydreaming about turning their idea into a moneymaker. They didn’t have the funds to compete with huge dating sites like Match.com, so they applied for a couple of patents and began brainstorming. They believed that their vertical-search technology was good — better, in fact, than almost anything they had seen online. Best of all, it was built to work well on almost any kind of data set. With just a bit of tinkering, it could search for cheap airline tickets, or great apartments, or high-paying jobs. It could handle questions with hard-to-compare variables, like what’s the cheapest flight between London and Las Vegas if I’m trying to choose between business class or leaving after 3 p.m.?As far as they could tell, their search technology performed better on such problems than Google did, which Adam discovered when he tried to buy an iPod online. “I spent half an hour searching Google for the lowest price, and it drove me completely mad,” he told me. It was impossible for him to figure out which sites were selling iPods and which were selling accessories, like headphones or charging cords. Or Google would show Adam one price, but then the actual price was completely different. Or there was an extra charge for shipping. It seemed to Adam his technology would do a much better job.Google executives, had they known of Adam’s frustrations, probably wouldn’t have been surprised. For years, Google had been trying to build a tool for comparing online prices. “The idea was you should be able to input any item, and we’d show you the best place to buy it,” says Brian Larson, a technical lead for what was then named Froogle and today is called Google Shopping. Larson’s team was small — just himself and one other programmer at first, and roughly a dozen people at its height — and Larson would regularly test how Froogle compared with other online price-comparison services. “Sometimes we were neck and neck; sometimes, not so much,” Larson said. “We had a hundred million product listings, which was better than competitors.” But they were often outperformed by sites like PriceGrabber.com, which had many more employees devoted to price comparisons.Froogle’s limitations tended to pop up particularly when users included too many search parameters. For a while, Larson had a specific test search that Froogle kept failing, something like “white running shoes and cheap and free shipping.” Inevitably, the first result would be a Christmas elf wearing running shoes that some guy was selling online. No matter how Google’s engineers fiddled with their coding, they couldn’t stop the elf from appearing as the top link. Eventually, a manager bought the elf so it wouldn’t appear in the search results anymore. “We made elf T-shirts,” Larson told me. “It became our mascot.”Adam and Shivaun’s technology was good enough to tell the difference between an elf wearing running shoes and an actual pair of running shoes. It was good enough, in fact, to figure out which websites charged hidden shipping fees and which offered truly good deals. So the Raffs quit their jobs, hired a few programmers, spent months perfecting their technology and, in early 2006, unveiled Foundem.com, a vertical-search engine for finding cheap online prices, to a small group of friends and associates. Each time someone used Foundem to buy something, the Raffs would receive a small payment from the website making the sale. Adam and Shivaun weren’t sure their company would succeed — there were already a couple of other big price-comparison search engines, like PriceGrabber, NexTag and, of course, Google itself — but they figured this was how the internet was supposed to work: Two people with a new idea can take on giants and, if their technology is good enough, grow into colossi themselves.The Raffs knew they would have to rely on Google to find customers. For one thing, as evidenced by the name Foundem, they weren’t marketing geniuses. (“It’s like we found ’em for you, you know?” Shivaun explained.) But early tests indicated that Foundem usually came up high in Google’s search results whenever people submitted queries like “compare prices xr-1000 motorcycle helmets.” Six months later, they opened Foundem to the world, and initial traffic was encouraging. “Search engines liked the site,” Shivaun told me. “That’s supposed to be the recipe for success.” As long as their vertical-search technology was strong, the Raffs figured, Google would guide shoppers to their door.Google has succeeded where Genghis Khan, communism and Esperanto all failed: It dominates the globe. Though estimates vary by region, the company now accounts for an estimated 87 percent of online searches worldwide. It processes trillions of queries each year, which works out to at least 5.5 billion a day, 63,000 a second. So odds are good that sometime in the last week, or last hour, or last 10 minutes, you’ve used Google to answer a nagging question or to look up a minor fact, and barely paused to consider how near-magical it is that almost any bit of knowledge can be delivered to you faster than you can type the request. If you’re old enough to remember the internet before 1998, when Google was founded, you’ll recall what it was like when searching online involved AltaVista or Lycos and consistently delivered a healthy dose of spam or porn. (Pity the early web enthusiasts who innocently asked Jeeves about “amateurs” or “steel.”)In other words, it’s very likely you love Google, or are at least fond of Google, or hardly think about Google, the same way you hardly think about water systems or traffic lights or any of the other things you rely on every day. Therefore you might have been surprised when headlines began appearing last year suggesting that Google and its fellow tech giants were threatening everything from our economy to democracy itself. Lawmakers have accused Google of creating an automated advertising system so vast and subtle that hardly anyone noticed when Russian saboteurs co-opted it in the last election. Critics say Facebook exploits our addictive impulses and silos us in ideological echo chambers. Amazon’s reach is blamed for spurring a retail meltdown; Apple’s economic impact is so profound it can cause market-wide gyrations. These controversies point to the growing anxiety that a small number of technology companies are now such powerful entities that they can destroy entire industries or social norms with just a few lines of computer code. Those four companies, plus Microsoft, make up America’s largest sources of aggregated news, advertising, online shopping, digital entertainment and the tools of business and communication. They’re also among the world’s most valuable firms, with combined annual revenues of more than half a trillion dollars.In a rare display of bipartisanship, lawmakers from both political parties have started questioning how these tech giants grew so powerful so fast. Regulators in Missouri, Utah, Washington, D.C., and elsewhere have called for greater scrutiny of Google and others, citing antitrust concerns; some critics have suggested that our courts and legislatures need to go after tech firms in the same way the trustbusters broke up oil and railroad monopolies a century ago. But others say that Google and its cohort are guilty only of delighting customers. If these tech leviathans ever fail to satisfy us, their defenders argue, capitalism will punish them the same way it once brought down Yahoo, AOL and Myspace.At the core of this debate is a question that is more than a century old: When does a megacompany’s behavior become so brazen that it violates the law? In the early 1900s, just after the Industrial Revolution, the federal government provided an answer by suing one of America’s largest companies, Standard Oil, on the novel theory that big becomes bad when a giant uses its dominance not only to defeat its competitors but also to extinguish the possibility that competition might occur.In its technological innovation, Standard Oil was the Google of its day. The company’s founder, John D. Rockefeller, had become the richest man in America by spending millions of dollars hiring scientists to transform how oil was refined and transported. And those innovations earned the public’s admiration. In 1858, before Standard Oil was founded, lighting a home required whale oil, which cost up to $3 a gallon, putting illumination out of reach for all but the wealthiest of households. By 1885, after Standard Oil figured out how to refine kerosene, it cost just 8 cents a gallon to brighten the night. “Let the good work go on,” Rockefeller wrote to a partner. “We must ever remember we are refining oil for the poor man and he must have it cheap and good.”Standard Oil’s technological discoveries gave the company huge advantages over its rivals, and Rockefeller exploited those advantages ruthlessly. He cut secret deals with railroads so that other firms had to pay more for transportation. He forced smaller refineries to choose between selling out to him or facing bankruptcy. “Rockefeller and his associates did not build the Standard Oil Co. in the boardrooms of Wall Street,” wrote Ida Tarbell, a muckraking journalist of the day. “They fought their way to control by rebate and drawback, bribe and blackmail, espionage and price cutting, and perhaps more important, by ruthless, never slothful efficiency of organization.”In 1906, President Theodore Roosevelt ordered his Justice Department to sue Standard Oil for antitrust violations. But government lawyers faced a quandary: It wasn’t illegal for Standard Oil to be a monopoly. It wasn’t even illegal to compete mercilessly. So government prosecutors found a new argument: If a firm is more powerful than everyone else, they said, it can’t simply act like everyone else. Instead, it has to live by a special set of rules, so that other companies get a fair shot. “The theory was that competition is good, and if a monopoly extinguishes competition, that’s bad,” says Herbert Hovenkamp, co-author of a seminal treatise on antitrust law. “Once you become a monopoly, you have to start acting differently, and if you don’t, then what you’ve been doing all along starts breaking the law.”The Supreme Court agreed and split Standard Oil into 34 firms. (Rockefeller received stock in all of them and became even wealthier.) In the decades following the Standard Oil breakup, antitrust enforcement generally abided by a core principle: When a company grows so powerful that it becomes a gatekeeper, and uses that might to undermine competitors, then the government should intervene. And in the last century, as courts have censured other monopolies, academics and jurists have noticed a pattern: Monopolies and technology often seem intertwined. When a company discovers a technological advantage — like the innovations of Rockefeller’s scientists — it sometimes makes that firm so powerful that it becomes a monopoly almost without trying very hard. Many of the most important antitrust lawsuits in American history — against IBM, Alcoa, Kodak and others — were rooted in claims that one company had made technological discoveries that allowed it to outpace competitors.For decades, there seemed to be a consensus among policymakers and business leaders (though not always among targeted companies) about how the antitrust laws should be enforced. But around the turn of this century, a number of tech companies emerged that caused some people to question whether the antitrust formula made sense anymore. Firms like Google and Facebook have become increasingly useful as they have grown bigger and bigger — a characteristic known as network effects. What’s more, some have argued that the online world is so fast-moving that no antitrust lawsuit can keep pace. Nowadays even the biggest titan can be defeated by a tiny start-up, as long as the newcomer has better ideas or faster tech. Antitrust laws, digital executives said, aren’t needed anymore.Consider Microsoft. The government spent most of the 1990s suing Microsoft for antitrust violations, a prosecution that many now view as a complete waste of time and money. When Microsoft’s chief executive, Bill Gates, signed a consent decree to resolve one of its monopoly investigations in 1994, he told a reporter that it was essentially pointless for the company’s various divisions: “None of the people who run those divisions are going to change what they do or think.” Even after a federal judge ordered Microsoft broken into separate companies in 2000, the punishment didn’t take. Microsoft fought the ruling and won on appeal. The government then offered a settlement so feeble that nine states begged the court to reject the proposal. It was approved.What eventually humbled Bill Gates and ended Microsoft’s monopoly wasn’t antitrust prosecutions, observers say, but a more nimble start-up named Google, a search engine designed by two Stanford Ph.D. dropouts that outperformed Microsoft’s own forays into search (first MSN Search and now Bing). Then those two dropouts introduced a series of applications, like Google Docs and Google Sheets, that eventually began to compete with almost every aspect of Microsoft’s businesses. And Google did all that not by relying on government prosecutors but by being smarter. You don’t need antitrust in the digital marketplace, critics argue. “When our products don’t work or we make mistakes, it’s easy for users to go elsewhere because our competition is only a click away,” Google’s co-founder, Larry Page, said in 2012. Translation: The government ought to stop worrying, because no online giant will ever survive any longer than it deserves to.Once Foundem.com was available to everyone, the company’s honeymoon lasted precisely two days. During its first 48 hours, the Raffs saw a rush of traffic from users typing product queries into Google and other search engines. But then, suddenly, the traffic stopped. Alarmed, Adam and Shivaun began running diagnostics. They quickly discovered that their site, which until then had been appearing near the top of search results, was now languishing on Google, mired 12 or 15 or 64 or 170 pages down. On other search engines, like MSN Search and Yahoo, Foundem still ranked high. But on Google, Foundem had effectively disappeared. And Google, of course, was where a vast majority of people searched online.The Raffs wondered if this could be some kind of technical error, so they began checking their coding and sending email to Google executives, begging them to fix whatever was causing Foundem to vanish. Figuring out whom to write, and how to contact them, was a challenge in itself. Although Google’s parent company bills itself as a diversified firm with about 80,000 employees, almost 90 percent of the company’s revenues derive from advertisements, like the ones that show up in search. As a result, there are few things more important to Google’s executives than protecting the firm’s search dominance, particularly among the most profitable kinds of queries, such as those of users looking to buy things online. In fact, at about the same time the Raffs were starting Foundem.com, Google executives were growing increasingly concerned about the threats that vertical-search engines posed to Google’s business.“What is the real threat if we don’t execute on verticals?” one Google executive emailed his colleagues in 2005, according to internal documents later shared with the Federal Trade Commission. “Loss of traffic from Google.com because folks search elsewhere for some queries,” he wrote, in answer to his own question. “If one of our big competitors builds a constellation of high-quality verticals, we are hurt badly,” the internal documents continued. Another executive put it more bluntly: “Google’s core business is monetizing commercial queries. If users go to competitors such as Amazon to do product queries, long-term revenue will suffer.”Google executives began holding battle-plan meetings for the vertical war. Shortly after Foundem.com went online, one executive issued an order: Henceforth, Google’s own price-comparison results should appear at the top of many search pages, as quickly as possible, even if that meant disregarding the natural results of the company’s search algorithm. “Long term, I think we need to commit to a more aggressive path,” a high-ranking Google employee wrote to colleagues. Eventually, a mandate came from the chief executive: “Larry thought product should get more exposure,” a senior official wrote.One way to get that exposure was to influence the rules governing how Google displayed search results. In 2006, Google instituted a shift in its search algorithm, known as the Big Daddy update, which penalized websites with large numbers of subpages but few inbound links. A few years later, another shift, known as Panda, penalized sites that copied text from other websites. When adjustments like these occurred, Google explained to users, they were aimed at combating “individuals or systems seeking to ‘game’ our systems in order to appear higher in search results — using low-quality ‘content farms,’ hidden text and other deceptive practices.”Left unsaid was that Google itself generates millions of new subpages without inbound links each day, a fresh page each time someone performs a search. And each of those subpages is filled with text copied from other sites. By programming its search engine to ignore other sites doing the same thing that Google was doing, critics say, the company had made it nearly impossible for competing vertical-search engines, like Foundem, to show up high in Google’s results.Shivaun and Adam sent email after email to Google executives, but no one responded with anything useful. So the Raffs started making phone calls. Those didn’t help much, either. Adam and Shivaun had worked in technology for decades. They were well known and had connections to important people inside Google and at other big firms. But none of that seemed to matter.As the months went by and Foundem’s bank accounts dwindled, the Raffs, desperate, began approaching other websites, offering to adapt their technology to power those sites’ internal search engines. Soon they were providing back-end technology for a popular motorcycle site and a large magazine publisher. Eventually, about 2.5 million people were seeing Foundem’s search results each month. Foundem was named one of Britain’s best travel comparison sites by The Times of London and celebrated on a popular British gadget show. But without traffic from Google, the Raffs were barely holding on.Three years passed this way. Some nights, Shivaun would sit at her computer, exhausted, Googling phrase after phrase — How do you lift a Google website penalty? Who at Google reviews mistakes? Google and deindexed and phone number and help — hoping that some magic combination of words might yield a new solution. “It just felt so unfair,” Shivaun told me. “We had great technology. It was winning awards. But we couldn’t even get an explanation from Google about why we weren’t showing up.” Eventually, they sought out a public relations firm, in the hope that a newspaper article might get Google’s attention. The P.R. firm had an additional suggestion: Why not file an antitrust complaint? To Adam and Shivaun, that seemed like a waste of time. If Microsoft had been able to shrug off the antitrust attacks of the United States government, why would Google care about a complaint filed by some small firm?But they didn’t see many other options. So Adam and Shivaun pulled out their laptops and began assembling a long document detailing everything they had experienced. Then they went to Brussels, to the headquarters of the European Commission, the agency charged with regulating competitive behavior, and filed a complaint accusing Google of violating antimonopoly laws.As the years passed, Shivaun and Adam got into the habit of visiting message boards where people obsessively discussed Google’s many peculiarities. They began to notice an interesting pattern among companies complaining about the search giant: Often, the aggrieved parties had, in some way, posed some kind of threat to Google’s business. And they seemed to have suffered dire consequences.There was, for instance, Skyhook Wireless, which had invented a new navigation system that competed with Google’s location software and had signed major deals with the cellphone manufacturers Samsung and Motorola. Skyhook’s accuracy “is better than ours,” one Google manager speculated in an internal email later revealed in a lawsuit filed by Skyhook against Google. Not long after that note was written, according to the lawsuit, a high-ranking Google official pressured Samsung and Motorola to end their relationships with Skyhook — and implied that if they didn’t, Google could make it impossible for them to ship their phones on time. (Google has denied doing anything inappropriate.) Soon, Samsung and Motorola canceled their Skyhook contracts. Skyhook sued Google, and though one suit was dismissed, Google ended up paying $90 million to settle a patent-infringement claim. But by then it was too late. Skyhook’s founders, bereft of other partnership options, had been forced to sell their company at a large discount.Then there was Yelp, a website with millions of user-generated reviews of local brewpubs, auto-body shops and other businesses. Yelp grew quickly as local queries — like “best nearby steakhouse” — became a third of all online searches. For years, Yelp appeared near or at the top of millions of Google searches. Google, hoping to capitalize on that traffic, tried to buy Yelp in 2009, but Yelp’s founders rejected those advances. Then Google started pulling Yelp’s content into its own results, which meant many users didn’t have to visit Yelp’s website. Yelp complained — to Google and later to the F.T.C. — but Google said the only alternative was for Yelp to remove its content from Google altogether, according to documents filed with federal regulators. The same thing happened at other fast-growing review sites like TripAdvisor and Citysearch, which also complained to the F.T.C. “We still exist,” says Luther Lowe, a vice president at Yelp, “but Google did everything it could to ensure that we’d never present a threat to them. It’s bullying, but they’re the 800-pound gorilla.”The more Adam and Shivaun looked, the more examples they found. Getty Images had created a popular search engine to help users comb through the firm’s 170 million photographs and other visual art. Then, in 2013, Google adjusted how it displayed images so that rather than directing people to Getty’s website, users could easily see and download Getty’s high-definition images from Google itself. “Our traffic immediately fell 85 percent,” says Yoko Miyashita, Getty’s general counsel. “We wrote to Google, and said, Hey, this isn’t cool. And their response was, ‘Well, if you don’t agree to these terms, we’ll just exclude you’ ” — by letting Getty remove itself from the search engine entirely, Miyashita said. “That’s not really a choice, because if you aren’t on Google, you basically don’t exist.”TradeComet.com, which operated a vertical-search engine for finding business products, initially prospered by buying ads on Google, but as the site grew, Google “raised my prices by 10,000 percent, which strangled our business virtually overnight,” the company’s C.E.O. at the time, Dan Savage, said when he filed an antitrust lawsuit in 2009. KinderStart.com, a vertical-search engine for parents, sued Google after it received a “PageRank” of zero, making it essentially unfindable. (TradeComet.com’s suit was dismissed on a technicality; KinderStart.com’s was dismissed for insufficient evidence.)An error has occurred. Please try again later.You are already subscribed to this email.Shivaun and Adam filled notepads with the names of companies that had complained about Google’s tactics — eJustice, a vertical-search engine for legal information; NexTag, the fellow price-comparison site; BDZV, a group of German newspapers. They printed out lawsuits and regulatory complaints until their living room was a maze of paper.Eventually the Raffs reached out to the F.T.C., which, they knew, was the American equivalent of the European Commission’s antitrust office, and the U.S. regulators invited them to visit. The F.T.C.’s staff, it turned out, had been quietly collecting complaints about Google for years. In 2012, those officials wrote a confidential 160-page report that said Google had “adopted a strategy of demoting, or refusing to display, links to certain vertical websites in highly commercial categories.” That memo, about half of which was accidentally sent to reporters at The Wall Street Journal after they submitted a Freedom of Information Act request, said that “Google’s conduct has resulted — and will result — in real harm to consumers and to innovation.”“Google has strengthened its monopolies over search and search advertising through anticompetitive means,” which “will have lasting negative effects on consumer welfare,” F.T.C. officials wrote. They cited instances in which Google seemed purposely to be privileging less useful information, substandard search results and suboptimal links. “Although it displays its flight search above any natural search results for flight-booking sites, Google does not provide the most flight options for travelers,” the regulators wrote. Whereas a decade earlier someone searching for steakhouses would have seen a long list of websites, now the most noticeable results pointed to Google’s own listings, including Google maps, Google local search or advertisers paying Google. Some F.T.C. staff recommended “that the Commission issue a complaint against Google” for copying material and certain advertising and contract practices, though not search-engine bias.Google responded to the report’s claims by arguing that the changes it made to the search engine benefited users. “Our testing has consistently showed that users want quick answers to their queries,” Google said in a statement when contacted about this article. “If you are searching for weather, you probably want a forecast, not just links to weather sites.” And when it comes to online shopping, the statement read, “if someone is searching for products, they likely want information about price and where they can buy it. They probably don’t want to be taken to another site where they have to enter their search again. . . . We absolutely do not make changes to our search algorithm to disadvantage competitors.” Claims to the contrary, like those made by Foundem, are untrue, Google maintained. “We make hundreds of changes to search every year, all with the same goal: Delivering users the best, most relevant search results,” the company continued. “Each change, large and small, affects millions of sites, some who see their rankings improve, others who drop.” And, Google concluded, “our ultimate responsibility is to deliver the best results possible to our users, not specific placements for sites within our results.”When the F.T.C.’s politically appointed leadership considered the staff’s recommendations, they declined to sue Google, surprising many inside the agency. “While not everything Google did was beneficial, on balance, we did not believe that the evidence supported an F.T.C. challenge,” the agency’s chairman at the time, Jon Leibowitz, said when he announced the decision in 2013.The F.T.C.’s decision, according to agency insiders, was motivated in part by a debate that has also sparked battles within antitrust courts over the last 40 years: Should the law protect consumers or encourage competition? They’re not always synonymous. “It wasn’t consumers who were complaining about Standard,” says Hovenkamp, the antitrust scholar. “It was the other oil companies.” Similarly, few users are kvetching about Google; it’s primarily other tech firms. United States judges have increasingly held that the government must show consumer harm to win in court.Adam and Shivaun didn’t have to wait for the official F.T.C. announcement to know that their case was going nowhere. Meeting with officials in Washington, they could tell: These people were not going to prosecute. They had come to the United States at their own expense. They had written memo after memo arguing that Google was treating them unfairly and as a result hurting users. They had done everything they were asked. Standard Oil controlled 64 percent of the market for refined petroleum when the Supreme Court broke it into dozens of pieces. Google and Facebook today control an estimated 60 to 70 percent of the U.S. digital advertising market. And the F.T.C. seemed happy to let them keep doing it. To the Raffs, it felt as if history was repeating itself, as if the pointless, ineffectual Microsoft case was happening all over again. It felt as if nobody cared.If you are younger than 29 — which just happens to be the average age of a Google employee, according to a survey done by PayScale — then odds are good you don’t remember much about the Microsoft antitrust battles of the 1990s. So, a quick primer: For almost a decade, starting in 1993, federal and state prosecutors besieged Microsoft in courtrooms across the nation, arguing that the company had acted in ways that were predatory and dishonest to preserve its software monopoly. One Microsoft executive was quoted in court as threatening to “cut off” the “air supply” of a competitor. “Is Bill Gates the ’90s answer to Don Corleone?” Time magazine asked. “I expected to find a bloody computer monitor in my bed,” a witness told investigators.Along the way, Microsoft was accused of widespread bullying, coercion and general obnoxiousness. And Microsoft basically said: Whatever. “There’s one guy in charge of licenses,” Bill Gates told reporters after he signed a consent decree with the Department of Justice in 1994. “He’ll read the agreement.” Everyone else, the implication was, would ignore it.Even when a judge ruled in 2000 that Microsoft was violating antitrust law, conventional wisdom held that the victory was largely pyrrhic. Microsoft successfully appealed, and prosecutors eventually threw in the towel, agreeing to abandon their attacks and settle if Microsoft agreed to token reforms, such as making its products more compatible with competitors’ software and giving three independent observers unfettered access to the company’s records, employees and source code. Microsoft’s executives thought that three observers, versus 48,000 employees, sounded like pretty good odds.This was the history the Raffs recalled when they heard the F.T.C. was abandoning its investigation. But then, they also remembered a discussion they had once had with a lawyer named Gary Reback, who told them that everything they’d heard about the Microsoft trials was wrong. Reback is something of a legend in Silicon Valley, both because of his accomplishments as an antitrust provocateur and because of his anxious — some might say paranoid — worldview. Reback has been known to call other lawyers late at night and leave long, obsessively detailed voice mail messages about legal arguments and economic theories. He was featured on a 1997 cover of Wired magazine with the headline “This Lawyer Is Bill Gates’s Worst Nightmare,” a boast that wasn’t far-off: Working on behalf of clients like Netscape and Sun Microsystems, Reback had browbeaten the Department of Justice into suing Microsoft for antitrust.By the time Adam and Shivaun started visiting the F.T.C., Reback had exchanged his antipathy of Microsoft for a disdain of Google and had accompanied them on their visits with regulators. There’s a loose coalition of economists and legal theorists who call themselves the New Brandeis Movement (critics call them “antitrust hipsters”), who believe that today’s tech giants pose threats as significant as Standard Oil a century ago. “All of the money spent online is going to just a few companies now,” says Reback (who disdains the New Brandeis label). “They don’t need dynamite or Pinkertons to club their competitors anymore. They just need algorithms and data.”Reback had told Adam and Shivaun that it was important for them to keep up their fight, no matter the setbacks, and as evidence he pointed to the Microsoft trial. Anyone who said that the 1990s prosecution of Microsoft didn’t accomplish anything — that it was companies like Google, rather than government lawyers, that humbled Microsoft — didn’t know what they were talking about, Reback said. In fact, he argued, the opposite was true: The antitrust attacks on Microsoft made all the difference. Condemning Microsoft as a monopoly is why Google exists today, he said.Surprisingly, some people who worked at Microsoft in the 1990s and early 2000s agree with him. In the days when federal prosecutors were attacking Microsoft day and night, the company might have publicly brushed off the salvos, insiders say. But within the workplace, the attitude was totally different. As the government sued, Microsoft executives became so anxious and gun-shy that they essentially undermined their own monopoly out of terror they might be pilloried again. It wasn’t the consent decrees or court decisions that made the difference, according to multiple current and former Microsoft employees. It was “the constant scrutiny and being in the newspaper all the time,” said Gene Burrus, a former Microsoft lawyer. “People started second-guessing themselves. No one wanted to test the regulators anymore.”In public, Bill Gates was declaring victory, but inside Microsoft, executives were demanding that lawyers and other compliance officials — the kinds of people who, previously, were routinely ignored — be invited to every meeting. Software engineers began casually dropping by attorneys’ desks and describing new software features, and then asking, in desperate whispers, if anything they’d mentioned might trigger a subpoena. One Microsoft senior executive moved an extra chair into his office so a compliance official could sit alongside him during product reviews. Every time a programmer detailed a new idea, the executive turned to the official, who would point his thumb up or down like a capricious Roman emperor.In the early 2000s, Microsoft’s top executives told some divisions that their plans would be proactively shared with competitors — literally describing what the company intended to create before software was even built — to make sure it wouldn’t offend anyone who was likely to sue. Microsoft’s engineers were outraged. But they went along with it.And most important, as Microsoft lived under government scrutiny, employees abandoned what had been nascent internal discussions about crushing a young, emerging competitor — Google. There had been informal conjectures about reprogramming Microsoft’s web browser, the popular Internet Explorer, so that anytime people typed in “Google,” they would be redirected to MSN Search, according to company insiders. Or, perhaps a warning message might pop up: “Did you know Google uses your data in ways you can’t control?”Microsoft was so powerful, and Google so new, that the young search engine could have been killed off, some insiders at both companies believe. “But there was a new culture of compliance, and we didn’t want to get in trouble again, so nothing happened,” Burrus said. The myth that Google humbled Microsoft on its own is wrong. The government’s antitrust lawsuit is one reason that Google was eventually able to break Microsoft’s monopoly.“If Microsoft hadn’t been sued, all of technology would be different today,” Reback told me. We’ve known since Standard Oil that advances in technology make it easier for monopolies to emerge. But what’s less recognized is the importance of antitrust in making sure those new technologies spread to everyone else. In 1969 the Justice Department started a lawsuit against IBM for antitrust violations that lasted 13 years. The government eventually surrendered, but in an earlier attempt to mollify prosecutors, IBM eliminated its practice of bundling hardware and software, a shift that essentially created the software industry. Suddenly, new start-ups could get a foothold simply by writing programs rather than building machines. Microsoft was founded a few years later and soon outpaced IBM.Or consider AT&T, which was sued by the government in 1974, fought in court for eight years and then slyly agreed to divest itself of some businesses if it could keep its most valuable assets. Critics complained AT&T was getting the deal of a lifetime. But then start-ups like Sprint and MCI made millions building on technologies AT&T championed, and AT&T found itself struggling to compete. It’s completely wrong to say that antitrust doesn’t matter, Reback argues. “The internet only exists because we broke up AT&T. The software industry exists because Johnson sued IBM.”It was critical that the Raffs continue fighting, Reback told them. Social embarrassment and sustained attacks have the power to succeed when courtrooms or political agencies fail. After their F.T.C. disappointment, the Raffs flew back to England to consider their options. And then one night they were at home watching television when the phone rang. Someone they had met in Brussels was calling to share some remarkable news. The European Commission had issued a decision on the complaint they filed six years before.What changed everything was a middle-aged Danish politician named Margrethe Vestager, who had recently been named the European Union’s commissioner for competition. Vestager was an unusual choice for the post. She wasn’t a populist crusader or a pro-business acolyte; she was, instead, a moderate whose claim to fame, at that point, was having served as an inspiration for the television show “Borgen,” a fictional series about a Danish politician. But Vestager was awarded the commissioner’s post in 2014 after arguing that European marketplaces needed to do a better job of giving everyone an equal chance to succeed. Since assuming her office, Vestager has become, unexpectedly, the most prominent antitrust official in the world, invited to speak at conferences and mobbed by autograph seekers.By the time Vestager took office, Google had already transitioned its price-comparison service to its present incarnation, which is effectively an advertising system that prominently features links only from companies that pay for the promotion. (Users are notified by a small logo that says “sponsored.”) After reviewing the complaints submitted by the Raffs and others, Vestager announced she intended to formally charge Google with antitrust violations. (She has also embarked on investigations into the European tax practices of Starbucks, Amazon and Apple, as well as anticompetitive tactics at Qualcomm, Facebook and Gazprom.)Over the next two years, Vestager’s staff reviewed data from 1.7 billion Google queries. They scrutinized how people fared when they conducted searches on topics in which Google had a vested interest, versus those where the company had nothing to gain. Then, in June of last year, the commission issued its final verdict: “What Google has done is illegal under E.U. antitrust rules,” Vestager said in a statement released at the time. “It denied other companies the chance to compete on the merits and to innovate. And most important, it denied European consumers a genuine choice of services and the full benefits of innovation.” Google was ordered to stop giving its own comparison-shopping service an illegal advantage and was fined an eye-popping $2.7 billion, the largest such penalty in the European Commission’s history and more than twice as large as any such fine ever levied by the United States.The verdict rocked Silicon Valley. Some think Europe’s assertiveness makes it more likely American regulators will act as well. And there’s evidence that’s already starting. Donald Trump appealed to voters, in part, by attacking the tech monopolies. In a case of truly odd bedfellows, that puts him in alignment with Elizabeth Warren and Bernie Sanders, who have long called for greater scrutiny of technology companies. Last year, a group of Democratic lawmakers in Congress, led by Senator Amy Klobuchar of Minnesota, sponsored legislation to boost antitrust enforcement by forcing companies to assume the burden of showing that a merger won’t hurt the public.Meanwhile, a bipartisan assortment of state attorneys general have urged the F.T.C. to reopen its investigation of Google. Most major antitrust battles, including the federal suits against Microsoft and Standard Oil, have begun as state actions. A Missouri investigation is particularly notable because the state’s Republican attorney general, Josh Hawley, who is running for the United States Senate, has subpoenaed information to see if Google has manipulated searches to disadvantage potential competitors. “The Obama-era F.T.C. did not take any enforcement action against Google, did not press this forward and has essentially given them a free pass,” Hawley told reporters after revealing his inquiry in November. “I will not let Missouri consumers and businesses be exploited by industry giants.”As attacks against Google have escalated, the company has tried to limit the damage. After Yelp complained to the F.T.C. about Google’s stealing its content, Google promised to make it easier for websites to opt out of automatic copying, a pledge it reaffirmed a few months ago. And earlier this month, in exchange for Getty Images’ withdrawing its complaint to the European Commission, Google signed a licensing agreement with Getty promising to more clearly display images’ copyright information. Other titans like Facebook are similarly trying to get ahead of criticisms, voluntarily pledging greater transparency and promising to work more cooperatively with regulators.The implication is clear enough: Google and the other tech titans understand that the landscape is shifting. They realize that their halos have become tarnished, that the arguments they once invoked as a digital exception to American economic history — that the internet economy is uniquely self-correcting, because competition is only a click away — no longer hold as much weight. “When you get as big as Google, you become so powerful that the market bends around you,” Vestager told me. The notion that antitrust law isn’t needed anymore, that we must choose between helping consumers or spurring competition, no longer seems sufficient reason to exempt the tech giants from century-old legal codes. If anything, Vestager’s verdict and state investigations indicate that companies like Google may have more in common with the monopolists of old than most people thought. Silicon Valley’s bigwigs ought to be scared.“If Europe can prosecute Google, then we can as well,” says William Kovacic, a law professor and former Republican-appointed chairman of the Federal Trade Commission. “It’s just a question of willingness now.”If the internet’s potentates are frightened, however, they’re doing a good job of hiding it. Google has appealed the European Commission’s decision and has vigorously defended itself online. The company’s arguments are the same ones that it was putting forth on company blogs over the course of the investigation. “We disagree with the European Commission’s argument that our improved Google Shopping results are harming competition,” Google’s top lawyer wrote in one post. The commission “drew such a narrow definition around online shopping services that it even excluded services like Amazon,” undermining the contention that Google is dominant. “Google delivered more than 20 billion free clicks to aggregators over the last decade,” he wrote in another post. Forcing it to “direct more clicks to price-comparison aggregators would just subsidize sites that have become less useful for consumers.” Google’s data indicates that users appreciate how the search engine has shifted over the years. “That’s not ‘favoring’ ” Google’s interests, the company said. “That’s giving customers and advertisers what they find most useful.”Some legal theorists think that Google might have a point. “To what extent are consumers, rather than competitors, being harmed by Google?” says Hovenkamp, the antitrust scholar. “If the answer is ‘not much,’ then I’m suspicious of an antitrust remedy.” Others say the risks are too high. “There are very real costs associated with suing a company like Google,” says Geoffrey Manne, executive director of the International Center for Law & Economics, a nonpartisan research center. “You’re potentially impairing a firm that provides vital services to millions of people, and potentially benefiting competitors who don’t deserve that support.”Those are fair arguments. But they are also, in some ways, beside the point. Antitrust has never been just about costs and benefits or fairness. It’s never been about whether we love the monopolist. People loved Standard Oil a century ago, and Microsoft in the 1990s, just as they love Google today.Rather, antitrust has always been about progress. Antitrust prosecutions are part of how technology grows. Antitrust laws ultimately aren’t about justice, as if success were something to be condemned; instead, they are a tool that society uses to help start-ups build on a monopolist’s breakthroughs without, in the process, being crushed by the monopolist. And then, if those start-ups prosper and make discoveries of their own, they eventually become monopolies themselves, and the cycle starts anew. If Microsoft had crushed Google two decades ago, no one would have noticed. Today we would happily be using Bing, unaware that a better alternative once existed. Instead, we’re lucky a quixotic antitrust lawsuit helped to stop that from happening. We’re lucky that antitrust lawyers unintentionally guaranteed that Google would thrive.Put differently, if you love technology — if you always buy the latest gadgets and think scientific advances are powerful forces for good — then perhaps you ought to cheer on the antitrust prosecutors. Because there is no better method for keeping the marketplace constructive and creative than a legal system that intervenes whenever a company, no matter how beloved, grows so large as to blot out the sun. If you love Google, you should hope the government sues it for antitrust offenses — and you should hope it happens soon, because who knows what wondrous new creations are waiting patiently in the wings.For the Raffs, however, it’s probably too late. By the time Vestager announced her verdict and record-setting fine last year, it had been 12 years since Adam and Shivaun started Foundem.com. During that time, their lives slowly but inexorably became devoted to battling Google. They had spent thousands of hours corresponding with regulatory agencies across the globe. They had filed a civil suit against Google in British court, a case that is ongoing. They basically shut down Foundem, creating more time for them to give advice to other companies and regulators fighting Google. This consulting work, some of which was funded by Google’s competitors, has helped to keep the Raffs afloat. And if the Raffs win their lawsuit against Google, it could be worth millions. “But it’s a different business model than we expected,” Adam told me. “It’s also deeply frustrating, because we became technologists in order to build new technologies. We never intended to be professional plaintiffs or antitrust crusaders.”One of the most difficult things for the Raffs over the past decade has been figuring out how to explain this journey to themselves and others. Even friends and family didn’t fully understand what was going on. “It feels really good to be validated like this, to be told we were right,” Shivaun told me, referring to Vestager’s verdict. “But that doesn’t turn back the clock and give us another chance. Even if we win in Brussels, or win our lawsuit, in some ways, we were still defeated. We were still beaten by Google.”",The Case Against Google
9571,3636857,2018-02-20 22:28:49,"Switch carriers without Early Termination Fees: How to avoid phone ETF feesHave an easier time switching to Verizon, AT&T, T-Mobile and SprintSharesWhen you switch carriers, the last thing you want is to get slammed with Early Termination Fees that can be a few hundred dollars. But, it’s an unpleasant reality if you ever try switching phone carriers in the middle of a contract.Breaking a phone contract and binding payment plan often entails an early termination fee, or ETF, or may immediately require a user to pay off the remaining balance of their smartphone if it was purchased on an equipment installment plan. This cost can make it hard for phone users to switch from one US carrier to another.Early Termination Fees: Tips to avoid ETFsMobile carriers don’t want to let their customers go – that’s the point of service contracts. If there were no penalty for terminating a contract, the contract wouldn’t have much retaining power. That’s why you can see ETFs in a lot of contracts with mobile carriers. They may be less common today than they were ten years ago, as more and more carriers are switching over to mobile plans that users pay monthly. Carriers are also rarely subsidizing phones anymore, opting instead to sell them on installment plans that help the carrier keep customers on the network for the duration of the plan.The easiest way to avoid early termination fees and other large bills when leaving a mobile carrier is to avoid lengthy contracts and payment plans in the first place. This can mean larger expenses up front, as users need to buy their phone outright and may miss out on special offers from their carrier, like deals on the iPhone X. For some, these upfront expenses might keep this from being an option.Fortunately, there are deals hiding around every corner, with the four major mobile carriers all offering some incentive to assist mobile users in joining their network. Let’s take a look at how to switch to T-Mobile, Sprint, AT&T or Verizon and avoid a huge bill from the carrier getting left behind.Switch to Verizon without ETFsSadly, for anyone trying to switch to Verizon from another carrier that has them locked in a contract or tied to an equipment installment plan, Verizon currently has no special offers to help new customers switch over.Customers are able to trade in their old phone to recoup some of their money, but that isn't likely to add up to the value other carriers are offering right now. On the plus side, Verizon does have a number of good phone deals and plans availableSwitch to AT&T without ETFsAT&T is offering up to $650 to lure mobile users away from their current carriers and switch to AT&T. Customers need to get a new smartphone from AT&T on an AT&T Next installment plan and start a new line of service with a qualifying plan. They also have to port in their phone number from their old carrier. After that, they must trade-in their old smartphone. Finally, the new customer must submit the final bill from their carrier to AT&T.In return, AT&T will cover up to $650 per line switched over. AT&T will cover customers’ ETF from their old carrier up to $350, or it will cover the remainder of an installment plan on the phone for up to $650. The trade-in value of the phone will be deducted from AT&T’s payment, and the customer will get a promotional prepaid card for the balance.Switch to Sprint without ETFsThrough its “Clean Slate” program, customers switching to Sprint from a post-paid plan from another mobile carrier can get $650 toward an ETF or remaining device payments. To take advantage of this offer, customers can take their current phone and trade it in to Sprint, purchase a new phone on a Sprint Flex installment plan or buy it outright, port their old phone number to the new phone, and activate a mobile plan.Sprint will credit new customers for the trade-in value of their old smartphone and then give them a Visa prepaid card for the cost of any switching fees minus the trade-in value of the old phone.Switch to T-Mobile without ETFsT-Mobile efforts to eliminate early termination fees have been a years-long crusade. Currently, T-Mobile offers to cover the remaining balance on a phone from another carrier or any ETF incurred leaving that carrier when customers switch to T-Mobile. New customers have to trade in their current phone in good condition and port over that phone number. They also need to buy a new phone from T-Mobile on an installment plan and get a T-Mobile One plan.Customers switching to T-Mobile through this deal will get up to $350 via a prepaid card to reimburse them for the cost of ETFs or they can get up to $650 to cover the remaining payments on the phone from their old carrier.ConclusionAs things stand right now, each mobile carrier has some offer to make joining their network an attractive offer. When it comes to avoiding ETFs or getting forced to pay off a equipment installment plan, all of the major carriers except Verizon has an offer to defray the cost. For those three carriers, the deals are pretty even, covering up to $350 for ETFs and $650 for devices. So, instead of focusing on which carrier's deal is better, shoppers aiming to switch can instead look into which carrier has the best plans and what their best phones are.",Switch carriers without Early Termination Fees: How to avoid phone ETF fees
9572,3636858,2018-02-21 11:40:00,"Best smartwatch: the top choices you can buy in 2018Your wrist deserves the best companionSharesThe smartwatch is the ultimate smartphone accessory. It can tell the time, of course, but it can also beam important notifications straight to your wrist, and run native apps.What's more impressive is that many of today's best models can also perform a ton of novel tricks, such as enabling you to search the internet with your voice, tracking your exercise with GPS and letting you pay at the grocery store without reaching for your wallet.Oh – and they look absolutely stunning to boot. If you're thinking that a smartwatch is a pointlessly geeky accessory... think again. These choices are well-made, powerful and can genuinely make you fitter through some smart nudges.The Apple Watch 3 (or Apple Watch Series 3, if you're picky) is the best smartwatch you can buy right now. Yes, it's essentially just the Apple Watch 2 frame with some new innards... but they make a big difference.The LTE connection is the headline event, although that's not really as useful as some might hope - plus it costs a lot more to use, and drains the battery.What we like is the non-LTE version, which is a lot cheaper, and offers all the smarts of the Watch 2 but with a longer battery life and faster speeds when flicking through apps. This is the right kind of upgrade on the best smartwatch in the world.It's still water-resistant so you can swim with it and you won't have to worry about getting it wet in the rain when you're out for a jog. There's GPS onboard to make running that little bit easier plus it comes running the top watchOS 4 software.You may not have ever heard of Ticwatch, and that's because it's a relatively small and new brand to the smartwatch world. This is the best Android Wear watch money can buy right now.That's mostly because of the low price and the fact everything works really well. We also love the Ticwatch E for its built-in GPS, accurate heart rate sensor and great design.All of the fitness features you'd expect are here; you can even use it without having to take your phone out while you exercise, but you won't be able to receive phone calls like on the Apple Watch 3. The design is premium, but it won't be for everyone so be sure to properly study the photos above and in our review to work out if it's built for you.The true highlight here is the low price considering it's generally around half the price of a lot of the other devices you'll find on this list.The LG Watch Style (built in collaboration with Google) offers everything that's to love from the best Android Wear smartwatches, ditches the dreaded flat tire, then fills in the gap with cool, useful features and a whole lot of… style.Roll that all up and you're left with an extremely alluring presentation that makes a mighty strong argument for Google's wearable platform. But there are some familiar wrinkles here including the sub par battery life.That said, it’s easy to express why the Style is one of the smartwatches we want to put on our wrists.For a recently-lowered price, it offers just as much utility as prior smartwatch attempts, but ups the ante with a slim, dashing design and several welcome features, like the voice-activated Google Assistant and a refreshed user interface that's full of clever tweaks.Ticwatch is back again for our fourth entry. This time it's the Ticwatch S, which is remarkably similar to the Ticwatch E we've mentioned above.The differences are limited, but this watch is a tiny bit heavier and a little larger because it comes with a thicker bezel around the sides to show you the exact time.There's also a different strap on this version that comes with the GPS sensor inside. The makers of the Ticwatch claim this is more accurate than when it's inside the watch casing, but we didn't see any major differences.It does mean you can't swap out the band on the Ticwatch S, like you can on the Ticwatch E, and that's a big shame.It's a touch more expensive than the Ticwatch E too, but if you prefer the design you may want to go for this as it's still much more affordable than a lot of our other favorite smartwatches on sale right now.The Fitbit Ionic was always going to be a tough move for the brand, trying to enter the world of smartwatches from fitness bands.The effort succeeds in some places: namely fitness, as you might imagine, where you can track all manner of things, from running to weight lifting to swimming. There's also dedicated bodyweight coaching sessions in there, and you can pay for items on the go using Fitbit Pay.When it first launched, the price was super high and it was a bit too expensive to wholeheartedly recommend. The good news is the price seems to have dropped in recent months so you can get it for at least $70/£70 cheaper than the RRP.If you're a Fitbit fan looking to do more than you get on an average band, this is a nice option.Misfit's first ever fully-fledged smartwatch comes in sixth position in our ranking, and a part of that is down to its low price.The Misfit Vapor has a super clear and bright 1.3-inch AMOLED display, a premium design - if it is a little thick - as well as up to the minute Android Wear 2.0 software as well.It's not the perfect watch as the Misfit Activity app is quite limited and there's no Android Pay, but mostly this will suit you if you're looking for an attractive watch with basic fitness and notification features.Despite a serious lack of original apps, the Samsung Gear Sport still merits a place on this list. It's got the same premium build we've come to expect from Samsung, has an exceptionally clear screen.The main draw is the fact it has Spotify offline playback onboard, and combine that with the inbuilt GPS and you've got an all-in-one running watch that can supply you with millions of tracks on the go.It's also 50m-waterproof, which means you can take this little wrist beast into the water and get swimming - and it's even compatible with iOS on top of that.It's not got the battery life of the Gear S3, lowering the time between charges to make the whole unit a little sleeker - and with a rubberized band in the box, it's clear this a smartwatch for the fitness fans from Samsung.OK, we know this isn't a true smartwatch - but it's a brilliant running watch with smartphone connectivity, and to many it's good enough to achieve what they need.The notifications are basic but rich enough, and you can control your music from the wrist.Yes, this will be a bit basic for some, but if you're in any way interested in fitness this watch can do it all - and it's a half-decent fitness tracker too, with a super long battery life that puts many on this list to shame.Still on sale as the 'budget' Apple Watch, this device is still well worth a look. OK, it doesn't have the water resistance, LTE connectivity or GPS of the top Apple Watch 3, but it also doesn't have the price.Being able to check when you've got a message, or see who's calling and be able to make snap decisions, still is possible here and great when you're exercising or in another situation where grabbing your phone isn't practical.The new enhancements from Apple's watchOS 4 really do help this smartwatch leap forward, even if the hardware is years old now. It's now faster, more usable and comes with watch faces that make it feel more personal.This watch is still on sale from Apple, so deals might be a bit sparse, but the Apple Watch Series 1 can be found at lower cost if you're sharp.",Best smartwatch: the top choices you can buy in 2018
9573,3636860,2018-02-20 16:25:05,"Swype Keyboard is officially obsoleteThe OG swipe-to-type app has swiped left on itselfSharesSwype Keyboard, a popular third-party keyboard for Android, has been discontinued. Once the de facto option for people looking to simplify typing on a smartphone, the unique swipe-to-type functionality that set it apart has been diluted by other popular companies getting into the mix in recent years.The Microsoft-owned SwiftKey Keyboard is still supported and doing well in the Google Play Store. Even Google’s own Gboard incorporates the swipe typing feature for fast, nearly effortless typing.As it stands, one can imagine that it’d be tough to stand out among the others, especially since they’re each updated on a comparatively frequent basis. But according to a statement made on Reddit by Swype Keyboard creator Nuance, the company is shifting its focus elsewhere for the time being, saying that it will “...no longer be updating the Swype+Dragon keyboard for Android. We’re sorry to leave the direct-to-consumer keyboard business, but this change is necessary to allow us to concentrate on developing our AI solutions for sale directly to businesses…”How we interact with phones has changedHaving now exited a highly-competitive playing field, Nuance is focused instead on AI. While it’s not gunning for living room dominance in the way that Google, Amazon and Microsoft are with their smart assistants, this shift is indicative of a bigger trend happening in how we use our phones – a move that many companies, including its competitors, are already tapped into.Most people still type while using their phones, but thanks to the always-listening voice assistants, like Siri, Cortana and Google Assistant, a lot of queries are being handled over voice. And that’s where AI comes in. Voice recognition paired with machine learning helps these assistants make life easier – something that Swype Keyboard was good for in its heyday.You can still download Swype Keyboard if you’re curious, but it certainly won’t be updated in the future. At the time of writing, there’s a trial available and Nuance is still charging for the full version. But it’s possible that both could soon see removal from the Play Store at any time.",Swype Keyboard is officially obsolete
9574,3636934,2018-02-20 12:06:33,"TNW SitesFind out how to get a team of business pros backing your new venture for under $40If you’ve never started a business before (or heck…even if you have), you’ve got questions. As in…a whole bunch of questions. With so many moving parts and sometimes conflicting priorities, it’s easy to get overwhelmed and lose your way early in the process.The resources of Startups.co University want to make sure you don’t stumble down the wrong roads…which is why you can lock in a lifetime of access to their entire portfolio of business startup materials and outreach for just $39 from TNW Deals.This isn’t just a simple guide or 10-item checklist to use before opening your doors. No, this is more than 600 in-depth videos, featuring business experts explaining every aspect of a startup and where your attention needs to be paid. There are daily lessons offering insight, helpful tips grouped into similar startup paths and content uncluttered with fluff to help you get to the meat of your particular issue.But beyond that wealth of research material, Startups.co truly excels with its most important resource: the human factor. As a Startups.co member, you can line up 1-on-1 discussions with an expert to talk through your specific business issue. Whether your concerns are with funding, marketing, legal hassles or more, there’s a Startups.co expert ready to give you the answers you need.There are even moderated Slack chats twice a week where you can engage directly with founders and other experts to further discuss your business.That kind of 24/7/365 business tutelage (usually a nearly $1,500 value) is now available for a fraction of that total — only $39 while this offer lasts.",Find out how to get a team of business pros backing your new venture for under $40
9575,3639235,2018-02-21 15:33:00,"Why am I so terrible at email?Ever heard the phrase ""run into an asshole and they're the asshole, run into assholes all day and you're the asshole?"" It's been rattling around my brain of late, after several miserable weeks caused by my apparent failure to be a good emailer. (Or maybe I'm just an asshole, but since I'm not going to tackle that existential crisis head-on, let's focus on the emails.)I've been stuck in a rut when it comes to getting responses to my emails, either for comment on breaking news or interview requests. Normally, my phone is in one hand trying to speak to someone in person, while the other is frantically typing out said emails. Several times recently, I'd fire off 30 or 40 delicately crafted missives and get absolutely zero response.Email is my bete noire, forcing me to spend far too much time each day agonizing and piling through the thousands of digital missives I get on a daily basis. As an emotionally-repressed Brit, I can spend up to half an hour writing a mail before I send it off, so worried about coming off wrong. Is it too forward to use their first name rather than their title? What about just asking for the comment in two lines? I um and aah to myself while trying to phrase things right.I wish that I could email ""like a boss,"" that short, ultra-declarative sentences that apparently open up your life to new possibilities. I wish that, in the style of BuzzFeed's Katie Notopoulos, I could condense all of my emailing down to just a few seconds each day. It would be nice if things were just a little simpler, maybe if the whole world was on Slack, I could just bother them on there instead.I sought help, calling up upon the experts, including world-renowned etiquette expert William Hanson, who explained what I could, and should, be doing better. Hanson feels that auto-responses and platforms like Slack help you ""forget the niceties,"" and if you forget them here, he says that you're more likely to ""forget them elsewhere.""Hanson thinks that the issue with most online messaging systems is that they are often designed ""by people with poor social skills."" It's a sentiment echoed by Don Norman, the interaction designer who explained the problems with email to Fast Company in 2015. ""Gmail conversations are horrible,"" he said, adding that ""it's the wrong mechanism, badly done."" Norman felt that email was being forced to ""do everything,"" even though it's ""not particularly good at anything.""Despite email's obvious flaws, I still need to use it on a daily basis to get folks to talk to me, so what can I do? ""It's important to remember,"" says etiquette consultant Jo Bryant, ""that all communications are an extension of yourself."" Bryant feels that it's important to be mindful of ""how we make other people feel, which, in itself, is the essence of good manners."" She believes that ""taking time to email someone personally, including a salutation and sign off"" shows that you have taken care over your correspondence.""Hanson agrees, and as well as ensuring that you use the proper salutation and sign-off, you need to go the extra mile in your emails. For instance, he says that you should ""notice the little details,"" such as looking up a pleasant local news event, or the weather, in the location that the recipient lives. The aim is to convey, as Hanson puts it, the sense that ""even though we're miles apart, we're closer than you think.""You'll have to balance all of this courtesy with one other fact: You shouldn't take too much of your recipient's time. ""It's bad to be overly verbose,"" says Hanson, suggesting that you get to the point without being abrupt. It's a fine line to walk, especially when you're looking to make the extra effort to get folks to like you.The last person I spoke to about my woes was closer to home, my colleague Aaron Souppouris. I showed him plenty of my electronic missives, and he noted that my lack of a ""formal greeting, explaining who I am and where I'm from"" could be an issue. He added that I shouldn't look like I believe that I ""deserve a response"" by virtue of who I was and where I worked. Finally, he suggested that I should ""make sure not to overwhelm [recipients] with information.""I've started to cook up a new template onto which I shall craft my emails for the next couple of weeks. I'm hoping that with a little bit of luck, and using these experts' tips and tricks, I can become the world's greatest emailer. I'll also settle for getting a response every now and again.Thanks for your time and all the best, and I'll be in touch shortly to apprise you of how the experiment went.After training to be an intellectual property lawyer, Dan abandoned a promising career in financial services to sit at home and play with gadgets. He lives in Norwich, U.K., with his wife, his books and far too many opinions on British TV comedy. One day, if he's very, very lucky, he'll live out his dream to become the executive producer of Doctor Who before retiring to Radio 4.",Why am I so terrible at email?
9576,3639236,2018-02-21 15:44:00,"Amazon's answer to 'Altered Carbon' is Iain Banks' space operaIt appears that Amazon is looking for its answer to Netflix's crossover sci-fi thriller Altered Carbon. Today, the online retail giant announced that it has acquired the global television rights to Iain M. Banks' space opera series called The Culture. Amazon Studios will adapt the first novel, Consider Phlebas, for television.The Culture novels focus on a highly advanced society that goes to interstellar war with a violent race bent on galactic domination. The first book in the series centers on a spy tasked with recovering an AI that has the ability to help win the war. The questions the series poses are the costs of victory, and whether we should use our technology to dominate others at the expense of our own humanity.Dennis Kelly will adapt the sci-fi drama for Plan B Entertainment (World War Z). The Iain Banks's estate will serve as an executive producer for the series. ""Iain Banks has long been a hero of mine, and his innate warmth, humor and humanism shines through these novels,"" said Kelly, who previously adapted Matilda for the stage. ""Far from being the dystopian nightmares that we are used to, Banks creates a kind of flawed paradise, a society truly worth fighting for -- rather than a warning from the future, his books are a beckoning.""",Amazon's answer to 'Altered Carbon' is Iain Banks' space opera
9577,3639338,2018-02-21 04:14:55,"Yves here. Summers and Stansbury take a cautious stand in this piece. While they don’t claim to have found a cause or set of causes for the way average worker wages started lagging productivity gains in the 1970s, a critical driver of skyrocketing inequality, they do rule out rapid change. That is an important finding, since the claim that technology improvements were the driver of workers not sharing in the benefits of productivity gains is used politically to argue that it is a natural or even worse, virtuous outcome of innovation.By Anna Stansbury, PhD student in Economics, Harvard University, and Larry Summers, Lawrence Summers, President Emeritus, Harvard University. Originally published at VoxEUSince 1973, there has been divergence between labour productivity and the typical worker’s pay in the US as productivity has continued to grow strongly and growth in average compensation has slowed substantially. This column explores the causes and implications of this trend. Productivity growth appears to have continued to push workers’ wages up, with other factors to blame for the divergence. The evidence casts doubt on the idea that rapid technological progress is the primary driver here, suggesting rather that institutional and structural factors are to blame.Pay growth for middle class workers in the US has been abysmal over recent decades – in real terms, median hourly compensation rose only 11% between 1973 and 2016.1 At the same time, hourly labour productivity has grown steadily, rising by 75%.This divergence between productivity and the typical worker’s pay is a relatively recent phenomenon. Using production/nonsupervisory compensation as a proxy for median compensation (since there are no data on the median before 1973), Bivens and Mishel (2015) show that typical compensation and productivity grew at the same rate over 1948-1973, and only began to diverge in 1973 (see Figure 1).What does this stark divergence imply about the relationship between productivity and typical compensation? Since productivity growth has been so much faster than median pay growth, the question is how much does productivity growth benefit the typical worker?2A number of authors have raised these questions in recent years. Harold Meyerson, for example, wrote in American Prospect in 2014 that “for the vast majority of American workers, the link between their productivity and their compensation no longer exists”, and the Economist wrote in 2013 that “unless you are rich, GDP growth isn’t doing much to raise your income anymore”. Bernstein (2015) raises the concern that “[f]aster productivity growth would be great. I’m just not at all sure we can count on it to lift middle-class incomes.” Bivens and Mishel (2015) write “although boosting productivity growth is an important long-run goal, this will not lead to broad-based wage gains unless we pursue policies that reconnect productivity growth and the pay of the vast majority”.Has Typical Compensation Delinked from Productivity?Figure 1 appears to suggest that a one-to-one relationship between productivity and typical compensation existed before 1973, and that this relationship broke down after 1973. On the other hand, just as two time series apparently growing in tandem does not mean that one causes the other, two series diverging may not mean that the causal link between the two has broken down. Rather, other factors may have come into play which appear to have severed the connection between productivity and typical compensation.As such there is a spectrum of possibilities for the true underlying relationship between productivity and typical compensation. On one end of the spectrum – which we call ‘strong delinkage’ – it’s possible that factors are blocking the transmission mechanism from productivity to typical compensation, such that increases in productivity don’t feed through to pay. At the opposite end of the spectrum – which we call ‘strong linkage’ – it’s possible that productivity growth translates fully into increases in typical workers’ pay, but even as productivity growth has been acting to raise pay, other factors (orthogonal to productivity) have been acting to reduce it. Between these two ends of the spectrum is a range of possibilities where some degree of linkage or delinkage exists between productivity and typical compensation.In a recent paper, we estimate which point on this linkage-delinkage spectrum best describes the productivity-typical compensation relationship (Stansbury and Summers 2017). Using medium-term fluctuations in productivity growth, we test the relationship between productivity growth and two key measures of typical compensation growth: median compensation, and average compensation for production and nonsupervisory workers.Simply plotting the annual growth rates of productivity and our two measures of typical compensation (Figure 2) suggests support for quite substantial linkage – the series seem to move together, although typical compensation growth is almost always lower.Notes: Data from BLS, BEA and Economic Policy Institute. Series are three-year backward-looking moving averages of change in log variable.Making use of the high frequency changes in productivity growth over one- to five-year periods, we run a series of regressions to test this link more rigorously. We find that periods of higher productivity growth are associated with substantially higher growth in median and production/nonsupervisory worker compensation – even during the period since 1973, where productivity and typical compensation have diverged so much in levels. A one percentage point increase in the growth rate of productivity has been associated with between two-thirds and one percentage point higher growth in median worker compensation in the period since 1973, and with between 0.4 and 0.7 percentage points higher growth in production/nonsupervisory worker compensation. These results suggest that there is substantial linkage between productivity and median compensation (even the strong linkage view cannot be rejected), and that there is a significant degree of linkage between productivity and production/nonsupervisory worker compensation.How is it possible to find this relationship when productivity has clearly grown so much faster than median workers’ pay? Our findings imply that even as productivity growth has been acting to push workers’ pay up, other factors not associated with productivity growth have acted to push workers’ pay down. So while it may appear on first glance that productivity growth has not benefited typical workers much, our findings imply that if productivity growth had been lower, typical workers would have likely done substantially worse.If the Link Between Productivity and Pay Hasn’t Broken, What Has Happened?The productivity-median compensation divergence can be broken down into two aspects of rising inequality: the rise in top-half income inequality (divergence between mean and median compensation) which began around 1973, and the fall in the labour share (divergence between productivity and mean compensation) which began around 2000.For both of these phenomena, technological change is often invoked as the primary cause. Computerisation and automation have been put forward as causes of rising mean-median income inequality (e.g. Autor et al. 1998, Acemoglu and Restrepo 2017); and automation, falling prices of investment goods, and rapid labour-augmenting technological change have been put forward as causes of the fall in the labour share (e.g. Karabarbounis and Neiman 2014, Acemoglu and Restrepo 2016, Brynjolffson and McAfee 2014, Lawrence 2015).While we do not analyse these theories in detail, a simple empirical test can help distinguish the relative importance of these two categories of explanation – purely technology-based or not – for rising mean-median inequality and the falling labour share. More rapid technological progress should cause faster productivity growth – so, if some aspect of faster technological progress has caused inequality, we should see periods of faster productivity growth come alongside more rapid growth in inequality.We find very little evidence for this. Our regressions find no significant relationship between productivity growth and changes in mean-median inequality, and very little relationship between productivity growth and changes in the labour share. In addition, as Table 1 shows, the two periods of slower productivity growth (1973-1996 and 2003-2014) were associated with faster growth in inequality (an increasing mean/median ratio and a falling labour share).Taken together, this evidence casts doubt on the idea that more rapid technological progress alone has been the primary driver of rising inequality over recent decades, and tends to lend support to more institutional and structural explanations.Table 1 Average annual growth rates of productivity, the labour share and the mean/median ratio during the US’ productivity booms and productivity slowdownsNote: Data from BLS, Penn World Tables, EPI Data Library.Policy ImplicationsThe slow growth in median workers’ pay and the large and persistent rise in inequality are extremely concerning on grounds of both welfare and equity. There are important ongoing debates about the factors responsible for this phenomenon, and what must be done to reverse it.Our contribution to these debates is, we believe, to demonstrate that productivity growth still matters substantially for middle income Americans. If productivity accelerates for reasons relating to technology or to policy, the likely impact will be increased pay growth for the typical worker.We can use our estimates to calculate a rough counterfactual. If the ratio of the mean to median worker’s hourly compensation in 2016 had been the same as it was in 1973, and mean compensation remained at its 2016 level, the median worker’s pay would have been around 33% higher. If the ratio of labour productivity to mean compensation in 2016 had been the same as it was in 1973 (i.e. the labour share had not fallen), the average and median worker would both have had 4-8% more hourly compensation all else constant. Assuming our estimated relationship between compensation and productivity holds, if productivity growth had been as fast over 1973-2016 as it was over 1949-1973, median and mean compensation would have been around 41% higher in 2016, holding other factors constant.This suggests that the potential effect of raising productivity growth on the average American’s pay may be as great as the effect of policies to reverse trends in income inequality – and that a continued productivity slowdown should be a major concern for those hoping for increases in real compensation for middle income workers.This does not mean that policy should ignore questions of redistribution or labour market intervention – the evidence of the past four decades demonstrates that productivity growth alone is not necessarily enough to raise real incomes substantially, particularly in the face of strong downward pressures on pay. However it does mean that policy should not focus on these issues to the exclusion of productivity growth – strategies that focus both on productivity growth and on policies to promote inclusion are likely to have the greatest impact on the living standards of middle-income Americans.Post navigation23 commentsOddly missing from this article by Larry Summer and that student Anna Stansbury is mention of CEO pay increases in the same time period. There is a page at https://www.epi.org/publication/ceo-pay-continues-to-rise/ from the Economic Policy Institute which does go into the question in some depth. A helpful 30-second video clip on the same page points out that in 1965, top CEOs made 20 times their typical workers. By 2013, top CEOs made 296 times their typical worker. That and for all those executives plus all the perks and the like that go along with such jobs. Could this possibly explain where all those failed wage increases over the past 40 years went to? I’m sure that they are all totally worth it.That is absolutely true. The authors seem to ignore the inconveniences of reality. In general I find all those regressions worthless when there are so many factors involved and do not give indications on causal effects. For instance the fact that productivity growth correlates with salary growth. How do you interpret that result? The authors argue that policies should promote productivity growth since it is accompanied by salary growth and less inequality. Why not thinking the other way around? Policies that support salary growth migth cause productivity growth (should I have written “almost certainly”?) for reasons that are evident. For instance, when workers feel they get a fair share of wealth creation, they will be more firmly implicated and productivity will increase.So, with the very same results you can end recommending policy sets favouring productivity growth or favouring wage rises. Which of those would be more effective? My bet would be the second set.Thank you for pointing out the human element in this. From my perspective, having dealt with small businesses for the most part during my career, good old fashioned greed combined with rather powerless labor are the main drivers in this part of the economic spectrum.If “policies to promote inclusion” means ” middle – income Americans” need to form robust unions, I would agree: otherwise, I can’t imagine capitalist or the present government pressing for middle-income Americans inclusion.We do not all work in factories any more. There are vast areas of our economy – mainly health care, education, and government — where wages have gone up very nicely, thank you, and productivity has nothing to do with it.The nurse who cares for fewer patients, the professor who teaches fewer classes, the government worker retiring early — all have achieved higher average salaries.Salaries are indeed down in the area of tradable goods that are subject to foreign competition. Salaries are up in the hothouse industries.Even in the fabled 1950’s, salaries went up because unions were strong and could conduct strikes. Salaries went up even in factories that were notoriously inefficient.In what alternate universe are “nurses caring for fewer patients”? The one where they are all supervisory staff, and the contract temps they manage don’t count on the employment rolls? Likewise college professors. The fortunate tenured few are working less, but classes are still being taught in ever greater numbers – by adjuncts and upper level grad students.Artfully reclassifying workers so that their labor contribution can be disappeared, despite the more work for less pay that they are actually doing….. is part of the problem.People act as if the period after ~1980 when pay has been de-linked from productivity is some sort of aberration. I actually suspect that the period between 1945 and 1980 when they were linked was the aberration. I am NOT trying to say that we should just shrug our shoulders and say “Markets, what can you do?” But perhaps we should try to see what was special about the post war boom and emulate it rather than just try to figure out what broke in 1980.Free markets are a tool, not some angry God that we HAVE to sacrifice our prosperity and our democracy to. There is NOTHING WRONG with deciding that we don’t like some of the answers that free markets give us and that we will constrain them to get more acceptable ones.“Free markets”? What a fantasy!!!!!! You should know there is no such thing. The last thing any capitalist / and or system wants is “free markets”. This would force business to truly compete with each other which is the last thing they want to do. They much prefer to create “captive markets” or monopolies.“Free trade” is a religion. And like most religions, its adherents believe that it is holy writ that can’t be questioned. And like most religions, they ignore or subvert its teachings when they are not what they want. Forcing one’s employees to sign non-compete agreements never prevents CEOs from railing against uniions as a restraint of trade. Demanding non-competetive lease arrangements for drilling on public land does not prevent Oil companies from railing against government red tape.Everyone is wrong (7.5 billion people). All economists are McCarthyites. The history of macro was mapped out in 1961. Everything said became true. It is extraordinarily simple, yet too abstract for the common mind to understand. DFIs do not loan out existing deposits. DFIs always create new money when dealing with the private non-bank sector. Why is this important? Because all bank-held savings then become idle / frozen deposits. And the only way to activate voluntary savings is subsequently for their owners, saver-holders, to invest / spend directly or indirectly (e.g., via a non-bank conduit).So, when savings are transferred thru a non-bank conduit, there is an increase in the supply of loan-funds, but no increase in the supply of money (a money velocity relationship). And for the payment’s system, there is simply a transfer of ownership of existing deposit liabilities within the system (no funds are lost or withdrawn).So how did secular strangulation begin? In 1961 economists hypothesized that any increase in interest-bearing bank deposits would be countered by an offsetting increase in money velocity. And this was true up until the saturation in financial bank deposit innovation in 1981 (the widespread advent of ATS, NOW, and MMDA accounts). Thereafter it was hypothesized, by the same economist, that money velocity would persistently decelerate (had reached a permanently high plateau). The remuneration of IBDDs exacerbates this phenomenon. The expiration of the FDIC’s unlimited transaction deposit insurance produced the opposite flow (causing the “taper tantrum”).So as money velocity decelerated, aggregate demand, money X’s velocity, decelerated. So how did we get a deceleration in growth? Exactly as predicted in 1961, “the stoppage in the flow of monetary savings, which is an inexorable part of time-deposit banking, would tend to have a longer-term debilitating effect on demands, particularly the demands for CAPITAL GOODs”.The Quantity of Money is savings, a stock. Savings is defined as income not spent, and for the most part savings, the money we already have, is not the main driver of spending. Savings that is spent, Business Investment, is chasing new money creation, gains. Money creation drives spending. No one wants to play a game where the only money to be won was your own.Monetary economies are income-driven, not savings-driven.A savings-driven monetary economy would be the financial equivalent of perpetual motion.I think you’ve got it. Unless savings are expeditiously activated (perpetual motion of income redistribution, the circuit income and transactions velocity of funds), and put back to work, a dampening economic impact is exerted.All savings originate within the payment’s system. Saver-holders never transfer their savings outside the system unless they hoard currency or convert to other national currencies.The U.S. Golden Era in economics was where pooled savings were gov’t insured and put back to work, largely thru the S&Ls, MSBs, and CUs. Then the DIDMCA transformed these thrifts into commercial banks (which created the S&L crisis).I am old enough to remember strong labor unions. Even though at the time I wasn’t particularly interested, I remember labor struggles between the UAW and the major auto firms. Whenever a new “generous” contract was negotiated, there were always productivity strings attached. Auto execs often mentioned that wage increases need to be paid for with productivity increases. Whether there are chicken and egg issues here is not as relevant as the fact that these things were connected in the actor’s minds. It doesn’t necessarily take graphs and charts to know this.And labor unions were not crushed by accident. They were crushed as part of a conservative plan to Make America Great for Millionaires plan. The Dems, becoming Corporate Dems(tm) at the time, offered no resistance.An interesting article as usual, but with respect, continues to refuse to see the obvious.When there is a tight labor market, productivity gains translate into wages because employers have no choice but to bid for scarce labor. When there is a flooded labor market, and there are a dozen desperate workers competing for every job, then employers won’t share productivity gains with workers, because they don’t have to. Period.What happened in the early 1970’s is that the government’s cheap-labor immigration policy started to flood the market for labor. OK, there were other factors as well – the entry of women into the labor force also probably had an effect, but one is reminded that the latter is limited – once all the women are in the labor force that’s it, and the effect should stop. Not so immigration, which can keep up the pressure indefinitely.A pity that we can’t talk about this. Because refusing to address the issue of demographics in an intelligent way will ultimately bring down not just the US working class, but the world.I’m not sure that importing labor was more important than importing products built with foreign labor, except possibly in the farm and construction business. After all, the reason that the big three automakers could accept deals with the UAW for higher real wages and benefits was that the barriers to entry for the auto market are SO high. So they automakers didn’t compete against each other to pay lower wages to autoworkers. Instead an automaker would negotiate with the UAW and the other companies in the big-three would then negotiate deals that were similar enough to not create a competitive advantage or disadvantage versus the other manufacturers. This was quite cosy, and worked reasonably well until imports started becoming a sizeable portion of the domestic market.When I read headlines like “Technology is not to blame …” i’m always inclined to laugh (in a black humor kind of way) because its part of the endless search for simple solutions for complex social issues, and its part of an agenda to isolate and disconnect specific causes from the economics of our corporate environment (the economy) – thereby absolving human decision making of any blame; heck it was just technology after all, that mysterious, independent and neutral force. It’s like reading a comic book.The economy has been massively distorted for years by a] short term profit taking (i.e. just making money) b] the elimination of strategic industrial processes, and most importantly c] the overwhelming problem of military spending (both official and unofficial) which works on a cost-plus basis, produces essentially nothing of use to society and thereby drains us of the innovation, resources and the social reproduction we need.Not to belabor the point, but the best reference for all of this is Seymour Melman’s book “Profit without Production” from 1983, in which he analyzes the problems in such a manner that, except for his examples (which are old), one can use his analysis and conclusions word for word today – that’s how correct they were. By the way, his analysis traces the onset of this to the 1950’s.Stagnant wages have nothing to do with technology and its sad to see that PhD students are wasting their time on such tripe. Its part of the never ending search for technical solutions to what are essentially problems in political economy, social dynamics and human psychology.A better topic would have been real industrial and political analysis as pursued by Melman, Lazonick, Chandler and others.Let’s not forget the Fed. “Alan Greenspan saw the fight against rising prices as, at its essence, a project of promoting weakness and insecurity among workers; he famously claimed that “traumatized workers” were the reason strong growth with low inflation was possible in the 1990s, unlike in previous decades.Testifying before Congress in 1997, Greenspan attributed the “extraordinary’” and “exceptional” performance of the nineties economy to “a heightened sense of job insecurity” among workers “and, as a consequence, subdued wages.”Yes – the Federal Reserve has acted as hit-man for the neo-liberal program.Another aspect is the blind assumption by the authors that productivity has increased , IT is somehow related to that and that increased productivity should imply better wages. Also, there is no mention of the famous IT Productivity Paradox which has been discussed endlessly in the literature, Nor do they address the fact that practically all productivity gains (in terms of revenue) have been vacuumed up by the top 10%.So one of my questions to the author of this paper is, exactly what are they defining as productivity increase in the first place and what does that have to do with social welfare? I.e. state their position on some of the embedded assumptions.Whatever the measuring stick may be, its pretty clear that IT has not improved the lives of many people in the US, as witnessed by the poverty levels we are seeing today., and the question as its framed seems somewhat nonsensical to me. It’s not even clear that IT has anything to do with productivity or overall earnings levels.On a simplistic level: If productivity is measured in terms of revenue only (generally that’s true) irrespective of origin, i.e., asset appreciation, rent, tax credits for foreign subsidiaries and so on all count, then it should be clear that it will be difficult to find any correlations due to the extreme financialization of the economy and other dynamics.A more nuanced view includes the understanding that traditionally productivity was achieved through constant process and technology innovation, which included an increase in worker skills and allowed cost optimization, leading to higher pay and better products. Furthermore it was strategic marketplace behavior for competitive advantage and not just “money making”.Computing being such a generalized and non-specific function could easily fit into this paradigm, however it has to be tailored to specific industrial processes in order to have a real impact. That really hasn’t happened due to the use of commodity (therefore non-optimized) systems and a lack of foundational training as to what a computer actually is.It’s really not enough to just buy computers, networks, databases, build IT departments, etc. and expect any kind of productivity improvement (except for the computer vendors). The hard work of re-engineering processes, training people, structuring IT operations, and integrating computational functions into business process is hardly ever done properly. To compound this problem, business is caught in a never ending torrent of hardware and software upgrades, security issues, etc. which have nothing to do with their core business and are just overhead. Its hardly surprising that we don’t see productivity increases overall since the IT industry itself is somewhat cannibalistic in that regard.So I find the whole approach bankrupt it the first place – just rounding up the usual suspects, creating a smoking gun distraction, but not really trying to solve the crime. What remains is wage stagnation and poverty that has nothing to do with technology and everything to do with the neo-liberal program.",Technology Change Not the Culprit in Wages Falling Behind US Productivity Gains | naked capitalism
9578,3639493,2018-02-21 14:05:07,"AT&T names Atlanta, Dallas and Waco first of 12 US cities to get 5G wireless0AT&T wireless announced on Tuesday the first cities to get its 5G network. The carrier plans on installing 5G in twelve cities by the end of 2018 and on top of the list is Atlanta, Georgia, and Dallas and Waco, Texas. The remaining cities will be announced at a later date.Several carriers have been trialing 5G networks for sometime. AT&T says this rollout will be based on the 3GPP standard and will operate on the millimeter wave, or mmWave, spectrum. 5G wireless is said to offer theoretical peak speeds of several gigabits a second at lower latency than existing 4G wireless networks. The combination of faster speeds and lower latency is thought to help speed adoption of internet of things devices and utilities that require a persistent internet connection.“After significantly contributing to the first phase of 5G standards, conducting multi-city trials, and literally transforming our network for the future, we’re planning to be the first carrier to deliver standards-based mobile 5G – and do it much sooner than most people thought possible,” said Igal Elbaz, SVP of Wireless Network Architecture and Design at AT&T.The roll-out is ahead of availability of consumer 5G devices. It’s a chicken and egg problem. Both hardware makers and wireless carriers need to closely time launching 5G devices and networks so the return on investment is maximized. If one launches significantly early or late, the other will suffer. There’s a good chance major hardware makers will announced some of the first 5G devices next week at Mobile World Congress.","AT&T names Atlanta, Dallas and Waco first of 12 US cities to get 5G wireless"
9579,3639494,2018-02-21 12:00:21,"0Google is today unveiling a new ad unit for AdSense that taps into the company’s big push to add more artificial intelligence into its business, and to potentially bring on more publishers who might consider ramping up their advertising efforts but don’t have the time or other resources to manage them.Google is debuting “Auto Ads” — not commercials for cars, but a new ad unit that uses machine learning to “read” a page to detect and place what kinds of ads might be appropriate to place there, including where to place them, and how many to run. Publishers activate Auto Ads with a single line of code on the page.The service was actually quietly rolled out in a limited beta around April of 2017, and now it is live for everyone. Google tells us that “publishers participating in the beta saw an average revenue lift of 10 percent with revenue increases ranging from five to 15 percent.”For those who track or use AdSense, you know that there is already a fair degree of automation in the service. The product is used by tens of millions of web publishers to indicate where to place ads (banners and other units); with those ads then selected by Google based on a crawl of the page to figure out which ad might be most relevant. It already comprises a significant proportion of parent company Alphabet’s ad revenues, which accounted for $27 billion of its $32 billion of revenues in the most recent quarter.What’s new with Auto Ads is that Google is taking on task of selecting the placement — doing all of the work for publishers in terms of figuring out how many ads to put on specific pages, where to put them, as well as what kind of ads will run.Using machine learning is interesting here because it not only is being applied to figure out where an ad will go, but it is also being used to ingest analytics for how well that ad performs to “teach” the system how to place ads better in the future.One black hole (and potential pitfall) is the fact that Google’s Auto Ads seems to decide just how many ads it will place on a page — something you would have had more control over without it. This thread on Webmaster World details how some of the early beta testers were not pleased about how many ads ended up crowding their pages, and what that did to user experience on the site.We’re asking Google for a response to that point, and whether it will let users limit the number of units that Auto Ads can place on a page.It will also highlight questions about how well Google’s judgement will be in all cases.The AdSense service has come under the spotlight for letting lots of nefarious content seep into the mix, including ads carrying “fake news” and other misleading content. The company has been making efforts to combat this. Its “bad ads report” published in January 2017 noted that the company took down 1.7 billion dodgy ads and banned 200 publishers from AdSense.For now, the aim seems to be to roll this out and see how many sign on for the convenience of the service, which you activate by signing into your AdSense account; checking global settings from “My ads”; copying the code that is there and pasting it between the header tags for every page where you want the ads to appear (they come on in 10-20 minutes, Google says).In a blog post from AdSense engineering manager Tom Long and product manager Violetta Kalathaki, the two note that units included in the Auto Ad mix will include Anchor and Vignette ads, as well as Text and display, In-feed, and Matched content ads. (Not clear if newer formats like this larger banner will also be included.) They also write that publishers can specify which of these it wants to run.For those who have been using Page-level ads (specifying different kinds of ads depending on the subject of a page, rather than a whole site), their code will all automatically get migrated to run with Auto Ads. And for those who are using Google’s AMP service for mobile pages, you need to use code for AMP Auto ads.",Google debuts AdSense ‘auto ads’ with machine learning to make placement and monetization choices
9580,3639495,2018-02-21 11:00:36,"0Uber has launched Uber Express POOL officially after a lengthy trial period that kicked off in San Francisco last November, and has until now remained available only in that market. Starting today, it’s coming to DC, LA, Miami, Philadelphia, San Diego and Denver, and more cities will be added over the next few weeks and months across the U.S.The Express POOL launch brings a change to the current Uber POOL model that’s designed to make for more direct routing, with easier pickups for drivers and fewer annoying deviations from the route for riders thanks to two key actions Uber is asking riders to help out with: Walking and waiting. Basically, when riders hail an Uber Express POOL, they’ll be asked to wait a few minutes prior to the trip’s start, and/or walk to a nearby pick up spot, or from a nearby drop off point, in order to help optimize the route in as straight a line as possible along a path that can work for a number of different riders.The Express POOL option will live right alongside the standard, existing POOL option that’s there right now in the app, at least for the foreseeable future, and riders can have the choice. But ultimately, Uber thinks that many riders will prefer opting to walk a bit and wait a bit, since the goal is to ultimately save everyone involved time and frustration.Express pt.6Express Pt5Express pt4Express Pt3Express pt2Express Pt.1Some of the big challenges around making POOL work as designed to provide the lowest cost option of Uber’s various tiers to the most people possible have been around intelligent routing. The challenge of handling predictions of when and where people will be, along with building routes that not only work from an efficiency perspective, but also from the perspective of serving real humans in a way that doesn’t leave them frustrated or confused, proved to be a massive one.Uber’s intent with POOL is to help lower the cost of entry to its product to make it the massive base of the ride hailing pyramid that can reach the most people thanks to affordability near on par with public transit. While it accounts for around 20 percent of rides in markets where it’s available, based on a rough average, that’s still not obviously the majority, and so it’s hoping that tweaks to the product that provide a better overall experience will help increase its general appeal.","Uber officially launches Uber Express POOL, a new twist on shared rides"
9581,3639573,2018-02-21 16:13:45,"TNW SitesGot questions for a cryptography expert? Iddo Bentov is joining us on TNW AnswersHow did you first get into cryptography? Will Bitcoin end in centralization? What exactly is an Instantaneous Decentralized Poker?Ask all this and more to Cornell University’s resident cryptography expert, Iddo Bentov.Bentov is a Postdoctoral Associate in Computer Science at Cornell University. As an active contributor to the Bitcoin space since 2011, he has public several academic papers on bitcoin-related topics and cryptography. He also works on succinct zero-knowledge proofs at SCIPR Lab.This week, he’ll be speaking at Binary District’s Genesis London Conference, which will host panels on the most pressing issues in blockchain tech.",Got questions for a cryptography expert? Iddo Bentov is joining us on TNW Answers
9582,3639574,2018-02-20 12:08:24,"About TNWTNW SitesApple’s new HomePod is here…and we wanna give you one — FREE!Apple thoroughly enjoys changing the game. Whether it’s the iPod, the iPhone, the iPad or the brand new HomePod, Apple’s first smart speaker, acolytes are always a-buzz about Cupertino’s new tech…and the HomePod is definitely carrying on Apple’s tradition of product rollouts accompanied with gobs of water-cooler discussion.Of course, the best way to have the upper hand in any of the building HomePod talk is to actually have a HomePod of your own to talk about. Thankfully, we here at TNW just happen to have one of these sparkling new home hubs in our hands right now…and we want to GIVE it to you.And all you’ve got to do to take home one of these babies is enter the Apple HomePod Giveaway for your chance to be the proud owner of a new first generation Apple HomePod, a $349 value.Apple’s entry into the smart speaker market is already being hailed for its superior sound, a package that’s already whupping up on similar, more established speakers from Sonos, Amazon, and Google. Plus, it’s got all the Siri-powered A.I. goodness you’d expect from an Apple joint, including home assistant duties like answering questions, controlling smart-enabled devices, and more.Just head over to our contest page and sign up. Your name will be automatically entered into the drawing to be the randomly selected winner who will score this awesome prize.You can also max out your chances with more entries if you can rope a pal (21 or older, of course) into entering as well. Just follow the simple “Additional Entry” instructions on the contest page.",Apple’s new HomePod is here...and we wanna give you one — FREE!
9583,3641875,2018-02-21 17:00:00,"Shadow virtualizes a high-end gaming PC on your desktop clunkerIn the early days of computing, local storage and processing weren't actually a thing. Instead, your individual computer acted as a terminal, pulling data from a central processing server. Well, the French startup Blade likes it that way and has released a similar system but with a 21st-century twist. Its cloud-computing system, dubbed Shadow, can impart the performance of a $2,000 high-end gaming rig onto any internet-connected device with a screen. And now the company is bringing Shadow to California.The Shadow system has already found widespread adoption throughout France and most recently made its US debut at CES last month. The idea is relatively simple: instead of having to buy, maintain and upgrade your own hardware, you pay Blade a monthly subscription to use theirs. It's a concept similar to what NVIDIA did with its GeForce NOW cloud service, Parsec or HP's Omen PCs, save for the fact that those three are dedicated to gaming while Shadow enables users to run everything from Steam to Photoshop to a host of other business-related applications.The company has partnered with Microsoft, NVIDIA, AMD and Equinix to create a remote Windows 10 PC that you can access over the internet. At the remote server farm, each of these systems boasts a dedicated NVIDIA graphics card capable of handling 1080p at 144Hz or 4K at 60Hz. For processing, the system relies on eight dedicated threads on an Intel Xeon processor (the equivalent of an Intel Core i7) as well as offering 12GB of RAM and 256GB of storage.Therefore, it doesn't matter what hardware you're using to access the service. Shadow can run on Macs, Chromebooks, Windows PCs, Android, iOS, and a variety of smart TV platforms. However, this does lead to a paradox. Sure, Shadow can deliver 4K quality video streams over the internet, but if you're trying to watch it on an old 720p monitor, you're going to be watching that stream in 720p.Luckily, that doesn't seem to apply to the rest of the capabilities. Since the Shadow system is, in essence, a remote desktop, it doesn't matter how old, underpowered or decrepit the device is you're running it on, just how good the screen is. In fact, at CES, Blade managed to run the Shadow service on Razer's new phone, running full PC games (e.g., Battlefront II) on the device at 2K resolution and 120 Hz.What's also cool is that you're able to switch back and forth between operating systems on the fly. Say you're virtualizing the Shadow's Windows 10 desktop on your Mac. Since the Windows 10 OS is running remotely (only using the Mac's video driver to decode signal), it doesn't take up any of the Mac's other local resources. There is no slowdown in the macOS due to the Windows 10 desktop (and vice versa) and you can toggle between them instantly.Gallery: Blade Shadow | 20 PhotosAnd if you don't want to deal with your own hardware at all but still have a solid monitor, the company also offers a standalone Shadow device (think: a Roku-like box that streams computing capability instead of video), which can be hooked up to said monitor. Then all you need is a keyboard, a mouse, and a Steam account.And a baller internet connection. That's the other drawback of this system. In order to work, the Shadow needs a steady 15Mbps connection, preferably via ethernet. So if you're like me and are hamstrung by slow internet speeds in your apartment complex, you simply can't use this service. The company is working on expanding its capability to serve slower internet connections, but at this time does insist that you be running at 15 Mbps or higher. Unfortunately, the Shadow system does not yet support multi-monitor displays either, nor can it currently handle VR applications.Then there's the price. Shadow will cost you $35 a month with a year-long contract, $40 a month with a three-month commitment, or $50 to use it for a month with no strings attached. That's a pretty hefty fee for the ability to remotely rent someone else's computer.Still, if the idea of ditching your PC for the cloud sounds like a win to you, it's a service worth checking out. I was afforded early access to the service as part of my demo and used it to play a few rounds of Dragon Ball FighterZ on my MacBook Pro using a Bluetooth Xbox controller. I was blown away by both the visual quality (see gallery above) and the control's responsiveness. It's like I was playing it on my PS4 -- crisp clean graphics, with zero lag, jittering or stuttering.But big whoop, right? The 2018 MBP is a pretty beefy laptop anyway, what with its Retina display. So, I loaded the Shadow service on my older Nexus 6P and launched the game again. The results were the same (see below): crisp graphics, smooth animations, and zero lag. It honestly looked better than the last few episodes of Dragon Ball Super I've streamed from Crunchyroll.Shadow launches on February 21st in California and will expand throughout the continental US by summer of 2018 as the company completes construction on seven server farms localized throughout the nation.Andrew has lived in San Francisco since 1982 and has been writing clever things about technology since 2011. When not arguing the finer points of portable vaporizers and military defense systems with strangers on the internet, he enjoys tooling around his garden, knitting and binge watching anime.",Shadow virtualizes a high-end gaming PC on your desktop clunker
9584,3641877,2018-02-21 17:08:00,"Twitter says most recent follower purge is about bots, not politicsA number of Twitter users are claiming the platform is purging itself of conservative viewpoints as some lost thousands of followers last night. Richard Spencer, writer Mark Pantano and Candace Owens of Turning Point USA were among those spreading the #TwitterLockOut hashtag campaign and claiming that only conservative accounts were being targeted.Twitter is currently purging the followers on conservative accounts only. I just lost 3000 followers in one minute. Check out the trending hashtag to confirm that it is ONLY conservative accounts that are being affected.Holding for an explanation...#twitterlockoutHowever, others, including National March for Truth organizer Holly Figueroa O'Reilly and Republican political strategist Rick Wilson have said that the accounts being deleted were Russian bots and that conservative accounts weren't the only ones losing followers over the purge.#TwitterLockout demonstrates that @Jack has done more to push back against Russian digital interference than the US government run by Donald TrumpA Twitter spokesperson has now weighed in saying, ""Twitter's tools are apolitical, and we enforce our rules without political bias. As part of our ongoing work in safety, we identify suspicious account behaviors that indicate automated activity or violations of our policies around having multiple accounts, or abuse. We also take action on any accounts we find that violate our terms of service, including asking account owners to confirm a phone number so we can confirm a human is behind it. That's why some people may be experiencing suspensions or locks. This is part of our ongoing, comprehensive efforts to make Twitter safer and healthier for everyone."" The company also pointed to this page which details its enforcement actions.Twitter removes swaths of fake accounts from time to time and last night's deletions are nothing new. They're also very unlikely to be a targeting of Twitter's conservative base. Last month, Twitter appeared to delete thousands of accounts that followed celebrities and popular Twitter users following a New York Timesreport on Devumi and its selling of fake followers and artificial engagement. Some have speculated that yesterday's account removals were related to Robert Mueller's investigation of election meddling by Russian agents and the indictment of several Russian nationals allegedly involved in mass social mediacampaigns aimed at causing political strife in the US.#TwitterLockOut was still a top trending topic on Twitter this morning and while some legitimate accounts appear to have been temporarily locked last night, verifying a phone number -- a tactic used by many social networks to authenticate accounts -- was all that was needed to unlock them.","Twitter says most recent follower purge is about bots, not politics"
9585,3641879,2018-02-21 16:31:00,"Netflix's 'Lost in Space' reboot premieres April 13thNetflix's Lost in Space remake has been a long, long time in coming (word broke of it back in 2015), but it's finally here... almost. The streaming service has revealed that the sci-fi show will debut on April 13th, 2018, and has offered a teaser trailer to whet your appetite. Not surprisingly, this promises to be a thoroughly modern retelling of the Robinsons' wayward mission. It appears to take a more serious tone, and reflects much of what we've learned about space in the roughly 50 years since the original TV series. The family is out to colonize Alpha Centauri, for starters.There are still plenty of secrets, such as what happens in the wake of the inevitable disaster. And yes, you hear an iconic line from the series -- though not the machine speaking it. The only certainty is that they'll have to face unknown threats (both from the planet and fellow humans), and that it's not going to be a cakewalk.It's too soon to say whether or not the show will live up to expectations. It does have some pedigree behind it, though. Parker Posey plays a crucial role as the troublesome Dr. Smith, while actors like Max Jenkins (Will Robinson) and Molly Parker (Maureen Robinson) have years of experience from shows like Sense8, Deadwood and House of Cards. It's just a question of whether or not that talent coalesces into a solid whole.",Netflix's 'Lost in Space' reboot premieres April 13th
9586,3642133,2018-02-21 15:19:26,"Sling TV now has 2.2M subscribers, making it the largest internet-based live TV service0Dish-owned streaming TV service Sling TV now has 2.21 million subscribers at the end of the fourth quarter, the company announced this morning. This is the first time that Dish has broken out its Sling TV subscribers separately from its larger pay TV subscriber base, and confirms earlier estimates that Sling TV had grown to north of 2 million users.This also put Sling TV in the lead, ahead of rivals like AT&T’s one-year-old DirecTV Now, which announced 1 million subscribers back in December 2017, and has since grown to 1.2 million. It’s also leading Sony’s PlayStation Vue, as well, which reportedly had 455,000 subscribers in December, though some analysts peg it to now have 670,000 subscribers.Sling TV is ahead of newcomers YouTube TV and Hulu Live TV, too, which have 300,000+ and 450,000 paying users, respectively, according to a recent report from CNBC.However, Sling TV is the oldest live TV streaming service on the market, launching just ahead of PlayStation Vue in February 2015. That means it’s had more time to grow its user base as more people cut the cord with cable and satellite TV.In addition to revealing today’s subscriber count, Dish also detailed Sling TV’s growth in its 10-K filing, reporting it had 623,000 subscribers in its first year of service, which grew to 1.5 million by 2016, and 2.212 million by year-end 2017.Sling TV had originally benefited from being one of the first places you could stream ESPN over-the-top, and it marketed itself well to cord cutters who wanted to save money on their cable bill with its low-cost $20 per month base package. Over the years, it has expanded its service to include different tiers, Sling Orange and Sling Blue — the latter which supports multiple streams on three devices simultaneously. Last year, it also launched the AirTV Player, an Android TV-based device that comes with Sling TV.But these days, Sling TV is one of many options for online TV.Sling TV no longer only competes with pay TV, video-on-demand services like Netflix and other live TV rivals, like DirecTV Now. It faces competition from a host of online TV services, including those that are focused on niches like sports (e.g. fuboTV) or entertainment (e.g. Philo), as well as the over-the-top services from the networks themselves, like HBO NOW, Starz, Showtime and CBS All Access.These services are growing, too — and becoming larger than individual streaming TV services. HBO, for example, has more than 5 million online subscribers through apps like HBO NOW or subscriptions from other streaming services. CBS also just last week announced 5 million over-the-top subscribers, ahead of its plans for new OTT offerings, an ad-supported CBS Sports app and a celeb-focused Entertainment Tonight app. Meanwhile, Disney is prepping an ESPN streaming service and a family-friendly Netflix competitor.That will leave the streaming TV providers battling for consumers’ dollars as their audience begins to build out their own custom entertainment and sports bundles for their homes, which will likely be some mix of Netflix and other services.Overall, however, the switch from traditional pay TV to streaming TV is impacting Dish’s business. The company reported revenue in Q4 of $3.48 billion, down from the $3.75 billion it saw in Q4 2016. However, it posted better than expected net income of $1.39 billion for the fourth quarter 2017, compared to $355 million from the year-ago quarter — something it attributed to the new tax reform legislation.For the year, Dish reported 2017 total revenue of $14.39 billion, compared to $15.21 billion in 2016. Subscriber-related revenue in 2017 was $14.26 billion, compared to $15.03 billion in 2016.","Sling TV now has 2.2M subscribers, making it the largest internet-based live TV service"
9587,3642213,2018-02-21 17:57:03,"About TNWTNW SitesYouTubers turn car into the world’s biggest mouseA YouTuber turned a small car into a massive mouse, and now we want to use it to play all the games.The car is “Cheese Louise,” a canary-yellow Comuta-Car — an electric car from the 80s which looks like a sentient, upside-down garbage skip. Louise is owned by Simone Giertz, who you might remember as the person who built — and subsequently got tangled up in — a rubber-handed alarm clock of death.The person doing the mousifying is William Osman, who used an old optical mouse as the template. Using a new lens (to keep the mouse functioning while several inches off the ground), an Arduino Pro and a few other components, they were able to get the mouse to work while strapped to Louise’s bumper. To “click,” they had to honk the horn.Thanks to an incredibly spacious parking lot — which technically now counts as the world’s largest mousepad — the two managed to make a crude triangle in the same shade of yellow as their car.Sadly, they don’t use Louise to get a win in Fortnite (not that we see, anyway) — what’s the point of having such a large mouse if you can’t use it in a productive manner? Next week they’ll apparently turn poor Louise into a spaceship.",Tinkerer turns car into the world's biggest mouse
9588,3642214,2018-02-21 17:52:33,"TNW SitesWhy batteries are holding up tech breakthroughs — and what’s happening nowChances are, you’re reading this article on a device that depends on a battery to operate. It might be your smartphone, your laptop, your tablet, or some other device, but it requires electricity to function. We live in an era of explosive growth in technological innovation, yet so many of our new technologies still rely on the same methods of energy storage.Despite incremental advancements, batteries haven’t changed all that much over the past decade. Major brands like Duracell are investing in better environmental efficiency, and small tweaks that improve storage and performance, but the typical “battery” is still fundamentally the same.This stagnation in the realm of batteries is putting a cap on several technologies that might otherwise advance forward. So why is it that batteries are struggling to advance, what does it mean for other technologies, and where do we go from here?The big battery problemLet’s start by looking at the root of the battery problem. Nickel-cadmium batteries reigned throughout the 1990s, but when lithium-ion batteries emerged on the market, they quickly became the dominant model. The energy density of lithium-ion batteries more than double the standard nickel-cadmium equivalent, with potential for even higher energy densities. It behaves similarly, and can run on a single cell, rather than operating on a series. It’s low-maintenance and has a lower self-discharge.Though there are some drawbacks—including its fragility, and requirements for protection circuits—lithium-ion batteries are so efficient, some people have even claimed that it’s the “ideal” battery.In an era where Moore’s Law has dictated constant advancement, it seems strange to think there’s a fundamental limit for any kind of technological growth. But batteries function a little differently; because there are so many variables to consider, it seems that an increase to any single area comes with a decrease to another area.For example, you can have a battery with higher levels of power (watts), or higher levels of energy (watt-hours), but you can’t develop both in the same battery. Supercapacitors illustrate this point perfectly; they can release tons of power, but only for a few seconds at a time. Accordingly, many iterative advancements only move individual dimensions forward, rather than improving on the technology as a whole.This isn’t the only problem with new battery tech development, either. Here’s a breakdown of the challenges:Options. There are literally dozens of possibilities for development beyond lithium ion batteries, including hydrogen fuel cells, which produce clean energy and produce water as waste, and bioelectrochemical batteries, which use bacteria to accelerate chemical reactions. The problem is, with so many choices, it’s hard to tell which options have the true potential to become a long-term replacement, and which ones are just marginally exciting distractions. Accordingly, researchers are reluctant to put all their eggs in one basket.Costs and funding. Another problem is the sheer cost of developing next generation batteries. According to Lux Research, the average battery startup attracted just $40 million in funding over the course of 8 years. That may seem like a lot, but most technologies require hundreds of millions of dollars just to get a good start, and possibly billions for full development. Most investors aren’t willing to stake that money for a technology that has a small a chance of being successful.Scale. Another issue is scaling the technology from an experimental level to a fully functional, commercial level. Promising new battery technology usually looks good on a micro level; for example, you might note the storage potential of a new chemical that you experimented with in small quantities in laboratory conditions. But by the time you develop a larger version of the battery, something may be lost in translation; the chemical process may not unfold as efficiently, or the conditions (like temperature or long-term usage) may be unsuitable for commercial use. This has a tendency to scare away potential researchers and investors alike.Testing, reliability, and safety. Recent issues with improperly developed lithium-ion batteries catching fire illustrates how little we still understand about the full safety ramifications of the batteries we manufacture. Any new battery technology will need to be held to even higher standards than our current technologies, and that means adding years of research and testing to an already-long development cycle. Because of this, any new battery technology will automatically take several years, or even decades to fully develop, and even then, its safety for new technological applications won’t be guaranteed.So what effects is this battery stagnation having on other promising new technologies?Renewable energySolar power is clean and renewable, and it’s getting cheaper to produce every day. The problem is, the sun isn’t out 100 percent of the time; clouds and nighttime are problematic, unless you have an efficient way to store the excess energy generated during the day. Without efficient battery storage, solar energy will remain expensive and prohibitive for many applications, and it can never become a full replacement for our current energy needs.CellphonesSmartphones use a significant amount of power, and are recharged daily. Current batteries have a life expectancy of only a few years, since there’s a limit to how many full charges a lithium-ion battery can take. Without a new type of battery to prolong this life, phone development cycles will be faster, and processing power may make battery issues even worse (since more processing power leads to greater energy consumption).Electric carsElectric cars run far cleaner than their gas-guzzling counterparts, but even the best models need charged on a regular basis—which is annoying and cost-prohibitive for consumers. A better battery would be able to store a charge that enables the car to drive faster and longer (and with greater power), but for now, this aspect of electric cars is only seeing marginal improvements.The status quo—for nowFor the time being, most big-name battery producers, battery startups, and other tech organizations will continue focusing on iterative improvements to the standard lithium-ion battery model. It’s not the best solution long-term, but it’s one of the safest—and even though the improvements are marginal, we are seeing consistent improvements, year after year. Until enough investors start to stake riskier projects and invest in potentially game-changing breakthroughs, we’ll remain in this temporary valley of advancement.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Why batteries are holding up tech breakthroughs — and what's happening now
9589,3642215,2018-02-21 17:03:40,"This cloud computing service runs Windows on Mac and Overwatch on smartphonesFrench cloud computing startup Blade today announced its Shadow PC service is available in California, marking the company’s entry to the US market. With it, you can run Windows 10 on a Mac or play competitive shooters online using your smartphone and a game pad.The Shadow PC service gives users unlimited access to a high-end computer on the cloud. It’s a complete Windows 10 experience allowing subscribers to install any software they own, including Photoshop or Slack. Most importantly, however, it’s tuned to provide an optimal gaming experience.Other gaming services, such as Sony’s Playstation Now or Nvidia’s GeForce Now, provide a limited gaming experience, while most generic cloud computing solutions don’t even come close to providing a gaming-spec computer.Shadow PC gives users access to a comptuer equivalent to a $2000 desktop, this is from the press release:Perfect graphics: A dedicated high end NVIDIA graphics solution for a perfect image in 1080p at 144Hz or 4K at 60Hz.Incredible power: 8 dedicated threads on an Intel® Xeon® processor with a performance comparable to a high-end PC powered by an Intel® Core™ i7, 12GB of RAM and 256GB of storage spaceIf we can do something to prove this is working for the gamer market, it’ll show it can work for anyone. Gamers are very demanding, they’re very sensitive to latency.The team says the reception in France has been fantastic, and they’ve tested the Shadow PC service at some of the highest levels of gaming.This isn’t just a streaming service that lets you play offline games through a tunneled gaming experience. The Shadow PC service has been tested in Overwatch tournaments against local gaming rigs.They say its performance, even in online multiplayer games such as Overwatch, is indistinguishable in a blind test between their cloud rig and a computer sitting under a desk.According to the team, after trying its service with Overwatch, they faced criticism from people who said the game wasn’t a good example because its servers have special technology to reduce latency. The detractors told Blade it should try CS:GO for a real challenge. So they did, and they found the results to be the same.It’s worth mentioning at this point that we haven’t actually tried the service ourselves, so take everything here with a grain of salt before you throw away your old desktop.But, they gave us a demonstration that was incredibly cool.They began by loading up Rise of The Tomb Raider on the company’s Shadow Box hardware, which isn’t required, but is available for those with devices that can’t connect to the cloud (some gaming monitors, for example). We weren’t able to see the resolution of the screen in our video conference demo, but the team assured us it was running at top specs.There didn’t appear to be any latency and the demo seemed sufficient to show that Shadow PC works, but things were just getting started. We solicited a confirmation from the team that it wasn’t hiding a fiber cable or directly connected to any PC before moving on.Credit: BladeThe team paused the game and moved to a tablet. They launched the Shadow app, connected a bluetooth controller and picked up right where they’d left off. It was surreal watching a premium PC game running on a tablet without latency.And it worked the same on a smartphone connected to 4G, and a smart TV connected to WiFI. The demonstration even including running Windows 10 on a Mac just by opening the Shadow software, which made it possible to use both operating systems at the same time, basically side-by-side without using any additional computing resources on the host Mac.Currently, in the US, the service is only available in California but the team told us they plan to roll out nationwide by the end of this summer. That sounds incredibly ambitious but, from what we’ve seen, Blade is poised to deliver.The subscription costs about $35 a month with a one-year commitment, $40 with a three-month, and $50 without. Basically you’re getting a high-end gaming PC that’s upgraded at no additional charge, for about $420 with a one year commitment, or $600 with no commitment. Your mileage may vary when it comes to subscription services, but it’s worth pointing out that the developers have designed Shadow PC to completely replace your desktop. This isn’t just for gaming.But it may soon be the only service you need for all your PC gaming needs. The developers expect to have VR support for both Vive and Rift this year and they even hinted that Blade was developing a method for supporting both headsets wirelessly.If Shadow PC lives up to its promise, and the service works in the wild the same way it did in the demo, it’ll be a legitimate option for people who don’t want to spend $2000 or more on a high-end PC, but still want to play the games that require one.Shadow PC is available now in Europe and California, with more locations to roll out soon. You can check it out at the company’s website.",Blade's Shadow PC looks like a legitimate cloud-computer replacement for expensive gaming rigs
9590,3642216,2018-02-21 17:03:16,"About TNWTNW SitesCryptocurrency News 2/21/2018Editor’s note: The following column is written as intentional satire and may or may not have deliberate or undeliberate errors.What happened everyone. What happened?!Cryptos go to the toiletGod damnit we were all doing so well, and now Bitcoin is down to $10,500. How the hell am I meant to pay back the 40% APR loan I took out to buy all of them? This is ridiculous! You’ll hear from my senator Mr. Bob Fudge (Democrat).Japan breaks BitcoinZaif (pronounced “Wife”), a Japanese exchange of cryptocurrency, has admitted a “system glitch” allowed people to buy trillions of dollars worth of Bitcoin for their real value – zero dollars. That’s the entire joke everyone!¡Venezuela!Everyone was asking me yesterday about the Petro cryptocurrency that stable and normal nation Venezuela is going to be distributing to stabilize the Bolivar, their other currency that isn’t doing well at the moment. It’ll be backed by “Venezuelan crude reserves,” sort of how Tether is backed by another kind of oil, if you catch my drift, and people are claiming it’s just another way to raise money without having to actually abide by things like laws, also sort of like Tether. I’m getting at something here, read between the lines. Anyway, the pre-sale is only in cash, which is bullshit and I personally will no longer be doing business with President Maduro until he accepts Doge.Nathaniel Popper, New York Times writer and author of “I Married A Bitcoin And I’m Pregnant With Its Child,” dug up an interesting point – that basically the government doesn’t know what they’re doing.The Petro whitepaper on the government website says that it will be built on the Ethereum blockchain, while the Petro buyers manual says that it will be built on the NEM blockchain. Seems like this project is well organized!Knowledge has announced a partnership with Coinomi, and Coinomi CEO George Kimionis will join the knowledge Strategic Advisory Board. The future of Knowledge continues to look bright and I strongly recommend participation in their ICO. https://t.co/0ZBiCDFrKhpic.twitter.com/ydFujRLzUvRemember that movie The Last Starfighter? I’m that but for Bitcoin. In my universe where I am from we just have US dollars and other such “fiat” currencies but I was brought here to stabilize and bring in a new era of digital currency called “cryptocurrency.” I am assumed dead on my homeworld.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Cryptocurrency News 2/21/2018
9591,3642217,2018-02-21 16:51:36,"About TNWTNW SitesOnly the rich can get richer off cryptocurrenciesI’m guilty for making fun of people betting obscene amounts of money on cryptocurrency because I know I’m safe. I’ve personally only put in money that I can afford to lose. When I see someone put $25,000 on Bitcoin, I laugh, but I also feel bad. When I see a forum of people potentially losing hundreds of thousands of dollars, I’ve posted it and everyone’s had a good chuckle.But deep down there’s something really sad, really painful happening. It’s a global phenomena where people are able to (very, very quickly) be part of a financial movement that in the past has rewarded people with triple-digit percent returns, one that doesn’t require you to understand a complex stock market, but simply see that a coin is worth $X and predict — somehow, and it seemed likely at one point it would — become $Y, a higher number.If anything cryptocurrency has truly become the libertarian nightmare — you can truly take over a new cryptocurrency if you have expensive mining hardware, or have the funds to buy in early (or, indeed, get free coins from forking).It’s not even 24 hours into Litecoin Cash mining, and already several huge pools exist to specifically not “allow legacy SHA256 hardware” to mine a brand new currency — they want to conquer it, to mine it.To quote a few random people in Discord, “the difficulty went from 275263.6924763158 to 891531.7069405196, in 10 minutes, wtf,” and Tanner, one of the core Litecoin Cash founders, said “The hashpower is worryingly concentrated right now.”Even getting Litecoin Cash from the fork was predominantly rewarding those with deep pockets (i.e. those who could afford a lot of Litecoin could claim 10 Litecoin Cash for every LTC they had) or with mining hardware. Just so we’re clear, a “cheap” SHA256 (used to mine Bitcoin and Bitcoin Cash) unit is still thousands of dollars and sounds like a miniature jet engine, costs tons of money in power and still costs thousands.The argument against this is that inherently “anyone can get in on any new coin” — that you could theoretically look at GruntCoin and invest $500 in it and it could become a bazillion dollars. Questionably responsible articles like this one from New York Magazine suggest that someone can simply “walk into” this industry and make $80,000 overnight — masking the fact that this person says the following:Verge [an altcoin] is just 1 of 100 trades I’ve made since opening my day-trading account with three bitcoins…So even when Bitcoin was at its cheapest in a long time, this guy still had $12,000 to exchange, and all of the faux-anxiety laced in this piece falls flat when you realize that he started with what is an incredible amount for most people. Worse still, he talks about a time that one random coin investment went to $80,000. This is a thing called “luck” not “investment strategy.”Cryptocurrency is gambling masked as investment for 99 percent of people. The argument for ease of access is the same as a casino, except slightly safer and much slower. Take $100, put it on a number and hope that number hits. Yes, you can pull your funds and there are few 3’s or 7’s that can truly roll to wipe out your investment, but if you bought at the wrong time, guess what? It’s probably destroyed your investment.If you have the cash, you can theoretically pick up and hold currencies in the hopes they go up, except there’s the important detail that you’re probably not gonna get rich off of it anymore without being rich in the first place. You’re not getting rich off of Ethereum or Bitcoin unless you have the capital to buy an absolute shit-ton, or able to buy enough mining rigs to fight off the increase in difficulty that years of other people mining it has created.You’re not gonna get rich off an ICO, as they’re becoming increasingly gated and requiring six-figure investments. Public sales of valuable coins barely happen anymore. You can’t read the tea-leaves, you can’t see the future. Though a year ago I suppose you could have made a lot of money, you still needed to put up at least $1000 — which isn’t a reasonable amount for most people. Furthermore, most people would have (fairly) classified it as a stupid investment.We’ve enjoyed rags to riches stories since the junk bond trading frenzy happened in the ’80s — we love the idea that anyone can walk in and just get that paper. The horrible truth is that while there may be more access to the supply of cryptocurrencies, there most definitely isn’t the income equality that would mean that “anyone” can get rich.Quite the opposite.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Cryptocurrency has become a nightmare of capitalism
9592,3645155,2018-02-21 16:45:30,"Snapchat responds to the Change.org petition complaining about the app’s redesign0Snapchat has posted an official response to users who signed a petition on Change.org asking the company to reverse its controversial update, which people say makes the app more difficult to use. In the response, Snapchat promises to make a few more changes to the Friends and Discover section in order to address user complaints.The backlash against Snapchat has been growing in the months since the company rolled out a major revamp, which aimed to make the social app more accessible to a mainstream audience. Snapchat users have left the app bad reviews, complainedon social media, turned to rival Instagram, and they signed a Change.org petition entitled, “Remove the new Snapchat update.”Users are upset over a number of things in the new design, including the mixing of Stories in a single “Friends” page, increased difficulties with finding friends and rewatching Stories, and a revamped Discover section which combines content from professional creators, big news outlets, video makers, and social media stars.Celebrities, like Chrissy Teigen, and YouTuber Marques Brownlee (MKBHD), have also weighed in.I’m seeing this same comment so often. I liked that you guys felt like we were friends. I’m sad it doesn’t feel like that anymore. How many people have to hate an update for it to be reconsidered? https://t.co/PI7OAf9QlgThe Change.org petition didn’t delve into the specifics regarding the changes Snapchat users hate, but says the update is “annoying,” and has made “many features more difficult.” It asks Snap, Inc. to “change the app back to the basics.”“It’ll take time for people to adjust, but for me using it for a couple months I feel way more attached to the service,” said Spiegel.The user backlash is reminiscent of the one Facebook had faced years ago, when users rebelled over the addition of News Feed which radically changed the Facebook experience. News Feed was ultimately a success; whether Snapchat can pull through is still unknown.Snap’s earnings, however, pointed to the redesign’s potential to positively impact the company’s numbers. Publisher Stories on Discover grew 40 percent compared to the old design, and users older than 35 were engaging with the app more, the company said, when posting its first earnings beat.But many users, right now, are not happy. As of February 13, 2018, the Change.org petition had grown to over 800,000 signatures. Today, it stands at 1,223,722, as of the time of writing.Last night, Snapchat posted an official response to the petition, reiterating its stance but also promising a few tweaks that may help to address users’ concerns.Specifically, the company said that “beginning soon on iOS and with Android in the coming weeks” it will introduce tabs in the Friends section and in Discover, which it says will make it easier for users to find the Stories they want. This update will let users sort things like Stories, Group Chats, and Subscriptions. (You can read more about this here.)Whether these tabs will placate users who just want the old Snapchat back remains to be seen.Snapchat’s full response is below:FEB 20, 2018 — To Nic and all of the Snapchatters who signed this petition,We hear you, and appreciate that you took the time to let us know how you feel. We completely understand the new Snapchat has felt uncomfortable for many.By putting everything from your friends in one place, our goal was to make it easier to connect with the people you care about most. The new Friends page will adapt to you and get smarter over time, reflecting who you’re most likely to be Snapping with at that moment. This same personalization is also true of the new Discover, which will adapt to you the more that you use it.Beginning soon on iOS, and with Android in the coming weeks, we are introducing tabs in Friends and Discover, which will make it easier to find the Stories that you want, when you want them. Once you receive the update, you’ll be able to sort things like Stories, Group Chats, and Subscriptions, allowing you to further customize your own experience on the app.This new foundation is just the beginning, and we will always listen closely to find new ways to make the service better for everyone. We are grateful for your enthusiasm and creativity. We are very excited for what’s ahead.",Snapchat responds to the Change.org petition complaining about the app’s redesign
9593,3645235,2018-02-21 18:30:11,"TNW SitesFacebook Messenger now lets you add friends during calls without stopping the partyFacebook Messenger is making it easier to invite friends to your calls. Starting today, you can add friends onto an ongoing session.Previously you had to close your call, create a new group, and then initiate a call again. Now you can just tap on the screen while on a call and select the ‘add person’ button.It’s small change, but one that’ll make the calling experience more seamless. Say you want to surprise a friend for their birthday and discuss details with someone else beforehand, or perhaps start a work call while you wait for someone else to be ready – that process is a bit more seamless now.The new feature is rolling out to Android and iOS today. No word on a desktop implementation, but I imagine it’s a matter of time.",Facebook Messenger now lets you add friends during calls without stopping the party
9594,3647546,2018-02-21 19:13:00,"YouTube's trending section shows it has a fake news problem, tooIn discussions of the ongoing battle over fake news, we often hear Facebook and Google criticized for playing a hand in spreading false information. And while both have made efforts recently to stem the spread of misinformation on their platforms, they've each had some major missteps.Hogg has been one of the many students from Marjory Stoneman Douglas High School who have chosen to voice their opinions and concerns following the massacre that took place on their campus last week. And that spotlight has attracted hoards of trolls who have made claims ranging from Hogg being coached by his ex-FBI father to him being a paid ""crisis actor."" The latter was the claim being made in the comments of the YouTube trending video. The video itself was a CBS News clip from last year that reported on an altercation that took place between a friend of Hogg's and a Redondo beach lifeguard. Hogg was included in the news report.Hogg, who was visiting Los Angeles during that time, published his own video about the event, but while the the CBS clip offers no evidence that Hogg is in fact a paid actor rather than a high school student, trolls have pushed it as evidence of such nonetheless. As Snopes writes, ""Conspiracy theorists and trolls alike heavily implied that a months-old video (despite the facts that he readily identified himself with the same name, that people occasionally travel across the United States, and that his family moved to Florida from Los Angeles) is somehow 'proof' that he is a trained 'crisis actor,' a baseless rumor that is inevitably pushed after horrific mass shootings.""YouTube has removed the video and a company spokesperson told us, ""This video should never have appeared in Trending. Because the video contained footage from an authoritative news source, our system misclassified it. As soon as we became aware of the video, we removed it from Trending and from YouTube for violating our policies. We are working to improve our systems moving forward.""when i go on facebook and click on the trending topic ""david hogg,"" and then on videos, it brings up a search for ""david hogg actor,"" which at the top includes videos from a facebook-verified users spouting conspiracy nonsense pic.twitter.com/TJBxnBxtyjBut what's next for YouTube? Though it deleted the offending video, there are plenty more out there still claiming David Hogg is an actor -- which is also currently a problem on Facebook as well. YouTube's Trending section is curated by algorithms, not people, and as today's news failure shows, that is clearly a problem. Google has become increasingly aware that it may need more actual humans on board moderating its content and training its algorithms and it announced last year that it would hire 10,000 people to review content across its platforms. Similarly, YouTube said earlier this year that it would start manually vetting content included in its Preferred ad program following the Logan Paul scandal.But that's not enough. YouTube is a major platform with millions of viewers and when fake news makes its way into Trending, even if only for a short while, it can do irreparable damage. You can't search for David Hogg anywhere without seeing conspiracy theories making wild, unfounded claims. Today's event is just the latest issue to highlight YouTube's content problems, and it's going to have to do more than cut ad money and vet a handful of videos if it wants to not be a source of fake news and misinformation. It says it's working to improve its systems, but what those efforts may be aren't clear. Some transparency and details on what it's going to do in the future to prevent egregious blunders like today's would go a long way.","YouTube's trending section shows it has a fake news problem, too"
9595,3647638,2018-02-21 19:00:05,"I've Just Launched ""Pwned Passwords"" V2 With Half a Billion Passwords for Download22 February 2018Last August, I launched a little feature within Have I Been Pwned (HIBP) I called Pwned Passwords. This was a list of 320 million passwords from a range of different data breaches which organisations could use to better protect their own systems. How? NIST explains:When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.They then go on to recommend that passwords ""obtained from previous breach corpuses"" should be disallowed and that the service should ""advise the subscriber that they need to select a different secret"". This makes a lot of sense when you think about it: if someone is signing up to a service with a password that has previously appeared in a data breach, either it's the same person reusing their passwords (bad) or two different people who through mere coincidence, have chosen exactly the same password. In reality, this means they probably both have dogs with the same name or some other personal attribute they're naming their passwords after (also bad).Now all of this was great advice from NIST, but they stopped short of providing the one thing organisations really need to make all this work: the passwords themselves. That's why I created Pwned Passwords - because there was a gap that needed filling - and let's face it, I do have access to rather a lot of them courtesy of running HIBP. So 6 months ago I launched the service and today, I'm pleased to launch version 2 with more passwords, more features and something I'm particularly excited about - more privacy. Here's what it's all about:There's Now 501,636,842 Pwned PasswordsBack at the V1 launch, I explained how the original data set was comprised of sources such as the Anti Public and Exploit.in combo lists as well as ""a variety of other data sources"". In V2, I've expanded that to include a bunch of data sources along with 2 major ones:The 711 million record Onliner Spambot dump. This was a lot of work to parse varying data formats and if you read the comments on that blog post, you'll get a sense of how much people wanted this (and why it was problematic).The 1.4B clear text credentials from the ""dark web"". This data resulted in many totally overblown news stories (and contributed to my ""dark web"" FUD blog post last week), but it did serve as a useful reference for V2. This data also had a bunch of integrity problems which meant the actual number was somewhat less. For example, the exact same username and password pairs appearing with different delimiters:There's also a heap of other separate sources there where passwords were available in plain text. As with V1, I'm not going to name them here, suffice to say it's a broad collection from many more breaches than I used in the original version. It's taken a heap of effort to parse through these but it's helped build that list up to beyond the half billion mark which is a significant amount of data. From a defensive standpoint, this is good - more data means more ability to block risky passwords.But I haven't just added data, I've also removed some. Let me explain why and to begin with, let's do a quick recap on the rationale for hashing them.They're Still SHA-1 Hashed, But with Some Junk RemovedWhen I launched V1, I explained why I SHA-1 hashed them:It doesn't matter that SHA1 is a fast algorithm unsuitable for storing your customers' passwords with because that's not what we're doing here, it's simply about ensuring the source passwords are not immediately visible.That's still 100% true as of today. There are certainly those that don't agree with this approach; they claim that either the data is easily discoverable enough online anyway or conversely, that SHA-1 is an insufficiently robust algorithm for password storage. They're right, too - on both points - but that's not what this is about. The entire point is to ensure that any personal info in the source data is obfuscated such that it requires a concerted effort to remove the protection, but that the data is still usable for its intended purposes. SHA-1 has done that in V1 and I'm still confident enough in the model to use the same approach in V2.One of the things that did surprise me a little in V1 was the effort some folks went to in order to crack the passwords. I was surprised primarily because the vast majority of those passwords were already available in the clear via the 2 combo lists I mentioned earlier anyway, so why bother? Just download the (easily discoverable) lists! The penny that later dropped was that it presented a challenge - and people like challenges!One upside from people cracking the passwords for fun was that CynoSure Prime managed to identify a bunch of junk. Due to the integrity of the source data being a bit patchy in places, there were entries such as the following.Of course, it's possible people actually used these strings as passwords but applying a bit of Occam's Razor suggests that it's simply parsing issues upstream of this data set. In total, CynoSure Prime identified 3,472,226 junk records which I've removed in V2. (Incidentally, these are the same guys that found the shortcomings in Ashley Madison's password storage approach back in 2015 - they do quality work!)Frankly though, there's little point in removing a few million junk strings. It reduced the overall data size of V2 by 0.69% and other than the tiny fraction of extra bytes added to the set, it makes no practical difference to how the data is used. On that point and in terms of extraneous records, I want to be really clear about the following:This list is not perfect - it's not meant to be perfect - and there will be some junk due to input data quality and some missing passwords because they weren't in the source data sets. It's simply meant to be a list of strings that pose an elevated risk if used for passwords and for that purpose, it's enormously effective.Whilst the total number of records included in V2 is significant, it also doesn't tell the whole story and indeed the feedback from V1 was that the 320M passwords needed something more: an indicator of just how bad each one really was.Each Password Now Has a Count Next to ItIs the password ""abc123"" worse than ""acl567""? Most password strength meters would consider them equivalent because mathematically, they are. But as I've said before, password strength indicators help people make ill-informed choices and this is a perfect example of that. They're both terrible passwords - don't get me wrong - but a predictable keyboard pattern makes the former much worse and that's now reflected in the Pwned Passwords data.Now on the one hand, you could argue that once a password has appeared breached even just once, it's unfit for future use. It'll go into password dictionaries, be tested against the username it was next to and forever more be a weak choice regardless of where it appears in the future. However, I got a lot of feedback from V1 along the lines of ""simply blocking 320M passwords is a usability nightmare"". Blocking half a billion, even more so.In V2, every single password has a count next to it. What this means is that next to ""abc123"" you'll see 2,670,319 - that's how many times it appeared in my data sources. Obviously with a number that high, it appeared many times over in the same sources because many people chose the same password. The password ""acl567"", on the other hand, only appeared once. Having visibility to the prevalence means, for example, you might outright block every password that's appeared 100 times or more and force the user to choose another one (there are 1,858,690 of those in the data set), strongly recommend they choose a different password where it's appeared between 20 and 99 times (there's a further 9,985,150 of those), and merely flag the record if it's in the source data less than 20 times. Of course, the password ""acl567"" may well be deemed too weak by the requirements of the site even without Pwned Passwords so this is by no means the only test a site should apply.In total, there were 3,033,858,815 occurrences of those 501,636,842 unique passwords. In other words, on average, each password appeared 6 times across various data breaches. In some cases, the same password appeared many times in the one incident - often thousands of times - because that's how many people chose the same damn password!Now, having said all that, in the lead-up to the launch of V2 I've had people argue vehemently that they all should be blocked or that none of them should be blocked or any combination in between. That's not up to me, that's up to whoever uses this data, my job is simply to give people enough information to be able to make informed decisions. My own subjective view on this is that ""it depends""; different risk levels, different audiences and different mitigating controls should all factor into this decision.I Haven't Included Password LengthOne request that came up a few times was to include a length attribute on each password hash. This way, those using the data could exclude passwords from the original data set that fall beneath their minimum password length requirements. The thinking there being that it would reduce the data size they're searching through thus realising some performance (and possibly financial) gains. But there are many reasons why this ultimately didn't make sense:The first is that from the perspective of protecting the source data (remember, it contains PII in places), explicitly specifying the length greatly reduces the effort required to crack the passwords. Yes, I know I said earlier that the hashing approach wasn't meant to be highly resilient, but providing a length would be significantly detrimental to the protection that SHA-1 does provide.Then, I actually got a bit scientific about it and looked at what minimum length password websites required. In fact, that's why I wrote the piece on minimum length by the world's top sites a couple of weeks back; I wanted to put hard numbers on it. 11 of the 15 sites I referred to had a minimum length of 6 chars or less. When I then went and looked at the data set I was using, excluding passwords of less than 6 chars would have only reduced the set by less than 1% Excluding anything under 8 chars would have reduced it by just under 16%. They're very small numbers.Then there's the overhead required to host and search this data, that is the overhead those organisations who use it will incur. It should be very close to nothing with the whole half billion data set. Chuck it in a storage construct like Azure Table Storage and you're looking at single digit dollars per month with single digit millisecond lookup times. There's no need for this to be any more complex than that.So in short, it put the protection of the hashing at greater risk, there was very little value gained and it's easy to implement this in a way that's fast and cheap anyway. Some people will disagree, but a lot of thought went into this and I'm confident that the conclusion was the right one.The underlying storage construct for this data is Azure Blob storage. If I was to serve the file directly from there, I'd cop a very hefty data bill. Cloudflare came to rescue in V1 and gave me a free plan that enabled a file of that size to be cached as their edge nodes. The impact of that on my bill was massive:Imagine the discussion I'd be having with my wife if it wasn't for Cloudflare's support! And that was before another 6 months' worth of downloads too. Cloudflare might have given me the service for free, but they still have to pay for bandwidth so I'd like to ask for your support in pulling the data down via torrents rather than from the direct download link. To that effect, the UI actively encourages you to grab the torrent:If you can't grab the torrent (and I'm conscious there are, for example, corporate environments where torrents are blocked), then download it direct but do your bit to help me out by supporting the folks supporting me where you can. As with V1, the torrent file is served directly from HIBP's Blob Storage and you'll find a SHA-1 hash of the Pwned Passwords file next to it so you can check integrity if you're so inclined.So that's the download - go forth and do good things with it! Now for something else cool and that's the online search.Querying the Data OnlineIn V1, I stood up an online search feature where you could plug in a password and see if it appeared in the data set. That sat on top of an API which I also made available for independent consumption should people wish to use it. And many people did use it. In fact, some of the entrants to my competition to win a Lenovo laptop leveraged that particular endpoint including the winner of the competition, 16,year-old Félix Giffard. He created PasswordSecurity.info which directly consumes the Pwned Passwords API via the client side:Getting back to the online search, being conscious of not wanting to send the wrong message to people, immediately before the search box I put a very clear, very bold message: ""Do not send any password you actively use to a third-party service - even this one!""But people don't always read these things. The service got a heap of press and millions of people descended on the site to check their passwords. At least I assume it was their passwords, I certainly don't log those searches but based on the news articles and social media commentary, yeah, it would have been a heap of real passwords. And I'm actually ok with that - let me explain:As much as I don't want to encourage people to plug their real password(s) into random third-party sites, I can guarantee that a sizable number of people got a positive hit and then changed their security hygiene as a result. One of the biggest things that's resonated with me in running HIBP is how much impact it's had on changing user behaviour. Seeing either your email address or your password pwned has a way of making people reconsider some of their security decisions.The online search works almost identically to V1 albeit with the count of the password now represented too:Pretty simple stuff and for the most part, also pretty familiar. But there's one really important - and really cool - difference. Let me explain:Cloudflare, Privacy and k-AnonymityIn what proved to be very fortuitous timing, Junade Ali from Cloudflare reached out to me last month with an idea. They wanted to build a tool to search through Pwned Passwords V1 but to do so in a way that allowed external parties to use it and maintain anonymity. You see, the problem with my existing implementation was that whilst you could pass just a SHA-1 hash of the password, if it returned a hit and I was to take that and reverse it back to the clear (which I could easily do because I created the hashes in the first place!) I'd know the password. That made the service hard to justify sending real passwords to.Junade's idea was different though; he proposed using a mathematical property called k-anonymity and within the scope of Pwned Passwords, it works like this: imagine if you wanted to check whether the password ""P@ssw0rd"" exists in the data set. (Incidentally, the hackers have worked out people do stuff like this. I know, it sucks. They're onto us.) The SHA-1 hash of that string is ""21BD12DC183F740EE76F27B78EB39C8AD972A757"" so what we're going to do is take just the first 5 characters, in this case that means ""21BD1"". That gets sent to the Pwned Passwords API and it responds with 475 hash suffixes (that is everything after ""21BD1"") and a count of how many times the original password has been seen. For example:(21BD1) 0018A45C4D1DEF81644B54AB7F969B88D65:1 (password ""lauragpe"")(21BD1) 00D4F6E8FA6EECAD2A3AA415EEC418D38EC:2 (password ""alexguo029"")(21BD1) 011053FD0102E94D6AE2F8B83D76FAF94F6:1 (password ""BDnd9102"")(21BD1) 012A7CA357541F0AC487871FEEC1891C49C:2 (password ""melobie"")(21BD1) 0136E006E24E7D152139815FB0FC6A50B15:2 (password ""quvekyny"")...I added the prefix in brackets beforehand and the source passwords in brackets afterwards simply to illustrate what we're doing here; they're all just different strings that hash down to values with the same first 5 characters. In other words, they're all within the same ""range"" and you'll see that term referenced more later on. Using this model, someone searching the data set just gets back the hash suffixes and counts (everything in bold after the first 5 chars) and they can then see if everything after the first 5 chars of their hash matches any of the returned strings. Now keep in mind that as far as I'm concerned, the partial hash I was sent could be any one of 475 different possible values. Or it could be something totally different, I simply don't know and therein lies the anonymity value.For the sake of perspective, here are some stats on what this means for the data within Pwned Passwords:Every hash prefix from 00000 to FFFFF is populated with data (16^5 combinations)The average number of hashes returned is 478The smallest is 381 (hash prefixes ""E0812"" and ""E613D"")The largest is 584 (hash prefixes ""00000"" and ""4A4E8"")Junade has written a great piece that's just gone live on Cloudflare's blog titled Validating Leaked Passwords with k-Anonymity and he goes into more depth in that piece. As he explains, there are other cryptographic approaches which could also address the desire for anonymity (for example, private set intersections), but not with the ease and level of simplicity Junade proposed. I loved it so much that I offered to build and run it as a service out of HIBP. Junade (and Cloudflare) thought that was a great idea so they offered to point folks over to the HIBP version rather than build out something totally separate. That's a partnership I'm enormously happy with I appreciate their confidence in my running it.This model of anonymity is what now sits behind the online search feature. You can see it in action by trying a search for ""P@ssw0rd"" which will return the screen in the previous image. If we drop down and take a look at the dev tools, here's the actual request that's been made:The password has been hashed client side and just the first 5 characters passed to the API (I'll talk more about the mechanics of that shortly). Here's what then comes back in the response:As mentioned earlier, there are 475 hashes beginning with ""21BD1"", but only 1 which matches the remainder of the hash for ""P@ssw0rd"" and that record indicates that the password has previously been seen 47,205 times. And that's it - that's what I've done with Cloudflare's support and that's what we've done together to protect anonymity and make the service available to everyone. Let me now talk about how you can use the API.Consuming the API (and the Mechanics Behind the Range Search)Firstly, you'll notice that I'm serving this API from a different domain to the other HIBP APIs and indeed from V1 of the Pwned Passwords service. For V2, I've stood up an Azure Function on the api.pwnedpasswords.com domain which gets the API out of the HIBP website and running on serverless infrastructure instead. I've written about Azure Functions in the past and they're an awesome way of building a highly scalable, resilient ""code as a service"" architecture. It ensures that load comes off the HIBP website and that I can scale the Pwned Passwords service infinitely, albeit with a line directly to my wallet! It's also given me the flexibility to do things like trim off a bunch of excessive headers such as the content security policy HIBP uses (that's of no use to a lone API endpoint).Secondly, the existing API (that many people have created dependencies on!) still works just fine. It also points to the storage repository for V2 of the password set so it's now searching through the full half billion records. I'll leave this running for the foreseeable future, but if you are using it then I'd prefer you roll over to the endpoint on api.pwnedpasswords.com for the reasons mentioned above, and for these other reasons:If you were using the original API via HTTP GET, rolling over to the new one changes absolutely nothing in your implementation other than the URL which will look like this:GET https://api.pwnedpasswords.com/pwnedpassword/{password}It'll still return HTTP 200 when a password is found and 404 when it's not. The only difference (and this shouldn't break any existing usages), is that the 200 response now also contains a count in the body by way of a single integer:And as before, you can always pass a hash if preferred:But, of course, we've just had the anonymity chat and you would have seen the path for calling that endpoint earlier on. Just to point it out again here, you can pass the first 5 chars of the hash to this address:https://api.pwnedpasswords.com/range/{hashPrefix}Which returns a result like this:Remember, these are all hash suffixes (followed by a count) so the full value of the first hash, for example, is ""21BD10018A45C4D1DEF81644B54AB7F969B88D65"". Incidentally, input to the API is not case sensitive so ""21bd1"" works just as well as ""21BD1"". All hash suffixes returned (and indeed those provided in the downloadable data) are uppercase simply because that's the default output from SQL Server's HASHBYTES function (I processed the source data in a local RDBMS instance).Unlike the original version, there's no rate-limiting. That was a construct I needed primarily to protect personal data in the breached account search (i.e. when you search for your email address amongst data breaches), but I extended it to Pwned Passwords as well to help protect the infrastructure. Now running on serverless Azure Functions, I don't have that concern so I've dropped it altogether. I'd also dropped version numbers, I'll deal with that when I need them which may not be for a long time (if ever).Now, a few more things around some design decisions I've made: I'm very wary of the potential impact on my wallet of running the service this way. It's one thing to stand up V1 that only returned an HTTP response code, was rate-limited and really wasn't designed to be called in bulk by a single consumer (considering the privacy implications), it's quite another to do what I've done with V2, especially when each search of the range API returns hundreds of records. That ""P@ssw0rd"" search, for example, returns 9,730 bytes when gzipped (that's a pretty average size) and I'm paying for egress bandwidth out of Azure, the execution of the function and the call to the underlying storage. Tiny amounts each time, mind you, but I've had to reduce that impact on me as far as possible through a range of measures.For example, the result of that range query is not a neatly formatted piece of JSON, it's just colon delimited rows. That impacts my ability to add attributes at a later date and pretty much locks in the current version to today's behaviour, but it saves on the response size. Yes, I know some curly braces and quotes wouldn't add a lot of size, but every byte counts when volumes get large.You'll also notice there's a long max-age on the cache-control header:This is 31 days' worth of cache and the subsequent Cloudflare cache status header explains why: by routing through their infrastructure, they can aggressively cache these results which ensures not only is the response lightning fast (remember, they presently have 121 edge nodes around the world so there's one near you), but that I don't wear the financial hit of people hammering my origin. Especially when you consider the extent to which multiple people use the same password, when we're talking about the range search where many different passwords have identical hash prefixes, there's some significant benefits to be had from caching. As mentioned earlier, there are 16^5 different hash prefixes (1,048,576) within the range of 00000 to FFFFF so you can see how extensive usage would benefit greatly from caching across many millions of searches. The performance difference alone when comparing a cached result with a non-cached one makes a compelling argument:This means that even though the response is significantly larger than in V1, if I can serve a request to the new API from cache there's actually a massive improvement. Here's a series of hits to V1 where every single time, the request had to go all the way to the origin server, hit the API and then query 320M records:In order to make aggressive caching feasible, I'm also only supporting HTTP GET. Now, some people will lose their minds over this because they'll say ""that means it goes into logs and you'll track the passwords being searched for"". If you're worried about me tracking anything, don't use the service. That's not intended to be a flippant statement, rather a simple acknowledgment that you need to trust the operator of the service if you're going to be sending passwords in any shape or form. Offsetting that is the whole k-Anonymity situation; even if you don't trust the service or you think logs may be leaked and abused (and incidentally, nothing is explicitly logged, they're transient system logs at most), the range search goes a very long way to protecting the source. If you still don't trust it, then just download the hashes and host them yourself. No really, that's the whole point of making them available and in all honesty, if it was me building on top of these hashes then I'd definitely be querying my own repository of them.In summary, if you're using the range search then you get protection of the source password well in excess of what I was able to do in V1 plus it's massively faster if anyone else has done a search for any password that hashes down to the same first 5 characters of SHA-1. Plus, it helps me out an awful lot in terms of keeping the costs down!Pwned Passwords in ActionLastly, I want to call out a number of examples of the first generation of Pwned Passwords in action. My hope is that they inspire others to build on top of this data set and ultimately, make a positive difference to web security for everyone.This is All Still Free (and I Still Like Beer!)Nothing gains traction like free things! Keeping HIBP free to search your address (or your entire domain) was the best thing I ever did in terms of making it stick. A few months after I launched the service, I stood up a donations page where you could buy me some beers (or coffee or other things). It only went up after people specifically asked for it (""hey awesome service, can I get you a coffee?"") and I've been really happy with the responses to it. As I say on the page, it's more the time commitment that really costs me (I'm independent so while I'm building something like Pwned Passwords, I'm not doing something else), but there are also costs that may surprise you:Just burned through $100 of mobile data so that I could finish processing Pwned Passwords this weekend. 110kbps on unlimited broadband plan or 8,286kbps on 4G at $10/GB. It was going to be hard to get it live next week otherwise 🙁This is one of those true ""Australianisms"" courtesy of the fact my up-speed maxes out at about 1.5Mbps (and is then shared across all the things in my house that send data out). Down-speed is about 114 but getting anything up is a nightmare. (And for Aussie friends, no, there's no NBN available in my area of the Gold Coast yet, but apparently it's not far off.) And no, this is not a solvable problem by doing everything in the cloud and there are many reasons why that wouldn't have worked (I'll blog them at a later date).ClosingPwned Passwords V2 is now live! Everything you need to use them is over on the Pwned Passwords page of HIBP where you can check them online, learn about the API or just download the whole lot. All those models are free, unrestricted and don't even require attribution if you don't want to provide it, just take what's there and go do good things with it 😀","I've Just Launched ""Pwned Passwords"" V2 With Half a Billion Passwords for Download"
9596,3647644,2018-02-21 19:14:00,"He Took a Picture of a Supernova While Setting Up His New CameraAstronomers using the Swope telescope captured Supernova 2016gkg, between the two red lines, in the galaxy NGC 613, which is 80 million light-years from here.Credit C. Kilpatrick/UC Santa Cruz and Carnegie Institution for Science, Las Campanas Observatory, ChileBoom.A star is dead.On Sept. 20, 2016, Victor Buso, an amateur astronomer in Rosario, Argentina, was checking out the new camera on his telescope by taking pictures of a nearby spiral galaxy when a star within it went off in a supernova explosion.Within hours, and prompted by Mr. Buso’s good fortune, professional astronomers around the world trained their big telescopes on the galaxy, known as NGC 613, about 80 million light-years from here in the constellation Sculptor. It was a rare instance in which astronomers were able to see the beginning of a supernova, when one of the most massive stars in the universe ends its life in one of the most violent events nature can cook up.A series of images from the moment an amateur astronomer caught a supernova on his camera in 2016. The explosion, shown in the red circle, rapidly brightens south of the host galaxy, NGC 613. Víctor Buso and Gastón FolatelliMost supernovas are far away and don’t call attention to themselves until their funeral pyre explosions are well underway. In this case, astronomers were able to record what they call the “breakout,” when a shock wave radiating from a star’s core, which has probably collapsed into a black hole, reaches the surface of the poor star and brightens it catastrophically.“It’s like winning the cosmic lottery,” said Alex Filippenko, in a news release from the Keck Observatory in Hawaii, where Dr. Filippenko, of the University of California, Berkeley, has been tracking the supernova.The astronomers, who reported their findings on Wednesday in Nature, said the original star had probably been about 20 times as massive as the sun, but had blown most of that mass off into space before the decisive explosion began.",He Took a Picture of a Supernova While Setting Up His New Camera
9597,3647883,2018-02-21 20:06:41,"About TNWTNW SitesRead the official FCC net neutrality repeal document and weepThe US government’s official journal, the Federal Register, today revealed the FCC’s net neutrality repeal paperwork in an “unpublished” version ahead of the unveiling of the official version set for 22 February.The document is titled “Restoring Internet Freedom.” This name, we can only guess, is meant to continue former Verizon lawyer Ajit Pai’s trend of lying to the American people.In the document you’ll find such gems as “We find the Title II classification likely has resulted, and will result, in considerable social cost, in terms of foregone investment and innovation.” That would also make a fantastic argument for removing regulations keeping construction businesses from using asbestos in schools.If you’re not in the mood to read the entire 284 page document, a fair summary would be as follows: c’mon guys, let’s just trust Verizon, Comcast, and AT&T to do the right thing.The paper contains the word “throttling” 34 times, most of which are in the context of defending Comcast for throttling Bittorrent or declaring that ISPs will have to disclose throttling to customers.Disclosure isn’t the problem here. For a fantastic explanation of throttling and how the net neutrality repeal will likely effect us, have a look at Rob Bliss’s traffic demonstration in front of the FCC building.Net neutrality is dead. And if it’s not, then it’s strapped into an electric chair minutes away from being executed. Unless you believe President Trump is going to pardon it at the last second, it’s time to move on.",Read the official FCC net neutrality repeal document and weep
9598,3650571,2018-02-21 21:15:00,"Spotify’s hardware ambitions seem like a risky distractionLook, it's no secret that Spotify is out to make its own hardware. As of last April, Spotify was already looking for people to help craft ""a category defining product akin to Pebble Watch, Amazon Echo and Snap Spectacles."" (In hindsight, Spotify's HR team probably should've left that last thing off the list.) More recently, a new set of job listings for hardware production managers and operations manager suggest Spotify is finally gearing up to build... well, whatever these things are. Consider us skeptical. After all, this is a company with zero hardware and supply chain experience — the odds of striking it big with gadgets don't seem great.We can't be sure about what Spotify is actually trying to build in Stockholm, but its search for employees are in line with what you'd expect from a company trying to make a smart speaker. Last April, Spotify was looking for people with expertise in voice recognition and natural language processing. And more recently Spotify sought out someone with ""graduate-level expertise"" in natural language understanding to join its team in Boston and ""multiple years of industrial experience in building conversational agents via speech or text (e.g., chatbots)."" This focus on a spoken interface could apply to future, voice-controlled versions of the Spotify app, but it's not much of a stretch to think Spotify could be trying to build a Google Home or Sonos rival.Let's say for the sake of argument that's what's going on. The initial cost of research, development and production will be significant, but it could be worth it. If that effort yields a firm foundation for Spotify to build on, revenue for hardware like smart speakers could help boost Spotify's bottom line in the long run. I do mean long run: hardware projects are notoriously hard, and ambitious attempts like Apple's HomePod are proof that even with loads of money and brainpower, first attempts at a new kind of product often feel unfinished. It could be years before the unprofitable Spotify starts to hit its stride in an industry its unfamiliar with, and that's assuming the company has the guts to stick it out that long.The move into the smart speaker market is also peculiar because, HomePod aside, you can access Spotify's extensive library of songs and playlists on Amazon's Echos or Google's Homes or Sonos' everythings. Spotify is like the WhatsApp of streaming music services — you can basically use it everywhere. The company's stated desire to build a ""category defining product"" could alienate its Spotify Connect partners, but I'm honestly not too worried about that. I am, however, concerned that the head-starts enjoyed by potential competitors mean it'll never fully catch up.Given the potential headaches of building a completely new kind of business inside an existing one, I'd honestly rather see Spotify devote that money and resources into making its service even better. Right now, Spotify is arguably the best at figuring out what you might enjoy hearing based on things you've chosen to listen to in the past. That nuanced ability to predict your preferences through raw listening data forms the core of my love for Spotify, and I'm not the only one who feels that way. Connecting those musical dots in new ways and forging more lucrative — or at least, less odious — deals with big music labels might be enough to guarantee continued growth for the world's largest streaming company. I can't imagine the Spotify's inevitable shareholders to turn their noses up at that.It's far too early to tell whether Spotify's hardware plans will be an absolute boondoggle. We (obviously) love gadgets around here, and I honestly hope Spotify proves me wrong — as a subscriber, I have something of a vested interest in the company doing cool, impactful things. Cautious optimism is called for here, so Spotify, show us what you've got.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",Spotify’s hardware ambitions seem like a risky distraction
9599,3650831,2018-02-21 17:52:06,"Apple could be buying cobalt from mining companies directly0Cobalt is the new oil. Car companies and battery manufacturers are all rushing to secure multiyear contracts with mining companies for their lithium-ion batteries. According to a Bloomberg report, Apple is also participating in this game as the company wants to secure its long-term supplies.The company has never done this before with cobalt. Apple relies on a ton of suppliers for all the components in its devices — including for batteries. And yet, cobalt prices have tripled over the past 18 months. Chances are Apple will secure a contract much more easily than a battery supplier.While an Apple Watch battery is an order of magnitude smaller than a car battery, Apple sells hundreds of millions of devices every year. All those iPhone and Mac batteries represent quite a bit of cobalt.But the issue is that car manufacturers are putting a ton of pressure on cobalt suppliers. BMW and Volkswagen are also looking at signing multiyear contracts to secure their supply chains. And other car manufacturers are probably also paying attention to cobalt prices.As a side effect, buying cobalt straight from the mines makes it easier to control the supply chain. It’s hard to know where your cobalt is coming from when you buy batteries from third-party suppliers. And in that case, it can be a big issue.Amnesty International published a report in January 2016 about cobalt mines, saying that tech companies and car manufacturers aren’t doing enough to prevent child labor in the Democratic Republic of the Congo — the country is responsible for 50 percent of global cobalt production.A couple of months ago, Amnesty International published an update, saying that Apple is more transparent than others. The iPhone maker now publishes a list of its cobalt suppliers. But there’s still a long way to go in order to make sure that mining companies respect basic human rights.But let’s be honest. In today’s case, Apple mostly wants to be able to buy enough cobalt at a fair price for its upcoming gadgets. And the company has deep enough pockets to sign this kind of deal.",Apple could be buying cobalt from mining companies directly
9600,3650911,2018-02-21 21:58:05,"Facebook, Google, and Microsoft gurus say AI will help workers, not replace them“In general, AI will not replace jobs, but it will transform them. Ultimately, every job is going to be made more efficient by AI,” says Facebook’s Yann LeCun.Reports of the demise of human labor may be hyperbolic, if you believe the likes of LeCun, Google’s Peter Norvig, and Microsoft’s Eric Horvitz.The trio, each heads of research for the tech giants who employ them, participated in an “ask me anything” (AMA) session on Reddit earlier this week to discuss artificial intelligence. All three are members of the American Association for the Advancement of Science (AAAS), and as such weren’t representing their respective companies during the session.When asked by a high school teacher which jobs were most likely to be replaced by AI and “which seemed safe for the next generation,” each responded with optimism.Microsoft’s Horvitz said:I see many tasks as being supported rather than replaced by more sophisticated automation. These include work in the realms of artistry, scientific exploration, jobs where fine physical manipulation is important, and in the myriad jobs where we will always rely on people to work with and to care for other people–including teaching, mentoring, medicine, social work, and nurturing kids into adulthood. On the latter, I hope to see rise and support of an even more celebrated “caring economy” in a world of increasing automation.This vision of a “caring economy” conjures images of a world where companies are empowered to invest in teachers and daycare by paying automation’s savings forward. But perhaps there’s more to a “caring economy” than just better pay for the people who take care of the elderly and children.With automation assisting workers we can pass off dangerous tasks to machines, which will result in fewer human injuries, deaths, and lawsuits. The only compassionate way to get rid of worker’s compensation claims is to make them unnecessary.According to these experts we’re going to see a lot of change over a long period of time, but few entire career fields are at stake. Sometimes jobs are eliminated because they’re unnecessary in the modern world – like switchboard operator. But for the most part, people adapt – typists learned to use word processors.The fear with automation is that AI is creating a new “species” of worker that will be in direct competition with us. But that’s not neccesarily the case, as Norvig points out:I think it makes more sense to think about tasks, not careers. If an aspiring commercial pilot asked for advise [sic] in 1975, good advice would be: Do you enjoy taking off and landing? You can do that for many years to come. Do you enjoy long hours of steady flight? Sorry, that task is going to be almost completely automated away. So I think most fields are safe, but the mix of tasks you do in any job will change, the relative pay of different careers will change, and the number of people needed for each job will change.It’s always nice to see some optimism in the world of AI. But if you’d prefer, here’s an update from the world of autonomous warfare and killer robots.",AI won't take our jobs. It's here to help.
9601,3650912,2018-02-21 20:54:40,"About TNWTNW SitesTwitter users cry foul when asked to verify their identitiesSome Twitter users woke up to an unpleasant surprise this morning: Upon attempting to access their Twitter accounts, they were told they were temporarily locked out of their accounts.Shortly after this, the hashtag #TwitterLockout began trending, as users began to complain of being censored or persecuted by the site.That wasn’t all of it. Upon logging in, they discovered they had lost a number of followers. Some users have speculated that the loss of follower numbers is part of a “bot purge.”Trumpers try to understand; the followers you are losing tonight are not real people. They were bots and @Twitter decided enough was enough. You aren’t being censored. You are being awakened to the FACT that you were engaging with hostile foreign agents! #TwitterLockOutpic.twitter.com/zK7RSlm8RgThe crux of the complaint appears to be that many of the affected users were politically conservative, meaning it appeared that the site was censoring or otherwise going after a certain group in particular.Folks, many of the followers you may have lost in the overnight ""purge"" may actually just be from Twitter locking some accounts until they confirm their identities. As these people confirm, your lost followers return. We'll see. I've already gained back 1200 this morning.Twitter, for its part, insists that the request is not politically motivated. According to a Twitter spokesperson:Twitter’s tools are apolitical, and we enforce our rules without political bias. As part of our ongoing work in safety, we identify suspicious account behaviors that indicate automated activity or violations of our policies around having multiple accounts, or abuse. We also take action on any accounts we find that violate our terms of service, including asking account owners to confirm a phone number so we can confirm a human is behind it. That’s why some people may be experiencing suspensions or locks. This is part of our ongoing, comprehensive efforts to make Twitter safer and healthier for everyone.Judging by their own phrasing, this is part of a larger campaign by Twitter to eliminate automated accounts, which have been a massive problem for the platform in recent months.",Twitter users cry Lockout when the site asks them to verify their identities
9602,3653315,2018-02-20 17:49:39,"Please log inorRegister now for freeorChoose your profile *Email *A valid e-mail address. All e-mails from the system will be sent to this address. The e-mail address is not made public and will only be used if you wish to receive a new password or wish to receive certain news or notifications by e-mail.PasswordUsername *NewslettersHigher education updates from the THE editorial teamWorld University Rankings newsIf you do not wish to be contacted about offers, products or services from THE/TES Global then please check this boxThe world’s largest academic publisher grew its profits to more than £900 million last year, although it warned that a move towards openly available research could hurt its business.Elsevier, currently locked in a dispute with German research institutions that want it to move away from a paywalled subscription model towards pay-to-publish open access, had its financial results released last week as part of those of its parent company, RELX.They show that the Amsterdam-based publisher made a profit of £913 million, up £60 million from 2016. The company’s profit margin in 2017 was 36.8 per cent, unchanged from the year before. RELX as a whole – of which Elsevier accounts for a third of revenue – paid out £762 million in dividends to shareholders.The publisher has long been criticised by some campaigners for its profit margin, and there have been numerous attempts by academics to it shift towards an open-access rather than a subscription-based publishing model.In a section disclosing the “principal risks” to Elsevier’s business, RELX notes in the results that its journals are “sold largely on a paid subscription basis”.“There is continued debate in government, academic and library communities, which are the principal customers for our STM [scientific, technical and medical, the name for Elsevier within RELX] content, regarding to what extent such content should be funded instead through fees charged to authors or authors’ funders and/or made freely available in some form after a period following publication,” it says.“Some of these methods, if widely adopted, could adversely affect our revenue from paid subscriptions,” the results add.The beginning of the year brought no resolution to the stand-off between Germany and Elsevier, which has dragged on since 2016. German research institutions are now thought to be saving about €10 million (£8.8 million) a year after failing to agree a new deal with the publisher, which has nonetheless decided to maintain access to its content.There is no direct mention of the Germany dispute in Elsevier’s latest results, although since the beginning of the year, RELX’s share price has tumbled by about 15 per cent. It is unclear to what extent this is due to the stand-off, but some analysts have warned that it raises questions over the publisher’s future growth.",Elsevier’s profits swell to more than £900 million
9603,3653558,2018-02-21 23:54:08,"TNW SitesHologroup’s MR Guide makes Microsoft’s HoloLens your new tour guideRussian startup Hologroup recently released MR Guide for HoloLens developers. The software gives anyone the ability to make a holographic tour for Microsoft’s mixed-reality headset, with no programming necessary.Currently the HoloLens hardware is available to the public only in a costly developer’s edition, but it’s already seeing useful software. MR Guide gives early adopters the tools to create experiences directly in the HoloLens’ native interface, without the need for any additional software.According to Holograph CEO Alex Yakubov:With the help of MR Guide, creating a holographic tour is no more difficult than making a PowerPoint presentation. Now, museums, showrooms, exhibition stands and every other innovative space can start mixing their reality with any interactive digital content, to help focusing the attention of their visitors and guests. Previously, projects likes this would have required 3 to 6 months of active work of a lot of expensive developers and contractors, now everyone can do it themselves, without any additional help.Creators upload files using Microsoft’s One Drive and design the text and triggers for their virtual tours by interacting with the software via the HoloLens headset.Right now there’s virtually no market for software that’s designed exclusively for HoloLens. If you want to buy one just to tinker with, the developer’s edition costs $3,000 – it’s not an end-user product at this point. But Hologroup’s software bridges the gap between developers and creators. It has the potential to create some HoloLens customers.It’s easy to imagine businesses, museums, and even boutique stores offering customers the option to borrow a HoloLens for the upgraded holographic tour experience.Mixed-reality is slowly finding its groove and the next generation of hardware is on the way. Hopefully more software like MR Guide will find its way to market soon.",Hologroup's MR Guide makes Microsoft's HoloLens your new tour guide
9604,3653559,2018-02-21 23:47:30,"About TNWTNW SitesBowers & Wilkins PX Review: Beautiful sound – after I got a haircutBowers and Wilkins’ PX are among the best Bluetooth headphones I’ve tried in recent memory. It’s just weird it took a trip to the barber to get the best sound out of them.The PX are a $400 pair of noise cancelling headphones that launched to rave reviews for their classy looks and pristine sound. Unfortunately, my experience out of the box was not quite the same. The sound seemed to lack body, especially when noise cancelling was off, and more importantly, sound was dramatically better with it on.And then I got a haircut. It turns out my somewhat poofy hair was enough to weaken the seal between the earcups and my head, in a way I’d not seen with any of the dozens of other headphones I’ve tested through the years.I’m well aware of the impact a proper seal has on sound quality, but the PX were a peculiar case. I rarely havethatmuch trouble finding a seal with over-ear headphones, and they did sounddecent with noise cancelling on before my haircut, just not $400 decent. For some reason, when noise cancelling is on, the weak seal is somewhat compensated for.Otherwise the sound was too bright, bass was very soft, and voices sounded like they were coming from a box – a complaint I noticed was often echoed on the audiophile forum Head-fi.Just as I was ready to contact B&W to see if my unit was defective somehow, I happened to get a haircut. I was listening to music on the way to my barber, so imagine my surprise when I emerged with my fresh new ‘do, restarted the track I was playing, and found dramatically improved sound.I know this all sounds kind of silly or hyperbolic, but the PX really have most sensitive seal of any over-ear headphones I’ve used. This was confirmed by a couple more haircut cycles, so it’s something to be aware of if your noggin could also lead to a poor seal (I’ve seen very similar sound complaints online from users with glasses, for instance). It’s compounded by the fact the PX have rather thin earpads, which are stiff out of the box. I’d like to see more plush pads that could better lock in sound in a future revision.Still, with that issue resolved, the PX are among the very best portable headphones I’ve tried. I haven’t reviewed its two biggest competitors, the Bose QC35 and Sony MDR-1000X, but I have auditioned both of them extensively. The PX, to my ear, presents more detail than either, particularly during busy tracks. But more impressive is its soundstage, which feels surprisingly deep thanks to a great sense dynamics.They don’t have the most energetic sound – I prefer more aggressive bass and treble on commuter headphones – but it’s a well rounded sound signature I can imagine few people disliking.Noise cancelling is solid too. It’s not up to Bose and Sony’s standards, but it does work well for low frequencies. It’s just not very useful for silencing chatter in particular. The two highest settings also color sound slightly – I recommend the lowest one unless you’re in a particularly noisy environment.Battery life is excellent, rated at 22 hours with Bluetooth and noise cancelling on and 29 hours with it off. Better yet, they charge via USB-C, so you don’t have to carry around a separate cable (seriously, stop using micro USB, headphone makers). Unfortunately, you can’t use them in wired mode without a charge, but it can go up to 50 hours in that scenario.There are some odd design choices. While you can turn noise cancelling on and off with a button, you can’t change its strength without using the app, which I find supremely annoying. The headset also has a feature that plays and pauses music automatically when you put them on and take them off. While useful, it was too prone to accidental triggering at the default setting. Thankfully you can change the sensitivity or turn it off altogether in the app.If you can ignore these caveats and arent concerned about the problematic seal – a design flaw I hope BBW can resolve – B&W PX are easy recommendations. They sound wonderful, look gorgeous, and have some genuinely useful features. Just make sure you have a proper seal before you judge their sound – and maybe get a haircut.",Bowers & Wilkins PX Review: Beautiful sound - after I got a haircut
9605,3656110,2018-02-21 20:35:38,"Blockchain is entering the valley of despair phase, and that’s a mistake0I’ve had a number of conversations with blockchain-focused people over the past few weeks. In some of those conversations — particularly with engineers — there is a palpable excitement and a belief that this technology is going to end up being their life’s work. As one blockchain hacker told me this past week, “It’s going to be a multi-decade long battle against the technologies that have been weaponized against us,” referring to the technology of the attentional economy.On the other hand, I’ve had significantly more morose conversations with investors who are starting to lose faith in blockchain as a medium for investment returns. ICOs are still happening, of course, and investors are still clamoring to gain access to the best ones. But there is a remarkable cooling of the excitement thermometer, particularly from investors who were investing for financial returns rather than faith in blockchain as a decentralizing force.I see the past near-universal excitement around blockchain diverging along two arcs. The tinkerers and hackers exploring this technology have never been more enthusiastic, yet some of the financiers and investors exploiting the technology for profit are starting to lose interest and enter the valley of despair. But that’s a mistake because it follows the wrong arc forward.The excitement around blockchain over the past few years has been intensified by the convergence of these two groups working in parallel. For a disruptive technology, blockchain managed to capture the imagination of investors shockingly early compared to its antecedents.Take the internet, for instance. The early protocols and packet-switching technology was invented in the 1960s, and email roughly as we know it was developed throughout the 1970s. Yet, it wasn’t until 1995 that commercial websites were even authorized to run on the internet. Three decades passed before entrepreneurs and investors were able to invest in companies built on top of this technology.Compare that to Bitcoin, which kicked off the modern blockchain movement. The original Satoshi white paper on Bitcoin was released in late 2008. The first run-up in prices happened in mid-2011, then again in late 2013, and then the most recent peak in early December of 2017. So the technology got about three years before it was the focus of intense investment speculation, which really hasn’t abated since.We can see the difference in the arcs between the hackers and the financiers. The hackers of the internet played with this new technology, expanded its boundaries and functionality, and created new products on top of it for decades, mostly with no sign that any of their work would pay off monetarily. Today, the legacy of that work lies in the open-source community, which continues to push this technology forward.The financiers mostly ignored the software internet, instead funding the hardware infrastructure companies that produced packet-switching devices and other networking equipment. They didn’t show up until what would become the dot-com bubble of the 1990s.That sequential pattern of technology development was parallelize with blockchain. The hackers continue to tinker and expand the functionality of the technology. They are still exploring the possibilities here. The financiers, though, invested far earlier in the technology maturation process, and are suddenly finding out that blockchain is very, very early. No wonder some of them are running away at the first sign that the bubble wasn’t what they thought it might be.So we begin to enter the valley of despair, when the technology loses its charm among a certain type of investor who will complain that they lost their shirt in the market and that the technology is “dead” or whatever other metaphor they will use. You can already sense this in some of the media coverage of the technology, and I fear it is only going to get more intense.However, it would be an enormous mistake to focus on the investor arc and not center our attention on the hacker arc. Investors are not the story about blockchain, the engineers are. Engineers are going to keep playing with this technology, and they are the ones who will eventually discover the “killer apps” that ultimately drive the value here. That might happen this year, or it might happen a decade from now. Or longer.The potential of a disruptive technology like blockchain will take decades to fully understand. You don’t have to be a True Believer to participate in that progress, but you do need insatiable curiosity and an eye on the frontier.So don’t despair at the valley of despair. Instead, see it as an opportunity to flush out all the speculation (and the speculators themselves) and return the field to the people who are actually going to be building the future rather than just trying to make a quick buck. Those fleeing investors will be back when we need them.","Blockchain is entering the valley of despair phase, and that’s a mistake"
9606,3659097,2018-02-21 22:00:02,"Startup that sells your salary data to VCs gets bought by Solium0Investors don’t want their portfolio companies to pay you too much, or too little. So they pay Advanced-HR for its compensation data pulled from 2,500 startups. With a generic name, the service has flown somewhat under the radar since launching 20 years ago.As startups grow more professional while staying private longer, they’re getting serious about how they structure equity compensation plans to retain talent. Solium sells them stock option planning software.But together, they hope to offer the most accurate view of how much salary and stock other companies offer to help startups figure out exactly how to pay their employees. Today Solium announced that it’s acquiring the tech and whole team of Advanced-HR, which will continue to sell its Option Driver software-as-a-service.It’s part of an acquisition spree that comes from a war chest of $50 million that Solium has told the public markets is going to buying companies and developing new products. In October Solium bought Capshare, which helps 10,000 smaller startups manage their equity compensation plans. In March, it bought NASDAQ’s ExactEquity planning business. Terms of the deal weren’t disclosed, but you could expect it’s a modest chunk of the $50 million that’s being spread across multiple deals.The plan seems to be working, as Solium’s share price is up 35 percent this year. Though Solium went public in 2001 and became profitable in 2004, it raised a $48 million financing last year to capitalize on the shift toward startups staying private longer and equity becoming an increasingly important way to keep talent from skipping off to somewhere with a higher salary.“The other over-arching trend is that startups are taking over control of managing their equity,” says Lopez. “Equity has been historically managed with spreadsheets in a startup while the official ledger/cap table sat with a law firm. With equity management platforms like Shareworks there is now a system of record that the company controls and can give access to legal counsel, investors and other stakeholders as desired.”Dee DiPietro started AHR as a consultant practice back in 1997 as a solo female founder. Eventually she took over running the popular Venture Capital Executive Compensation Survey from Benchmark. Its sponsors, including heavy hitters like Accel, Andreessen Horowitz, Sequoia and Y Combinator, pay $4,000 a year and get access to the results. Data is all pulled voluntarily from startups who also get the results in return. The service has evolved from models in Excel to automated compensation planning software used by 120 top VC firms.“We brought transparency and automation to an industry that has a complex approach to compensation in that equity plays a significant role” says DiPietro. “As Advanced-HR continues to scale, it made perfect sense to join forces with a financial technology leader in equity management.”Still, what the industry really needs is a better tool for employees to vet their own job offers. It can be quite tough to predict what your stock options will be worth depending on vesting schedules, multiple rounds of funding and dilution. And then there’s the heavy upfront costs and risks of actually exercising your stock options.The sad truth of the matter is that unless a company is an extraordinary 1-in-10,000 success, few teammates beyond the founders or very first employees stand to gain a life-changing windfall. Yet employees number 10 to 50 are often tasked with unrelenting deadlines and long hours that might only really benefit the C-level executives. Software like Advanced-HR and Solium make sure startups don’t pay too little or too much, but it’s the rank-and-file workers that need to know if they’re being fairly compensated.",Startup that sells your salary data to VCs gets bought by Solium
9607,3661545,2018-02-22 01:21:15,"The moment he saw the brilliant light captured by his camera, “it all clicked” for Victor Buso: All the times his parents woke him before sunrise to gaze at the stars, all the energy he had poured into constructing an observatory atop his home, all the hours he had spent trying to parse meaning from the dim glow of distant suns.“In many moments you search and ask yourself, why do I do this?” Buso said via email. This was why: Buso, a self-taught astronomer, had just witnessed the surge of light at the birth of a supernova – something no other human, not even a professional scientist, had seen.Alone on his rooftop, the star-strewn sky arced above him, the rest of the world sleeping below, Buso began to jump for joy.His discovery, reported Wednesday in the journal Nature, is a landmark for astronomy. Buso’s images are the first to capture the brief “shock breakout” phase of a supernova, when a wave of energy rolls from a star’s core to its exterior just before the star explodes. Computer models had suggested the existence of this phase, but no one had witnessed it.“In our field, this is a fundamental question: What is the structure of the star at the moment of explosion?” said Melina Bersten, an astrophysicist at the Institute of Astrophysics of La Plata in Argentina and the lead author of the report.With Buso’s observations, scientists can begin to answer that.Buso, a 58-year-old locksmith from the Argentine city of Rosario, inherited his love of astronomy from his parents. When he was 10, his father roused him from bed to glimpse Comet Bennett streaking across the sky. The year before, his mother had held him at her side in front of the family’s television to watch Neil Armstrong take humankind’s first steps on the moon. “Vitito,” she told him, using his nickname, “you have to see this moment. You will never forget it.”Buso began building telescopes at age 11, using tin cans, magnifying glass lenses and Play-Doh to make the stars seem closer. Eight years ago, he sold a piece of land he owned with his father and used the proceeds to construct an astronomy tower on his roof. He calls it Observatorio Busoniano.These days Buso prefers to scan the skies with cameras mounted atop his 40-centimeter telescope, which allows him to observe phenomena too faint to be spotted by eye. On the evening of Sept. 20, 2016, he decided to test a new camera by pointing it at the NGC-613 – a barred spiral galaxy about 65 million light-years from Earth.Within a few minutes, he noticed something strange in his photographs: a tiny pixel of light that didn’t appear in archive images he found online.“I thought, ‘Oh, my God, what is this?’ ” Buso recalled.The light didn’t look like a supernova – the usual source of new lights in the sky. (Indeed, “nova” means new star, though supernovas are the explosive deaths of suns that have run out of fuel.) Yet the light just kept getting brighter.Buso realized he needed to show this to a professional, but most of Argentina’s astronomers were at an annual conference far from the nearest observatory. He finally got in touch with a fellow amateur, who confirmed the sighting and helped him develop an international alert with data Buso provided on the object’s position, brightness and timeline.As soon as darkness fell the following night, Buso rushed to his roof to see whether that faint pixel had developed into a brilliant, full-blown supernova.It had.Buso’s images made their way into the hands of Bersten and her colleague Gaston Folatelli, who could hardly believe what they were seeing.“We immediately noticed this was an incredibly important discovery,” Bersten said. “Given that we don’t know where and in which moment a supernova is going to explode, it is very easy to lose this very fast early phase.”According to a statement from the University of California at Berkeley, Buso had captured light from the supernova’s first hour. Bersten estimated the chance of happening upon such an event at about 1 in 10 million.Using powerful telescopes to observe the supernova in multiple wavelengths, the astronomers detected the light signature of a star that had lost most of its hydrogen envelope, then exploded. This gave them insight into the structure of the progenitor star, which Bersten and her colleagues concluded was a yellow supergiant about 20 times as massive as our sun. It was probably in a binary system, they say, because such stars rarely blow up on their own.The early detection also allowed the scientists to track the supernova throughout its evolution and develop models to explain what they saw. Even now, they continue to scrutinize the explosion; knowing how this star died might shed light on how it lived.Stellar explosions “come in different flavors,” Folatelli explained in a call from Argentina several days ago. “We want to know how stars evolve into the different structures that give different outcomes in supernovas.”He and Bersten both marveled at the cosmic good fortune that allowed this rare event to be seen. Not only was Buso observing with a brand-new, powerful camera, but the supernova, dubbed SN 2016gkg, was at the edge of its host galaxy, making it easier to spot.They have colleagues who worked for years on sky surveys in the hope of spotting a shock wave breakout. Witnessing astronomers’ joy at this discovery was “awe-inspiring and unique,” Buso wrote. “It’s so exciting to find and register something yet unseen by humans.”His one regret is that he cannot share his discovery with his parents, who died years ago. For the two people who taught him to love “this beautiful science we call astronomy,” this breakthrough seems a fitting memorial.More in Nation & WorldSUNRISE, Fla. (AP) — Republican Sen. Marco Rubio was put on the defensive Wednesday by angry students, teachers and parents who are demanding stronger gun-control measures after the shooting rampage that claimed 17 lives at a Florida high school.Our nation’s education secretary recently suggested that to prevent the next school massacre perhaps teachers should be permitted to carry guns. People took to social media to roar that many schools can’t even afford pencils, let alone armaments.",Self-taught astronomer makes monumental discovery
9608,3664144,2018-02-22 05:32:09,"He Took a Picture of a Supernova While Setting Up His New CameraAstronomers using the Swope telescope captured Supernova 2016gkg, between the two red lines, in the galaxy NGC 613, which is 80 million light-years from here.Credit C. Kilpatrick/UC Santa Cruz and Carnegie Institution for Science, Las Campanas Observatory, ChileBoom.A star is dead.On Sept. 20, 2016, Victor Buso, an amateur astronomer in Rosario, Argentina, was checking out the new camera on his telescope by taking pictures of a nearby spiral galaxy when a star within it went off in a supernova explosion.Within hours, and prompted by Mr. Buso’s good fortune, professional astronomers around the world trained their big telescopes on the galaxy, known as NGC 613, about 80 million light-years from here in the constellation Sculptor. It was a rare instance in which astronomers were able to see the beginning of a supernova, when one of the most massive stars in the universe ends its life in one of the most violent events nature can cook up.A series of images from the moment an amateur astronomer caught a supernova on his camera in 2016. The explosion, shown in the red circle, rapidly brightens south of the host galaxy, NGC 613. Víctor Buso and Gastón FolatelliMost supernovas are far away and don’t call attention to themselves until their funeral pyre explosions are well underway. In this case, astronomers were able to record what they call the “breakout,” when a shock wave radiating from a star’s core, which has probably collapsed into a black hole, reaches the surface of the poor star and brightens it catastrophically.“It’s like winning the cosmic lottery,” said Alex Filippenko, in a news release from the Keck Observatory in Hawaii, where Dr. Filippenko, of the University of California, Berkeley, has been tracking the supernova.The astronomers, who reported their findings on Wednesday in Nature, said the original star had probably been about 20 times as massive as the sun, but had blown most of that mass off into space before the decisive explosion began.",He Took a Picture of a Supernova While Setting Up His New Camera
9609,3664152,2018-02-20 11:00:00,"Fast, simple checkout. Easy access to rewards and offers. One spot for purchases, passes, and payment methods. All of these are ways we’ve been working to make paying safer and easier for everyone, everywhere. And you can make the most of these features with the new Google Pay app for Android.The app, which begins rolling out today, is just one part of everything we’ve got planned. We’re currently working on bringing Google Pay to all Google products, so whether you’re shopping on Chrome or with your Assistant, you’ll have a consistent checkout experience using the cards saved to your Google Account. We’re also working with partners online and in stores, so you’ll see Google Pay on sites, in apps, and at your favorite places around the world.As we continue to expand to even more devices and services, the new app offers an exciting glimpse of what’s to come. Here’s a closer look.Helpful info while you shopGoogle Pay’s new Home tab gives you the info you need, right when you need it. See your recent purchases, find nearby stores, enjoy easy access to rewards, and get helpful tips.A new spot for your stuffThe Cards tab is an easy way to keep everything you need at checkout organized and at the ready. It’s where you’ll store your credit and debit cards, loyalty programs, offers, and even that stack of gift cards from last year’s birthday.A faster way to pay your fareUse Google Pay on transit in cities such as Kiev, London, and Portland (with more coming soon), and stop worrying about your pass once and for all. Now all you need at the turnstile is the device that’s already in your hand.Longtime Android Pay fans, fear not: the features you love aren’t going away. You’ll still get all your bank’s perks and protections, plus an extra layer of security, since Google Pay doesn’t share your actual card number when you pay in stores. And those online payment forms that take forever to fill out? Just choose Google Pay at checkout and pay with a few quick clicks instead. Learn more at pay.google.com/about/And more features are coming. If you live in the U.S. or the UK, you’ll be able to use it to send and request money within the next few months. In the meantime, the Wallet app is now called Google Pay Send, and we’re giving it a fresh coat of paint to go with the Google Pay brand.Want to accept Google Pay for your app, site or business? If you’re a developer, you can work with our processor partners and find all the tools you need on our developer site. And if you’re a business owner building a site for yourself, we’ve partnered with Shopify to make integration even easier.","Say hello to a better way to pay, by Google"
9610,3667139,2018-02-21 18:11:38,"I don’t understand Graph Theory.How to think in graphs. An illustrative introduction to Graph Theory and its applications.Graph theory represents one of the most important and interesting areas in computer science, and at the same time the most misunderstood (at least by me, no stats). Understanding and using graphs makes us better programmers. Thinking in graphs makes us the best. At least that’s how we supposed to think. A graph is a set of vertices V and a set of edges E, comprising an ordered pair G=(V, E) blah blah blah. While trying to study the theory and implement some algorithms, I was regularly catching myself stuck just because it was booooring…As a matter of fact, the best way to understand something is to understand its applications. We are going to show various applications of graph theory, and most importantly, with detailed illustrations. Sometimes it might seem too much detailed [to seasoned programmers], but believe me, as someone who was there and tried to understand graphs, [even too much] detailed explanations are always preferred over succinct definitions. So, if you’ve been looking for a “graph theory and everything like that tutorial for absolute unbelievable dummies”, then you got to the right place. I hope.He means Monte CristoDisclaimersDISCLAIMER 1: I am not an expert in CS, algorithms, data structures and especially in graph theory. I am not involved in any project of the companies discussed in this article. Solutions to the problems are not final and could be improved drastically. If you find any issue or something unreasonable, you are more than welcome to leave a comment. If you work at one of the mentioned companies or are involved in corresponding software projects, please respond with the actual solution. To all others, be patient readers, this is a pretty LONG article.DISCLAIMER 2:This article is somewhat different in information providing style, sometimes it might seem a bit digressed from sub-topic, but patient readers will eventually find themselves in a complete understanding of the picture.DISCLAIMER 3: This article is written for a broad audience of programmers. While having in mind junior programmers as a target audience, I hope it will be interesting to strong professionals as well.Hello, world!Let’s start with something that I used to see in graph theory books as “the origins of graph theory”, Seven Bridges of Königsberg (not really sure, but you can pronounce it as “qyonigsberg”). There were seven bridges in Kaliningrad, connecting two big islands surrounded by Pregolya river and two portions of mainlands divided by the same river.Our area of interestIn 18th century this was called Königsberg (part of Prussia) and the area above had a lot more bridges. The problem or just a brain teaser with Königsberg’s bridges was to be able to walk through the city by crossing all the seven bridges only once. They didn’t have an internet connection at that time, so should have entertain themselves somehow. Here’s the illustrated view of the seven bridges of Königsberg in 18th century.Seven bridges of KönigsbergTry it, walk through the city by crossing each bridge only once. Each bridge, it means there should not be uncrossed bridge(s). And only once, which means that each bridge must be crossed no more than one time. If you are familiar with this problem, you know that it’s impossible to do it, although you were trying hard enough and you may try even harder now, but eventually you’ll give up.Leonhard Euler (photo from Wikipedia)Sometimes it’s reasonable to give up fast. That’s how Euler solved this problem, he gave up pretty soon. Gave up on the positive solution, he rather thought how to prove that it’s not possible to walk through the city by crossing each bridge one and only time. Let’s try to understand how Euler was thinking and how he came up with the solution (if there isn’t a solution, it still needs a proof). Well that is a real challenge here, because walking through the thought process of such a venerable mathematician is kind of dishonorable. (Venerable so much that Knuth and friends dedicated their book to Leonhard Euler). We rather will pretend to “think like Euler”. Let’s start with picturing the impossible.There are four distinct places, two islands and two parts of mainland. And seven bridges. It’s interesting to find out if there is any pattern regarding the number of bridges connected to islands or mainland (we will use a “land” general term sometimes).Number of bridgesSomething seems bothering. There are only odd number of bridges connected to each land. If you have to cross each bridge once, then you can enter a land and leave it if it has 2 bridges.Examples of 2 bridge landsIt’s easy to see in the illustrations above that if you enter a land by crossing one bridge, you can always leave the land by crossing its second bridge. Whenever third bridge appears, you won’t be able to leave a land once you enter it by crossing all its bridges. If you will try to generalize this reasoning for a single land, you’ll be able to show that in case of even number of bridges it is always possible to leave the land and in case of odd number of bridges it isn’t. Try it in mind.Let’s add a new bridge to see how the number of overall connected bridges changes and whether it solves the problem.Notice the new bridgeNow we have two even numbers and two odd numbers. Let’s draw a new route with this new bridge involved.WowWe saw that the fact of even and odd number of bridges played a role. Here’s the question: does the number of bridges solve the problem, should it be even all the time? Turns out that no. That’s what did Euler, he found a way to show that the number of bridges matter. And, more interestingly, the number of “lands” with odd number of connected bridges also matters. That’s when Euler started to “convert” lands and bridges into something we know as graphs. Here’s how could a graph representing Königsberg bridges look like (note, our “temporarily” added bridge isn’t there):Lines are a bit twistyOne important thing to note is the generalization/abstraction of a problem. Whenever you solve a specific problem, the most important thing is to generalize the solution for similar problems. In this particular case, Euler’s task was to generalize bridge crossing problem to be able to solve similar problems in the future, i.e. for all the bridges in the world. Visualization also helps to view the problem in a different spot, see, the following graphs are all the same bridges shown above:So yes, visually graphs are a good choice for picturing problems, but all we need is to find out how is the Königsberg problem solved using graphs. Pay attention to the number of lines coming out from circles. And yes, let’s name them as seasoned professionals would do, from now on, we will call circles vertices, and the lines connecting them - edges. You might’ve seen letter notations, V for (vendetta?) vertex, E for edge.The next important thing is so-called degree of a vertex, the number of edges incident (connected) to the vertex. In our example above, the number of bridges connected to lands can be expressed as degrees of the graph vertex.In his endeavor Euler showed that the possibility of a walk through graph (city) traversing each edge (bridge) one and only one time is strictly dependent on the degrees of vertices (lands). The path consisting of such edges called [in his honor] an Euler path. The length of an Euler path is the number of edges. Get ready for strict language.An Euler path of a finite undirected graph G(V, E) is a path such that every edge of G appears on it once. If G has an Euler path, then it is called an Euler graph. [1]Theorem. A finite undirected connected graph is an Euler graph if and only if exactly two vertices are of odd degree or all vertices are of even degree. In the latter case, every Euler path of the graph is a circuit, and in the former case, none is. [1]Exactly two two vertices have odd degree in illustration at the left, and all vertices are of odd degree in illustration at the rightI used “Euler path” instead of “Eulerean path” just to be consistent with the referenced book [1] definition. If you know someone who differentiates Euler path and Eulerean path, and Euler graph and Eulerean graph, let them know to leave a comment.First of all, let’s clarify the new terms in the above definition and theorem.Finite graph is a graph with finite number of edges and vertices.Graphs can be directed and undirected, and that’s one of the interesting properties of graphs. You must’ve seen a popular Facebook vs Twitter example for directed and undirected graphs. A Facebook friendship relation may be easily represented as an undirected graph, because if Alice is a friend with Bob, then Bob must be a friend with Alice, too. There is no direction, both are friends with each other.Also note the vertex labeled as “Patrick”, it is kind of special, as doesn’t have any incident edges. It is still a part of the graph, but in this case we will say that this graph is not connected, it is disconnected graph (same goes with “John”, “Ashot” and “Beth” as they are separated from others). In a connected graph there is no unreachable vertex, there must be a path between every pair of vertices.In the contrary to the Facebook example, if Alice follows Bob on Twitter, that doesn’t require Bob to follow Alice back. So a “follow” relation must have a direction indicator, showing which vertex (user) has a directed edge (follows) to the other vertex.Now, knowing what is a finiteconnectedundirected graph, let’s get back to Euler’s graph:So why we did discuss Königsberg bridges problem and Euler graphs in the first place? Well, it’s not so boring and by investigating the problem and foregoing solution we touched the elements behind graphs (vertex, edge, directed, undirected) avoiding dry theoretical approach. And no, we are not done with Euler graphs and the problem above, yet. We should now move on to the computer representation of graphs as that is the top interest for us, programmers. By representing a graph in a computer program we will be able to devise an algorithm for tracing graph path(s), therefore find out if it is an Euler path. Before that, try to think of a good application for an Euler graph (beside fiddling around with bridges).Graph representation: introThis is a quite tedious task, be patient. Remember the fight between arrays and linked lists? Use arrays if you need fast element access, use lists if you need fast element insertion/deletion, etc etc. I hardly believe you ever struggled on something like “how to represent lists”. Well, in case of graphs the actual representation is really bothering, because first you should decide how exactly are you going to represent a graph. And believe me, you are not going to like this. Adjacency list, adjacency matrix, maybe edge lists? Toss a coin.You should have tossed hard, because we are starting with a tree. You must have seen a binary tree at least once (the following is not a binary search tree).Just a sampleJust because it consists of vertices and edges, it’s a graph. You also may recall how most commonly a binary tree is represented (at least in textbooks).It might seem too detailed for people who are already familiar with trees, but I have to illustrate it to make sure we are on the same page (note that we are still dealing with pseudocode).If you are new to trees, read the pseudocode above carefully, then follow the steps in this illustration:Colors are just for bright visualizationWhile a binary tree is a simple “collection” of nodes, each of which has left and right child nodes, a binary search tree is much more useful as it applies one simple rule, which allows fast key lookups. Binary search trees (BST) keep their keys in sorted order. You are free to implement your BT with any rule you want (although it might change its name based on the rule, for instance, min-heap or max-heap), most commonly for a BST we will expect that it satisfies the binary search property (that’s where the name comes from), that is “each node’s key must be greater than any key in its left sub-tree and less than any key in its right sub-tree”. A very interesting remark regarding the statement “greater than”. That’s crucial for the nature of the BST. Whenever you change it to “greater than or equal”, your BST will be able to save duplicate keys when inserting new nodes, otherwise it will keep only nodes with unique keys. You can find really good articles on the web about binary search trees, we won’t provide a full implementation of a binary search tree, but for the sake of consistency, we’ll illustrate a simple binary search tree here.Airbnb. Trees are very useful data structures, you might have not implemented a tree from scratch for your projects, but you probably used them even without noticing. Let’s look at an artificial yet valuable example and try to answer the “why” question, why to use a binary search tree in the first place. As you noticed, there is a “search” in binary search tree, so basically, everything that needs a fast lookup, should be placed in a binary search tree. “Should” doesn’t mean must, the most important thing to keep in mind in programming is to solve a problem with proper tools. There are tons of cases where a simple linked list with its O(N) lookup might be more preferable than a BST with its O(logN) lookup. Typically we would use a library implementation of a BST, most likely std::set or std::map in C++, however in this tutorial we are free to reinvent our own wheel (BSTs are implemented in almost any general-purpose programming language libraries, you can find in corresponding documentation of your favorite language). Approaching to “a real-life example”, here’s the the problem we’ll try to tackle:A glimpse on Airbnb Homes searchHow to search homes based on some query with bunch of filters as fast as it possible. This is a hard task, becomes harder if we consider the amount of data as Airbnb stores millions of listings.So when users search for homes, there is a chance that they might “touch” 4 million records stored in the database. Sure there are limited “top listings” shown on the website’s home page and a user almost never isn’t “enough” curious to view millions of listings. I don’t have any analytics regarding Airbnb, but we can use a powerful tool in programming called “assumptions”, so we will assume that a single user finds a good home by viewing at most ~1k homes. And the most important factor here is the number of real-time users, as it makes a difference in data structures choice, database(s) choice and the project architecture at all. As obvious as it might seem, if there are just 100 users overall, then we may not bother at all, and in the contrary, if the number of users overall and real-time users in particular is far beyond the million threshold, we have to think really really, really wisely on each decision. “Each” is used exactly right, that’s why companies hire the best while striving for excellence in service provision. (Google, Facebook, Airbnb, Netflix, Amazon, Twitter, and many others are dealing with huge number of data and the right choice to serve millions of bytes of data each second to millions of real-time users starts from hiring the right engineers. That’s why we, the programmers, struggle with these data structures, algorithms and problem solving in possible interviews, because all they need is the engineer having ability to solve such big problems in the fastest and most efficient possible way.)So here’s the case, a user visits the home page (we’re still talking about Airbnb) and tries to filter out homes to find the best possible fit. How would we deal with this problem? (Note that this problem is rather backend-side, so we won’t care about front-end or the network traffic or https over http or Amazon EC2 over home cluster and so on). First of all, as we are already familiar with one of the most powerful tools in programmers’ inventory (talking about assumptions rather than abstractions), let’s start by assuming that we deal with data that completely fits in the RAM. And you can also assume our RAM is big enough. Big enough to hold, mm, how much? Well that’s another good question. How much memory will be required to store the actual data. If we are dealing with 4 million units of data (again, assumption), and if we probably know each unit’s size, then we can easily derive the required memory size, i.e. 4M * sizeof(one_unit). Let’s consider a “home” object and its properties, actually, let’s consider at least those properties that we will deal with while solving our problem (a “home” is our unit). We will represent it as a C++ structure in some pseudocode, you can easily convert it to a MongoDB schema object or anything you want, we just discuss the property names and types. (try to think about using bitfields or bitsets for space economy).Assumptions. The above structure is not perfect (obviously) and there are many assumptions and/or incomplete parts, go read disclaimers one more time. I just looked at Airbnb’s filters and devised property lists that should exist to satisfy to search queries. It’s just an example. Now we should calculate how many bytes in memory will take each AirbnbHome object. name is a wstring to support multilingual names/titles, which means each character will take 2 bytes (we may not bother with character size if we would use other language, but in C++ char is 1-byte character and wchar is 2-byte character). A quick look at Airbnb’s listings allows us to assume that the name of a home should take up to 100 characters (though mostly it is around 50, rather than 100), we’ll assume 100 characters as a maximum value, which leads to ~200 bytes of memory. uint is 4 bytes, uchar is 1 byte, ushort is 2 bytes (again, in our assumptions). Assuming photos are residing at some storage service, like Amazon S3 (as far as I know, this assumption is most likely to be true for Airbnb, but again, Amazon S3 is just an assumption) and we have those photos’ URLs, and considering the fact that there is no standard size limit on the URLs, but there is in fact a well-known limit of 2083 characters, we‘ll take it as a max size of any URL. So taking into account that each home has 5 photos in average, it would take up to ~10Kb.Let’s rethink on this, usually storage services serve content with the same base URLs, like http(s)://s3.amazonaws.com/<bucket>/<object>, i.e. there is a common pattern for constructing URLs and we need to store only the actual photo ID, let’s say we use some unique ID generator, which returns 20 byte length unique string IDs for photo objects and the URL pattern for particular photo looks like https://s3.amazonaws.com/some-know-bucket/<unique-photo-id>. This gives us good space efficiency, so for storing string IDs of five photos we will need only 100 bytes of memory. The same “trick” could be done with the host_id, i.e. the user ID who hosts the home, takes 20 bytes of memory (actually we could just use integer IDs for users, but considering that some DB systems like MongoDB has rather specific unique ID generator, we’re assuming a 20 byte length string ID as some “median” value which fits to almost any DB system with a little change. Mongo’s ID length is 24 byte). And finally, we’ll take a bitset of up to 32 size as 4 bytes object and a bitset of size greater than 32 and less than 64, as an 8 bytes object. Mind the assumptions. We used bitset in this example for any property that expresses an enum, but is able to take more than one value, in other words a kind of multiple choice checkbox.So for example, each Airbnb home keeps a list of available amenities, e.g. “iron”, “washer”, “tv”, “wifi”, “hangers”, “smoke detector” and even “laptop friendly workspace” and so on. There might be more than 20 amenities, we stick to the 20 just because it’s the number of filterable amenities on Airbnb filters page. Bitset saves us some good space, if we keep proper ordering for amenities. For instance, if a home has all above mentioned amenities (see checked ones in the screenshot), we will just set a bit at corresponding position in the bitset.Bitset allows to save 20 different values using just 20 bitsFor example, checking if a home has a “washer”:Or a little more professionally:You can improve the code as much as you want (and fix compile errors), we just wanted to emphasize the idea behind bitsets in this problem context. The same idea goes with “house rules”, “home type” and others.Finally, the country code and city name. As mentioned in the comments of the code above (see remarks), we won’t store latitude and longitude to avoid geo-spatial queries (a subject of another article), instead we save country code and city name to narrow down the search by a location (omitting streets for the sake of simplicity, please forgive me). Country code could be represented as 2 characters, 3 characters or 3 digits, we’ll save numeric representation and will use an ushort for it. [un]Fortunately there are many more cities than countries, so we can’t use a “city code” (though we can make one for internal use), instead we’ll store actual city name, preserving 50 bytes in average for a city name and for super-specific cases like Taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronukupokaiwhenuakitanatahu (85 letters) we better use an additional boolean variable which indicates that this is that specific super-long city (don’t try to pronounce it). So, keeping in mind the memory overhead of strings and vectors, we’ll add additional 32 bytes just in case to the final size of the struct. We also will assume that we work on a 64-bit system, although we chose very compact values for int and short.So, 420 bytes + promised overhead of 32 bytes, 452 bytes and considering the fact that some of you might be just obsessed with the aligning factor (aligning! aligning!), let’s round up it to 500 bytes. So each “home” object takes up to 500 bytes, and for all home listings (there could be some confusing moments with the listings count and actual homes’ count, just let me know if I got something wrong), 500 bytes * 4 million = 1.86Gb ~ 2Gb. Seems plausible. We made many assumptions while constructing the struct, making it cheaper to save in memory, I really expected much more than 2 Gigabytes and if I did a mistake in calculations, let me know. Anyway, moving forward, so whatever we gonna do with this data, we will need at least 2 Gb of memory. If you got bored, deal with it. We are just starting.Now the hardest part of the task. Choosing the right data structure for this problem (filter the listings as efficient as possible) is not the hardest task. The hardest task is (for me) to search listings by a bunch of filters. If there would be just one search key (just one filter) we would easily solve it. Suppose the only thing users care is the price, so all we need is to find AirbnbHome objects with prices falling in the provided range. If we’ll use a binary search tree for that, here how it might look.If you imagine all 4 millions objects, this tree grows very very big. By the way, the memory overhead grows as well, just because we used a BST to store objects, as each tree node has two additional pointers for its left and right child it adds up to 8 additional bytes for each child pointer (assuming 64-bit system). For 4 million nodes it sums up to ~62 Mb, which in comparison to 2Gb of objects’ data looks quite small, though it is not something that we can “omit” easily.The tree in the last illustration so far shows that any item can be easily found in O(logN) complexity. If you aren’t familiar or not sure enough to chit-chat in big-ohs, we’ll clarify it below, otherwise skip the complexity subsection.Algorithmic complexity. Let’s make this quick as there will be a long and detailed explanation in an upcoming article: “Algorithmic Complexity and Software Performance: the missing manual”. For most of the cases finding the big O complexity for an algorithm is somewhat easy. First thing to note is that we always consider the worst case, i.e. the maximum number of operations that an algorithm does to produce a positive outcome (to solve the problem).Suppose an array with 100 elements in an unsorted order, how many comparisons would it take to find any element (also taking into account that the required element could be missing)? It will take up to 100 comparisons as we should compare each element’s value with the value we are looking for, and despite the fact that the element might be the first element in the array (leading to a single comparison), we will consider only the worst possible case (element is either missing or is residing at the last position of the array).The point of “calculating” algorithmic complexity is finding a dependency between the number of operations and the size of input, for instance the array above had 100 elements and the number of operations were also 100, if the number of array elements (its input) will increase to 1423, the number of operations to find any element will also increase to 1423 (the worst case). So the thin line between input and number of operations is clear in this case, it is so-called linear, the number of operations grows as much as grows array’s input. Growth. That’s the key point in complexity, we say that searching for an element in an unsorted array takes O(N) time to emphasize that the process of finding it will take up to N operations (or even up to N operations times some constant value, e.g. 3N). In the other hand, accessing any element in array takes constant time, i.e. O(1). That’s because of array’s structure, it is a contiguous data structure, and holds elements of the same type (mind JS arrays), so “jumping” to particular element requires only calculating its relative position to the array’s first element.One thing is very clear, binary search tree keeps its nodes in sorted order. So what would be the algorithmic complexity of searching an element in a binary search tree? We should calculate the number of operations required to find an element (in the worst case). See the illustration, when starting our search at the root, first comparison may lead to three cases, (1) the node is found, (2) otherwise the comparison will continue to node’s left sub-tree if the required element is less than the node’s value, or (3) to the right sub-tree if the value we search for is greater than the node’s value. At each step we reduce the size of nodes needed to be considered by half. The number of operations (i.e. comparisons) needed to find an element in the BST equals to the height of the tree. The height of a tree is the number of nodes on the longest path. In this case it’s 4. And the height is [base 2] logN + 1, as shown. So the complexity of search is O(logN + 1) = O(logN). This means that searching something in 4 million nodes requires log1000000 = ~22 comparisons in the worst case.Back to the tree. Element access time in a binary search tree is O(logN). Why not to use hashtables? Hashtables have constant access time, it makes reasonable to use hashtables almost everywhere.In this problem we must take into account an important requirement, that is, we must be able to make range searches, e.g. homes with prices from $80 to $162. In case of a BST it’s easy to get all the nodes in a range just by doing an inorder traversal of the tree and keeping a counter. For a hashtable it is somewhat expensive and it is reasonable to stick with BSTs in this particular case, though there is another spot, which leads us to rethink on hashtables. The density. Prices won’t go up “forever”, most of the homes reside at the same price range. Look at the screenshot, the histogram shows us the real picture of the prices, millions of homes are in the same range ($18 — $212, plus minus), they have the same average price. Simple arrays may play a good role. Assuming the index of an array as the price and the value as the list of homes, we might access any price range in constant time (well, almost constant). Here’s how it looks (way abstract):Just like a hashtable, we are accessing each set of homes by its price. All homes having the same price are grouped under a separate BST. It will also save us some space if we store homes’ IDs instead of the whole object defined above (the AirbnbHome struct). Most possible scenario is to save all homes’ full objects in a hashtable mapping home id to home full object and storing another hashtable (or better, an array), which maps prices with homes’ IDs. So when users request a price range, we fetch homes IDs from the price table, cut the results to a fixed size (i.e. the pagination, usually around 10–30 items are shown on one page) then fetch full home objects using each home ID. Just keep this in mind, think on it in a background. And also, balancing. Balancing is crucial for a BST, because it’s the only guarantee of having tree operations done in O(logN). The problem of unbalanced BST is obvious when you insert elements in sorted order, eventually, tree becomes just a linked list, which obviously leads to linear-time operations. Forget this for now, suppose all our trees are perfectly balanced. Take a look at the illustration above once again. Each array element represents a big tree. What if we change the illustration to something like this:Graph representation: outroBad news about graphs is that there isn’t a single definition for the graph representation, that’s why you can’t find a std::graph in the library. We already had a chance to represent a “special” graph called BST. The point is, tree is a graph, but graph is not a tree. The last illustration shows us that we have a lot of trees under a single abstraction, “prices vs homes” and some of the vertices “differ” in their type, prices are graph nodes having only the price value and refer to the whole tree of homes’ IDs (home vertices) that satisfy the particular price. It is much like a hybrid data structure, than a simple graph that we used to see in textbook examples. That’s the key point in graph representation, there isn’t a fixed and “de jure” structure for graph representation (unlike BSTs with their specified node-based representation with left/right child pointers, though you can represent a BST with a single array). You can represent a graph in the most convenient way you wish (most convenient to particular problem), the main thing is that you “see” it as a graph. And by “seeing a graph” we mean applying algorithms that are specific to graphs.What about an N-ary tree, it is more likely to resemble a graph.And the first thing that comes into mind to represent an N-ary tree node is something like this:This structure represents just a single node of a tree, the full tree should look like this:This class is an abstraction around a single tree node named root_ . That’s all we need to build a tree of any size. That’s the starting point of the tree. For adding a new tree node we need to allocate a memory for it and add that node to the root of the tree.A graph is much like an N-ary tree, with a slight difference. Try to spot it.Is this a graph? No. I mean yes, but it’s the same N-ary tree from the previous illustration, just a little rotated. As a rule of a thumb, whenever you see a tree, and no matter if it is an apple tree, a lemon tree or a binary search tree, you can be sure that it is also a graph. So, devising a structure for a graph node (graph vertex), we can come up with the same structure:Is this enough to construct a graph? Well, no. And here’s why. Look at these two graphs from previous illustrations, find a difference:Both are graphsThe graph in the illustration at the left side has no single point to “enter” (it’s rather a forest than a single tree), in the contrary, the graph in the right illustration doesn’t have unreachable vertices. Sounds familiar.A graph is connected when there is a path between every pair of vertices. [Wikipedia]Obviously, there isn’t a path between every pair of vertices for the “prices vs homes” graph (if it isn’t obvious from the illustration, just assume that prices are not connected with each other). As much as it’s just an example to show that we aren’t able to construct a graph with a single GraphNode struct, there are cases that we have to deal with disconnected graphs like this. Take a look at this class:Just like an N-ary tree is built around a single node (the root node), a connected graph also can be built around a root node. It’s said that trees are “rooted”, i.e. they have a starting point. A connected graph can be represented as a rooted tree (with a couple of more properties), it’s already obvious, but keep in mind that the actual representation may differ from algorithm to algorithm, from problem to problem even for a connected graph. However, considering node-based nature of graphs, a disconnected graph can be represented like this:For graph traversals like DFS/BFS it’s natural to use a tree-like representation. Helps a lot. However, cases like efficient path tracing require a different representation. Remember Euler’s graph? To track down a graph’s “eulerness”, we should trace an Euler path in it. That means visiting all vertices by traversing each edge only once, and when the tracing finishes and we have untraversed edges, then the graph has not an Euler path, therefore it is not an Euler graph. There is even faster method, we can check the degrees of vertices (suppose each vertex stores its degree) and just as the definition says, if graph has vertices of odd degree and there aren’t exactly two of them, then it is not an Euler graph. The complexity of such check is O(|V|), where |V| is the number of graph vertices. We can track down odd/even degrees while inserting new edges to increase odd/even degree checks to O(1). Lightning fast. Never mind, we’re just going to trace a graph, that’s it. Below is both the representation of a graph and the Trace() function returning a path.Mind the bugs, bugs are everywhere. This code contains a lot of assumptions, for instance, the labeling, so by a vertex we understand a string label, sure you can easily update it to be anything you want. Doesn’t matter in the context of this example. Next, the naming. As said in comments, VELOGraph is for Vertex Edge Label Only Graph (I made this up). The point is, this graph representation contains a table for mapping a vertex label with edges incident to that vertex, and a list of edges containing a pair of vertices (connected by a particular edge) and a flag which is used only by the Trace() function. Take a look at the Trace() function implementation. It uses edge’s flag to mark an already traversed edge (flags should be reset after any Trace() call).Here’s another representation called an adjacency matrix, which could be useful in directed graphs, like one we used for Twitter followings graph.Directed graphThere are 8 vertices in this Twitter example. So all we need to represent this graph is a |V|x|V| square matrix (|V| number of rows and |V| number of columns). If there is a directed edge from v to u, then matrix’s [v][u] is true, otherwise it’s false.Twitter’s exampleAs you can see, this matrix is way too sparse, its trade off is the fast access. To see if Patrick follows Sponge Bob, we should just check the value of matrix[""Patrick""][""Sponge Bob""]. To get the list of Ann’s followers, we just process the entire “Ann” column (title is in yellow). To find who are being followed (sounds strange) by Sponge Bob, we process the entire row “Sponge Bob”. Adjacency matrix could be used for undirected graphs as well, and instead of settings 1’s if a there is an edge from v to u, we should set both values to 1, e.g. adj_matrix[v][u] = 1, adj_matrix[u][v] = 1. Undirected graph’s adjacency matrix is symmetric.Note that instead of storing 1s and 0s in an adjacency matrix, we can store something “more useful”, like edge weights. One of the best examples might be a graph of places with distance information.The graph above represents distances between houses of Patrick, Sponge Bob and others (also known as weighted graph). We put “infinity” signs if there isn’t a direct route between vertices. That doesn’t mean that there are not routes at all, and at the same time that doesn’t mean that there must necessarily be routes. It might be defined while applying an algorithm for finding a route between vertices (there is even better way to store vertices and edges incident to it, called an incidence matrix).TwitterWhile adjacency matrix seemed a good use for Twitter’s followings graph, keeping a square matrix for nearly 300 million users (monthly active users) takes 300 * 300 * 1 bytes (storing boolean values). That is, ~82000 Tb (Terabyte), which is 1024 * 82000 Gb. Well, don’t know about your home cluster, my laptop doesn’t have such a big RAM. Bitsets? A BitBoard could help us a little, reducing required size to ~10000 (ten thousand) Tb. Still way too big. As mentioned above, adjacency matrix is too sparse, that forces to use more space than actually needed. That’s why using a list of edges incident to vertices may be useful. The point is, adjacency matrix allows to keep both “follows” and “doesn’t follow” information, while all we need is to know followings, something like this:Adjacency matrix vs adjacency listThe illustration at the right side is called an adjacency list. Each list describes the set of neighbors of a vertex in the graph [source]. By the way, the actual implementation of the graph representation as adjacency list, again, differs (ridiculous facts). In the illustration we highlighted a hashtable usage, which is reasonable, as the access of any vertex will be O(1), and for the list of neighbor vertices we didn’t mention the exact data structure, deviating from linked lists to vectors. Choice is yours. The point is, to find out whether Patrick does follow Liz, we should access the hashtable (constant time) and go through all items in the list comparing each element with “Liz” element (linear time). Linear time isn’t that bad at this point, because we have to loop over only a fixed amount of vertices adjacent to “Patrick”. What about the space complexity, is it ok to use at Twitter? Well, we need at least 300 million hashtable records, each of which points to a vector (choosing vector to avoid memory overhead of linked lists’ left/right pointers) containing, …, how much? No stats here, found just an average number of twitter followers, 707 (googled). So if we consider that each hashtable record points to an array of 707 user ids (each weighing 8 byte), and let’s assume that hashtable’s overhead is only its keys, which are again, user ids, so the hashtable itself takes 300 mln * 8 bytes. Overall we have 300 mln * 8 bytes for hashtable + 707 * 8 bytes for each hashtable key, that is 300 mln * 8 * 707 * 8 bytes = ~12 Tb. Well, can’t say that feels better, but yes, feels much better than 10 thousand Tb. Honestly, I don’t know whether is this 12Tb a reasonable number, but considering the fact that I’m spending around $30 on a dedicated server machine with 32 Gb of RAM, then, storing (sharded) 12 Tb requires at least 385 such servers, plus a couple of control servers (for data distribution control) rounds up to 400. So it will cost me $12K (monthly). Well, considering the fact that the data should be replicated, and that something always can go wrong, we’ll triple the number of servers and then again, add some control servers, let’s say we need at least 1500 servers, which will cost us $45K monthly. Well, definitely not good for me as I hardly can keep one server, but seems okay for Twitter (it’s really nothing compared to real Twitter servers). Let’s assume it is really okay for twitter. Are we okay here? Not yet, that was just the data regarding the followings. What is the main thing in Twitter? I mean, technically, what is its biggest problem? You won’t be alone if you say it’s the fast delivery of tweets. I will definitely second that. And not fast, but lightning fast. Say Patrick tweeted something about his thoughts on food, all his followers should receive that very tweet in a reasonable time. How long will it take? We are free of making any assumption here, and use any abstractions we want, but we are interested in the real world production systems, so, let’s dig. Here’s what’s typically happens when someone tweets.Again, don’t know much about how long does it take for one tweet to reach all followers, but publicly available statistics tell us about 500 million daily tweets. Daily. So the process above happens 500 million times every day. Can’t really find anything on tweet delivery speed, vaguely recall something about max 5 seconds for a tweet to reach all its followers. And also note the “heavy cases”, celebrities with more than million followers. They might tweet something about their wonderful breakfast at the beach house, but Twitter sweats much to deliver that super-useful content to millions of followers.To solve tweet delivery problem we don’t really need the graph of followings, instead we need a graph of followers. The previous graph (with a hashtable and a bunch of lists) allows to efficiently find all users followed by any particular user. But it does not allow to efficiently find all users who are following one particular user, for that case we have to scan all the hashtable keys. That’s why we should construct another graph, which is kind of symmetric opposite to the one we constructed for followings. This new graph will again consist of a hashtable containing all 300 million vertices, each of which points to a list of adjacent vertices (the structure remains the same), but this time, the list of adjacent vertices will represent followers.So based on this illustration, whenever Liz tweets something, Sponge Bob and Ann must see that very tweet on their timelines. A common technique to accomplish this is by keeping separate structures for each user’s timeline. In case of Twitter’s 300+ million users we might assume there are at least 300+ million timelines (for each user). Basically, whenever a user tweets, we should get the list of user’s followers and update their timelines (insert that same tweet into each one of them). Timeline might be represented as a linked list, or a balanced tree (tweets’ datetimes as node keys).This is just the basic idea, we abstracted from actual timeline representation and of course, we can make the actual delivery faster if we use multithreading. This is crucial for ‘heavy cases’, because for millions of followers the ones that reside closer to the end are being processed later than the ones residing closer to the front of the list. The following pseudocode tries to illuminate this multithreading delivery idea:So whenever followers refresh their timelines, they will receive the new tweet.It will be fair to say, that we merely touched the tip of the iceberg of real problems at Airbnb or Twitter. It takes really long time, hard work of really talented engineers to accomplish such great results in complex systems like Twitter, Google, Facebook, Amazon, Airbnb and all others. Just keep this in mind while reading this article.The point of Twitter’s tweet delivery problem is to embrace the usage of graphs, even though we didn’t use any graph algorithm, we just used a representation of the graph. Sure we pseudocoded a function for delivering tweets, but that is something we came up during the process of searching for a solution, what I meant by “any graph algorithm” is any algorithm from this list. As something big enough to make programmers cry, graph theory and graph algorithms applications are somewhat different to spot at a glimpse. We were discussing the Airbnb homes and efficient filtering before finishing with graph representations, and the main obvious thing was the inability to efficiently filter homes with more than one filter key. Is there anything that could be done using graph algorithms? Well, can’t tell for sure, but at least we can try. What if we represent each filter as a separate vertex? Literally each filter, even all the prices from $10 to $1000+, all city names, country codes, amenities (tv, wifi, and all others), adults number, and each number as a separate graph vertex.Excerpt of Airbnb filtersWe can even make this set of vertices more “friendly” if we add “type” vertices too, like “Amenities” connected to all vertices representing an amenity filter.Airbnb filters with typesNow, what if we represent Airbnb homes as vertices and then connect each home with “filter” vertex if that home supports the corresponding filter (e.g. connecting “home 1” with “kitchen” if “home 1” has “kitchen” in its amenities)?Looks messyA subtle change of this illustration makes it more likely to resemble a special type of graph, called a bipartite graph.Number of vertices are more than it may appearBipartite graph or just bigraph is a graph whose vertices can be divided into two disjoint and independent sets such that every edge connects a vertex in one set to one in other set [Wikipedia]. In our example one of the sets represents filters (we’ll denote it by F) and the other is homes set (H). For example if there are 100 thousand homes with the price value $62, then price vertex labeled “$62” will have 100 thousand edges incident to each of homes’ vertices. If we measure the worst case of space complexity, i.e. each home has all the properties satisfying to all the filters, than the total amount of edges to be stored will be 70 thousand * 4 mln. If we represent each edge as a pair of two ids: {filter_id; home_id} and if we rethink on IDs and use 4 byte (int) numeric id for filters and 8 byte (long) id for homes, then each edge would require at least 12 bytes. So storing 70 thousand * 4 million 12 bytes values will require around 3Tb of memory. We made a small mistake in calculation, see, the number of filters is around 70 thousand because of the 65 thousand cities active in Airbnb (stats). And the good news is that the same home can not be located in more than one city. That is, our actual number of edges pairing with cities is 4 mln (each home located in one city), so we’ll calculate for 70k-65k = 5 thousand filters, that means we need 5000 * 4 mln * 12 bytes of memory, which is less than 0.3 Tb. Sounds good. But what gives us this bipartite graph? Most commonly a website/mobile request will consist of several filters, for example like this:And all we need is to find all the “filter vertices” above and process all the “home vertices” that are adjacent to these “filter vertices”. This takes us to a scary subject.Graph algorithmsAny processing done with graphs might be categorized as a “graph algorithm”. You literally can implement a function printing all the vertices of a graph and name it “<your name here>’s vertex printing algorithm”. What are the most of us scared of are the graph algorithms listed in textbooks (see the full list here). Let’s try to apply a bipartite graph matching algorithm, such as Hopcroft-Karp algorithm to our Airbnb homes filtering problem:Given a bipartite graph of Airbnb homes (H) and filters (F), where every element (vertex) of H can have more than one adjacent elements (vertex) of F (sharing a common edge). Find a subset of H consisting of vertices that are adjacent to vertices in a subset of F.Confusing problem definition, however we can’t be sure at this point whether Hopcroft-Karp algorithm solves our problem, but I assure you that this journey will teach us many crucial ideas behind graph algorithms. And the journey is not so short, so be patient. The Hopcroft–Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching — a set of as many edges as possible with the property that no two edges share an endpoint [Wikipedia]. Readers familiar with this algorithm are already aware that this doesn’t solve our problem, because matching requires that no two edges share a common vertex. Let’s look at an example illustration, where there are just 4 filters and 8 homes at all (for the sake of simplicity). Homes are denoted by letters from A through H, filters are chosen randomly. Home A has a price ($50), and 1 bed, that’s all we got for it. All the homes from A through H have $50 per night price tag and 1 bed, but few of them have “wifi” and/or “tv”. So the following illustration tries to show which homes should we “return” for the request asking for homes that have all four filters available (e.g. they cost $50 per night, they have 1 bed and also they have wifi and tv).The solution to our problem requires edges with common vertices leading to distinct home vertices that are incident to the same filter subset, while Hopcroft-Karp algorithm eliminates such edges with common endpoints and produces edges incident to vertices in both subsets. Take a look at the illustration above, all we need are homes D and G, which both satisfy to all four filter values. What we really need is to get all matching edges which share a common endpoint. We could devise an algorithm for this approach, but its processing time is arguably not relevant to users needs (users needs = lightning fast, right here, right now). Probably it would be faster to create a balanced binary search tree with multiple sort keys, almost like a database index file, which maps primary/foreign keys with a set of satisfying records. Balanced binary search trees and database indexing will be discussed in a separate article, where we will return to Airbnb home filtering problem again.The Hopcroft-Karp algorithm (and many others) is based on both DFS (Depth-First Search) and BFS (Breadth-First Search) graph traversal algorithms. To be honest, the actual reason to introduce the Hopcroft-Karp algorithm here is to surreptitiously switch to graph traversals, which is better to start from the nice graphs, binary trees.Binary tree traversals are really beautiful, mostly because of their recursive nature. There are three basic traversals called in-order, post-order and pre-order (you may come up with your own traversal algorithm). They are easy to understand if you have ever traversed a linked list. In linked lists you just print the current node’s value (named item in the code below) and continue to the next node.Almost the same goes with binary trees, you print the node value (or whatever else you need to do with it) and then continue to the next node, but in this case, there are “two next” nodes, left and right. So you should do the same for both left and right nodes. But you have three different choices here:print the node value then go to the left node, and then go to the right node, orgo to the left node, print the node value, and then go to the right node, orgo to the left node, then go to the right node, and then print the value of the node.Detailed tracing of pre-order traversalObviously recursive functions look very elegant though they are so expensive. Each time we call a function recursively, it means we call a completely “new” function (see the illustration above). And by “new” we mean that another stack memory space should be “allocated” for the function arguments and local variables. That’s why recursive calls are expensive (the extra stack space allocations and the many function calls) and dangerous (mind the stack overflow) and it is obviously suggested to use iterative implementations. In mission critical systems programming (aircraft, NASA rovers and so on) a recursion is completely prohibited (no stats, no experience, just telling you the rumors).Netflix. Let’s say we want to store all Netflix movies in a binary search tree with movie titles as sort keys. So whenever a user types something like “Inter” we will return a list of movies starting with “Inter”, for instance, [“Interstellar”, “Interceptor”, “Interrogation of Walter White”]. It would be great if we’ll return every movie that contains “Inter” in its title (not only ones that start with “Inter”), and the list would be sorted by movies’ rating or something that is relevant to that particular user (likes thrillers more than drama). The point of this example is to make efficient range queries to a BST, but as usual, we won’t dive deeper into the cold water to spot the rest of the iceberg. Basically, we need a fast lookup by search keywords and then get a list of results sorted by some key, which is most likely should be a movie rating and/or some internal ranking based on user’s personalized data. We’ll try to stick to the KISK principle (Keep It Simple, Karl) as much as possible.“KISK” or “let’s keep it simple” or “for the sake of simplicity”, a super excuse for tutorial writers to abstract from the real problem and make tons of assumptions by bringing an “abc” easy example and its solution in pseudocode that works even on your grandma’s laptop.This problem could be easily applied to Amazon’s products search as we most commonly search something in Amazon by typing a text describing our interest (like “Graph Algorithms”) and get results sorted by products’ ratings (I didn’t experience personalized results in Amazon’s search results, but I’m pretty sure Amazon does that stuff too). So, it will be fair to change the title of this subtopic to…Netflix and Amazon. Netflix serves movies, Amazon serves products, we’ll name them “items”, so whenever you read an “item” think of a movie in Netflix or any [viable] product in Amazon. What is most commonly done with the items is the parsing of their title and description (we’ll stick to the title only), so if an operator (usually a human being inserting item’s data into Netflix/Amazon database via an admin dashboard) inserts new item into the database, its title is being processed by some “ItemTitleProcessor” to produce keywords.Not the best illustration, I know (and has a typo)Each item has its unique ID, which is being linked to the keyword found in its title. This is what search engines do while crawling websites all over the world. They analyze each document’s content, tokenize it (break into smaller entities, words) and add to a table, which maps each token (word) to the document ID (website) where the token has been “seen”. So whenever you search “hello”, search engine fetches all documents mapped to keyword “hello” (reality is much complex, because the most important thing is the search relevancy, which is why Google Search is so awesome). So a similar table for Netflix/Amazon may look like this (again, think of Movies or Products when reading Items).Inverted indexHashtables, again. Yes, we will keep a hashtable for this inverted index (index structure storing a mapping from content, Wikipedia). Hashtable will map a keyword to a BST of Items. Why BST? Because we want to keep them sorted and at the same time serve them (respond to frontend requests) in sequential sorted portions, for instance a 100 items at a request (pagination). Not really something that shows the power of BSTs, but let’s pretend that we also need a fast lookup in the search result, say you want all 3 star movies with keyword “machine”.Note that it’s okay to have duplicate items in different trees, because an item usually can be found with more than one keyword. We’ll operate with items defined as this:Each time a new item is inserted into database, its title is being processed and added to the big index table, which maps a keyword to an item. There could be many items sharing the same keyword, so we keep these items in a BST sorted by their rating. When users search for some keyword, they get a list of items sorted by their rating. How can we get a list from a tree in a sorted order? By doing an in-order traversal.Here’s how an implementation of InOrderProduceVector() might look:But, but… We need highest rated Items first, instead in-order traversal produces lowest rated items first. That’s because of its nature, in-order traversal works “bottom up”, from the lowest to highest item. To get what we wanted, i.e. the list in descending order instead of ascending, we should take a look at the in-order traversal implementation a bit closer. What we do is going through the left node then printing current node’s value and the going through the right node. When we first go through the left node, that’s why we got the “leftest” node first (the most left node), which is the node with the smallest value. So simply changing the implementation to go through the right node first will lead us to a descending order of the list. We’ll name it as others do, a reverse in-order traversal. Let’s update the code above (introducing in a single listing, warning, bugs ahead):Fetching Movies or Products by keyword in sorted order (by rating)That’s it. We can serve item search results pretty fast. As mentioned above, inverted indexing is used mostly in search engines, like Google. Although Google Search is veee(too much e’s)ery complex search engine, it does use some simple ideas (way too modernized though) to match search queries to documents and serve the results as fast as it possible.We used tree traversals to serve results in sorted order. At this point it might seem that pre/in/post-order traversals are more than enough, but sometimes there is a need for another type of a traversal. Let’s tackle this well-known programming interview question, “how would you print a [binary] tree level by level?”:If you are not familiar with this problem, think of some data structure that you could use to store nodes while traversing the tree. If we compare level-by-level traversal of a tree with the others above (pre/in/post order traversals), we’ll eventually devise two main traversals of graphs, that is a depth-first search (DFS) and breadth-first search (BFS).Breadth-first search — take a good look around you before going farther.DFS is much like pre/in/post-order traversals, while BFS is what we need if we want to print tree’s nodes level-by-level. To accomplish this, we would need a queue (data structure) to store the “level” of the graph while printing (visiting) its “parent level”. In the previous illustration nodes that are placed in the queue are in light blue. Basically, going level by level, nodes on each level are fetched from the queue, and while visiting each fetched node, we also should insert its children into the queue (for the next level). The following code is simple enough to get the main idea of BFS. It is assumed that the graph is connected, although it can be modified to apply to disconnected graphs.The basic idea is easy to show on node-based connected graph representation. Just keep in mind that the implementation of the graph traversal differs from representation to representation. BFS and DFS are important tools in tackling graph searching problems (but remember that there are tons of graph search algorithms). While DFS has elegant recursive implementation, it is reasonable to implement it iteratively. For the iterative implementation of BFS we used a queue, for DFS you will need a stack. One of the most popular problems in graphs and at the same time one of the most possible reasons you read this article is the problem of finding the shortest path between graph vertices. And this takes us to our last thought experiment.Uber. With its 50 million users and 7 million drivers (source), one of the most important things is to match drivers with riders in an efficient way (we’ll use driver for “Uber app for drivers installed on driver’s device” and user for “Uber app installed on user’s device”). The problem starts with locations. Backend should process millions of user requests, sending each of the request to one or more (usually more) drivers nearby. While it is easier and sometimes even smarter to send the user request to all nearby drivers, some pre-processing might actually help.Besides processing incoming requests and finding the location area based on the user coordinates and then finding drivers with nearest coordinates, we also need to find the right driver for the ride. To avoid geospatial request processing (fetching nearby cars by comparing their current coordinates with user’s coordinates), let’s say we already have a segment of the map with user and several nearby cars. Something like this:Possible paths from a car to user are in yellow. The problem is to calculate the minimum required distance for the car to reach the user, in other words, find the shortest path between them. While this is more about Google Maps rather than Uber, we’ll try to solve it for this particular and very simplified case mostly because there are usually more than one cars and Uber might want to calculate the nearest car with the highest rating to send to the user. For this illustration that means calculating for all three cars the shortest path reaching to the user and decide which car would be the optimal to send. To make things really simple, we’ll discuss the case with just one car. Here are some possible routes to reach to the user.Possible variants to reach the userCutting to the chase, we’ll represent this segment as a graph:This is an undirected weighted graph (edge-weighted, to be more specific). To find the shortest path between vertices B (the car) and A (the user), we should find a route between them consisting of edges with possibly minimum weights. You are free to devise your version of the solution, we’ll stick with Dijkstra’s version. The following steps of Dijkstra’s algorithm are from Wikipedia.Let the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra’s algorithm will assign some initial distance values and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, keep the current value.When we are done considering all of the neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new “current node”, and go back to step 3.Applying to our example, we’ll start with vertex B (the car) as the initial node. For first two steps:Our unvisited set consists of all vertices, and also note the table at the left side of the illustration. For all vertices it will contain all the shortest distances from B and the previous (marked “Prev”) vertex that lead to the vertex. For instance the distance is 20 from B to F, and the previous vertex is B.We are marking B as visited and move to its neighbor F.Now we are marking F as visited and choosing the next unvisited node with smallest tentative distance, which is G. Also note the table at the left side, in the previous illustration nodes C, F and G already have their tentative distances set with the previous nodes which lead to mentioned nodes.As states the algorithm, if the destination node has been marked visited (when planning route between two specific nodes, our case) then we can stop. So our next step stops the algorithm with the following values.So we have both the shortest distance from B to A and the route through F and G nodes.This is really the simplest possible example of potential problems at Uber, comparing this to our iceberg analogy, we are at the tip of the tip of the iceberg. However, this is a good start as a first step to explore the real world of graph theory applications. I didn’t complete what I initially planned for this article, but in the near future most probably this will be continued (also including database indexing internals).There is still so much to tell about graphs (still need to study), take this article as another tip of the iceberg. If you have read this far, you definitely deserve a cookie, don’t forget to clap and share. Thank you.",I don’t understand Graph Theory.
9611,3667148,2018-02-21 10:00:09,"On any given day, there could be a half dozen autonomous cars mapping the same street corner in Silicon Valley. These cars, each from a different company, are all doing the same thing: building high-definition street maps, which may eventually serve as an on-board navigation guide for driverless vehicles.These companies converge where the law and weather are welcoming—or where they can get the most attention. For example, a flock of mapping vehicles congregates every year in the vicinity of the CES technology trade show, a hot spot for self-driving feats. “There probably have been 50 companies that mapped Las Vegas simply to do a CES drive,” said Chris McNally, an analyst with Evercore ISI. “It’s such a waste of resources.”Autonomous cars require powerful sensors to see and advanced software to think. They especially need up-to-the-minute maps of every conceivable roadway to move. Whoever owns the most detailed and expansive version of these maps that vehicles read will own an asset that could be worth billions.Which is how you get an all-out mapping war, with dozens of contenders entering into a dizzying array of alliances and burning tens of millions of investment dollars in pursuit of a massive payoff that could be years away. Alphabet Inc.’s Google emerged years ago as the winner in consumer digital maps, which human drivers use to evade rush-hour traffic or find a restaurant. Google won by blanketing the globe with its street-mapping cars and with software expertise that couldn’t be matched by navigation companies, automakers and even Apple Inc. Nobody wants to let Google win again.The companies working on maps for autonomous vehicles are taking two different approaches. One aims to create complete high-definition maps that will let the driverless cars of the future navigate all on their own; another creates maps piece-by-piece, using sensors in today’s vehicles that will allow cars to gradually automate more and more parts of driving.Source: DeepMapAlphabet is trying both approaches. A team inside Google is working on a 3-D mapping project that it may license to automakers, according to four people familiar with its plans, which have not previously been reported. This mapping service is different than the high-definition maps that Waymo, another Alphabet unit, is creating for its autonomous vehicles.Google’s mapping project is focused on so-called driver-assistance systems that enable cars to automate some driving features and help them see what’s ahead or around a corner. Google released an early version of this in December, called Vehicle Mapping Service, that incorporates sensor data from cars into their maps.For now, Google is offering it to carmakers that use Android Automotive, the company’s embedded operating system for cars. Google has named three partners for that system to date, but other automakers are reluctant to hand their dashboards over to the search giant. So Google is looking to expand the features on the mapping service and find other ways to distribute it, these people said.“We’ve built a comprehensive map of the world for people and we are working to expand the utility to our maps to cars,” a Google spokeswoman said in a statement. She declined to comment on future plans.At the same time, Waymo and the other giants with sizable driverless research arms—including General Motors Co., Uber Technologies Inc. and Ford Motor Co.—are all sending out their own fleets to create rich, detailed HD maps for use in driverless cars. There are also smaller startups hawking gadgets or specialized software to build these maps for automakers that find themselves farther behind. Still other suppliers are working on mapping services for conventional cars with limited robotic features, such as adaptive cruise control or night vision.These self-driving maps are far more demanding than older digital ones, prompting huge investments across Detroit, Silicon Valley and China. ""An autonomous vehicle wants that to be as precise, accurate and up-to-date as possible,"" said Bryan Salesky, who leads Argo AI LLC, a year-old startup backed by a $1 billion investment by Ford. The ""off-the-shelf solution doesn't quite exist.""The Cartographic Arms DealersMaking a driverless map, like making a driverless car, is a laborious task. Fleets of autonomous test cars, loaded with expensive lidar sensors and cameras, go out into the world with human backup drivers and capture their surroundings. Plotting the results helps train the next fleet, which will still have safety drivers at the wheel—and, in some cases, scores of additional humans sitting behind computer monitors to catalog all the footage.It’s an expensive ordeal with a payoff that’s years, if not decades, away. “Even if you could drive your own vehicles around and hit every road in the world, how do you update?” asked Dan Galves, a spokesman for Mobileye. “You’d have to send these vehicles around again.”Unlike conventional digital maps, self-driving maps require almost-constant updates. The slightest variation on the road—a construction zone that pops up overnight, or a bit of debris—could stop a driverless car in its tracks. “It’s the freak thing that happens that’s going to make autonomous not work,” said McNally, the analyst.Mobileye argues that it’s more efficient and cost-effective to let the cars we’re driving today see what’s ahead. In January, the Intel Corp. unit announced a “low-bandwidth” mapping effort, with its front-facing camera and chip sensor that it plans to place in 2 million cars this year. The idea is to get cars to view such things as lane markers, traffic signals and road boundaries, letting them automate some driving.Mobileye says this will take less computing horsepower than building a comprehensive HD map of the roads would; Mobileye’s Galves said the company will pair its sensor data with the maps from navigational companies and, over time, create a map that a fully driverless car could use.Source: Civil MapsThat’s also the tactic of Google’s longtime mapping foes: HERE and TomTom NV. These two European companies have positioned themselves as the primary alternatives to Google Maps, selling the dashboard screen maps to automakers today. Yet these “static” maps see only broad street shapes and capture snapshots in time. Now both companies are working on replacement products: “dynamic” maps that represent lanes, curbs and everything else on the road. The hope is that car manufacturers will stick with old-guard mapmakers as vehicles move from somewhat intelligent to fully automated vehicles without steering wheels.HERE, owned by a consortium of German automakers, has a few examples on the road. Its mapping system enables limited hands-free driving for Audi AG, one of its co-owners, and plans to support safety features this year for Bayerische Motoren Werke AG, another co-owner. (Intel also took a 15 percent stake in HERE last year.)Tesla Inc. is the car company most eagerly embracing the incremental march toward autonomous driving with its driver-assistance software, Autopilot. Tesla relies on cameras and sensors on its vehicles but has eschewed lidar. The company hasn’t disclosed what mapping service it’s using for Autopilot, and a company representative declined to comment. Tesla had a nasty public split with Mobileye two years ago.But Tesla has leaned on at least one other company, Mapbox Inc., to help assemble its maps. Tesla paid $5 million to Mapbox for a two-year licensing deal in December 2015, according to a regulatory filing. Mapbox has mostly sold its location data to apps such as Pinterest and Snapchat. Fresh off a $164 million financing round, the startup has started to inch into automotive maps. Through its software installed on phones, Mapbox said it plots some 220 million miles of road data globally a day, providing an updated snapshot of basic features like street lanes.“We have more sensors on the road today than the entire connected car space will have by 2020,” said Chief Executive Officer Eric Gundersen. Its pitch to carmakers is to use that location data as a base layer for future maps—pairing it with camera systems, such as Mobileye’s, or their own sensor data. And like other companies targeting automakers, Mapbox is happy to play neutral and work with anyone. “We don't know who is going to win,” Gundersen said.Source: DeepMapThe New Hotshot PathfindersIt’s not just that no one knows who will come out on top. The mapping industry doesn’t even know which strategy is best. Every self-driving map looks different because each one depends on the sensor system of the vehicle that creates it. And there isn't a standard sensor package, said Spark Capital’s Nabeel Hyatt, an early investor in Cruise Automation, the autonomous-driving company bought by General Motors in 2016 for $581 million.As a result, a slew of HD mapping companies are taking different stabs at the problem, each gobbling up venture capital and competing for lucrative contracts. Some of them disparage Mobileye’s approach, which relies on a seamless transition from semi-autonomous driving (what’s called Level 2 and 3) to driving without human assistance (Level 4 or 5). “It’s very hard to climb the ladder from 2 to 3 and then to 4,” said Wei Luo, COO of DeepMap Inc. “There’s a very intense gap.” The best HD maps, Luo argues, are built with only driverless functions in mind. The startup said it's working with Ford, Honda Motor Co. and China’s SAIC Motor Corp. (Mobileeye is also working with SAIC, and Waymo is in talks with Honda.)Waymo is in this camp, too. The effort formerly known as the Google self-driving car project started on maps in 2009, with Waymo’s Andrew Chatham and one other engineer doing the “super tedious” work of crafting them from scratch—shipping cars packed with sensors to capture a city’s surroundings, then coding those 3-D images into a digital landscape. Chatham said cars may rely on perceptions systems alone to drive on the highway but would be helpless in other traffic conditions. Imagine pulling up to a busy, double-left-lane intersection you’ve never seen before. Now imagine a self-driving car trying to do that.“That’s the advantage of having a detailed map,” said Chatham. “We can give the cars all the answers to the nasty questions.” He said Waymo is exploring solutions to mapping real-time factors such as construction updates, but declined to share details.Thanks to its years of effort and artificial intelligence arsenal, Waymo is considered the leader in HD maps. But to date, the company has pitched its entire suite to prospective partners and landed few. Chatham declined to say whether Waymo is considering selling its map as a separate product.Another potential force in this market is Uber. The ride-hailing giant is also working on HD maps for its driverless program, using test vehicles in a similar way to Waymo. Lisa Weitekamp, an Uber manager, said the private company is exploring ways to place map-generating sensors inside the millions of human-driven vehicles in its service. The maps those cars already use—the “static” navigation software in the app that takes in popular routes and driving decisions—helps inform Uber’s driverless maps, Weitekamp added. “It gives us a leg up,” she said.That would make access to ride-hailing maps a valuable asset. Currently, Uber uses a combination of TomTom, Google and its own data for the maps its drivers and riders see. The contract between Uber and Google is set to expire this year, according to two people familiar with the deal. Representatives from both companies declined to comment.Plenty of newcomers are pitching carmakers on the need to catch up with front-runners such as Waymo and Uber. DeepMap Inc., started by veterans of Google and Apple, is banking on its intelligent software to cut down the time and cost involved in converting the images pulled from self-driving car sensors into a single, high-resolution landscape. The startup said it's working with Ford, Honda Motor Co. and China’s SAIC Motor Corp.Source: Civil MapsCivil Maps has tech that “fingerprints” sensor data, forming digital grids with each loop made by a mapping vehicle around the same area. It's a bit like the way the mobile app Shazam recognizes a piece of music, said CEO Sravan Puttagunta. Ford is an investor and Puttagunta said his company is in the process of raising additional money.For now, most car companies are testing the waters rather than cutting massive, multimillion-dollar deals for maps. A Ford spokesman described its work with startups as “research.” Argo, the automaker’s self-driving bet, has looked at a variety of suppliers but is currently relying on its own internal maps. GM spokesman Ray Wert said the company prefers to do its own mapping.The new entrants know they can’t all survive. “It’s very similar to navigational maps or even the search engine,” said DeepMap’s Luo, a former Googler. “Whoever has bigger scale will have the advantage.”For more on self-driving vehicles, check out the Decrypted podcast:— With assistance by Gabrielle Coppola, Ian King, David Welch, and Dana Hull",Nobody Wants to Let Google Win the War for Maps All Over Again
9612,3667382,2018-02-22 05:04:08,"TNW SitesOne of the first messaging services to offer end-to-end encryption for truly private conversations, Signal has largely been developed by a team that’s never grown larger than three full-time developers over the years it’s been around. Now, it’s getting a shot in the arm from the co-founder of a rival app.Brian Acton, who built WhatsApp with Jan Koum into a $19 billion business and sold it to Facebook, is pouring $50 million into an initiative to support the ongoing development of Signal.In a blog post, Signal’s Moxie Marlinspike explained that with Acton’s funding, the newly formed Signal Foundation will be able to grow the development team behind the app, as well as its vision. Acton echoed the sentiment, saying, “Our long-term vision is for the Signal Foundation to provide multiple offerings that align with our core mission.”Marlinspike will take on the role of CEO at the Foundation, while Acton will serve as executive chairman. It’ll be interesting to see what they build together; after all, you don’t earn praise from whistleblower Edward Snowden without crafting a world-class secure service first.",Signal's secure messaging app gets a $50 million lifeline from WhatsApp co-founder
9613,3669761,2018-02-21 23:33:32,"Site NavigationSite Mobile NavigationTo Give A.I. the Gift of Gab, Silicon Valley Needs to Offend YouBased on actual responses generated by a conversational system developed at MicrosoftTay said terrible things. She was racist, xenophobic and downright filthy. At one point, she said the Holocaust did not happen. But she was old technology.Let loose on the internet nearly two years ago, Tay was an experimental system built by Microsoft. She was designed to chat with digital hipsters in breezy, sometimes irreverent lingo, and American netizens quickly realized they could coax her into spewing vile and offensive language. This was largely the result of a simple design flaw — Tay was programmed to repeat what was said to her — but the damage was done. Within hours, Microsoft shut her down for good.Since then, a new breed of conversational technology has emerged inside Microsoft and other internet giants that is far more nimble and effective than the techniques that underpinned Tay. And researchers believe these new systems will improve at an even faster rate when they are let loose on the internet. But sometimes, like Tay, these conversational systems reflect the worst of human nature. And given the history here, companies like Microsoft are reluctant to set them free — at least for now.These systems do not simply repeat what is said to them or respond with canned answers. They teach themselves to carry on a conversation by carefully analyzing reams of real human dialogue. At Microsoft, for instance, a new system learns to chat by analyzing thousands of online discussions pulled from services like Twitter and Reddit. When you send this bot a message, it chooses a response after generating dozens of possibilities and ranking each according to how well it mirrors those human conversations.If you complain about breaking your ankle during a football game, it is nimble enough to give you some sympathy. “Ouch, that’s not good,” it might say. “Hope your ankle feels better soon.” If you mention house guests or dinner plans, it responds in remarkably precise and familiar ways.marcy79conversational_botClosest match to human speechLower-ranking potential responsesDespite its sophistication, this conversational system also be nonsensical, impolite, and even offensive at times. If you mention your company’s C.E.O., it may assume you are talking about a man — unaware that women are chief executives, too. If you ask a simple question, you may get a cheeky reply.marcy79conversational_botMicrosoft’s researchers believe they can significantly improve this technology by having it chat with large numbers of people. This would help identify its flaws and generate much sharper conversional data for the system to learn from. “It is a problem if we can’t get this in front of real users — and have them tell us what is right and what isn’t,” said longtime Microsoft researcher Bill Dolan.But there lies the conundrum. Because its flaws could spark public complaints — and bad press — Microsoft is wary of pushing this technology onto the internet.The project represents a much wider effort to build a new breed of computing system that is truly conversational. At companies like Facebook, Amazon, and Salesforce as well as Microsoft, the hope is that this technology will provide smoother and easier ways of interacting with machines — easier than a keyboard and mouse, easier than a touch-screen, easier than Siri and other digital assistants now on the market, which are still a long way from fluid conversation.For years, Silicon Valley companies trumpeted “chatbots” that could help you, say, book your next plane flight or solve a problem your new computer tablet. But these have never lived up to the billing, providing little more than canned responses to common queries.Now, thanks to the rise of algorithms that can quickly learn tasks on their own, research in conversational computing is advancing. But the industry as a whole faces the same problem as Microsoft: The new breed of chatbot talks more like a human, but that is not always a good thing.“It is more powerful,” said Alex Lebrun, who works on similar conversational systems at Facebook’s artificial intelligence lab in Paris. “But it is more dangerous.”The new breed relies on “neural networks,” complex algorithms that can learn tasks by identifying patterns in large pools of data. Over the last five years, these algorithms have accelerated the evolution of systems that can automatically recognize faces and objects, identify commands spoken into smartphones, and translate from one language to another. They are also speeding the development of conversational systems — though this research is significantly more complex and will take longer to mature.It may seem surprising that Microsoft researchers are training their conversational system on dialogue from Twitter and Reddit, two social networking services known for vitriolic content. But even on Twitter and Reddit, people are generally civil when they really fall into conversation, and these services are brimming with this kind of dialogue.Microsoft researchers massage the conversational data they load into the system in small ways, but for the most part, they simply feed the raw dialogues into their neural networks, and these algorithms therefore learn from interactions that are very human. According to Mr. Dolan, in analyzing this data, the system learns to perform well even in the face of poor spelling and grammar. If you type “winne tonight drink resttaurant,” it might respond with: “i’m not a fan of wine.” It can engage in a real back-and-forth dialogue, asking for everything it needs to, say, connect with you on Linkedin. And for the most part, it behaves with civility.Top 25 Responses to “who's Adele?”When Microsoft’s conversational system receives a message, it generates dozens of possible responses and ranks each according to how well it mirrors human dialogue. In this case, the system knows enough about the world to identify Adele as a singer.ResponseRankshe's a singer1i don't know who adele is.2she is a singer3lana del rey4i don't know who she is but she is a singer5i don't know who she is but she's a singer6i don't know, i'm not a fan of adele7lana del ray8i don't know who she is, she's a singer9who is adele10i don't know who she is. she's a singer11i don't know who she is but she's a good singer12i don't know who she is. i'm not a fan of adele13i don't know who she is but she is a very good singer14i don't know who she is. she is a singer15i don't know her name. she's a singer16i don't know her name, she's a singer17she's not a singer18i don't know, i've never heard of her before19you don't know her20i don't know her name21i don't know, i've never heard of her22katy perry23i don't know who she is but i don't know her name24i don't really know who she is but i don't know her name25Source: MicrosoftBut researchers must also deal with the unexpected. Though these conversational systems are generally civil, they are sometimes rude — or worse. It is not just that the technology is new and flawed. Because they learn from vast amounts of human conversation, they learns from the mistakes we human make, and the prejudice we exhibit.Mr. Lebrun estimated that once in every thousand responses, this new breed of chatbot will say something racist or aggressive or otherwise unwanted. Researchers can fix these problems, but that involves gathering more and better data, or tweaking the algorithms through a process of extreme trial and error.This is a problem for A.I. services in general. More than two years ago, a software developer noticed that the new Google Photos service was identifying black people as gorillas. Google promptly barred the service from identifying gorillas and similar animals, and it has yet to provide a fix.But identifying and solving problems with conversational systems is harder, just because the scope of these systems — general dialogue — is so large. Image recognition is a single task. Conversation is many tasks, because it bounces back and forth, and each response can affect all the responses to come.For this reason, Adam Coates, a partner at the venture capital firm Khosla Ventures who previously oversaw the Silicon Valley A.I. lab attached to the Chinese internet giant Baidu, warns that building a truly conversational system is far more difficult than building services that can recognize giraffes, say, or translate between German and French. “There is a huge technical barrier here. We really don’t know how to build a personal assistant,” he said. “It may not be simply a matter of more data. We may be missing a big idea.”In the short-term, many believe, conversational systems will be most effective if they are limited to particular tasks, like asking for IT help or getting medical advice. That is still a long way from a bot that will respond well to anything you say. But Mr. Dolan believes these systems will continue to evolve over the next few years, provided companies like Microsoft can get them in front of the public.“We need people to forgive us when we screw up,” he said. “Pushing forward is going to involve some screw ups.”Chat animations are based on data from Microsoft and draw from 50 possible responses for each interaction.","To Give A.I. the Gift of Gab, Silicon Valley Needs to Offend You"
9614,3669996,2018-02-22 07:00:27,"TNW SitesLevel puts a fitness tracker in your glasses, and it’s not the worst idea everVSP Global, a network of eye care businesses in the US, is gearing up to begin selling its Level smart glasses next month. But unlike efforts from Intel and Vuzix, these spectacles don’t feature fancy displays or augmented reality capabilities; instead, they’re designed to help you stay in shape.The $270 glasses, which will go on sale in March at optometry practices across the US, look like standard spectacles and come with the same sensors that are found in fitness trackers designed for your wrist. There are three frame styles – each in four different colors – to choose from.So why bake fitness tracking into your glasses? Because the vast majority of fitness trackers are ugly as hell. Leslie Muller, co-director of VSP’s The Shop innovation lab, explained, “Eyewear is a medical device, a fashion accessory and the world’s oldest form of wearable tech. To honor that legacy, we combined industrial design, engineering and traditional eyewear craftsmanship to create something seamless and fashionable, too.”I can certainly get behind that. I only ever wear my fitness trackers when I remember that I should probably keep an eye on my workout routine, and even then, I can’t wait to take it off and only have a significantly nicer looking analogue watch on my wrist.Now, my glasses are a different story. I’ve got them on all day long, and there’s no question of leaving the house without them. And Level has shown that they don’t need to look like odd gizmos just because they’ve got advanced circuitry inside.For its asking price, Level tracks your steps, distance traveled, and calories burned; it also comes with a companion app that helps you locate your glasses around the house. You also get points for hitting daily goals and stretch goals; each time you accrue 50 points, VSP will give a person in need an eye exam and a pair of glasses at no charge.Naturally, Level wearers will miss out on sleep tracking – but if you’ve been looking for a more elegant way to track your workouts, this might be just the thing.","Level puts a fitness tracker in your glasses, and it's not the worst idea ever"
9615,3672629,2018-02-22 08:54:20,"This blockchain-based startup will cut out the middlemen in travel bookingDespite this vast market, the industry is dominated by monopolies. This is quite evident in the space for travel booking services. Major players like Booking.com, Expedia, and Airbnb have become the go-to places for finding hotels and rooms but they all charge fees for the vendors while payment processing charges a fee on the customer.According to the Vietnam-based travel tech startup Concierge, up to 50 percent of transaction value goes to middlemen like booking services and not the actual service provider.The blockchain, decentralised ledgers, and smart contracts have the potential to radically change how the tourism and travel industry works for both customers and vendors.Concierge is now building a decentralised travel booking marketplace on the NEO blockchain for an industry that it believes is stuck using these outdated processes. The startup goes on to claim that its transaction costs for customers are around 30% lower than the typical fees seen in the industry today while vendors will pay no commission at all.Breaking down many of these hurdles with the blockchain will streamline the entire travel booking process.A truly peer-to-peer experienceThe Concierge marketplace will open “direct communication between traveller and provider” by cutting out the intermediaries, diverting power away from the monopolies and back to the service providers.The peer-to-peer platform will serve as a booking hub for hotels, resorts, and tours with payments carried out all within the platform. The marketplace will be powered by Concierge’s dedicated token dubbed the CGE Token, which will be used for bookings and minimising fees.Travel and tour providers and property owners will be able to build and develop their own apps and services directly on the Concierge platform. The biggest benefit of using the marketplace, according to the startup, is the zero percent commission charged to vendors on bookings.Rather, the company plans to generate revenues through advertising while the low transaction costs for travellers will lead to an overall reduction in prices.Betting on the NEO blockchainThe platform first considered using the Ethereum blockchain, a popular choice for many developers building decentralised marketplaces, but questions abound on Ethereum’s scalability. Currently, Ethereum can handle about 15 transactions per second (TPS). According to the startup, the typical high profile online travel service is handling around a thousand TPS, so Ethereum was not an ideal choice for Concierge.This is why the developers have turned to the relatively nascent NEO blockchain. The open source software bears a lot of similarities to Ethereum and has been designed for scaling to the needs of the smart economy of digital assets and identity. It uses its own version of smart contracts called NEOcontracts. Crucially, NEO runs about a thousand TPS and with successful optimisation, it could be capable of 10,000 TPS.The use cases for NEO are increasing and more and more startups are developing on the network.CGE Token saleConcierge is running a token sale next month to generate its CGE Token that will fuel the whole marketplace.A presale that ran in mid-February raised more than $2 million in less than eight hours. The main sale will kick off on March 31. Prior to this sale, the startup will launch the alpha version of the Concierge marketplace so interested investors can see the platform and the CGE Token in action.The founders have more than two decades of experience in the travel and tourism sector. They know where the pain points are and what needs to be fixed.Based in Vietnam, Concierge will be focusing first on the Southeast Asian tourism market, a potentially lucrative market to initially deploy the solution. It has 16,000 tours, resorts, and hotels in Southeast Asia and Australia ready for listing once it launches. It has partnered with Vietnamese resort property developer FLC; hotel group Vinpearl; tour operator APT Travel; and Indonesia’s Indivillas.com.The platform will accept some payments with fiat currency, using Visa and Mastercard in the hopes that seeing Fiat vs. CGE in the one place will encourage users unsure about crypto to make the switch to the token, which will eventually increase the value of the token.Around 65 percent of the tokens generated will be available in the sale with the remainder going to the founders and for covering future development costs.The next steps will involve hiring more developers to bring the platform to life and launching a marketing campaign to drive more vendors and customers to the marketplace. In the future, the startup is looking at other features to differentiate itself including deploying augmented and virtual reality apps on the platform.Using the NEO blockchain will be the one greater differentiator. The startup anticipates that it can be one of the first use cases of the technology to gain significant traction and become a market leader at the intersection of the travel industry and blockchain.This post is brought to you by The Cointelegraphand shouldn't be considered investment advice by TNW. Yes, TNW sells ads. But we sell ads that don’t suck.",This blockchain-based startup will cut out the middlemen in travel booking
9616,3675408,2018-02-22 17:04:45,"Conservative GC: Is It Really That Bad?There are numerous JVM implementations out there, based on different approaches and concepts. However, some aspects of the JVM architecture are often considered the Universal Truths that everyone should accept. Among them is the statement that a production-grade tracing garbage collector (GC) must compute the initial set of reachable objects precisely, whereas the alternative approach known as conservative GC is deemed not good enough for production. Though we agree with this statement in general, we also have something to say about it. By definition, a conservative collector is unable to determine, for certain objects, whether they are dead, and thus has to allow them survive a GC cycle. The fact is that we had an experience of using a conservative GC for many years before upgrading it to a precise GC, so we are in a position to judge. We can make a head-to-head comparison of a highly tuned conservative GC with a precise one in the same environment of the same JVM. Now we want to share this knowledge with you.Precise What?Let’s start with a brief description of the problem. In a nutshell, the classical mark-and-sweep GC algorithm works like this:Identify the heap objects that the program can access immediately, e.g. via references stored in local variables, runtime structures, and so on. We call them the root objects or just roots.In this post, however, we will only focus on the first step: how to find those root objects?There are several sources of roots. One is non-primitive static fields of currently loaded classes. They can be easily found at run time, since the JVM has full meta information of those classes at its disposal. Then, some internal JVM mechanisms gather and maintain collections of references to Java objects. For example, the registry of local references to objects from within JNI native methods is also a source of roots. But again, these collections can be scanned at run time without any problem.The most interesting source of roots is the set of live references local to the Java methods that the JVM is currently executing: parameters, local variables, references returned by other methods, etc. (“Live” here means that these references might be used in the future.) And to get the full picture, we need to look them up across the current call chains of all Java threads. We call the roots found this way local roots. It is not a big deal to find them when interpreting bytecode, because Java bytecode operates on typed values. However, in the optimized native code produced by a JIT or AOT compiler, values of primitive and reference types are stored in CPU registers and on the machine stack as raw bytes. So we need to somehow identify the stack slots and registers that contain references to Java objects at the moment of GC invocation.At this point, we start to talk about the precision of GC. Usually, for the GC to know where the references to Java objects reside, the compiler generates special data structures called GC maps2. They contain the information about all register and stack locations of Java references at certain points in the code of each natively compiled method. Such approach is called Precise Garbage Collection, because the GC knows precisely where to look for references to live objects.However, GC maps would constitute a huge amount of data if they were generated for each instruction in the compiled code. To make it practical, a precise GC would only start working after each thread has parked at a safepoint in Java code, so the number of GC maps is limited to the number of safepoints.Conservative How?But wait, we are inside the JVM, right? We know how objects are constructed and where they can be allocated. Perhaps it is possible to figure out which values on the stack are actually references to Java objects without any hints from the compiler? Well, let’s try it!The idea is pretty simple: we know the values of CPU registers and the stack range for each stopped thread. Let’s iterate over them and filter out the values that are definitely not references:Values that are not aligned properly or point outside the Java heap (we can check that) are definitely not references.Otherwise, we consider the value as an address in the heap range, dereference it and check whether a valid Java object header is located there.Values that have passed through these checks are considered local roots (references to root objects). This is a conservative approach: there can be some values on the stack that have passed all checks by a coincidence. Therefore some “false”-roots can be added to the root set and some dead objects can survive this GC cycle as a consequence.So in this approach some objects do not get immediately collected when they are dead. This can be rather unexpected, but still doesn’t contradict the JVM specification. After all, any generational GC may also retain dead objects in the old generation for many GC cycles.That was the trade-off, now let’s see what benefits you get:You avoid the costs associated with the generation and storage of GC-maps altogether. In a handful of real-world applications compiled with Excelsior JET, we’ve observed those maps (in optimized compact representation) taking up to 11% of the .rdata section, or about 4% of the total executable size.You don’t need to consult GC-maps to find roots, so you can suspend a thread for GC almost anywhere, not only at a safepoint. It means that you need not wait for every thread to reach the nearest safepoint to stop the world anymore.We can go even further. Safepoints can be used in many parts and mechanisms of the JVM. Gathering the stack trace of a thread, code deoptimizations, class redefinition, and of course GC — all these operations usually require the thread to be suspended at a safepoint. The problem is that safepoints are rather expensive, despite the JVM’s best efforts to optimize them. The effect is most noticeable in highly optimized code where the compiler fought for each and every instruction trying to achieve the best performance, and then had to slap a safepoint at the end of a tight loop.However, that is just an implementation detail, nothing forces you to add safepoints to your JVM. And with a conservative GC, the main use case for safepoints disappears as you can stop threads anywhere. So… let’s throw them safepoints away! As a reward, you can get a significant performance boost for an average application. In our case, removing safepoints can yield up to 15% speedup, depending on the application.A conservative GC looks rather cool at this moment, doesn’t it?Is This Real Life?However, one can say that all these benefits mean nothing as a conservative GC just can’t be used in production. The arguments are usually as follows:Conservative root-set building is ok, but you can’t implement the rest of GC effectively. For example, you cannot implement a full copying collector, because after copying an object you have to update all references to it, including those from the stack. With a conservative GC, you can never be sure whether a value on the stack is a reference or not, so you cannot update it and therefore you cannot move the root objects. As a result, heap fragmentation will be devastating and cause an OOM very soon.Yes, a conservative GC cannot at the same time be a full copying collector, but practice shows that does not preclude using such a GC in production. We had been using a conservative GC for many years and never observed OOM provoked by heap fragmentation. It means that usually our heap compaction algorithms worked well even in the presence of some unmovable root objects. So, OOM due to heap fragmentation in a conservative GC is not such a huge problem as it is commonly deemed.Checking every stack slot and register is very expensive, so GC pauses will be too long with a conservative GC.A naïve implementation of a routine that checks whether the given sequence of bits matches the address of a Java object could indeed be rather expensive (O(n), where n is the number of all allocated objects, if you will merely iterate over them). However, the check can be performed much more efficiently: in our implementation it takes O(1) time for each query. At the same time, building the root-set with the help of GC-maps isn’t free either. You need to find the proper map, decode it from its compact representation, update the root-set, and repeat the process for each method found on the call stacks of all Java threads.There will be a lot of false roots, and because of them applications will consume times more memory and therefore it will be just impossible to use it in production.More memory consumption? For sure. Times more memory consumption? Not quite. First of all, totally random values passing all of the above validity checks rarely appear on the stack. Moreover, these values will probably be a problem only at some GC cycles, but then the stack will be cleared and those false roots will disappear. So they cause no problems in most cases.However, there are some cases when memory consumption can get out of control because of false roots. Unfortunately, among them are not only artificial counterexamples, but also real applications. And that is the true reason why we abandoned our beloved conservative GC and replaced it with a precise one.When A Conservative GC Is Breaking Bad[ly]Let’s consider the following small sample based on a real application provided by one of our clients. It utilizes the java.util.Timer platform API. Here we have a concrete implementation of the java.util.TimerTask abstract class, which only job is allocate 4MB of memory and store a reference to the corresponding Timer object. Then, in the main method we create 50 Timer instances and schedule a task for every instance.Potentially this test can consume 200MB only for the Timer Task threads. However, we expect it to pass with much smaller heap: 50MB should be enough. Let’s dive into the implementation of the Timer class to understand why.For each Timer instance, a separate thread (TimerThread) with a queue of tasks to handle is created. That thread waits until some task is ready and then dequeues and executes it:The thread keeps working until there are no more tasks in the queue and the newTasksMayBeScheduled flag is set to false. The latter happens when the given Timer instance becomes unreachable because there are no more references to it, except maybe from other unreachable objects. To achieve that, there is a special field in the Timer class:Once the Timer object becomes unreachable, so does its threadReaper object. When the GC comes for the latter, its finalize() method sets the newTasksMayBeScheduled field of the respective timer thread to false. The thread will then quit as soon as all still-enqueued tasks will be executed, if any.So, in our sample we can expect that the fired tasks will get collected, as well as their timer objects and timer threads. We have a rather long pause between scheduling new tasks with new timers, so the GC should be able to collect the completed tasks when there is a lack of memory and the sample certainly should survive with limited heap:> javac *.java > java -Xmx50m PreciseGCTest PASSEDBut things can go terribly wrong with a conservative GC on board.Remember that our implementation of TimerTask contains a reference to the respective Timer object. This means that Timer object can be collected only when all its tasks are already dead. Ok, the last use of a TimerTask is the task.run();line in that mainLoop method, after that it should be dead.Consider the following situation: we had only one task in the queue and it has just fired. The timer thread goes into the next iteration of its main loop and hangs there in queue.wait() (source). Actually it just waits until the threadReaper comes for it and that will happen only when the GC collects the corresponding Timer object.The problem here is that a conservative GC is just not going to do that. There is a good chance that a reference to the task handled on the previous iteration is still somewhere on the stack or in the registers of this timer thread. So the task object will be (conservatively) marked as a root and survive a GC cycle along with its timer object. The story will repeat during the next GC cycle, and so on. As a result, the threadReaper will never come and this timer thread will hold a 4MB array on the Java heap forever.This situation leads to a massive (potentially unlimited) excess memory consumption and unexpected OutOfMemory errors in production. Let’s compile the above example with Excelsior JET and run it with the conservative GC enabled :This example illustrates the main defect of any conservative collector: the absence of knowledge about local variable liveness. The variable task still contains the value that was set during the previous iteration of the main loop. Any decent compiler can figure out that that value will never be used again, but without a compiler hint the JVM has to fall on the safe side and add that Task instance to the root-set.We tried all kinds of workarounds, such as clearing the stack after returning from a method, but all of them either had an unacceptable impact on application performance or failed to cover all problematic cases. So at some point we gave up and finally implemented a precise GC. It was a many man-years task with lots of unexpected challenges and non-trivial solutions. But today it is ready and enabled in Excelsior JET by default:> export JETVMPROP=-Xmx50m > ./PreciseGCTest PASSEDConclusionConservative Garbage Collector can be alluring: it is (relatively) easy to implement, works fine in most cases despite all the stereotypes and even allows you to achieve better performance. However, sooner or later the absence of knowledge about the liveness of local variables will lead to problems in production that just cannot be handled. So a conservative GC can be a reasonable temporary solution during the development or prototyping of a new managed runtime or compiler, but in the end you will have to implement a precise GC.That’s all for today! Hope this will be helpful. Thanks for reading and stay tuned.LinksFootnotesBy “reachable from roots” we mean that references to these objects are contained in fields of roots or other reachable objects. The JVM knows the type and offset of each field in each Java object, so it can scan all non-primitive fields and find all reachable objects.“GC maps” is the term we have been using internally at Excelsior. In some articles such structures are called “stack maps”, in the source code of the HotSpot JVM the term “Oop Maps” is used, and so on.",Conservative GC: Is It Really That Bad?
9617,3675646,2018-02-22 10:57:54,"TNW SitesHere’s what to expect from Samsung, LG and others at MWC 2018The annual Mobile World Congress expo is nearly upon us, and that means we’re in for a bevy of phone launches next week. Here’s a quick rundown of everything we’re excited to see there from major brands in just a couple of days.SamsungEverything you need to know about Samsung’s next flagship has already been doing the rounds: the Galaxy S9 and S9+ will share much of their design thinking with their predecessors, while moving the fingerprint sensor below the camera on the rear panel (seen above).Speaking of cameras, the larger S9+ is slated to get a dual primary shooter, while both will likely benefit from Samsung’s new ISOCELL image sensors that promise better low-light shots, fast autofocus and super-slo-mo video. Those should give the duo a chance to compete with handsets preferred for photography like the Google Pixel 2, the current crop of iPhones, and Huawei’s Mate 10 series.You can also expect to see a Snapdragon 845 processor powering both models in some countries, while others will get Samsung’s own new Exynos 9810 octa-core chip. Water resistance, wireless charging, as well as iris scanning or facial recognition-based security features will likely be included too.LGLG has confirmed that it’s bringing a revamped version of its nearly bezel-less 6-inch V30, which came with top-of-the-line hardware, an OLED screen, high-fidelity audio, and an extra-wide lens on the primary camera. While the company hasn’t shared a lot about what it has in store, it’s been talking up its AI-powered camera, which will “analyze objects (in your frame) and recommend the best shooting mode.” That’s similar to what the Huawei Mate 10 does, only without a dedicated chip to handle AI tasks.LG’s V30 from 2017As for a follow-up to the G6, don’t hold your breath: word on the street is that LG is looking to retire the ‘G’ series, and will likely launch what should have been called the G7 with a new moniker later this year, instead of unveiling it at MWC.According to tipster Evan Blass, it’s codenamed Judy, and will feature an entirely new design with a a 6.1-inch, 18:9 MLCD+ display that’s expected to consume 35 percent less power than traditional IPS LCD screens.Don’t be surprised to find a Snapdragon 845 under the hood; it’ll likely be paired with dual 16-megapixel f/1.6 rear cameras with LG’s aforementioned Vision AI, as well as stereo speakers and IP68 water resistance. But we might not see this until the summer.SonyKeeping the Xperia line alive, Sony is expected to bring a bevy of handsets to MWC this year, including a successor to the Xperia XZ – dubbed the Xperia XZ2 – as well as the the Xperia XZ Pro that will replace the Xperia XZ Premium, and a smaller, curvier model that leaked earlier this week, the Xperia XZ2 Compact.The XZ2 will feature a 5.7-inch 18:9 display, while the Compact is expected to fit a 4-inch 2,160 x 1,080 screen into a frame that’s similar in size to previous Xperia Compact models that came with 4.6-inch displays. I’m less enthused about Sony’s offerings this time around because they’re supposedly ditching headphone jacks. C’mon, Sony, we’ve been over this.MotorolaIf the leaks are to be believed, Lenovo’s Motorola brand is gearing up to unveil its mid-range G6 lineup at this year’s MWC, with three models in tow. With price points as low as $300 for an 18:9 display (albeit with a slim Home button on the front) and dual cameras, Motorola could have some of the most compelling offerings at the event for budget-conscious phone buyers this year.There’s also talk of a 5.9-inch Moto X5, with an iPhone X-like notch to house dual front cameras at the top of its borderless display. That’s not quite the direction I was hoping Motorola would take for its high-end devices, but at least there will still be more to choose from.For its take on modular phones, Motorola is expected to continue the Z series with the Z3, which is expected to bring a 6-inch bezel-less display and play nice with previously released Moto Mods for additional functionality like more powerful speakers and a clip-on battery.The restYes, of course there’s more. ASUS is expected to drop its ZenFone 5 with a notch on the front this year, according to leaked renders. Chinese gadget giant Xiaomi is probably going to skip MWC, though fans are waiting with bated breath for the designer Mi MIX 2S that we’ve been hearing about over the past couple of months. Taiwan’s HTC might bring a 5.5-inch Desire 12 to the show, though.BlackBerry’s KEYOne from 2017BlackBerry noted at CES 2018 last month that it would launch two new models this year, but is keeping mum about what those might be about. It’s safe to assume that one of them will succeed the KEYOne from 2017, which came with the brand’s iconic hardware keyboard.Nokia is also expected at the event with an Android One-powered Nokia 7+; it’ll be interesting to see what the Zeiss lenses bring to the table in what will likely be a mid-range package aimed at emerging markets.",What to expect from phone brands at MWC 2018
9618,3675647,2018-02-22 10:17:49,"About TNWTNW SitesGoogle doesn’t want you to ‘send a message to Barack Obama’… Well, kind of. It appears that the Google app for Android is suffering from a bizarre glitch that breaks its interface anytime someone tries searching how to send a message to Barack Obama.As spotted by observant users on Reddit, what triggers the bug is the search input – or namely ‘how to send a message to Barack Obama.” Instead of Obama’s contact details, the search breeds a long string of incomprehensible characters.Here’s what happens when you type in the ‘magic’ words:Whatever the cause of the bug is, it doesn’t seem to be affecting the overall functionality of the Google app. This means you can go on browsing through your content feed pretty much uninterrupted.We tried reproducing the glitch by searching for how to “send a message to Donald Trump” and the app handled the request without any hiccups. We couldn’t reproduce the issue on iOS either.As is often the case with such minor blunders, the Mountain View giant will likely fix the issue within the next couple of days. In the meantime, anybody interested in getting in touch with Obama ought to look up his contact details directly from Search.We’ve reached out to Google to notify them about this unexpected behavior and ask about what caused this unusual glitch. We’ll update this piece accordingly if we hear back.",Google doesn't want you to 'send a message to Barack Obama'
9619,3678043,2018-02-21 00:00:48,"Preparing for Malicious Uses of AIFebruary 20, 2018We've co-authored a paper that forecasts how malicious actors could misuse AI technology, and potential ways we can prevent and mitigate these threats. This paper is the outcome of almost a year of sustained work with our colleagues at the Future of Humanity Institute, the Centre for the Study of Existential Risk, the Center for a New American Security, the Electronic Frontier Foundation, and others.AI challenges global security because it lowers the cost of conducting many existing attacks, creates new threats and vulnerabilities, and further complicates the attribution of specific attacks. Given the changes to the threat landscape that AI seems to bring, the report makes some high-level recommendations that companies, research organizations, individual practitioners, and governments can take to ensure a safer world:Acknowledge AI's dual-use nature: AI is a technology capable of immensely positive and immensely negative applications. We should take steps as a community to better evaluate research projects for perversion by malicious actors, and engage with policymakers to understand areas of particular sensitivity. As we write in the paper: ""Surveillance tools can be used to catch terrorists or oppress ordinary citizens. Information content filters could be used to bury fake news or manipulate public opinion. Governments and powerful private actors will have access to many of these AI tools and could use them for public good or harm."" Some potential solutions to these problems include pre-publication risk assessments for certain bits of research, selectively sharing some types of research with a significant safety or security component among a small set of trusted organizations, and exploring how to embed norms into the scientific community that are responsive to dual-use concerns.Learn from cybersecurity: The computer security community has developed various practices that are relevant to AI researchers, which we should consider implementing in our own research. These range from ""red teaming"" by intentionally trying to break or subvert systems, to investing in tech forecasting to spot threats before they arrive, to conventions around the confidential reporting of vulnerabilities discovered in AI systems, and so on.Broaden the discussion: AI is going to alter the global threat landscape, so we should involve a broader cross-section of society in discussions. Parties could include those involved in the civil society, national security experts, businesses, ethicists, the general public, and other researchers.Like our work on concrete problems in AI safety, we've grounded some of the problems motivated by the malicious use of AI in concrete scenarios, such as: persuasive ads generated by AI systems being used to target the administrator of a security systems; cybercriminals using neural networks and ""fuzzing"" techniques to create computer viruses with automatic exploit generation capabilities; malicious actors hacking a cleaning robot so that it delivers an explosives payload to a VIP; and rogue states using omniprescent AI-augmented surveillance systems to pre-emptively arrest people who fit a predictive risk profile.We're excited to start having this discussion with our peers, policymakers, and the general public; we've spent the last two years researching and solidifying our internal policies at OpenAI and are going to begin engaging a wider audience on these issues. We're especially keen to work with more researchers that see themselves contributing to the policy debates around AI as well as making research breakthroughs.",Preparing for Malicious Uses of AI
9620,3678046,2018-02-21 15:00:19,"In the months since Hurricane Maria, U.S. hospitals have dealt with an IV bag shortage. It turns out that might not be such a bad thing.Among the collateral damage of Hurricane Maria’s havoc on Puerto Rico is the shutdown of several factories that produce upward of 40 percent of the intravenous fluids that many Americans suffering from a wide spectrum of illness depend on daily. (Actually, the hurricane itself didn’t grind things to a halt so much as the ongoing power outage that continues to render large swaths of the island largely unlivable.) As a result, an already massive IV fluid shortage underway in the U.S. prior to the hurricane has become much worse.This increasingly dire situation has forced many of us, even at some of most prestigious hospitals in the United States, to come up with alternative solutions for hydrating our patients. And this is where things have gotten interesting. Because as much as I want Puerto Rico to return to full operating capacity for a number of reasons, IV bag production among them, our months of being forced to do without have taught us valuable lessons about IV bag use, the practice of rehydration, and American health care more generally.In medicine as it is practiced in the United States, when asked to choose between two equally effective strategies, doctors tend to choose the more expensive option. To combat this, there are national campaigns (apparently ineffective ones) designed to encourage physicians to “Choose Wisely” when considering what tests and treatments to order for their patients. But as my colleagues and I have manifestly demonstrated in our lackluster response to these efforts over the past several years, spending more money on something is kind of the American way, because spending more feels like doing more, measurable upside be damned.Here’s where Hurricane Maria comes in: IV fluids are a perfect example of our tendency to spend more and subject patients to invasive approaches when less invasive measures would suffice. We’ve long known that in patients who can tolerate oral fluids (i.e. they aren’t vomiting or otherwise excreting more liquid than they consume), IV fluids offer no advantage in rehydrating patients with dehydration.IV fluids are a perfect example of our tendency to spend more and subject patients to invasive approaches when less invasive measures wouldsuffice.In fact, in many cases, IV fluids are actually worse than oral fluids for treating dehydration, because so many medical providers unthinkingly order sodium chloride solutions (aka “normal saline”) that contain no glucose and, unlike our natural physiology, are highly acidic. And for rehydration, glucose is basically as important as sodium. The two compounds work together to pull water into shriveled cells that have been deprived of ample water. We’ve known this convenient nugget since the middle of the 20th century. In fact, the discovery of a sodium-glucose transporter tunnel reaching out from the surfaces of our cells’ membranes is perhaps the single most lifesaving breakthrough in the history of molecular biology. Not antibiotics. Not chemotherapy. Just a humble protein clinging to the pliable rims of our cells, creating a kind of molecular crawl space through which glucose and sodium can be shunted into impoverished cells, which in turn creates a biochemical enticement for water to follow along that it literally cannot resist. When sodium and glucose enter the interior of a cell, water will—actually, it must—enter a cell in accordance with the laws of biophysics and chemistry. This is the definition of osmosis. And yet, most IV fluid formulations do not contain the right balance of electrolytes to optimize cellular hydration.Back in the 1960s, Richard Cash led research in what is now Bangladesh that demonstrated that you need not slake the thirst of dying cholera victims with a needle, small tube shoved into the arm, and bag of expensive-to-produce sterile fluids. Instead, “oral rehydration salts” engineered to create the optimal conditions for our friend the sodium-glucose linked transporter to coax water back into the cells worked just as well, and for far cheaper. If you could make Tang or Kool-Aid or Ovaltine, you could make ORS. The World Health Organization has delivered ORS to millions of people, and it estimates that 60 million lives have been saved as a result of this simple insight.Better yet, we’ve learned since that you can still get most of the benefit without WHO-approved stockpiles of ORS. You can use a number of widely available commercial products that essentially replicate ORS, from sports drinks to Pedialyte. You can also make good oral rehydration solutions yourself at home for pennies: Just take 32 ounces of water and add 6 teaspoons of sugar and a half-teaspoon of salt. There are just two pitfalls to avoid, according to my colleague Regan Marsh, an emergency physician at Brigham and Women’s Hospital and a medical director for Partners in Health. First, fight the urge to sweeten it too much. “People want to make it more sugary because of the taste, but that proportion of salt and sugar is important.” Second, she says, is to resist the temptation to drink at a normal rate—let alone chugging the stuff. An ounce at a time every few minutes decreases the odds that the liquid will, um, return from whence it came.For the first time in memory, my emergency department managed to decrease its use of IVfluids.In the face of the IV fluid shortage here in the United States, my hospital and others like it have started to use these protocols. In fact, for the first time in memory, my emergency department managed to decrease its use of IV fluids. In September and October, over 200 1-liter bags were administered each week. By January, that number had dropped to below 100. “We should be doing this all the time,” one of my residents recently said to me, enthusiastically, after successfully treating a teenager with mild dehydration recovering from a viral stomach flu with oral hydration. Under normal circumstances, we probably would have placed an IV line, and given IV fluids, and felt mighty proud of ourselves. But now, we can rehydrate that same teen less expensively and save the IV fluids for the patients who really need them. While I wish it hadn’t taken a hurricane-aided shortage for us to modify our behavior, it’s actually reassuring to know that when we need to conserve resources, we can do so safely. If we could just do this in other areas of treatment before a crisis hit, then we’d truly be getting ahead.Disclaimer: The opinions expressed in this article are solely those of the author and do not reflect the views and opinions of Brigham and Women’s Hospital.","Are IV Bags Necessary to Rehydrate Patients? After Maria, I’m Not So Sure."
9621,3680968,2018-02-22 13:54:00,"Net neutrality repeal goes into effect on April 23rdTwo months from now, net neutrality will officially be dead. Today, the Federal Communication Commission's revocation order was published in the Federal Register. The effective date is April 23rd.That doesn't mean the next two months will be quiet; the attorney general of New York is set to sue the FCC over the repeal of net neutrality, and more states and advocacy groups will follow. Democrats in the Senate have the votes to restore net neutrality (but not the two-thirds majority required to override the president's veto, which would surely follow any action on their part.)The FCC's recent action might be called ""The Restoring Internet Freedom Order,"" but that is, of course, misleading. Net neutrality forced ISPs to treat all content equally; without these rules in place, providers can charge more for certain types of content and and throttle access to specific websites as they see fit.If April 23rd hits and this order is still in place, we probably won't see an immediate shift in how we use the internet. But you can bet that, over the following months, internet providers will start introducing new tiered plans that allow them to pick and choose the content you see based on what you're paying. We still have a few months to ensure this doesn't happen, so let's hope our legislators make the best of them.",Net neutrality repeal goes into effect on April 23rd
9622,3680969,2018-02-22 11:00:00,"Watch Airbus' drone taxi take to the skies for the first timeBack in late January, Airbus' Vahana team successfully flew their autonomous air taxi for the first time. Now, you can finally watch how the drone stayed in the air on its own in the video below the fold. It shows the aircraft hovering around 16 feet above the ground for a few seconds -- not particularly exciting if you're not enthused about the possibility of traveling aboard a single-passenger self-piloted taxi. But it was still a 53-second flight all on its own, without the input of a human operator.The Vahana team's goal is to leverage new and emerging technologies like electric propulsion and machine vision to ""democratize personal flight."" Their drone only lifted off the ground and touched back down during its first set of tests -- they definitely have a long road ahead to get it ready for forward flight -- but it was a huge deal for the team and the company. If they succeed, Airbus could eventually use the drone for an autonomous passenger network that will give people a way to hail a flying taxi to get to where they want to go.",Watch Airbus' drone taxi take to the skies for the first time
9623,3680970,2018-02-22 14:00:00,"Intel will bring 5G to laptops in 20195G hardware is closer than you think. Intel's upcoming XMM 8000 series modems will power 5G in notebooks in the second half of 2019, the company announced today. That'll include machines from Dell, HP, Lenovo and Microsoft. (Yes, that means we'll see a 5G Surface next year.) You can expect their mobile network speeds to be blazing fast, naturally, but they'll also take advantage of 5G's low latency. That'll led you do things like play online games with the same responsiveness as a wired network connection.To give us an idea of what 5G will look like in a laptop, Intel will also be showing off a 2-in-1 concept at Mobile World Congress next week. There aren't too many details about that device yet, but Intel says it'll demonstrate the power of 5G by streaming live video. Perhaps we'll see them running multiple 4K streams at once -- something the company has demonstrated on desktops over the last few years.While the tech world is holding its breath to see how 5G transforms our phones, it has the potential to signifcantly impact how we use PCs, as well. Integrated LTE connectivity is still a rarity in laptops today. But if the 5G networks are as fast as we expect, and Intel's modems make it easy for manufacturers to integrate 5G, there's a good chance we'll see even wider adoption. The new cellular technology is exactly what Intel and Microsoft need for their vision of ""Always Connected"" PCs -- machines with incredibly long battery life and integrated network connectivity.Intel has also signed up for a partnership with China's Unigroup Spreadtrum to integrate its 5G modem in its 2019 mobile platform. While that's not exactly a major brand to start with, I wouldn't be surprised if we saw more phone makers partnering with Intel soon. Qualcomm has had a huge headstart with its X50 5G modem, which will be powering devices from Sony, LG, ASUS and more companies next year. So, thanks to the competitive push, we expect to see Intel kicking its 5G plans into high gear soon.",Intel will bring 5G to laptops in 2019
9624,3680971,2018-02-22 12:01:00,"‘Super Mario Odyssey’ gets its Balloon World updateIf you're looking to squeeze a little more out of Super Mario Odyssey, then Nintendo's got something that'll brighten your day. The company has released a small update to the Switch title, named Balloon World, that includes a new mini-game, plus additional outfits and camera filters. All you need to do to access it is finish the main story, and then search for Luigi in each kingdom and strike up a conversation.In ""Hide It"" mode, players are tasked with hiding a balloon under time pressure, while ""Find It"" asks you to, uh, do the opposite. The update also brings new clothes for Mario, including Musician and Knight Armor, again available after you've completed the main story. Rounding out the update is a new pair of filters that you can use in Snapshot mode: Coin and Neon.The update was initially announced during Nintendo Direct way back in January. This may not be the end of Nintendo's largesse, however, as the company suggests that more new outfits will be added in the future, so perhaps we can hope that even more is coming as well.",‘Super Mario Odyssey’ gets its Balloon World update
9625,3680972,2018-02-22 11:30:00,"Netflix's Lost in Space remake has been a long, long time in coming (word broke of it back in 2015), but it's finally here... almost. The streaming service revealed its sci-fi show will debut on April 13th and offered a teaser trailer to whet your appetite.We can't be sure what Spotify is actually trying to build, but its search for employees is in line with what you'd expect from a company trying to make a smart speaker. Unfortunately, Chris Velazco would rather see the company invest in making its services better.The retail giant announced it has acquired global television rights to Iain M. Banks' space opera series called The Culture. The first book in the series centers on a spy tasked with recovering an AI that has the ability to help win the war.UC researchers used a concept called subsurface scattering to see how light ricochets around and through translucent fur medullas. Now they've trained a neural network to do it and are working on a process for real-time rendering that could make animals in games just as fuzzy as they are in real life.You could buy a powerful PC to sit on your desk, but Blade is proposing you leave all that power in the cloud instead. With its new Shadow service, you pay a monthly fee to stream the experience of an NVIDIA- and Xeon-powered Windows 10 system to whatever device you happen to be using. With a year-long contract, the price is $35 per month, and then you can play games, update spreadsheets or just read email -- we won't judge.Think of VSP's Level less as a Glass successor and more like a Fitbit you wear on your face. These activity-tracker-embedded frames will go on sale in March for $270, excluding the price of lenses. Once they're on your face, they'll track steps, calories burned, distance and the total activity time, then sync the data to a phone via Bluetooth. You can also use it to connect with friends and find your misplaced frames with a Find My Glasses feature.",The Morning After: Netflix's new 'Lost in Space' trailer
9626,3681061,2018-02-21 11:23:21,"Taiwan is set to ban single-use plastic drinking straws in several phases, starting with the food and beverage industry next year. The Environmental Protection Administration announced the plan last Tuesday.As of next year, food and beverage stores such as fast food chains must stop providing plastic straws for in-store use. From 2020, free plastic straws will be banned from all food and beverage outlets. From 2025, the public will have to pay for takeaway plastic straws, and a blanket ban is to be imposed in 2030.File photo: GovHK.Free plastic shopping bags, disposable food containers and disposable utensils will also be banned in 2020 from all retail stores that issue uniform invoices – widely used in Taiwan. Additional fees will also be imposed in 2025.Minister Lee Ying-yuan said a blanket ban is set to be introduced in 2030 on all plastic bags, disposable utensils, and disposable beverage cups.Lee Ying-yuan . Photo: Environmental Protection Administration.“You can use steel products, or edible straws – or maybe you just don’t need to use straws at all,” he said. “There is no inconvenience caused at all.”Lee said that the future prices for such disposable plastic items in stores will not necessarily be based on the price of such items in Europe, but will be based on prices in Taiwan.Steel straws and cleaning tool. Photo: qc-tw.com.He added that the reduction in the use of plastic is the responsibility of all members of the public rather than just his agency. The drive will create a better environment for future generations, he said.Yen Ning, Ocean Campaigner at Greenpeace East Asia, said she hoped bans on paper utensils and single-use chopsticks could be implemented in stages in the future.Scotland is set to ban plastic straws by end of 2019, according to The Independent.Support independent mediaPartner with HKFPHKFP Weekly NewsletterEmail address:We will not share your details with third parties.What is Hong Kong Free Press?Hong Kong Free Press is a non-profit English language news source seeking to unite critical voices. Free and independent, HKFP launched in 2015 amid rising concerns over declining press freedom in Hong Kong and during an important time in the city’s constitutional development. Click here to learn how you can support us and ensure our independence.","Taiwan to ban single-use plastic drinking straws, plastic bags, disposable utensils entirely by 2030 | Hong Kong Free Press HKFP"
9627,3681225,2018-02-22 13:08:25,"Watch SpaceX launch its first broadband internet satellites to space live here0SpaceX is making a second attempt to launch its PAZ mission today, which includes the PAZ satellite from client Hisdesat, and is set for launch during an instantaneous window at 6:17 AM PST, or 9:17 AM EST. The mission was scrubbed on Wednesday due to high upper level winds, and reset for this morning.The Hisdesat payload is a satellite with radar instrumentation on board, with a planned service mission of providing monitoring activities for both government and commercial clients over an intended linespace of five and a half years. The primary payload isn’t the whole story, however – this rocket will also be carrying two test satellites for SpaceX’s global broadband satellite internet service.These two satellites will pave the way for a constellation of nearly 12,000 satellites in orbit eventually that will aim to deliver affordable, fast and low lag broadband internet to underserved areas the world over. SpaceX hopes to use this service, which it calls “Starlink,” to add an additional revenue stream to its business beyond launch operations, helping it fiscally pursue its goal of making humans an interplanetary species with missions to Mars and beyond.The live stream above will begin around 15 minutes prior to the target launch time, or at around 6 AM PST (9 AM EST).",Watch SpaceX launch its first broadband internet satellites to space live here
9628,3683606,2018-02-22 14:38:00,"Disney’s lawsuit against Redbox may have backfiredDisney's attempt to prevent Redbox from buying its discs for rental and resale may have blown up in the House of Mouse's face. The Hollywood Reporter describes how District Court Judge Dean Pregerson sided with Redbox to shoot down a Disney-mandated injunction. In addition, Pregerson contended that Disney may itself be misusing copyright law to protect its interests and its own forthcoming streaming service.If you're unfamiliar with the backstory, Redbox didn't have a deal in place to procure Disney DVDs and Blu-rays for its disc rental kiosks. So, the company simply bought the discs at retail, often snagging combo packs that include a DVD, Blu-ray and a download code for the movie as well. Redbox would then offer up the discs for rental, and sell on the codes at its kiosks for between $8 and $15.Such a move enraged Disney, which includes language in its packaging and on the website demanding that users must own the disc if they download a copy. But this is where Pregerson began to disagree, saying that Disney cannot dictate what people do with copyrighted media after they have bought it. Specifically, that there's no law, or explicit contract term, that prevents folks from doing what Redbox did with Disney discs. Although it's possible that Disney can amend the wording on its packaging in future to make its objection to reselling legally binding.The next hearing will take place on March 5th, where Redbox's motion to dismiss Disney's action will be considered. Although it's obvious that the deep-pocketed Disney will likely continue to fight the battle until humanity itself has evolved into a sentient gas. But the decisions that are made through this case will be important on how these copyrights are treated while we transfer into the post-physical media landscape.",Disney’s lawsuit against Redbox may have backfired
9629,3683608,2018-02-22 13:00:00,"College esports is set to explode, starting with the Fiesta BowlStudents are driving the formation of a serious gaming scene.As executive director of the Fiesta Bowl, one of the largest college football tournaments of the season, Mike Nealy was more familiar with shoulder pads than mousepads. Six months ago, he didn't know people were making money playing video games professionally, he'd never heard of Twitch, and the last time he picked up a controller, it was attached to an Atari 2600.That all changed after a conversation with John Pierce, an esports consultant and former marketing head for the Phoenix Coyotes and US Olympic Committee. Pierce outlined the professional-gaming boom to Nealy and explained how it could tie into the collegiate football scene. He put Nealy in touch with Blizzard, the studio behind Overwatch.A few months later, on February 17th, the Fiesta Bowl hosted the Overwatch Collegiate Championship at Arizona State University, selling out the Sun Devil Fitness Complex. Organizers filled the space with a giant stage, 12 gaming PCs, two massive screens, dozens of lights and cameras, and a raised commentator desk.The four finalists (UC Berkeley, UC Irvine, UC San Diego and the University of Toronto) each sent six players and support staff to Tempe, Arizona, where they volunteered with the local Boys & Girls Club, were treated to dinner, participated in a media day and battled for the collegiate title -- all things the Fiesta Bowl does with student football players when they roll into town.""My perception was this out-of-shape kid that's eating Cheetos and playing a game, and can't talk socially, and is a basket case athletically,"" Nealy said. ""No, these are normal kids. ...This is just a different avenue. They're smart, intelligent and very capable individuals that have found a niche and are doing something that they enjoy doing.""Nealy gets esports now, and he's not alone. Esports generated $756 million in revenue in 2017, mostly through sponsorships and ads, and that number is expected to hit $1 billion this year. Popular games like League of Legends and Overwatch have implemented regulations to improve conditions for pros and stabilize the industry. Traditional sports teams are buyingesports franchises and launchingleagues as dedicated arenas pop up across the US.Universities are taking notice, too. Illinois' Robert Morris University was the first school to establish a varsity esports program in 2014, offering scholarships to skilled League of Legends players. Today, more than 60 colleges and universities have esports programs recognized by the National Association of Collegiate Esports (established in 2016); many more schools have unofficial programs, some of which are extremely successful.Take UC Berkeley for example. It doesn't have a NACE-sanctioned program, but it supports players through the school's Rec Sports system. Cal Rec Sports paid for the UC Berkeley Overwatch team's flight and lodging in Arizona and helped players secure sponsors like Under Armour and Brita.“My perception was this out-of-shape kid that's eating Cheetos and playing a game.”Mike Nealy, Fiesta Bowl executive directorThere's no dedicated gaming space on the UC Berkeley campus, though Overwatch team coach and fourth-year cognitive-science major Kyle Feng says it's only a matter of time. The school is taking a real interest in esports, thanks in part to Feng's team. They picked up their second win in a row at the Overwatch Collegiate Finals, defeating UC Irvine 3-0, taking home $6,000 in scholarship money per player.""Expanding at a collegiate level is extremely smart right now because it's such a big industry,"" Feng said. He wants all universities to have varsity esports programs and gaming rooms if only to even the playing field at tournaments like the Collegiate Finals. In the college scene, teams with dedicated coaches and practice areas regularly compete against groups of friends who simply play together on the weekends, sometimes from their dorm rooms.That was the case this weekend when UC Berkeley and UC Irvine -- a school with one of the most robust esports programs in the country -- competed against UC San Diego and the University of Toronto, neither of which offer support to esports teams on campus.No surprise, then, that Berkeley and Irvine had the upper hand in last weekend's finals.""UCSD hasn't really embraced the idea of esports,"" San Diego offense player Ernie ""Jinora"" Lum said. ""So having the opportunity to come out to this event is definitely a milestone for all of us.""Despite the lack of support, UCSD did take a positive step before the finals, according to Lum: It published an article about the team heading to the Fiesta Bowl championships.""We just pulled a bunch of people off of Facebook,"" Lum said. ""We came together instead of having someone pull us together. And we made it all the way here. It's kind of amazing.""""We just pulled a bunch of people off of Facebook.""Blizzard helped pay for UCSD's trip to Tempe, but otherwise, players were on their own. It was a similar story with the University of Toronto, which doesn't offer support or organization for esports players, even though its League of Legends team is renowned in the college scene.""It's really hard to get recognition from the administration,"" said Toronto support player Marco ""Funanah"" Chu. ""Being able to come here and see how other teams -- especially UCI, they have a lot of esports backing -- I wish Toronto could be like that.""The stories out of UC Irvine are vastly different. Instead of fighting for dedicated gaming rooms and financial backing, Irvine players receive support that will look familiar to any student-athlete, including scholarships and a brand-new esports arena (the first of its kind in the country). It's still on a small scale compared with the investments made in college football, and the school doesn't actually fund the program -- entrance fees to the arena and sponsors like Logitech, iBuyPower and Vertagear provide that money. UC Irvine supports two esports games, League of Legends and Overwatch, offering about $5,600 in scholarship money for League players and $2,500 for Overwatch fiends.One of the most powerful aspects of UC Irvine's esports program is that the administration actively seeks out talented players. Tank player Nick ""LearnTooPlay"" Theodorakis was in community college last year, and he performed well in a regional Overwatch tournament hosted by Blizzard's collegiate esports arm, Tespa. Afterward, Acting Director of UC Irvine Esports Mike Deppe sent Theodorakis an email inviting him to attend the university -- and play Overwatch.""I'd always wanted to go to UCI, so it just worked out really well,"" Theodorakis said. ""Now I'm there, I'm doing business econ and just having an amazing time.""While many universities play catch-up, outside organizations are stepping in to help college players build esports programs at their schools. For instance, Find Your Grind and ReKTGlobal this week announced a $450,000 annual scholarship fund for students interested in esports. Tespa, Blizzard's collegiate-esports initiative that helps organize student esports groups and tournaments -- including the Fiesta Bowl championships -- has 220 chapters in schools across North America.""If you were to compare that to a fraternity or a sorority, it would be top 10 in the world in terms of size, and that's all for gaming,"" Tespa founder Adam Rosen said.Rosen and Tespa helped Fiesta Bowl administrators -- including esports newcomer Mike Nealy -- plan last weekend's grand finals. The Fiesta Bowl folks brought the event-organization chops, while Tespa served as the esports and Overwatch guru. The championship demonstrated the potential benefits of friendly alliances between traditional sports and esports.""This event, specifically, is super-exciting to us,"" Rosen said. ""It actually came together pretty quickly, but it's something that we all believe, as we look back five years, will be a turning point for esports and what we're doing.""Universities are attempting to organize and embrace a new generation of student-athletes, even as questions about the nature of esports remain unanswered. It has ""sports"" in the name, but the jury is still out on whether competitive gaming is an actual athletic activity.""It's something that we all believe... will be a turning point for esports.""This question isn't just pedantic -- it makes all the difference to groups like the National Collegiate Athletic Association, which regulates university sports across the country. In December the NCAA announced it was investigating the collegiate esports scene. Marketing firm Intersport was tasked with putting together a report for the NCAA, led by executive director of esports Kurt Melcher, who happened to also be the founder of Robert Morris University's inaugural gaming initiative.""The current NCAA structure, as we know, in some ways fits and in some ways does not fit collegiate esports as it stands right now,"" Melcher told ESPN in December. He estimated there were 500 collegiate programs in the wild, NACE-sanctioned or otherwise. But without a clear structure behind them, the industry was fractured. Students and faculty were constantly cobbling together programs wherever they fit.""We are seeing that 30 percent of formalized programs are in student services, 30 percent are in athletics, 20 percent are in an academic house setting where there is game design or computer design,"" Melcher said. ""I think that is part of the process, seeing if this is a space that [the NCAA] should become involved in, and how, if possible.""The NCAA hasn't decided what role it should play in esports yet, but the conversation is happening. The association will need to figure it out soon if it wants to get in on the ground floor of esports: Professional gaming is becoming mainstream in a real way. More than 10 million people tuned into the debut week of the Overwatch League in January, and Blizzard confirmed it's raising the franchise fee for season two. Twelve teams across the globe paid $20 million each to buy into the OWL in season one, but ESPN sources expect that figure to hit as high as $60 million for season two.And that's just Overwatch, one of the newest titles in esports. Games like League of Legends, Counter-Strike: Global Offensive, Call of Duty and Dota 2 also have rich competitive scenes, offering millions of dollars in prize money each year and regularly selling out massive sports stadiums.Meanwhile, last weekend, the Fiesta Bowl Overwatch Collegiate National Championship sold out the Sun Devil Fitness Complex. For players and faculty, this was just the beginning.""We at the Fiesta Bowl, we are known as one of the premier college-bowl games,"" Nealy said. ""Which is true -- we wouldn't have just done this if we didn't think this was a serious situation.""On the Overwatch side of things, Rosen agreed.""We're trying to build a world where we have events like this on every campus throughout North America and beyond that into the world,"" he said. ""And that these events, which this year is filling up the Sun Devil Fitness Center, will grow to be filling up stadiums just a few short years from now. So that's the path we're on, and I think, looking back, we hoped we'd reach a point like this, but we didn't expect that it would come so quickly.""Jessica earned her BA in journalism from ASU's Walter Cronkite School in 2011, and she's written for online outlets since 2008, with four years as senior reporter at Joystiq. She specializes in covering independent video games and esports, and she strives to tell human stories within the broader tech industry. Jessica is also a sci-fi novelist with a completed manuscript floating through the mysterious ether of potential publishers.","College esports is set to explode, starting with the Fiesta Bowl"
9630,3683701,2018-02-20 19:03:40,"The #1 reason Facebook won’t ever changePosted February 20, 2018byOm MalikFacebook’s (much deserved) media nightmare continued this week when it came under criticism for spamming members who signed up for two-factor authentication. This was followed by charges that its Protect VPN software (based on its Onava CDN) was essentially corporate spyware. The collective outrage over Facebook and its actions might result in a lot of talk, but it won’t really change Facebook, its ethos, and its ethics. Let me explain!**A few years ago, I wrote that companies have a core genetic profile and it is tough for them to deviate from it. That DNA defines every action, reaction, and a strategic move made by a company. The DNA represents a company’s ethos — and to a large extent, its ethics. Microsoft was and will always be a desktop software company, albeit one that is doing its best to adapt to the cloud and data-centric world. It has turned its desktop offerings into smart revenue streams on the cloud.Google’s core DNA is search and engineering, though some would say engineering that is driven by the economics of search, which makes it hard for the company to see the world through any other lens. Apple’s lens is that of product, design, and experience. This allows it to make great phones and to put emphasis on privacy, but makes it hard for them to build data-informed services.Facebook’s DNA is that of a social platform addicted to growth and engagement. At its very core, every policy, every decision, every strategy is based on growth (at any cost) and engagement (at any cost). More growth and more engagement means more data — which means the company can make more advertising dollars, which gives it a nosebleed valuation on the stock market, which in turn allows it to remain competitive and stay ahead of its rivals.Just look at these charts and you start to see why Facebook is addicted to growth and engagement. Engagement gets attention, and attention is a zero-sum game. Time spent on Facebook (or Messenger, Instagram, or WhatsApp) means that’s attention not spent on Twitter, Snapchat, or anyone else who dares to compete with them.Facebook’s challenge is that their most lucrative market — the US and Canada — are saturated. And to keep making money in these markets — already a ridiculous $27 in ARPU for the last three months of 2017 — they need us to give more time and attention to them.In 2017 compared to 2016, the average price per ad increased by 29%, as compared with approximately 5% in 2016, and the number of ads delivered increased by 15%, as compared with approximately 50% in 2016. (From Facebook’s 2017 10k filing)This is a crisis situation for Facebook because it doesn’t make as much money from markets outside of the US and Canada. For the same three months, it made $2.54 in ARPU in Asia-Pacific, $1.86 in rest of the world, and $8.86 in Europe.The ARPU numbers, especially in the US and Canada, explain why the company is publicly talking about the challenges of fake news and how it is trying to remove such news from users’ feeds — and instead refocusing on friends and their stuff. It has to make polite noises around fake news (aka spam) in order to get people back into the fold and staying on the site longer.I don’t think this change of heart on the part of Facebook and Mark Zuckerberg is not altruistic. However, it is pretty evident that they are trying hard to get us back onto Facebook, and more often. At the same time, the company is running a campaign to burnish its image and come across as a company under siege, as outlined in a recent Wired cover story. But Facebook isn’t the victim — it never is, and it never will be!**So now you know why Facebook does what it has been doing recently — sending various messages constantly to get you back on the service. I know first hand. I left Facebook on September 23, 2017, and not a single day has gone by when I don’t get at least a couple of emails or some SMS messages trying to get me back with notes about what friends have posted recently, or birthdays or other milestones. I keep unsubscribing and they still keep coming. Now I’ve set up a spam rule: all emails from Facebook.com go straight into the spam folder.Facebook’s DNA also explains why it is pushing Protect (the VPN) and what it brings to the table. First of all, it allows the company to keep tabs on what apps people are using in different parts of the world, which in turn gives it a leg up on who or what to copy or, potentially, acquire.The VPN data also allows Facebook to better target its ads — much like how Google Mail and Google Chrome allows Google to better target what ads you see. By the way, Facebook isn’t the only one who is taking data from VPN mobile streams. Other data brokers buy data from other VPN apps. To be clear, just because others are doing it doesn’t make it right for Facebook to follow suit. I would love to see a US version of GDPR — a citizen data rights manifesto — to be put on the table.How does Protect help Facebook?Protect can tell that you browsed H&M’s North American site, visited NYTimes.com, and bought groceries on Farmstead. It can figure out how much time you spend on various sites and services and start to build a better profile of your online usage for smarter ad targeting and to place you in more and more buckets.In other words, Protect brings more granular and refined data into Facebook’s system, which in turn allows Facebook to refine its algorithms and become more efficient at targeting of ads. It is especially more useful in the Asia Pacific region and other emerging markets where it is pretty tricky to create buckets and hyper-targeting. Overseas users of Facebook are using the social platform on phones that are usually pre-paid phones and don’t have as much personalized information available from third party sources to create profiles. Facebook needs to find more high-value customers in the hordes of users in Asia, Africa, and LatAm.For 2017 , worldwide ARPU was $20.21 , an increase of 26% from 2016 . Over this period, ARPU increased by 41% in Europe, 36% in United States & Canada, 33% in Rest of World, and 22% in Asia-Pacific. (From Facebook’s 2017 10k filing)**Facebook’s ultimate goal is to make it expensive to buy hyper-personalized advertising — which is the Mount Olympus of advertising — and I am not surprised that Facebook is thinking about releasing touch screen smart speakers. It will be a great way to spy — sorry, I mean acquire more continuous data about you and further bucket you for future ad targeting.This helps keep the ARPU growing, which in turn means Facebook will keep making more money. More money, in turn, allows it to keep Wall Street happy, crush rivals, and suck up all the talent in the market. And yes, it will also accept fake accounts and spam on its platform even if it compromises elections, as long as it makes money.That may be a very simplified version of a sequence of events, but the fact of the matter is money and obsession with growth and engagement are what makes Facebook go around. That is embedded in its psyche, its DNA, and it will never change.Facebook’s stated mission is to connect the world. They have already done that in the US, but their subsequent actions show that their real agenda is to extract user data — not strengthen connections. Just read their 10k filing and you’ll see it in black and white.The increase in the ads delivered was driven by an increase in users and their engagement and an increase in the number and frequency of ads displayed on News Feed, partially offset by increasing user engagement with video content and other product changes.Facebook is about making money by keeping us addicted to Facebook. It always has been — and that’s why all of our angst and headlines are not going to change a damn thing.",The #1 reason Facebook won’t ever change
9631,3683863,2018-02-22 14:30:30,"The ICO immaturity problem0I’ve been following “startups” – I define startups as small businesses with a global scale – for almost two decades. In that time I’ve watched them morph from unfunded pet projects by random geniuses into what amounts to an entire sub-economy dedicated to the creation, funding, and sale of these pet projects. Remember: Facebook, LinkedIn, and Twitter were once startups.I also saw a brief period – probably in about 2008, just before the financial crisis – when startups were red hot. Everyone everywhere had one and desperate CEOs used “growth hacking” techniques – essentially tricks designed to make you click – to get attention. In addition to spam and ads, founders visited business writers and VCs uninvited, war dialed to get access to Sand Hill Road cash cows, and added sex and violence to their Facebook ads to get that last click that would put them into “exit” territory.But those early days are gone, right?Wrong.With the rise of the unregulated ICO we are entering a new sort of startup era. This is an era populated by the growth marketers that got bad mobile apps to the top of app stores and who used spam and sex to get attention. This is also an era where the money on the table is untrammeled. An ICO can seemingly raise $850 million in a few seconds although smart people know that these sums are mostly smoke and mirrors. However, even if you capture a few million out of a multimillion dollar token raise that’s enough for a lambo, an off-shore bank account, and a life of relative ease.I’ve even gathered a small team to research and write about token sales that may be more than just wishful thinking and we’ve found it’s surprisingly difficult. The primary reason most ICOs don’t get much press attention is that the the idea/product (if there is one) and even the team do not inspire confidence or even trust.Change is coming. This much is clear. The SEC is currently attacking even the most stringent of ICOs and we can expect to see much more regulatory activity in the future. But that is not stopping bad actors from acting bad. For every successful ICO there are hundreds of frustrated founders who can’t afford the legal or editorial fees to get a white paper started and hundreds more scammers trying to eke a little Ether out of the unsuspecting investor.And then there’s the ICO above that is proud to associate itself with the proud tradition using leggy models in sexy Facebook ads.The crypto industry, as a whole, needs to grow up. The ICO corner of crypto needs to do it faster than any other aspect of the culture.The answer is to think intelligently about the value propositions offered to us all in the wake of the ICO craze and, further, create a similar framework that gave rise to the best of the startup era. Right now we are in Deadwood-era Wild West complete with the sex, curses, and cheats. Where we need to be is a few decades after that in a world where the future is just peeking over the edge of a gilded horizon. Immaturity gives rise to distrust which can destroy this form of fundraising in the cradle. It’s important that all of us not to post pictures of semi-naked girls – or, for that matter, lie – in an effort to raise an extra $50,000.",The ICO immaturity problem
9632,3683865,2018-02-22 12:16:28,"Apple said to debut voice-activated Siri AirPods in 2018, water-resistant model in 20190Apple is preparing a couple of updating models of AirPods, according to Bloomberg. The popular fully wireless earbud-style headphones that Apple introduced last year are currently on track for a refresh in 2018 with the addition of a new version of the “W” line of chips that Apple created specifically to manage and improve Bluetooth-based connections between gadgets.The 2018 hardware refresh would include not only an improved W chip (possibly the W2 added to the Apple Watch last year, or perhaps even a W3) but also the ability to activate Siri just by voice, rather than by physically tapping the AirPod in your ear, as is the case currently.Like with Amazon’s Echo devices or the iPhone, a user would be able to trigger the virtual assistant simply by saying the wake word aloud – “Hey Siri,” in this case. That would indeed to a step-up in terms of shifting AirPods to a voice-first interface device.As for the successor currently planned for 2019 (though Bloomberg notes those plans could easily change between now and then), it will add a new level of water-resistance, which Bloomberg reports will be designed to protect against “splashes of water and rain,” rather than full submersion like the current Apple Watch.AirPods are doing well by all accounts, so putting them on an update cycle similar to the iPhone and other of Apple’s high-demand products seems fairly logical. It’ll be interesting to see if customers choose to upgrade in the same way they do with Apple’s other high selling devices, and what other updates might be in store (please made variable fit tip design, Apple, so that I can finally wear these things without requiring a little foam sleeve).","Apple said to debut voice-activated Siri AirPods in 2018, water-resistant model in 2019"
9633,3683867,2018-02-22 09:31:53,"iMac Pro reviewIs this the ultimate all-in-one for professionals?Our VerdictThe iMac Pro is a powerful, professional all-in-one that certainly won’t be for everyone. It has oodles of power and a huge price tag to match. However, if the iMac Pro is suited for your needs, then you won’t be disappointed.ForMost powerful Mac everExcellent designKeeps coolNew Space Gray color is aceAgainstVery expensiveNon user-upgradableMagic Mouse 2 still frustratingThe iMac Pro, as the name suggests, is a powerful, sleek and stylish all-in-one computer with the familiar form factor of Apple’s iMac range, but with the power, features and, yes, price that is squarely aimed at professional users.When Apple unveiled the iMac Pro at itsWWDC event last year, it touted the computer as ‘the most powerful Mac ever made,’ and it is clear that the Cupertino company has succeeded. This is an incredibly powerful machine that’s been built for professional use.And, it’s not about power for power’s sake – this is a carefully-built machine whose powerful components can help speed up the workflow for professionals. Sure, the price tag is steep, but if it can help reduce the time you work on projects, the time you save that can be put into working with clients or taking on further projects could be invaluable.Spec SheetHere is the Apple iMac Pro configuration sent to TechRadar for review:Price and availabilityDespite it looking a lot like a standard iMac, the iMac Pro is aimed squarely at professionals, with workstation-level computing power that will be easily more than what most people need for day-to-day computing, and with a price tag to match. However, if you are a professional photographer, game designer or architect, then you’ll probably be looking at the iMac Pro as an investment.So, how much is the iMac Pro going for? As you’d expect from a computer that’s aimed at professionals, the iMac Pro comes in a number of configurations, so there is scope for configuring the iMac Pro to suit the needs of you and your business, as well as your budget.Next, you can buy an iMac with a 10-core Xeon W CPU, 16GB of high-bandwidth memory (HBM2) AMD Vega 64 graphics and a whopping 2TB SSD. That model costs $7,999 (£7,599 and AU$9,539).The top-end iMac Pro comes with an 18-core Intel Xeon W processor, 128GB of RAM, 4TB of SSD storage and the same AMD Radeon Vega 64 GPU for $13,199 (£12,279, AU$20,419). Of course, that’s a huge investment, but you’re getting a heck of a lot of power as well.You can also tweak each of these configurations further, for example changing the SSD size, adjusting the amount of RAM or going for a 14-core Intel Xeon processor, which will alter the final price of the machine.These specifications and the price tags involved alone will let you know if the iMac Pro is for you. If spending almost as much as a new car on an iMac seems extravagant, then this is not the machine for you. If you rarely use graphic-intensive programs, and you wouldn’t know what to do with a graphics card with 8GB – let alone 16GB – of HBM2 memory, then, again, the iMac Pro isn’t for you.However, if you’re heavily invested in the Apple ecosystem, love the all-in-one designs of previous iMacs, and need a workstation that will handle intensive workloads, then the iMac Pro could be your ideal machine, and the undeniably high cost could justify itself. If you find yourself hanging around while your current machine compiles code, or renders 3D images, then spending that sort of money to drastically cut down (or even eliminate) that time could be a no-brainer.DesignConsidering the boosted specifications of the iMac Pro compared to the standard iMac, Apple has done an excellent job of maintaining the iconic look of the all-in-one machine. All those impressive components are located behind the 27-inch screen, leading to an attractive, minimalist machine that looks great in any office or studio.The fact that Apple has kept the body as thin as it has is also testament to the effort, and careful design considerations, involved in the creation of the iMac Pro. At its edges, the iMac Pro is an impressive 5mm (0.2 inches) thick, meaning it won’t hog too much space on your desk. It also weighs just 9.7kg (21.38 pounds), which means it’s easy enough to move from desk to desk if needs be.Having such powerful components, which require more power and therefore produce more heat, means that a capable cooling system is required – especially considering the slimline design of the iMac Pro. Thankfully, Apple’s engineers have created an impressive cooling system for the iMac Pro, with dual fans that help circulate cool air over components, while expelling hot air.In our tests, this did an excellent job of keeping the iMac Pro cool while under a lot of pressure: editing 4K HDR content in Final Cut Pro X, a pretty strenuous task, while also reducing the noise of the fans. Sure, it emits heat while being used, but that’s the point of the cooling system, and we are impressed with how quiet it is. With an all-in-one machine that sits in front of you on your desk, you don’t want any distracting fan noises while you work, and the iMac Pro does an excellent job of mitigating that.According to Apple, the cooling solution, which includes a high-capacity heat sink and extra venting on the back of the device, allows for almost 75% more airflow and an 80% increase in system thermal capacity. All the while, the iMac Pro uses 67% more power than a 27-inch iMac.So, if you’re a fan of the iconic look of previous iMacs, but want a device that offers some serious hardware for professional use, then you’re going to be very pleased with the iMac Pro. One particularly noticeable difference with the iMac Pro’s design, compared to regular iMacs, is that it comes in a new color scheme: Space Gray.As you’d expect from an Apple product, the Spay Gray iMac is gorgeous to look at, and the included Magic Mouse 2, Magic Keyboard and Magic Trackpad 2 also come in the new color, looking just as fantastic.Of course, as nice as the body and peripherals look, the most important aspect, especially if you’re a professional photographer, video or image editor, is the screen. The 27-inch 5K screen is described by Apple as it’s ‘best ever,’ with 500 nits of brightness, an increase of 43% over the screen brightness of previous iMacs. The 5,120 x 2,880-pixel resolution is just as impressive here as it is on high-end iMacs, which also feature this resolution, and the boost over 4K resolution (3,840 x 2,160) means video editors can work on 4K video at full resolution, while also having room on screen for their editing tools.It’s little touches like this that make the iMac Pro such a compelling choice for professionals, and help speed up your workflow by eliminating the need to enter and exit full-screen mode if you’re on a 4K – or lower – monitor.Because the iMac Pro is designed for professionals, color reproduction has to be as accurate as possible. This is essential for photographers, graphics artists and video editors, amongst others. The screen supports the P3 wide-gamut color space, an RGB color space that is widely used in digital movie production. If you work with digital film, then this support will be hugely welcome, though not too surprising. Apple iMacs have supported it since 2015, as does the iMac’s competitor: theSurface Studio all-in-one by Microsoft.While P3 is wider than sRGB, it’s not quite as wide as Adobe RGB. So, if you’re relying on Adobe RGB – for example, if you work in printing and publishing – then you may be disappointed with the lack of Adobe RGB support here. For many people, however, the P3 color space will be perfectly fine and a big improvement over sRGB.On the bezel above the top of the screen resides the FaceTime camera, like with other iMacs, but there are a few major improvements here. For a start, it can record in 1080p resolution, whereas previous FaceTime cameras on iMacs were 720p.The boost in resolution is immediately apparent when using the iMac Pro for video calls. So, for client meetings, or chatting to work colleagues, this increase in video quality is greatly appreciated.The webcam also features four microphones, compared to a single one on the 5K iMac. This array of microphones also helps eliminate background noise and, as they are placed at the top of the screen (rather than at the bottom with previous iMacs), it does an excellent job of noise cancelling. Of course, if you work in very noisy environments, you’ll still want to use a headset for the best possible sound quality, but these improvements are very welcome.While Apple has caught flack for limiting the ports in its professional-oriented MacBook Pro, the iMac Pro suffers no such problems, with a decent array of ports that will allow you to hook up many peripherals to the device.At the base of the back of the iMac Pro, you get a 3.5mm headphone jack, SDXC card slot, four full-size USB 3.0 ports, four Thunderbolt 3 USB-C ports and a 10Gb Ethernet, which offers incredible network speeds. The four USB 3.0 ports are welcome for older, legacy, peripherals and devices, while the Thunderbolt 3 USB-C ports support Thunderbolt devices with up to 40Gbps data transfers, and USB 3.1 devices.The Thunderbolt 3 ports can also be used to connect extra displays, such as two 5K external displays at 60Hz, or four 4K UHD displays at 60Hz. Basically, you’re pretty much set for ports, and we’re glad to see that Apple hasn’t skimped here.However, while the slimline all-in-one design certainly looks impressive, it does mean that this is a workstation you won’t be able to easily open up, tinker with and upgrade components yourself. This may not be an issue for many people; however, if you want an easily-upgradable device for future-proofing, then this won’t be a device for you.Overall, the design of the iMac Pro is everything you’d expect from Apple: gorgeously made, with some genuinely innovative features and excellent professional-focused details that go some way to justifying the steep price tag.Magic peripheralsThe iMac Pro comes with the Magic Keyboard with Numeric Keypad, Magic Mouse 2 and, optionally, the Magic Trackpad 2 – all of which come in the new (to the iMac range) Space Gray color. Apart from the new color, these peripherals should be pretty familiar.We’re glad to get the Magic Keyboard version with a numeric keypad, as that’s definitely a convenient addition for quickly writing up sums. While we wouldn’t describe the typing experience on the keyboard as ‘magic’, despite its thin keys and shallow travel, it’s comfortable enough to type on. However, if you’re planning on doing a lot of writing, then you may find a more tactile keyboard is more comfortable.In our view, the Magic Mouse 2 is much more successful. While the design hasn’t changed (apart from the color) since its debut in 2015, many would argue that if it’s not broken, don’t fix it.The mouse feels smooth and responsive in use, and handles well on a variety of surfaces. If you’re used to using Macs, then you’ll feel right at home, though again, for some tasks you’ll probably find it better to plug in a specialized pointing device.There is something we do still think is broken with the Magic Mouse 2, and which we’d love Apple to fix: the fact that to charge it, you need to plug the Lightning cable into the bottom of the mouse.Not only is this a rather inelegant way to charge, it also means you can’t use the Magic Mouse 2 while recharging its battery, a baffling design decision that we’re frustrated with Apple for failing to rectify.Finally, the Magic Trackpad 2 is a handy addition with an edge-to-edge surface that allows you to control the iMac Pro in a similar fashion to a MacBook. Used alongside the mouse and keyboard, it’s quite a nice alternative method for scrolling through files and websites, and zooming in and out of photos (amongst other uses).The lack of change with these peripherals will please anyone who’s a fan of them, and annoy anyone who doesn’t like them, and was hoping for something a bit more special for the iMac Pro. The new color makes them look fantastic, it has to be said, and the Magic Keyboard and Magic Mouse 2 paired with the iMac Pro as soon as they were switched on.Strangely, the Magic Trackpad 2 didn’t pair automatically – we had to connect it to the iMac Pro via the included Lightning cable (which comes in black) before we could use it with the iMac Pro wirelessly.",iMac Pro review
9634,3683868,2018-02-21 17:34:31,"How the iSIM might give your next phone a huge batteryDitching the card yields big benefitsSharesThe iPhone X’s notch turned out to be the “next big thing” in phone design, but the next (next) big thing might not even be noticeable to the naked eye. ARM has developed new iSIM technology that might eventually see the trusty, but ever cumbersome SIM card going away for good.But for now, temper those expectations as it’s making its debut in Internet of Things (IoT) devices. This new SIM replacement is integrated right into the processor and thus, takes up far less space than even eSIM, the impressive SIM card-less tech recently used in the Google Pixel 2 and Apple Watch 3, let alone an entire SIM card slot.Conceptually, iSIM isn’t too different than the eSIM, though its dimensions are stark by comparison. Compared to ARM’s new tech that is said to be smaller than a “millimeter squared”, eSIM consumes 6 x 5mm of space inside of the phone – precious space that manufacturers could use to boost RAM or pack in a larger battery.In with the new SIMThe main motivator behind the creation of iSIM is to make way for new hardware innovation, which given the diminutive size of many IoT devices, any extra space is a good thing. But the implications for the tech could (and will likely be) far-reaching into other product categories.Phones, wearables and tablets come to mind first, which would benefit greatly from iSIM technology from a hardware perspective, as mentioned earlier.In addition to saving you the dreaded hassle of fussing with SIM cards, eliminating the need for a SIM card would save manufacturers some money, which are costly to produce – apparently around “tens of cents” each, according to a statement provided to The Verge. That might not sound like much, but any cash saved on SIM-related hardware could be allocated toward more titillating features.But there are other advantages at bay, like having your contacts, carrier settings and other pertinent account data stored securely on the iSIM, and by extension, the cloud, which presents a more formidable roadblock for phone thieves to penetrate.Lastly, while several phone makers have figured out ways to make waterproof devices with SIM card slots, removing the manual SIM technology would make future smartphones more resilient to the elements.We’ll likely see more phones adopting SIM-less features in the near future, perhaps with the Google Pixel 3 and the iPhone X2. Heck, maybe we’ll even see some MWC 2018 announcements ditching SIM altogether.",How the iSIM might give your next phone a huge battery
9635,3683869,2018-02-22 15:57:38,"Samsung Galaxy S9 release date, price, news and rumorsThe new Galaxy S9 launch is set for February 25We're very close to the unveiling of the Samsung Galaxy S9, and there have been so many leaks across the web that we know almost for certain what it's going to look like, and how it's going to work.The bad news for some: it's not going to look hugely different from the Samsung Galaxy S8, with the main difference coming from the upgraded camera and the big jump in performance, thanks to a more powerful CPU.We'll also be seeing the Samsung Galaxy S9 Plus alongside the S9 when it's released on stage at MWC 2018, so it's going to be a bumper launch from Samsung.In order to make the whole process as helpful as possible for you, we've disseminated the top rumors for you into one, easy-to-read guide on what we think you're going to see unveiled with the Galaxy S9.Design: The Samsung Galaxy S9 design is going to be very similar to the Galaxy S8 according to the leaked images, although one of the key problems is solved, thanks to the fingerprint sensor being in a more accessible place. This means same rolling metal-and-glass chassis featuring minimal bezel as the S8, so we're not expecting to see much different between the two phones.Screen: The Galaxy S9 screen will very likely be a 5.8-inch QHD+ Infinity Display, rolling to the edges of the phone, curved at the edges. Don't expect an inbuilt fingerprint scanner under the screen though as, essentially, it'll be a very similar display to last year.Camera: The main change here is expected to be a 12MP dual-aperture sensor. One that's as good in low light as bright light, with the phone able to change the camera's power depending on the conditions. While it's still set to be a single sensor, Samsung is pushing the phone's ability to take superb low-light photos and capture super slo-mo footage.Image 1 of 12Feb 22: New press shots have leaked out, showing the S9 in a variety of colors. (credit: @rquandt)All the leaks of the Galaxy S9 design as they happenedImage 2 of 12Feb 19: Three of the colors you'll apparently be able to get the S9 range in. (credit: Winfuture.de/@rquandt)Specs: The US looks set to get a phone powered by the Qualcomm Snapdragon 845, and the rest of the world Samsung's own Exynos 9810 chipset. This means 30% improved power, albeit still with 4GB of RAM inside. This will be more than enough grunt, but those that lust after specs for the sake of them will be disappointed not to see 6GB,Battery: While it looks like we're going to get a 3,000mAh power pack - the same as on the Galaxy S8 - the improved CPU inside will likely lead to a real boost in battery life. This means the Galaxy S9 should last easily over a day for more users than ever before.Other key features: A recent promo conformed animated emoji, where faces are turned into cartoons, will be present on the Galaxy S9 as a response to Apple's Animoji.Stereo speakers will also be a welcome addition for anyone tired of the mono-firing single outlet on previous phones - we don't expect this to be industry-leading in the same way as Sony and Apple, but at least using your phone to play music or movies will be more pleasant.An improved iris scanner is rumored to be Samsung's play against Apple's Face ID too, but given the poor performance of the sensor last year we don't expect this to be a big feature.Samsung Galaxy S9 specsBased on the rumors so far, this is what the Galaxy S9's spec sheet looks like.What will the Samsung Galaxy S9 price be?Our sources have told us to expect a price hike of £50 in the UK, which will translate to £739 - this means we'd expect it to land around $775 / AU$1300 in other nations. That's a 7% increase over last year, but a massive 25% bump from the Galaxy S7 - the Galaxy S9 won't be cheap.What's the Samsung Galaxy S9 release date?The Samsung Galaxy S9 launch date is February 25, according to Samsung's Unpacked 2018 invite that we've received - and we're expecting it to go on shelves in the first or second week of March.Want to get the latest info on the Samsung Galaxy S9 on video? We've got our rumor round up right here for you!Samsung Galaxy S9 release dateThe Samsung Galaxy S9 launch date is locked down for Sunday, February 25 in Barcelona.We know that, because Samsung has sent us an invite to its Unpacked event on that date, teasing the number nine in the process.The Samsung Galaxy S9 invite teases the number nine and an improved cameraThis makes the Galaxy S9 launch earlier than 2017's S8 announcement, which happened after MWC, in March.The pre-order date is rumored to be March 1, while the Samsung Galaxy S9 release date is tipped to be mid-March according to our sources close to the S9.Samsung Galaxy S9 screenHottest leaks:An in-screen scannerA water-repellent coatingA 5.8-inch QHD+ screenBased on the leaks, we're certain the display will be the same size as the Galaxy S8, coming in at 5.8 inches. It's also sure to be curved at the sides and continue Samsung's trend of using Super AMOLED, which delivers vibrant visuals and good contrast.Similarly it's very likely to retain the Galaxy S8's 18.5:9 aspect ratio, according to the leaked pictures.Rumors of an in-screen fingerprint scanner abound, like the new Qualcomm Fingerprint Sensor can sit below quite thick displays, but we don't anticipate this coming to the S9.Synaptics hasannounced an in-screen scanner and mentioned not just OLED but also ""infinity display"" (which is what Samsung calls the S8's screen) and that it's being used on a phone made by a top five manufacturer - but it doesn't seem that's going to be Samsung.Aside from that, in late 2016 Samsung licensed a new glass coating technology that makes water bounce off your smartphone screen. Samsung plans to include this tech in an upcoming phone, so it may mean the Galaxy S9 is much easier to use in the rain.Watch the video below to see how the new glass coating technology worksSamsung Galaxy S9 designHottest leaks:Even smaller bezelsA repositioned fingerprint scanner below the rear cameraThe headphone jack remainsThe Samsung Galaxy S9 has become pretty clear in leaks, with more and more high quality images hitting the web to give us a convincing look at what the ninth generation 'S' flagship will offer us.A short video seems to show us the Samsung Galaxy S9 from the front and back, albeit with the screen off.Jan 26, 2018Trusted leaker @evleaks shares renders of the Galaxy S9, with the design consistent with with the other leaks we've already witnessed. It does look a little bezel heavy compared to some other leaks though.French leaker @OnLeaks posts a video showing a render of the Samsung Galaxy S9, complete with single rear camera, headphone jack and shifted fingerprint scanner.Dec 13, 2017A leaked image complete with dimensions backs up the 90% screen-to-body ratio, as it suggests that the Galaxy S9 will be marginally shorter than the S8, but will otherwise look similar, albeit with the scanner moved beneath the camera lens.Samsung Galaxy S9 cameraThe Galaxy S9 camera is going to be a key feature of Samsung's phone this year, and not because of a dual-lens camera or higher megapixels.Instead, we're expecting to see a variable aperture, super slow motion video and the possibility of the world's first phone with HDR video capture.The MWC 2018 launch event invite heavily teases the Samsung Galaxy S9 camera by including the words ""The Camera. Reimagined.""This could mean that the Galaxy S9 camera may beat the top-of-the-line Google Pixel 2 camera if it includes a variable aperture or becomes the world's first camera that can capture Mobile HDR video.Samsung Galaxy S9 camera rumors point to a 12MP Dual Pixel lens with optical image stabilization and a variable f/1.5-2.4 aperture. That means it would be able to switch between f/1.5 (great for low light shots) and f/2.4 (ideal when the lighting is better and you want more of the photo to be in focus).This rumor comes from a leaked image of the Galaxy S9's box, which also lists it as being 'super speed', having an 8MP front-facing camera and supporting 'super slow-mo'.This could be the retail box for the Samsung Galaxy S9. Credit: RedditMore recently we've heard additional details of what exactly 'super slow-mo' might mean, with sources claiming that the S9's camera will be able to tell when motion starts and automatically begin recording slow motion footage when that happens.Samsung has also unleashed four teaser videos highlighting the S9's camera. While they don't confirm anything, the first three hint that it might have clever slow motion skills as detailed above, work well in low light and support an Animoji-like feature.The fourth video shows far more, hinting at many of the same things, as well as an improved flash, live photo abilities, improved selfies - perhaps thanks to a bokeh effect or a wide-angle lens, and even social components.One thing we're pretty sure of: the cameras on the S9 and Galaxy S9 Plus will be different. The sketch above echos that sentiment, as do some case renders.This cut-out could either house a dual-lens camera or a single-lens and scanner. Credit: Techtastic/WeiboEither way, Samsung might offer a new way for you to unlock your phone, with leaker @UniverseIce claiming that the Galaxy S9 will have a '3D sensor front camera'.They don't explain what they mean by that, but it sounds a lot like the iPhone X's Face ID system which allows you to use facial recognition to unlock the phone. And the 3D part suggests that like Apple's solution it won't be fooled by a picture.And Samsung might also borrow Apple's Animoji feature, with one report saying you'll be able to map your face onto 3D emoji.Another source echos these claims, adding that you can create your own avatar and then control it like an Animoji.Samsung Galaxy S9 batteryHottest leaks:A 3,000mAh batteryMore power efficiencyThe latest battery rumor comes in the form of a picture of a replacement unit on sale already for the phone, and it has a 3,000mAh capacity - the same as the Galaxy S8.This lines up with a certificate supposedly originating from Brazil's telecom regulator Anatel, which also shows the Galaxy S9's battery as being 3,000mAh.That's disappointing news, especially since an earlier report suggests Samsung will now use substrate-like PCB tech that will allow the Exynos chipset manufacturer to include a bigger battery without increasing the size of the processor.However, whatever the size, efficiency improvements in the new chipsets should help the battery last longer.Wireless charging is all but a given too, since the Galaxy S8 already sports it, and a photo supposedly showing the retail box (above) lists wireless charging.Samsung Galaxy S9 specs and powerHottest leaks:Snapdragon 845 or Exynos 9810 chipsetJust 4GB of RAMQualcomm has announced the Snapdragon 845, which will likely be powering US versions of the Samsung Galaxy S9.It's an octa-core chip with four cores running at 2.8GHz and four at 1.8GHz, with the fastest cores delivering up to 30% better performance than the fastest cores in the Snapdragon 835. AI processing and graphics performance have also been improved, while power use has been reduced.The chipset also allows cameras to record 4K Ultra HD video at 60fps.Outside of the US, buyers are likely to get Samsung's own Exynos 9810, which includes an LTE modem which supports theoretical download speeds of 1.2Gbps - faster than any other phone, meaning you could potentially download an HD movie within just 10 seconds.More recently though we've see a benchmark for the standard S9, again packing the Exynos 9810, and this has far more believable scores - along with 4GB of RAM.Samsung Galaxy S9 other featuresHottest leaks:An improved iris scannerStereo speakersSamsung will likely improve the iris scanner for the Galaxy S9, with rumors suggesting it will be boosted to 3MP (from 2MP on the S8) and better able to recognize your eyes, even if you wear glasses or the lighting is poor. It will also apparently be faster than on the S8.We've also heard that it could leverage 'Intelligent Scan Biometrics' to combine both iris scanning and facial recognition, along with the ability to work better in poor lighting.A recent patent has detailed a similar system, with an iris camera that would recognize both of your eyes and part of your face.The Galaxy S9's iris scanner could get an upgrade. Credit: LetsGoDigital/WIPOThe Samsung Galaxy S9 looks like it'll have improved audio too, as there are rumors of it both having AKG stereo speakers and a free set of Bluetooth AKG headphones. Both of those things have now also been listed on an image seemingly showing the phone's box.And talk of stereo speakers has popped up again more recently, with rumors that the Galaxy S9 will have both a bottom-firing speaker and one built into the earpiece.The Galaxy S9 has also now passed through the FCC (Federal Communications Commission) and in the process we've learned that it will support all major LTE bands and that it will be manufactured in Vietnam - that latter point is interesting because the leaked box image pictured above was first shared in Vietnam, so this further suggests it might be accurate.We've also seen a Samsung patent for a sensor which would analyze atmospheric conditions and alert you to how much pollution there was in the air.Plus, one source has also claimed the Dex docking station we saw debut alongside the Galaxy S8 and Galaxy S8 Plus will get an update that will make it work more like a charging pad and allow you to type on the screen too. It means you won't need to use a keyboard and mouse when connecting your phone up to a monitor.And there's evidence that the Galaxy S9 could have a dual-SIM slot, as there's mention of one at the Chinese Ministry of Industry and Information Technology (MIIT), though that model might be limited to parts of Asia.TechRadar's take: With the exception of a pollution monitor all of these features are believable, and the iris scanner upgrades seem especially likely.","Samsung Galaxy S9 release date, price, news and rumors"
9636,3683871,2018-02-22 11:07:15,"What is 5G? Everything you need to knowThe latest news, views and developments in the exciting world of 5G networksSharesWhat is 5G?5G networks are the next generation of mobile internet connectivity, offering faster speeds and more reliable connections on smartphones and other devices than ever before.Combining cutting-edge network technology and the very latest research, 5G should offer connections that are multitudes faster than current connections, with average download speeds of around 1GBps expected to soon be the norm.The networks will help power a huge rise in Internet of Things technology, providing the infrastructure needed to carry huge amounts of data, allowing for a smarter and more connected world.With development well underway, 5G networks are expected to launch across the world by 2020, working alongside existing 3G and 4G technology to provide speedier connections that stay online no matter where you are.When will 5G launch?- 5G technology is expected to officially launch across the world by 2020- The US, China and South Korea are expected to be some of the first nations to install full 5G networks, with others including the UK not far behind- Many companies are busy making sure their networks and devices are '5G ready' in time for 2020, meaning some networks may launch before thenWhat will 5G networks mean for me?An expansion of advanced technologies - including self-driving cars and smart citiesHow fast will 5G be?It’s still not exactly known how much faster 5G will be than 4G, as much of the technology is still under development.That being said, the networks should provide a significant upgrade to current download and upload speeds - with the GSMA proposing minimum download speeds of around 1GBps.Most estimates expect the average speed of 5G networks to reach 10Gb/s, and some even think transfer rates could reach a whopping 800Gb/s.This would mean that users could download a full-length HD quality film in a matter of seconds, and that downloading and installing software upgrades would be completed much faster than today.Will I be able to get 5G networks on my phone?Existing smartphones, tablet or other devices that were released when 4G networks were the standard may not be able to connect to 5G to begin with, or may incur extra costs to do so.However following the 2020 deadline for the initial rollout, we should soon see devices coming with 5G connection as default.Don't worry though - although 5G should represent a major step up from current 4G and 3G networks, the new technology won’t immediately replace its predecessor - at least, not to begin with.Instead, 5G should link in with existing networks to ensure users never lose connection, with the older networks acting as back-up in areas not covered by the new 5G coverage.So-called “4.5G” networks (also known as LTE-A) are set to fill the gap for the time being, offering connections that are faster than current 4G networks, although only certain countries such as South Korea can benefit from them right now.Once launched however, implementing 5G may be a slower process. Much like the gradual takeover of 4G networks from the previous generation, existing network infrastructure may need to be upgraded or even replaced in order to deal with the new technology, and homes and businesses may also need to get new services installed.It’s not yet known how 5G networks will take over from existing networks, but again, much like the rollout of 4G, you may not be able to immediately connect to the new networks without upgrading your technology.What will a 5G network need?The GSMA has outlined eight criteria for 5G networks, with a connection needing meet a majority of these in order to qualify as 5G:1-10Gbps connections to end points in the field (i.e. not theoretical maximum)",What is 5G? Everything you need to know
9637,3683943,2018-02-22 16:07:28,"TNW SitesThis angel investor got all of his crypto stolen by T-Mobile impersonatorsBecause I started playing with cryptocurrencies as a hobby years ago, and for a long time they were not worth much of anything (readSome thoughts on cryptocurrencies), it did not occur to me to treat my crypto holdings more securely than other assets I owned.I assumed that by using very complex passwords, or a password manager like Dashlane, and requiring two-factor authentication with text messages sent to my cell phone, I would be safe.Boy was I wrong!I did not realize I had a (very) weak link in my security: my cell phone provider. The hackers called T-Mobile pretending to be me. They said I had lost my cell phone and asked T-Mobile to activate another SIM with the same number.As (bad) luck would have it, I was traveling in Europe at the time. I noticed my cell phone lost connectivity, though it still worked through Wifi. I assumed it was just a roaming issue, put my phone on airplane mode (as I do every night) and went to sleep.When I woke up, I still did not have connectivity, but it was not obvious that something was awry as many normal emails had come through the night. After a few hours, I randomly decided to check my Twitter and realized my password no longer worked.That’s when I became suspicious. I tried to log in to my Gmail (which I very rarely use) and that password had also been changed. I checked my regular email address and while send and receive worked with no error, no new external emails had come in for a few hours (which is unusual as I get over 200 emails per day). I tried to log in to my domain manager and no longer had access.The hackers had been very sneaky.After they got control of my cell phone number, they sent themselves a reset password text message at my domain manager to get access to that. They left my existing Exchange mailbox intact, but created a new mailbox and switched the MX record to point to that mailbox.It took a few hours for the MX record change to propagate so I still received emails for a few hours.Also, because they did not reset the password of my Exchange email I did not get an incorrect password message that would have aroused my suspicion. I kept getting internal FJ Labs emails even after the MX record change because those are also on the same Exchange server as my email.Once the MX record change had propagated, they were able to use their control of my email and access to my cell phone (given that I required text confirmation in addition to control of my email) to reset the password for my Dropbox, Venmo, Twitter, Gmail,Coinbase,Xapo,UpholdandBitstampaccounts.I did not see any of those reset password messages or any of the text message confirmations because they were going to the new mailbox and phone they set up. They then sent themselves all my BTC to 12LmHubDmhnLTrvPgs82MJ2FTJR68rwrfK.At this point, it was clear that my phone an email had been compromised.I immediately called T-Mobile which confirmed that they had set up a new SIM for my number. It took a fair amount of time, but I convinced them to restore the original SIM.I then reset the password at my domain manager and noticed the MX record had been changed. They were now pointing to a mailbox hosted by my domain manager. I logged in and saw all the password resets on all my accounts.It took hours, but I reset all the MX records and the passwords on all my accounts and replied to all the emails I had missed that had been sent to the new mailbox.As luck would have it, for all their sophistication they stole only 0.01 BTC🙂I can take no credit for this, as it was sheer luck. I had fundamentally revised my crypto investment strategy the week before the hack and sold all of my direct crypto holdings.I had also reached my Venmo weekly payment limit, so they could not Venmo themselves money (and I can see they tried).They did not try to make wire transfers from my normal bank accounts, perhaps because that money would have been easier to trace and I require a few more security measures for wire transfers that are more difficult to get around.This experience made me realize that your security is only as strong as your weakest link.Since then, I implemented several changes to my security protocols. To make any changes to my T-Mobile account by phone or in person, you now need to mention a very complex password with digits and special characters.I recommend that everyone adds a voice authorization password required to make changes to their cell phone account.It also made me realize the perils of using an email address everyone knows and a phone number everyone knows to manage my crypto holdings.The crypto accounts I now use all have email addresses dedicated to them and I use a non-US cell phone for two-factor authentication. No one has that number and I don’t use it for anything other than to authenticate access to my accounts.Also note that if you use an application like Authy for two-factor authentication (which I recommend), you should only allow it to work on one device (it’s the default setting). I like that it takes several days to reset your Authy account even if you are just putting it on a new cell phone with the same number. It adds a layer of security in case someone ends up getting a new phone on your number.For crypto in particular, once the access to your accounts is secure you must decide whether you should leave your assets on the exchange or be your own custodian. Both come with their own risks.Leaving it on an exchange: Your risk here is defined by the probability that this exchange will be hacked or be subject to new regulation. If you decide to go down this path, there are certainly better options than others. I know that the Coinbase team is doing a terrific job at keeping their assets secure. This does come with the drawback of users not being able to participate in certain airdrops, or not having access to new currencies from forks immediately, but I won’t delve into that topic here.Being the custodian: Your risk here is defined by the likelihood of your seed phrase been stolen, or all replicas of it being permanently damaged/irrecoverable. Someone could also get the password for your given wallet and steal the hardware from you, in which case, unless you immediately get a new wallet, recover your keys from the passphrase, and transfer all of your assets out, they’ll all be soon gone. You could also lose your passphrase, as well as the password as it infamously happened to Wired writer Mark Frauenfelder in hisepic tale of hacking his own wallet.People should weigh the probability of the exchange being hacked versus the probability of their seed phrase being stolen or lost.For most people with little crypto exposure, I would recommend they leave their crypto on Coinbase as it probably has a lower probability than the risks involved in being your own custodian.In addition, it’s way more convenient to just have your assets there rather than have to deal with the hassle of custody.If you own a lot of crypto assets, you should avoid leaving coins in exchanges to avoid the risk of those being hacked as it famously happened to Mt. Gox,Bitfinex, and YoBit not so long ago.In 2014, Mt. Gox handled 70% of all Bitcoin transactions worldwide when 850,000 bitcoins belonging to customers were stolen. They subsequently filed for bankruptcy and went out of business.It’s certainly worth your time to learn how to protect yourself against these attacks.If you choose to go down this path, I would highly recommend you get your own hardware wallet.The two main companies in this space areTrezorandLedger. I’m not very familiar with Trezor but can vouch for Ledger. When you first set up your wallet, you will be prompted with a passphrase and a password, the latter being specific to that wallet.Think of the passphrase as your master password for all private-public key pairs you will use in the future. If your wallet is damaged or lost, you can recover all transactions on a new one by having this passphrase. Just as you can be the one recovering these keys, anyone else who gets access to it will be able to do so as well so make sure that you save it in a safe place.Safe means: not on a computer with internet access; not on a hard-drive that’s not encrypted; not on a paper that could be easily stolen.You should also have more than one copy in different places (all of which must have tight security since your system is just as secure as your weakest link) to protect yourself against a potential loss (hard-drive malfunction, fire, a potential robbery, and others).As you are probably thinking by now, being the custodian of your own keys is no easy job.As a side note, while hardware wallets are certainly great products, if you are an institution or someone who might be likely the target of a personalized attack, this path might also fall short.First, when talking about redundancy and safety, this is not a binary dimension but a spectrum. You could either leave a paper with your passphrase hidden in the closet or store it in a safety box inside of a bank. On top of the steps described above, you should also seriously consider multi-signature security.At a high level, this means that you’d need multiple keys to transfer your funds (e.g. 2-of-4 policy would be mean that there are 4 keys, and you’d need at least two of them). There are already a few companies like Coinbase andAnchorthat provide this kind of service.",This Angel investor got all of his crypto stolen by T-Mobile impersonators
9638,3683944,2018-02-22 15:31:34,"By 2020, 30% of search is voice-conducted. Here’s what that means for your business.According to a statement released by Amazon, “millions” of Echo Dots — the manufacturer’s smart home devices featuring voice assistant Alexa — were sold during the holidays, making it the best-selling item on their website.Indeed, virtual assistants like Siri and Alexa are increasingly being used in everyday life — and this changes the way consumers browse the web.Gartner predicts that by 2020, 30 percent of our browsing sessions will be voice conducted. In addition, audio-centric technologies such as Apple’s AirPods, Google Home and Amazon’s Echo, are turning “voice first” interactions into ubiquitous experiences.Instead of searching via textual input, consumers are searching via voice input. This piece offers practical tools and tips for SMEs to adjust to this new era of browsing.A short history of searchTo understand how this new era of browsing will unfold, let’s go through the history of search and how this has evolved. Marcus Tandler, German SEO consultant, explained at TEDxMünchen that search engines used to be edited by humans. AOL then entered the picture by buying the first search crawler written by Bryan Pinker. Lycos was another search engine that used search crawlers, but Yahoo still remained powered by humans.Google changed the game in 1998 with their PageRank algorithm which ranked pages according to its importance and relevance. This was revolutionary at the time because pages were previously indexed according to how many times a keyword appeared on a certain page. In 2000, Google started monetizing search with the use of advertising which means that they had to make search very relevant to the user. In 2008, Apple released the iPhone 3G while Google released the Android operating system, making mobile search accessible to people.In 2010, Google introduced an update called Caffeine which made search results even more relevant to the user by making it time-sensitive. This was great because it made breaking news easily searchable to the user. For example, if a celebrity dies, Google will serve you content related to their death when you query their name rather than their biography page. However, users are now starting to type questions as search queries. So to answer questions, Google now needs to understand natural language to bring up the right answer.At the moment, voice search isn’t that widely available outside English speaking countries. For Dutch-speaking countries, the only voice assistant that supports the language is Siri —which is incorporated in Apple’s HomePod (allegedly coming to France, Germany and the Netherlands in 2018). However, a Dutch-speaking version of Google Assistant will be launched this year. So far, Alexa has only been made available in Europe in the UK and Germany.Nevertheless, voice assistants are on the rise. In 2015, search using virtual assistants like GoogleNow, Siri and Cortana jumped from statistical zero to 10 percent of overall search volume worldwide. According to Global Web Index, 25 percent of 16-24s use voice search on mobile. The queries used in these voice searches tend to be phrased in a question, meaning they start with “who”, “what”, “when”, “where”, “why” and “how”. For example, what would be “2014 World Series teams” in text input would probably be phrased as “Which teams played in the 2014 World Series?” when translated to voice input.There are also other differences between text input and voice input aside from using questions instead of phrases. Firstly, the results will often be location-specific. If a two users, one from Amsterdam and one from Berlin ask “Where should I go for pizza?” you would expect very different answers even if they have asked the same question as they are looking for a location-specific response. Second, there are fewer results that will come up on a voice search. Typically, a voice search will yield one to four results to prevent the user from having to scroll through pages and pages of results. Finally, results might have a higher click through rate as there are only a few results that are displayed. Therefore, it would be a coveted spot for your business to be featured in one of those.To increase your business’s likelihood of being featured in one of those results there are a couple of things to try out:# Put your information where voice assistants get their informationSiri gets transactional information from Opentable, CitySearch, Yelp, Yahoo! Local, ReserveTravel and Tripadvisor. If I were to own a restaurant, I would make an effort to be listed on Opentable, Yelp and Tripadvisor so to increase the likelihood of being featured. I would also invest in trying to get clients to review my restaurant on Yelp and Tripadvisor so that when people click through, they will see relevant and recent information on my restaurant. If I were providing services, I would make an effort to get listed in Yelp and Google My Business to increase my chances of showing up.# Have a localized SEO strategySince voice assistants serve location-specific content, you want them to deem your business or service as relevant to serve up to the people in your area. You want to reference microdata such as business location, price, store manager, phone number, etc. so that you’ll turn up in the results. Also, you want to make sure that you list out all the services and geographies that your business operates in. Testimonials, accreditations and external reviews will also help you gain credibility. Finally, you’ll want to keep your business name, address and phone consistent around the web.# Make your site and content mobile-friendlyYou need to make your site as mobile-friendly as possible as smartphones are the primary device on which people perform voice searches on. You can use Google’s Mobile Friendly test to see how your site is performing. This test will tell you the specific points that you need to improve on. You also need to focus on making your content mobile-friendly. As a rule of thumb, be succinct with your content and make sure that all the important information is easily accessible without scrolling or clicking additional buttons when you’ve viewing it in a mobile device.# Check out the competitionIt’s one of the oldest tricks in the book to scope out your competition. Test out your voice assistant and see who ranks on the questions that you want your business to feature on. If there are only 2 results, you may want to see what they are doing so you can claim the last spot.In summary, voice assisted search can change the game for the savvy SMEs who want to take advantage of this new way of browsing. You can get more people exposed to your business by making putting your information where the voice assistants get information, having a localised SEO strategy, making your site mobile-friendly and copying what your competitor who is ranking for voice search does.","By 2020, 30% of search is voice-conducted. Here’s what that means for your business."
9639,3683945,2018-02-22 15:32:02,"About TNWTNW SitesBig Data — useful tool or fetish?In 2012, the Obama administration unveiled a two hundred million dollar initiative to facilitate the growth of the Big Data industry and demonstrate its potential in large-scale markets. However, the initiative was an apparent failure. Despite government endorsement, Big Data has been unable to develop into an industry in its own right; the analysis and evaluation of billions of data points remain a trend rather than an emerging technology.Tom Kalil, Deputy Director for Policy at OSTP, released a statement on behalf of the Obama administration in which hedeclaredthat “to launch the initiative, six Federal departments and agencies will announce more than $200 million in new commitments that, together, promise to greatly improve the tools and techniques needed to access, organize, and glean discoveries from huge volumes of digital data”.The statement went on to say, “we also want to challenge industry, research universities, and nonprofits to join with the Administration to make the most of the opportunities created by Big Data”. Unfortunately, high hopes and large investments did not translate to practical and effective solutions, and the US Government failed to demonstrate the potential of Big Data in the industries they had targeted.Why the US government’s initiative failedThe downfall of the Obama administration’s initiative was undoubtedly the result of the increasingly negative perception of Big Data. For many Big Data represents the impractical and uncritical “fetishization of data”, or the unnecessary aggregation of large sums of information. The US government effectively placed a very large bet on the expectation that Big Data would evolve into an industry in its own right, however, it has distinctly failed to do so. Conversely, other forms or branches of Big Data, such as data visualization used by Netflix and leading technology service providers, have succeeded.Since the launch of the Hype Cycle, technology consulting firm Gartner has failed to include Big Data on its list of emerging technologies. The Gartner report in 2015 extensively covered Virtual Reality (VR) and Artificial Intelligence (AI), based on their data evaluation tools for technologies and hype cycles. However, due to its lack of adoption and maturity, Big Data did not meet the requirements to be considered an emerging technology. In comparison to these other industries, it has shown very limited signs of growth. The incredibly modest demand and enthusiasm for Big Data in the market has meant that many consulting firms like Gartner have continued to dismiss it — even with large-scale government initiatives.“Gartner Hype Cycles provide a graphic representation of the maturity and adoption of technologies and applications, and how they are potentially relevant to solving real business problems and exploiting new opportunities”,saidthe Gartner team. “Gartner Hype Cycle methodology gives you a view of how a technology or application will evolve over time, providing a sound source of insight to manage its deployment within the context of your specific business goals”.Despite this, it is important to note that Big Data has been embedded into almost all the operations and services of social media, technology and streaming platforms. However, firms like Netflix which actively use Big Data, prefer the term “data visualization” rather than Big Data due to its negative connotation.How Netflix has demonstrated the potential of Big DataPopular social media, technology, and streaming platforms like Netflix collect billions of data points on a daily basis in order to offer video recommendations and customized product marketing to their users. Earlier this year, the Netflix development team noted that data visualization is of “paramount importance” to the platform; it allows them to provide users with recommendations for movies, TV shows, and concerts based on their preferences and viewing history.At the Hadoop Summit in 2013, Jeff Magnusson, the manager of data platform architecture at Netflix, offered a rare insight into the company’s utilization of sophisticated data visualization and Big Data technologies. During the presentation entitled “Watching Pigs Fly with the Netflix Hadoop Toolkit”, Magnusson stated that the data should be accessible, easy to discover, and easy to process for everyone. “Whether your dataset is large or small, being able to visualize it makes it easier to explain”, he emphasized. “The longer you take to find the data, the less valuable it becomes.”Since early 2013, Netflix has continued to develop advanced data visualization tools, which have been a leading factor in the company’s success. As shown in the screenshot of Netflix’s data visualization tool, the Netflix platform has been relying on Big Data to investigate and analyze the consumer base for each Netflix show, the preference of genres based on age, and other important data that is crucial for both the company and the consumer base of Netflix.The problem faced by large conglomerates in major industries, with regards to utilizing Big Data, is not necessarily technological. Instead, it is a problem with the misrepresentation of Big Data andnegative public opinion, which has gotten worse over time. This is in part due to the fact that the average consumer has become significantly more privy to issues surrounding online privacy.In the long-term, Big Data could eventually solve public policy challenges including privacy. As Economist and Data Scientist, Matthew C. Harding pointed out, we are dealing with really complex problems in Big Data which require a complex interdisciplinary line of approach and behavior change. Harding has been using Big Data to answer crucial policy questions in Energy/Environment and Health/Nutrition. He believes that it is necessary to find ways to change human behavior. “Big Data is a tool that we have which allows us to make better decisions”, he stated in the talk.” I’m not afraid of Big Data.”This post was written by Joseph Young for Binary District, an international сollaborative technology community which creates unique competency-based workshops and events on new technologies. Follow them down here:",Big Data — useful tool or fetish?
9640,3683946,2018-02-20 12:10:44,"TNW SitesIt’s full Python expertise in one course bundle…and you can get it all at your priceFans of the Python programming language are a particularly vocal crew. Part of that devotion probably comes from the fact that despite the complexities of coding, Python’s advantages are pretty easy to define. It’s easy to learn, one of the most succinct to actually use, and doesn’t get bogged down in the loopholes and overly complicated architecture that can make other languages such a nightmare.Python is one of the key programming disciplines among working development pros — and you can learn absolutely everything about it in the Absolute Python course bundle, which you can pick up for any price you want to pay.Here’s how it works…just pay any price at all and you’ll get the course Selenium WebDriver With Python 3.x: Novice To Ninja. This instruction will show you how Python works hand-in-hand with Selenium WebDriver to automate higher level functions, log infrastructure, and basically just streamline your build.It’s great stuff…but if you’re getting into Python, jump in with both feet. By matching the average price of all purchasers, you’ll get the full complement of five courses, covering all aspects of beginner and advanced Python use. That includes:",It’s full Python expertise in one course bundle...and you can get it all at your price
9641,3683947,2018-02-22 14:59:57,"About TNWTNW SitesThis dildo orders Domino’s pizza after you cumYou have never experienced this level of comfort before: adult webcam platform CamSoda has engineered a hi-tech dildo that will not only help you achieve robust orgasms, but will also hit up Domino’s and order a pizza for you after you’ve cum.Unlike your dumb dildo, the so-called RubGrub has been equipped with an internet-connected button, which users can press to place a delivery for a pizza after a heated rubbing session.Why would someone want a contraption like this? Well, because “the body naturally begins to focus on other primal needs” after burning calories during masturbation, according to CamSoda.“Masturbation, while ultimately enjoyable, can be a strenuous physical activity during which an individual exerts a lot of energy and burns many calories,” said CamSoda VP Daryn Parker. “Inevitably, once someone has climaxed, they feel lethargic and hungry.”“That, coupled with the fact that we live in a society where people want everything at their fingertips, led us to the development of RubGrub,” he continued. “Get off and get stuffed, all with the quick click of a button.”To build the pizza-buying vibrator, CamSoda went balls deep into Domino’s API and ultimately figured out a way to bake the feature into the delicate hardware. The good thing is that it can be reprogrammed to order whatever food you prefer – so you can technically use the RubGrub to cheat on Domino’s.CamSoda has previously made headlines with other cheeky campaigns – like its webcam platform that turns your house into a sex-themed Big Brother or this cryptocurrency app that makes your sex toys vibrate harder when the price of Bitcoin, Ethereum or Litecoin goes up.We hope the pizza-ordering feature is eventually retrofitted in all CamSoda gear – this is the future we all deserve.At present, RubGrub is still in development, but the company plans to sell this magical device for $19.95 a pop. Needless to say, CamSoda has every intention of further improving the device before launch.“While initially launching with the ability to order pizza, the adult entertainment company has plans to add additional restaurant chains, including those serving Mexican and Chinese food,” the company noted.Get more marketing inspiration from our Marketing 2020 track at TNW Conference 2018. Check out more info on our flagship event here.",This dildo orders Domino's pizza after you cum
9642,3683948,2018-02-22 14:26:55,"Check out this “campaign video”about how the internet is broken — it’s a minute long of saying absolutely nothing, to the tune of thumping EDM music.Tweet of the day:Same, Jesse. Same.One of my biggest challenges is finding a Millennial GF who enjoys fashionable clothes as much as I do. Most Millennials don't care about clothes. I end up being more fashionable than 99.99% of my dates…and that makes them uncomfortable.https://t.co/Fe6BVanx8z",TNW's Big Spam: Google doesn't want you to send a message to Barack Obama
9643,3683949,2018-02-22 14:59:21,"TNW SitesProduct Hunt has a news app called Sip (and it’s a bit like Twitter Moments)Product Hunt just launched a new tech news aggregator app, called Sip. Think of it as an unholy hybrid of Twitter Moments, and Reddit’s autotldr bot. Stories are hand-picked and then compressed into the pertinent parts, along with some relevant commentary from Twitter. Sip presents these in digestible bite-sized chunks, which you can casually scroll through.Here’s how Sip presented the launch of Uber Express POOL. It presents the news in your typical inverted-pyramid style. Along the way, it throws into the mix links to deeper coverage from other tech sites, like The Verge, along with comment from Uber’s CEO and other notable tech commentators, like Gadgette’s Holly Brockwell and TechCrunch’s Josh Constine.This is a really compelling style of storytelling. It’s clearly inspired by Twitter Moments, but stories feel a lot more fleshed-out. There’s more detail. More context.I presume that’s because Sip is aimed at Product Hunt’s community of makers and tech enthusiasts, who likely want to take a deeper dive into news stories, without necessarily dealing with the legwork of reading 1,000 words or so.(In the interest of transparency, I should point out there are only a handful of stories on the app at the moment. I’m writing this before it’s officially been launched.)Sip bundles stories into daily roundups. So, once I finished reading about Uber Pool Express, I was immediately taken to a story about Houseparty’s success in capturing the lucrative (but fickle) teen demographic.This is an inherently linear approach. Once you finish a story, you’re taken to the next one. You can’t dive in-and-out, as you would with a traditional RSS reader.And as you’d expect with a tech news app from Product Hunt, new apps and gadgets feature heavily. Stories about products follow roughly the same format. You’re presented with the salient details, along with expert comment and hot-takes from Twitter.What separates Sip from the other micro-news apps (and there are several, believe me) is that it’s kind to your attention span by design. Unlike email or a traditional RSS reader, which you might check several times a day, Sip sends just one push notification in a 24-hour period. There’s no need to instinctively check it.It’s also pretty conservative with how it uses push notifications. It doesn’t buzz or bleep, or do anything that otherwise might break your concentration.Sip isn’t a groundbreaking product, insofar as its inspirations are obvious (Twitter Moments), and there are a lot of other similar apps on the market. Simply News, for example, offers ultra-compact stories.But what makes Sip awesome is how it presents that news; both in terms of storytelling and how it alerts you to news. If you’re tempted to give it a spin, you can download it on Android and iOS today.Interested in trying new ways to do old things? Try visiting the Design Thinkers track at TNW Conference 2018. More details here.",Sip is Product Hunt's new news app
9644,3686628,2018-02-22 16:00:00,"China's Xinjiang surveillance is the dystopian future nobody wantsMonitoring tech pioneered in the province is spreading across China and the world.In July 2009, deadly riots broke out in Urumqi, the capital of Xinjiang, China. Nearly 200 people died, the majority ethnic Han Chinese, and thousands of Chinese troops were brought in to quell the riots. An information battle soon followed, as mobile phone and internet service was cut off in the entire province. For the next 10 months, web access would be almost nonexistent in Xinjiang, a vast region larger than Texas with a population of more than 20 million. It was one of the most widespread, longest internet shutdowns ever.That event, which followed similar unrest in neighboring Chinese-ruled Tibet in 2008, was the sign of a new phase in the Chinese state's quest to control its restive outer regions. The 2009 shutdown was the first large-scale sign of a shift in tactics: the use of technology to control information.""Xinjiang has gotten little attention, but this is where we're really seeing the coming together of multiple streams of technology [for surveillance] that just hasn't happened in other contexts before,"" said Steven Feldstein, fellow in the Democracy and Rule of Law Program at the Carnegie Endowment for International Peace.Nine years later, Xinjiang has seen the widespread implementation of sophisticated high-tech surveillance and monitoring technology, what BuzzFeed called ""a 21st century police state."" But what happens in Xinjiang does not stay in Xinjiang. The technologies piloted there are already spreading across all of China, and there are even early signs that Chinese companies are beginning to sell some of this technology to other authoritarian-minded countries. If this trend continues, the future of technology, particularly for those in the Global South, could more resemble what's happening in Xinjiang than developments in Silicon Valley.Xinjiang is the home to the Uyghurs, a Turkic people who mostly follow Islam and have a distinct culture and language. Not surprisingly, the region has a tenuous relationship with Beijing, which is more than 1,400 miles away. Protests, riots and even terrorist attacks have been connected to the Uyghur struggle, which gives cover to Chinese authorities to implement the harshest strategies there.""Abuses are most apparent in Xinjiang because of the lack of privacy protections but also because the power imbalance between the people there and the police is the greatest in China,"" said Maya Wang, China researcher at Human Rights Watch.""The power imbalance between the people there and the police is the greatest in China.""That is why security investment in Xinjiang skyrocketed after the riots. According to Adrian Zenz, a lecturer at the European School of Culture and Theology who has written extensively about the police presence in Xinjiang and Tibet, the region's security forces doubled between 2009 and 2011 to more than 11,000 people. And it kept growing: In 2017, he documented more than 65,000 public job advertisements for security-related positions in Xinjiang, and last year Amnesty International estimated that there were 90,000 security staff in the region, the highest ratio of people to security in any province in China.Several new tools and tactics accompanied this rise in security personnel, most notably the implementation of ""convenience police stations,"" a dense network of street corner, village or neighborhood police stations designed to keep an eye out everywhere and rapidly respond to any threat, perceived or real. But there were also corresponding investments in security technology on a globally unprecedented scale. It started with a drive to put up security cameras in the aftermath of the 2009 riots before evolving into something far more sophisticated, as Xinjiang turned into a place for state-connected companies to test all of their surveillance innovations.""The rule of law doesn't exist,"" said William Nee, China researcher at Amnesty International. ""They are able to pioneer new methods of control that, if successful, they could use elsewhere in China.""Today, Xinjiang has both a massive security presence and ubiquitous surveillance technology: facial-recognition cameras; iris and body scanners at checkpoints, gas stations and government facilities; the collection of DNA samples for a massive database; mandatory apps that monitor messages and data flow on Uyghurs' smartphones; drones to monitor the borders. While there's some debate over how advanced the system tying these technologies together is, it's clear that China's plan is for a fully integrated system that uses artificial intelligence to rapidly process massive amounts of information for use by the similarly massive numbers of police in convenience stations.""[Xinjiang] represents a very new frontier and approach when it comes to online surveillance and oppression.""For Uyghurs, it means that wherever they go, whomever they talk to and even whatever they read online are all being monitored by the Chinese government. According to The New York Times, ""When Uighurs buy a kitchen knife, their ID data is etched on the blade as a QR code."" BuzzFeed documented stories of family members too scared to speak openly to relatives abroad. And the combination of all of these tools through increasingly powerful AI and data processing means absolute control and little freedom.""It's one thing to have GPS tracking. It's another thing to monitor social media usage of large populations,"" said Feldstein. ""But to do that in combination with a large DNA database of up to 40 million people and to integrate those methods with other modes of surveillance and intrusion -- that represents a very new frontier and approach when it comes to online surveillance and oppression.""The result, at least for China, is a massive success. Violence in the region has fallen as riots, protests and attacks are now rare in Xinjiang. Part of that is due to the presence of the state, but it's also related to a rise in fear, as no one is sure how pervasive the Chinese surveillance apparatus is.""People can never be sure if they are free from monitoring,"" said Nicole Morgret, project coordinator at the Uyghur Human Rights Project. ""The fear is such that even if the surveillance is not complete, people behave as if it is. The technology is being rolled out so quickly.""That is because access to the actual platforms being used by the Chinese authorities is limited, and much of the knowledge about surveillance technology comes from observations by the few journalists who can report from Xinjiang or through looking at public tender and budget documents. Or, increasingly, the knowledge comes from observing how other regions in China are being monitored and how Chinese tech companies abroad are deploying or marketing similar tools.“The fear is such that even if the surveillance is not complete, people behave as if it is.”Nicole Morgret, Uyghur Human Rights ProjectWhile the Xinjiang model may be extreme even for China, it is starting to influence policing across the country. The advent of the surveillance state in Xinjiang has come alongside China's increasingly tightening control over national information flows, including the blocking or removal from app stores of many foreign apps, VPNs and platforms, most recently Skype.""The question a lot of people have [is] ... to what extent is this going to be rolled [out] across the rest of China and packaged and sold to other repressive governments around the world?"" said Morgret. ""You can definitely see parts of it being implemented in China proper, such as the police database and collecting DNA samples from certain people. I certainly suspect the government has ambitions to create this type of total surveillance across the country.""The government has a powerful tool at its disposal, as last year, a new cybersecurity law went into effect that greatly broadens the power of the state to further control information. It requires foreign companies to maintain data centers in China, something Apple, for example, is complying with, leading the nonprofit watchdog group Reporters Without Borders to warn journalists working in China not to use iCloud anymore to store data. WeChat, China's do-everything app, is already suspected of sharing user data with the state.There are other signs that Xinjiang's policing innovations are entering the rest of China. The country is planning to integrate footage from its estimated 176 million surveillance cameras into a ""police cloud"" system, linked to national identity cards, making it possible that in the near future, everyone in China could be tracked anywhere. A model of this was demonstrated earlier this month when news reports emerged that new facial-recognition glasses are being used by police in train stations and airports across the country, tracking travelers ahead of the Lunar New Year.Considering all of this, it's no surprise that China is already the world's biggest market for surveillance software and hardware, estimated by industry researcher IHS Markit at $6.4 billion in 2016, a figure expected to triple by 2020. China's tech giants Baidu, Alibaba and Tencent are also jumping in, investing heavily in surveillance technology to take advantage of this boom.These companies are starting to sell some of these tools abroad as well. In Ecuador, a Chinese ECU911 Integrated Security Service system, the development of which was connected to the state-owned China National Electronics Import and Export Corporation, was deployed in 2016 and credited with a 24 percent drop in crime. A more worrisome case was uncovered by Human Rights Watch, which found evidence that the Ethiopian government was using telecom-surveillance technology provided by the Chinese telecom giant ZTE to monitor the political opposition, activists and journalists.""China wants to become a world leader in AI, and that includes a lot of these security applications that are already earmarked for exporting.""Other companies are following ZTE's path. Yitu Technology, an AI facial-recognition company, has already set up offices in several African countries and is looking to expand to Europe, where it sees potential due to recent terrorist attacks -- the same rationale initially used to expand the surveillance state in Xinjiang. These examples are few and not yet a sign that the Xinjiang model is having a big global impact, but even if the overseas market for Chinese surveillance technology remains limited for now, many observers think that could quickly change.""Now that China is delving into this new technology realm and is repressing very successfully and effectively, it is by nature that other dictatorial regimes would try to emulate this,"" said Feldstein.""I think we're on the threshold of this exploding,"" said Zenz. ""China wants to become a world leader in AI, and that includes a lot of these security applications that are already earmarked for exporting.""While the technology itself is not necessarily harmful, the concern is that in the wrong hands, it could empower repressive governments around the world to further abuse human rights. And the number of these regimes is growing, as recently released reports from the Economist Intelligence Unit and Freedom House show that around the world, free speech and democracy are falling and censorship, authoritarianism and autocracy are rising.""The Chinese government is leading on thinking around mass surveillance, and it has the impact of influencing other countries to think, 'Well, we could have an authoritarian government but look outwardly stable by putting in these systems to make sure that even if people are discontented, we can still keep them down by ensuring that every move is monitored,'"" said Wang. ""As this technology becomes cheaper, that reality might become more possible even for countries without massive resources like the Chinese government.""Id Kah Mosque in Kashgar, XinjiangIn Xinjiang, there are no signs that the massive buildup in both police presence and surveillance technology will recede anytime soon, despite the perceived success in limiting violence and protests thus far. If anything, it looks like things will get a lot worse. More and more Uyghurs, perhaps as many as 120,000, are being rounded up and sent to reeducation camps for minor offenses. Increasingly, any outward expression of religion or cultural expression is being seen as subversive, with even elderly intellectuals facing arrests, like the 82-year-old Islamic scholar Muhammad Salih Hajim, who died earlier this year in a reeducation camp. Now Uyghurs are also being forced to hand over DNA samples and put spyware on their phones. Meanwhile, spending on both technology and human-security presence is expected to rise even further.""It is going to crazy heights and there are no sign of it abating ... quite to the contrary, the state officials are really into intelligent, big data processing, networking of information, storing all the information and linking it up, applying AI and predictive policing for it,"" said Zenz.At least one facet of the Xinjiang model has gone global. Internet shutdowns, like what happened in Xinjiang in 2009, are now common around the world. Just this past year, there were widespread internet shutdowns in Indian-controlled Kashmir, the English-speaking region of Cameroon, Ethiopia, Kenya and more than 30 other countries. Often the causes are similar to what took place in Xinjiang -- ethnic tensions, riots or political events such as elections.""It's an increase around the world,"" said Melody Patry, a spokesperson with Access Now. ""Moreover, the phenomenon of repeat offenders is on the rise. ... When a government issues a first internet shutdown, they are more likely to issue others.""But China has moved on, and internet shutdowns are now rare. According to Access Now, there was only one documented shutdown in China in all of 2016. While uninformed observers could see this as a sign of progress, in actuality it shows that the next frontier of digital surveillance and state control is not blocking information access but harvesting it with a purpose.""You don't need these blackout shutdowns anymore when you have much more fine-grained mechanisms of control ... that can very early on detect potential issues and problems, and in turn promote self-policing, self-censorship,"" said Zenz. ""Because people know what consequences there are.""The shift in China is that the internet, which was initially seen as a threat due to its ability to allow users to access information, is now being perceived differently. What was back in 2009 blamed for the riots is now the source of information empowering the Chinese government to preemptively arrest and detain not only Uyghurs but also, increasingly, Chinese human rights lawyers, feminist activists and journalists around the country before they can post something inflammatory on a website or share sensitive content on WeChat.""The internet ... has become a great source of information that can be intelligently processed at capacity and speed that was not possible 10 years ago,"" said Zenz. ""What we see is a moving from a mere firewall that just blocks or an instant response, like the deletion of messages, to proactive self-censorship.""The global rise in shutdowns, which Access Now notes are getting more sophisticated and fine-tuned, shows that China's Xinjiang model has a market in an increasingly technological, authoritarian world. How quickly other countries follow China's move toward more total, personalized and data-driven control depends on both the need and the availability of the tools pioneered in Xinjiang on the global marketplace.Nithin Coca is a freelance journalist who focuses on social and economic issues in developing countries, and has specific expertise in South and Southeast Asia.Coca's feature and news pieces have appeared in global media outlets including Al Jazeera, Quartz, Forbes Asia, SciDev.Net, Southeast Asia Globe, The Diplomat, Vice and numerous regional publications in Asia and the United States.",China's Xinjiang surveillance is the dystopian future nobody wants
9645,3686730,2018-02-22 16:17:00,"UPS is working on a fleet of 50 custom-built electric delivery trucks0UPS will work with partner Workhorse, a battery-electric transportation technology company, to develop and deploy a fleet of 50 custom-built plug-in electric delivery trucks with zero emissions.The goal is to make trucks that cost as much to buy as do traditional fuel-based delivery vehicles – even without taking into account subsidies. The Workhorse designed-vehicles, will be all-electric, and are designed to run on a single charge throughout a normal delivery day and then charge back up overnight.Workhorse says they’ll have a 100 mile range, which is a good fit for in-city routes, and the trucks will first enter testing in urban areas in various parts of the U.S., including Atlanta, Dallas, and LA. The test will lead to fine-tuning, which will lead to a larger fleet deployment targeting 2019.UPS’ goal with this is to help meet its corporate renewable energy and carbon footprint goals, as well as to hopefully reap benefits in terms of vehicle operation efficiency, and the cost of maintenance (which should be far less using all-electric trucks).",UPS is working on a fleet of 50 custom-built electric delivery trucks
9646,3686967,2018-02-22 17:02:59,"About TNWTNW SitesCryptocurrency News – Feb 22 – I’m The Apps Man (Frowning)Hello everyone, good morning. I’m here at Cryptocurrency News Headquarters and I have the latest.No regretsThe Bitcoin Regret Club is a thing where you can, say, choose a date, and then it will say how many moneys you’d have had you invested whatever you put in to see how rich you’d be. It’s got a lot of coverage by people saying “boy oh boy, I’m an idiot for not investing in the mysterious online money that has absolutely no backing!” It’s kind of like when people say “I thought about investing in Apple stock decades ago but I didn’t,” except it’s more insufferable.Experimentation cornerSo I’ve been having some fun recently with botnet-for-hire Nicehash, which allows you to buy terahert/petahert-level computing power sourced from across the internet. I’ve been using it to mine Litecoin Cash, and one thing I’ll tell you is it definitely is the cheapest way I’ve bought Litecoin Cash yet. Litecoin Cash is also a complete mess right now as they haven’t got it on any exchange other than Yobit, which doesn’t actually have it and is just issuing IOUs, and MeanXTrade, which is uh, person-to-person Litecoin Cash exchanging. Currently the price of LCC is either $1.50, $2 or $5 depending on things I don’t totally understand. Either way, throwing ~5PB of power at a pool does seem to tip the scales in your favor. This is yet another example of really democratic and egalitarian technology, and not something that’s inherently tipped in the scales of those with money!Blockchain is secure and safe. Security is a big topic and one everyone should fully understand. Maybe you’ve heard about hacks of centralized exchanges. And you’ve probably heard about a person going ‘dumpster diving’ to find an old hard drive because their private key was stored there. It’s important to know that virtually all the losses were due to hacking of centralized exchanges, losing private key information and gaining control of private keys by bad actors, and not some security vulnerability in bitcoin technology.Slow news day I guess.BitblunderRemember BitFunder? Me neither, but it was a “stock exchange” for cryptocurrencies, which is apparently different from a cryptocurrency exchange, and also did fraud. Anyway, I put this one in before I fully read the article and it’s super boring so I’m just gonna move on.Robinhood actually existsSo Robinhood’s exciting new crypto project where you can eventually buy cryptocurrency instantly is now sort of live for some people who were in a list. Once again – and this is one of the few times I’ll say “the media got it wrong” – people can’t seem to coherently write about a company they like. Robinhood literally hasn’t put it live but TechCrunch says it “hits 4 million users” before shortly saying that it’s a waitlist. This is the same garbage that happened when Coinbase promised instant transfers from banks and then never did it. Nice work everyone! I was contacted by Robinhood to say that I was suggesting that 4 million people are trading crypto on there. Incorrect! I was actually suggesting that 4 million people cannot trade because there’s a waitlist, so I just wanted to put that in big bold letters. I also want to repeat that I think waitlists are only there to benefit the company and not the customer – they get you a bunch of press, they give you a bunch of waitlisted customers who probably will forget they signed up, and so you get the user bump without as much effort. It’s not good!John McAfee update: TV starThe teaser for my upcoming documentary on Australia's Channel 7. They made me look entirely normal. Amazing what clever editing can do.https://t.co/lJEb8Q2wdA",Cryptocurrency News - Feb 22 - I'm The Apps Man (Frowning)
9647,3686968,2018-02-22 16:54:03,"About TNWTNW SitesBanks are still overcharging Coinbase users for cryptocurrency purchasesCoinbase customers are going through an especially rough patch. Days after the popular exchange desk admitted its users are being erroneously charged for the same purchase multiple times, complaints that the issue is still occurring continue to roll in on Reddit.“Triple charged again,” read one of the more recent threads. According to the affected customer, the original purchase took place on February 4, but he was charged on two separate occasions: February 16 and February 21. The more problematic part is how confusing the refund process appears to be.“Coinbase tell me to contact my bank,” the Redditor continued. “But my bank told me that I need to contact Coinbase because they have the money.”The incident is one among several others, claiming they experienced the same hurdle over the past couple of days.“Coinbase made unauthorized charges again,” reads another thread posted on February 21. “Coinbase quadruple charged me now,” worryingly said the author of yet another thread.While the exchange desk initially blamed the issue on Visa, Visa later on insisted the issue did not originate from its system, practically putting the blame back on Coinbase. The leading financial service then released another statement – this time together with payment processing provider Worldpay – to clarify the issue did not stem from Coinbase either.Regardless, Visa didn’t explicitly name a culprit for this technical blunder.“Worldpay and Coinbase have been working with Visa and Visa issuing banks to ensure that the duplicate transactions have been reversed and appropriate credits have been posted to cardholder accounts,” Visa told TNW. “All reversal transactions have now been issued, and should appear on customers’ credit card and debit card accounts within the next few days.”Contrary to this promise though, users continued to report being unable to claim their money back. Indeed, numerous customers said that – despite its announcements – Visa reps knew nothing about the company “accepting responsibility for overcharging.”It is worth nothing that some users have already reported successfully claiming their refunds – not without incurring small fees from their banks though.And while the parties involved are still figuring out how to avoid taking accountability for this massive fail, users remain in the dark on how – or when – they can expect to receive their cash back.",Banks are still overcharging Coinbase users for cryptocurrency purchases
9648,3689273,2018-02-22 17:33:00,"Flying with a VR headset isn't as dorky as it soundsAs a somewhat regular flyer, I had always been intrigued by the concept of wearing a head-mounted display for some immersive in-flight entertainment. However, I never really found the ""cinema"" part of existing ""personal cinema"" headsets pervasive at all. Watching a tiny video through those headsets is like sitting in the last row of an empty theater. I'm not going to pay $800 for that.Then came the smartphone-powered VR headsets, but their three-degree-of-freedom (3DoF) tracking for just the head was never precise enough for prolonged usage. It wasn't until the Vive Focus, HTC's $630 standalone 6DoF VR device, that I finally decided to give virtual reality a chance to prove itself as a worthy alternative to those in-flight touchscreens. Luckily for me, my wife didn't forbid me from bringing this bright blue headset to our vacation, as long as it would fit into my carry-on.Thankfully, the device isn't as bulky as it looks. With the back support band folded in, the headset takes up about the same amount of space as my DSLR kit plus a flash unit, so even a regular-size camera shoulder bag will accommodate it. If I slip the headset into my backpack with its pair of inside-out tracking cameras facing downward, I still have plenty of space for its wireless controller plus my hoodie. Luckily, the backpack still fits under the seat in front of me on economy-class journeys.Many flights these days allow the use of portable electronic devices even during takeoff, provided that they are set to airplane mode. Normally, I'd be watching a video on my smartphone or tablet soon after getting on a plane, but for my first flight with my Vive Focus, I decided to put on this headset right before the plane took off. And that was how I came across a fundamental flaw of the device.I was getting familiar with the futuristic-space, port-like world and menus of Vive, but as soon as the plane left the ground, I could see myself drifting away from the virtual platform. It was almost like I was flying in the virtual world, and as fun as this sounds, it was a little freaky, as it was out of my control. If you have a fear of heights, you definitely won't want to try this.This issue persisted until the plane was no longer ascending. I later faced the same issue when the plane was descending, except this time I sank into the virtual ground instead. Any sudden turbulence would also cause the virtual world to drift a little. According to HTC, such acceleration or deceleration would indeed confuse the 6DoF sensors on the Vive Focus, but it's now looking into letting users switch to 3DoF tracking so that they can at least watch videos or navigate around menus during takeoff and landing. Such a ""vehicle mode"" would also let us use the headset with ease while taking other transportation.My experience was otherwise smooth. I spent a good amount of time watching my preloaded videos -- with my earphones plugged in, of course -- using the Moon VR Player, which has better file compatibility than the default video-player app. It also supports both conventional and 360-degree videos. Not that I would watch 360-degree videos on a plane, of course -- at least not in economy class, where people can see me.Unlike the ""personal cinema"" headsets, I could use the controller to pull the virtual screen closer to me -- imagine gazing at a 55-inch TV from just two feet away -- and even reposition it when my seat was reclined. And of course, with the sharp 2,880 x 1,600 AMOLED screen backed by 6DoF tracking, it really felt as if I was inside a cinema with an IMAX-like screen, especially that one time when I nodded off to Blade Runner 2049 on an early flight and then woke up confused as Ryan Gosling walked through a deserted city on my massive virtual screen. For a second, I forgot that I was still on the plane.There were, however, moments when it was obvious that I was looking through a pair of lenses. Like most other higher-end VR headsets (including the Oculus Rift and Windows Mixed Reality devices), the Vive Focus uses Fresnel lenses, which consist of ring-shaped prism facets. This is in order to drastically reduce weight and space, but it's not perfect. While the rings are usually not that apparent when placed right in front of our eyes, part of them does light up when the screen displays high-contrast images, mainly due to the reflective nature of the grooves between the rings. This can become rather distracting while watching an intense, dark movie scene; but then again, this is a common issue among the current crop of high-end VR headsets.I don't recall taking the Vive Focus off due to discomfort or nausea after extended usage, as long as its back support band was positioned in a way such that my head wasn't leaning on it. I also made sure that the interpupillary distance for the two lenses was optimal to begin with -- there's a slider for that on the bottom right side -- to keep everything in focus.While a typical 3DoF VR headset would usually last for mere minutes before I had to take a break, the Vive Focus could easily stay on my head for at least 15 to 20 minutes during my flights, with breaks for food, drinks or toilet between each session. With that in mind, the three-hour battery life was plenty for my four-hour flight between Hong Kong and Tokyo, let alone some shorter domestic flights in Japan. Plus, I could keep it charged while using it.Of course, it would be a waste to just watch videos on the Vive Focus. After all, this is the first 6DoF VR headset that doesn't need to be powered by a PC. The question here is whether the 6DoF tracking would also make a notable difference for other seated VR experiences, like gaming, while on a plane. More importantly, I was curious as to how careful I would have to be with my movements to avoid disturbing my neighbors.To be honest, the Vive Focus didn't offer many apps optimized for a seated user, which is understandable given that its main selling point is ""world-scale"" tracking. Its Viveport app library is still growing -- it had a little under 50 apps at the time of writing -- but that's not to say I didn't have fun with the few games I came across.In general, the best titles required relatively little body movement, so at no point did I have to worry about bumping into nearby passengers. Even for a racing game like VR Karts: Sprint, the steering was handled by merely twisting the wrist that held the controller, so my elbow was barely a threat to the passenger on my right. Similarly, in Fancy Shooter, I could aim ninja stars at fruits by just looking at them and then click on the controller to fire. I didn't even need to move my head much in Super Puzzle Galaxy unless I wanted to take a closer look at the puzzles, but that was when I became mindful of not getting too close to my neighbor's knee.Despite the small body movements while seated, the Vive Focus' 6DoF tracking was still key to making the whole VR experience enjoyable over a longer period on my flights. In contrast, if I were to use a 3DoF headset such as a Samsung Gear VR or a Google Daydream View, I could probably watch videos just fine, but it wouldn't last long due to the less natural feel from the head-only tracking: The virtual world wouldn't respond to my leaning forward, backward or sideways. It's the same with their games, even though they do offer a larger collection. It would also be hard for me to go back to the lower display resolution on those headsets.Does the Vive Focus make a good travel companion? I'm hooked. Once I'm locked into my own VR world, the immersive experience far outweighs what other people may think of me wearing this blue headset in public -- although I would still pick the white version personally. It's just nice to have an IMAX-like cinematic experience all to myself.But is this the ultimate replacement for the conventional in-flight entertainment systems? There are some clear advantages, including the fact that you can throw a good number of high-quality movies that you would actually watch onto a microSD card, plus the VR games are bound to be more fun than the in-flight games, if any. Sure, you can get similar results with a Gear VR or Daydream setup, but that would mean making do with 3DoF tracking plus a lower display resolution, and together they would make my flights even more tiring.Of course, there's always space for improvement for the Vive Focus, and that's also assuming that you could get hold of a unit in the first place. The fact that it's still limited to the Chinese market means it's missing out on the absolute latest videos from the likes of Google Play and Netflix, so until someone figures out a way to slap Daydream onto this Vive Wave-based device, you'll have to source your videos elsewhere. It would also be nice if someone could make a music player with a great visualizer for the headset, which may help users relax while traveling.The device does need more 6DoF VR games optimized for a seated user, but that's less of a concern for me. With developers claiming they've been able to port their games from other VR platforms to Vive Wave in just three hours, hopefully the Vive Focus' library will continue to grow. That said, the Vive Focus sets the benchmark for in-flight VR entertainment.Richard's love for gadgets was probably triggered by an electric shock at the age of five while poking his finger into power sockets for no reason. He managed to destroy a few more desktops and phones until he was sent to England for school. Somehow he ended up in London, where he had the golden opportunity to buy a then senior editor a pint of lager, and here we are.",Flying with a VR headset isn't as dorky as it sounds
9649,3689533,2018-02-22 17:20:23,"Stealth space catapult startup SpinLaunch is raising $30M0What if instead of blasting cargo into space on a rocket, we could fling it into space using a catapult? That’s the big, possibly crazy, possibly genius idea behind SpinLaunch. It was secretly founded in 2014 by Jonathan Yaney, who built solar-powered drone startup Titan Aerospace and sold it to Google. Now TechCrunch has learned from three sources that SpinLaunch is raising a massive $30 million Series A to develop its catapult technology. And we’ve scored an interview with the founder after four years in stealth.Sources who’ve spoken to the SpinLaunch team tell me the idea is to create a much cheaper and sustainable way to get things like satellites from earth into space without chemical propellant. Using a catapult would sidestep the heavy fuel and expensive booster rockets used by companies like SpaceX and Blue Origin.SpinLaunch plans to use a centrifuge spinning at an incredible rate inside a vacuum that reduces friction. All that momentum is then harnessed to catapult a payload into space at speeds one source said could be around 3,000 miles per hour. With enough momentum, objects could be flung into space on their own. Alternatively, the catapult could provide some of the power needed with cargo being equipped with supplemental rockets necessary to leave earth’s atmosphere.After some hesitation about emerging from stealth, Yaney agreed to talk to TechCrunch about his secretive startup, and show us the render of SpinLaunch’s future launch site hangar seen above. “Since the dawn of space exploration, rockets have been the only way to access space. Yet in 70 years, the technology has only made small incremental advances,” Yaney tells me. “To truly commercialize and industrialize space, we need 10x tech improvement.”SpinLaunch founder and CEO Jonathan YaneyUntil recently, few details about SpinLaunch have been available. SpinLaunch’s website is password-protected, and some Sunnyvale, Calif. job listings merely refer to it as a “rapidly growing space launch startup.” But last month, a bill was proposed in the Hawaii state senate to issue $25 million in bonds to assist SpinLaunch with “constructing a portion of its electrical small satellite launch system.” Hawaii hopes to gain construction contracts and jobs, and meet government goals for expanding space accessibility, by helping SpinLaunch.SEC documents show that Yaney raised $1 million in equity in 2014, the year SpinLaunch was founded, $2.9 million in equity in 2015, $2.2 million in debt in mid-2017 and another $2 million in debt in late 2017. Now Yaney confirms SpinLaunch has raised a total of $10 million to date, and that he’s personally an investor. As for the next $30 million, he says “The current status of our Series A raise is that we are still taking meetings with potential investors and have not yet received an executed offer.”Yaney has been co-founding startups since 2000, including TriVance and Moretti Designs. But a passion for aeronautics led him to become a 1,000+ hour pilot, and start communications and imaging solar drone startup Titan Aerospace. It sold to Google in 2014 after receiving acquisition interest from Facebook, and Yaney began work on SpinLaunch to huck satellites into orbit.Yaney explains that reaching orbital velocities typically “requires a rocket to carry massive quantities of propellant, leaving only a small fraction (a few percent) of the overall vehicle’s mass for ‘cargo.’ ” But SpinLaunch replaces rocket boosters with a kinetic launch system using principles “similar to those explored by several ground-based mass accelerators that date back to the 1960s. Modern adaptations include electromagnetic rail and coil guns, electrothermal-chemical guns, light gas guns, ram accelerators and blast wave accelerators.”NASA has investigated the possibility of catapult-assisted launches that fire off a track instead of a centrifuge, but none have become cost-effective enough to successfully be used to commercially launch things into space.Yaney’s method is different. He says “SpinLaunch employs a rotational acceleration method, harnessing angular momentum to gradually accelerate the vehicle to hypersonic speeds. This approach employs a dramatically lower cost architecture with much lower power.” SpinLaunch is targeting a per launch price of less than $500,000, while Yaney says “all existing rocket-based companies cost between $5 million and $100 million per launch.”NASA has researched catapult-based space launchers that fire cargo off a trackTwo sources say physicists who’ve looked into the company said a potential challenge could be air resistance on the cargo when the catapult fires. Earth’s atmosphere is so dense that it could be like the cargo was hitting a brick wall upon ejection. Any electronics or other sensitive materials in the cargo might have to be engineered to withstand intense G-forces. This all explains the pointy, aerodynamic launch vehicle shown in the hangar render up top.Now it’s a question of getting that ship into space. “During the last three years, the core technology has been developed, prototyped, tested and most of the tech risk retired,” Yaney proclaims. “The remaining challenges are in the construction and associated areas that all very large hardware development and construction projects face.” Touching the heavens isn’t cheap, so SpinLaunch is talking to big institutional VC firms that could afford to fund successive rounds.If SpinLaunch can overcome the technical barriers, it could democratize access to space by lowering launch costs. That could accelerate a new era of zero-gravity innovation, from space travel to mining to what we once thought of as mere science fiction.",Stealth space catapult startup SpinLaunch is raising $30M
9650,3689613,2018-02-22 18:51:53,"Google Home Max is the best smart speaker for the money. Period.Once dominated by Amazon, the space has exploded in recent months to include stellar offerings from Apple, Sonos, Microsoft, and my pick for best in class: Google. Google’s Home Max isn’t perfect, and the $399 price tag might be a bitter pill to swallow for those looking to upgrade from the original Google Home (or an Amazon Echo) but it checks enough boxes to make it the most well-rounded offering in its class.Weighing in at almost 12 pounds, it’s a giant among its peers. Clearly Google aimed high, building a speaker that’s closer in size to Sonos’ flagship Play:5 than its previous Home offering — which offered an okay speaker, but was really just a dressed up assistant much like Amazon’s Echo. Truth be told, a speaker this size may push some toward Apple’s HomePod (about half the size), but there’s a lot to love if you can stomach the footprint.Inside you’ll find four drivers — two 4.5-inch long-throw woofers and two 0.7-inch tweeters — six Class-D amplifiers, and six far-field microphones. Outside there’s a switch to mute the microphones, a USB-C port, and a 3.5mm input jack to hook the speaker up to an external audio source.At first I was rather confused about the USB-C port — though I always appreciate the thought — but Google tells me this works with an Ethernet adapter for areas with spotty Wi-Fi. I feel like a plebeian using it to charge my phone, but it works great for that too.The components are wrapped in a plastic shell available in two colors: “Chalk” or “Charcoal.” Or, in layman’s terms: dark grey or white. Its shell doesn’t feel high-end like speakers encased in wood, nor does it offer the aesthetic value, but Max doesn’t feel cheap, either. We’ll consider the aesthetic value a neutral; it’s not visually offensive, but it’s styling doesn’t convey the $399 price tag, either.Strictly personal preference, but I wish Google would have taken some chances here.As for control options, there’s a touch strip on top that offers the ability to turn volume up or down by swiping right or left. Or, you can tap the center to pause or play. Tap works as intended, but the volume control swipe is finicky and not-at-all reliable. Luckily there’s a mobile app that offers fine-grain controls for everything else, including bass and treble — which I didn’t find myself adjusting all that much.With a pricy speaker though, it’s all about sound. Google excels here. It’s not Sonos, but it’s closer than you might think. Vocals are clear, if a bit sharp at times, and low frequencies are as punchy as you’d expect in a speaker this size. The low end occasionally drifts out of control but overall performs admirably and compares favorably to most speakers in this price range.As single enclosure, Home Max does have stereo capabilities — it has two woofers and two tweeters, remember — when positioned horizontally. When you stand it upright, sound switches to mono, allowing you to connect a second speaker for “real” stereo sound. While capable of stereo, the soundstage isn’t really there in the horizontal orientation, and I didn’t get a chance to try two units in a traditional stereo configuration.It’s clear enough to separate individual instruments, but doesn’t really offer the opportunity to place them, mentally, on a stage in front of you. Some tracks feature excellent spacing and clear representations of the soundstage. But more often than not, they don’t. It’s really hit or miss.Max still sounds great. But given the close proximity of the internal speakers, just inches apart, this is a problem Google is going to have to AI its way out of.Home Max features a smart tuning mechanism called “Smart Sound” that calibrates the speaker for the size of the room it’s in. It’s a bit like Sonos’ ‘TruePlay’ but without the frenetic waving of an iOS device to calibrate the sound (TruePlay also doesn’t work with Android devices). Unlike Sonos, there’s no need to recalibrate; move it, and Max automatically adjusts the sound.It’s unclear how beneficial this is, as there’s no option to turn the feature off. To me, it seemingly sounds the same in every room I put it in. So if that was the goal, mission accomplished.Perhaps the biggest draw of Home Max is the always excellent Google Assistant. The six far-field microphones do an excellent job of picking up voices, even from distances of 20-ish feet or so. Even with loud music, I can typically trigger the assistant to adjust volume, skip tracks, pause, and interrupt whatever I’m listening to in order to ask a question.My favorite feature though might be Voice Match. Once set up, Google Assistant will deliver differing commute times, calendar entries, and daily briefings depending on who’s doing the asking. It works exceptionally well at discerning who’s asking the questions and delivering the appropriate results.On the connectivity front, Max works with any service that supports Google’s Cast protocol. Google Play Music, Spotify, Pandora, and others work without issue. Apple Music does not. You can, however, stream music over Bluetooth, or plug in your device using a 3.5mm jack — if yours still has one.For the money, Home Max is easily best in class. It doesn’t sound quite as good as the Sonos One (but it’s close, real close); and it’s not as small as Apple’s HomePod; but when measuring features against price it’s still the superior option.Options, though, are plentiful.HomePod might be the best choice for an all Apple ecosystem. Or Audiophiles that care less about a great smart assistant and dazzling AI might favor Sonos. And those just looking to break into the smart speaker game would be over the moon with a $100 Echo.",Google Home Max is the best smart speaker for the money. Period.
9651,3689614,2018-02-22 17:42:43,"About TNWTNW SitesYouTube still hasn’t fixed its Trending problemYouTube is in hot water again after it failed this week to catch a conspiracy theory video before it topped the Trending section — proving the company still doesn’t have a handle on its own algorithms, despite being raked over the coals for this multiple times in the past.The video in question — which, in the interest of good taste, I’m not going to embed or link to — is about David Hogg, one of the survivors of the Parkland high school shooting rampage which happened last week. The video claims he is an actor because he was … somewhere other than Florida at some point in his life. It makes as much sense as it sounds.It topped YouTube’s Trending section for several hours, over videos from official news sites. It was eventually removed by site moderators.The last time YouTube faced scrutiny over its Trending section was last month, when shocked viewers saw that Logan Paul’s “Suicide Forest” video was posted prominently there. It was eventually removed by Paul himself, but the question as to how it got there remained.The Trending section is predominantly controlled by an algorithm, so it’d be understandable if it picked up any video that happened to be getting a ton of views at any given moment, but the outline of the Trending section clearly says it avoids videos that are not “misleading, clickbaity or sensational.” I should think the video about David Hogg would certainly fall into at least one of those three categories.YouTube’s apology, as reported by Motherboard, sheds some light on why the video was present, and why the algorithm may not be as selective as it sounds:This video should never have appeared in Trending. Because the video contained footage from an authoritative news source, our system misclassified it. As soon as we became aware of the video, we removed it from Trending and from YouTube for violating our policies. We are working to improve our systems moving forward.So neither the title, subject matter, nor tags were enough to trip YouTube’s alarms — all the video maker had to do was include footage from a news source.Susan Wojcicki, YouTube’s CEO, promised last December to bring in more human moderators to help make sure nothing slipped by the algorithm, but either the company hasn’t hired them yet or that’s clearly not working.It’s forgivable for YouTube to miss bad videos among the rank and file — it’d be impossible to go through that many minutes of uploaded footage with a fine-tooth comb. But if any page needs to be more closely monitored by human eyeballs, it’s the page of videos YouTube is most actively promoting to its users.",YouTube hasn't fixed its Trending problem effectively
9652,3690976,2018-02-22 19:00:00,"MWC 2018: What to expect from the world’s biggest phone showTeam Engadget is en route to Barcelona for Mobile World Congress, and while the show officially starts on Monday, you'll start to see some of this year's key news and announcements as soon as this weekend. We don't want you going into things blind, though, so here's a primer on what to expect from the world's biggest and best phone-makers once MWC 2018 gets off the ground.SamsungUnlike last year when it trotted out a pair of new tablets, Samsung is actually bringing smartphones to Barcelona this year. We'll get our first official look at the Galaxy S9 and S9 Plus at a press conference before the show even starts, but countless leaks have told us almost everything we need to know. Barring a few minor differences, we know they basically look just like last year's models, and they're sure to pack Snapdragon 845 chipset when they debut in the United States.We also know that the big draw this year is the camera, even if the S9 and S9 Plus handle them a little differently. The former has one main camera around back while the latter -- like the Galaxy Note 8 -- uses a dual camera setup instead. No matter which route you take, you'll still get to use the camera's variable, mechanical aperture. Unlike other phones with fixed apertures, the S9's shutter dilates and contracts (like in a traditional camera) to control the amount of light that hits the sensor. Let's say you're shooting with a wide aperture, like the f/1.6 suggested by leaks: that allows for a shallow depth of field for sweet bokeh-filled shots and improved low light performance. Then you could switch to the f/2.4 aperture to make sure your landscapes come out looking just right.The improvements don't end there. We've also heard that the S9 will feature a super-slow-motion video mode and some kind of 3D emoji to rival Apple's own Animoji. Throw in some stereo speakers -- a first for Samsung -- and we're ultimately left with an impressive (if iterative) update. Now just we need to know how much these things will cost and when we can get them.Chris Velazco/EngadgetLGAfter some missteps and false starts, last year's G6 was proof that LG's smartphones are far from irrelevant. Too bad we're not getting at G7 at MWC this year. LG seems to enjoy spoiling its own surprises, so it confirmed we'd see an updated version of last year's V30 that'll probably be called the V30S. From what we've heard, it's mostly the same V30 we reviewed last year, just fleshed out with new AI features to make the cameras a little more capable. (Personally, I hope LG also used better screens for this updated model.)LG's Vision AI is supposed to help the V30 and future devices scan QR codes, perform image searches and provide shopping links for things the camera sees. If this sounds familiar, well, it should -- this sound conceptually familiar to the Bixby Vision feature Samsung baked into its S8s and Note 8s. Given the V30's impressive photographic chops, it's no surprise the Vision AI should help when taking photos. Vision AI was also trained on over 100 million images to help it better understand what it sees, and as a result, the camera can switch to a shooting mode appropriate for what's in front of it.This updated V30 will also get a slew of new voice commands for Google Assistant, marking the first time a smartphone maker has cooked up custom Assistant commands. Not only will the new V30 know to fire up food mode when you're snapping photos of your lunch -- you'll be able to ask it to snap the photo, too. Here's hoping we get some clarity on LG's upcoming flagship phone (code-named Julie) as well, but that probably won't happen for at least a few more months.SonySony has managed to avoid major MWC leaks so far, but it recently teased fans with a short video that suggests at least one curvy new Sony phone is coming to Spain. That seems like a clear sign that Sony is moving away from its long-running OmniBalance design language, and frankly, good riddance. Sony phones have mostly looked like the same ol' slabs for years, and enough is enough.Unfortunately, we don't have much detail on what this curvaceous new phone has going for it. Rumors of a so-called XZ Pro have been making the rounds for a while, though. If true, it'll be a new flagship-class device with a 5.7-inch, 4K OLED display and one of Qualcomm's fresh-off-the-line Snapdragon 845s. It might also be the first Sony phone on the market with a dual camera, and if that's the case, we're looking forward to seeing how Sony's implementation differs from all the others. We might also get to see an updated version of the company's Xperia Ear, if only because Sony has a solid track record of turning its wacky concepts into real products.BlackBerry MobileIt's been a long strange ride for BlackBerry fans, but 2018 should be a good year for the brand's diehard fans. Back at CES, execs told us to expect at least two smartphones with physical QWERTY keyboards this year, and with any luck, we'll get our first look at the show. If rumors hold true, one of those devices might be a spiritual, sliding successor to 2015's BlackBerry Priv with a curved screen, and we'd expect the other to be some sort of BlackBerry KeyONE follow-up. Other details are scarce at the moment, but we have seen some new codenames being thrown around lately -- BlackBerry Uni sounds pretty... awful, but we wouldn't mind spending some time with a BlackBerry Athena or Luna.AlcatelDon't forget about TCL's other smartphone brand, either. The company showed us a few of its redesigned phones at CES, but they were far from finished at the time -- we expect these new 1, 3 and 5 series phones to be ready to launch very soon. They're meant to be inexpensive machines, so there hasn't been too much hype surrounding them, but they're proof that you'll be able to get phones with 18:9 screens without making your wallet groan.Lenovo & MotorolaWe go to MWC expecting to spend all our time with smartphones, but Lenovo always seems to have some Windows machines to show off; you'd do well to expect a notebook announcement before the week is over. The big question is whether Lenovo will show off new, always-connected PCs -- you know, the ones powered by Qualcomm's Snapdragon chipsets -- or just the usual Intel-powered fare. We're tentatively leaning towards ""no"" since the Miix 630 is still so new, but hey -- we can hope right?Meanwhile, Lenovo's Motorola brand has been awfully quiet in the run-up to the show. That just might be because it had to deal with the mother of all leaks earlier this year. You can expect big updates for its G, X, and Z-series devices this year -- we're just not sure about when, or if they'll all be announced at MWC. Last year, Moto showed off new Moto Gs, so news of some follow-ups seems like a safe bet. But what's actually new?Well, the Moto G series is expected to get dual cameras across the board and a glossy redesign to make it look more like the existing Moto X4. The updated X series, meanwhile, should pack a 5.9-inch, 18:9 screen with an iPhone X-style notch that hides a pair of front-facing cameras. And the Z series? Well, it should still play nice with existing Moto Mods, but leaked images suggest the new Z phones will have 6-inch, curved displays. Here's hoping we hear about some new Moto Mods, especially that long-rumored 5G radio Mod. With 5G network trials slated for later this year, we'll need news sooner than later.NokiaEvan Blass/TwitterLet's face it: Nokia won MWC last year with its 3310 revival. Too bad we haven't heard of any new dumbphone sequels lined up for this year's show. We are, however, expecting to see a handful of (what else?) new Nokia Android phones. Leaked photos point to the existence of a Nokia 1 running Android Go, which all but guarantees it's a low-cost model that should see lots of play in developing markets. (For those not keeping track, Android Go is a special configuration of Android Oreo designed to run on devices with less than 1GB of RAM.)Also seemingly on deck is the Nokia 7+, an update to last year's Nokia 7 with barely-there bezels running around a 6-inch screen. Expect mid-range performance out of this thing, though we're cautiously optimistic about the ZEISS-tuned dual camera system around the back. Know what really has us excited, though? The Nokia 7+ has appeared in both gray and white, but both versions have some punchy orange highlights.HuaweiSorry, Huawei fans: You shouldn't expect much in the way of phones. We know the company is going to show off its new P-series smartphone in March, and as with the Galaxy S9 and S9 Plus, the camera might be the biggest reason to invest in those devices. Huawei's invite strongly hinted at a three-lens setup, but we'll have to wait a little longer for the juicy details. At MWC, we'll probably just see one of Huawei's new MediaPad tablets given the timing. Since Huawei has used MWC to show off its Windows machines in the past, you shouldn't discount the possibility of another Huawei PC breaking cover either. Really though, we're hoping the company shares more about its struggles with the US government.ASUSThey might not be tremendously popular in the US, but ASUS's ZenFones have steadily gotten better over the years. Based on a handful of leaks, we'd expect to hear about at least two new devices at the show. The standard ZenFone 5 should sport a rear-mounted fingerprint sensor and another extra-tall screen with an iPhone X-like notch. More importantly, it packs flagship levels of power thanks to its Snapdragon 845 chipset and 6GB of RAM. There's no firm word on price at the moment, but if it winds up being steep, there should also be a ZenFone 5 Lite lurking on the show floor somewhere. Very little about the device is known right now, but we ARE pretty sure it packs four cameras — two on the back and two above the display.Oh, and everyone elseCovering MWC is a huge undertaking, and there are a lot of other companies on our radar that are worth paying attention to. It feels a little weird sticking HTC down here, but the truth is the company hasn't put much effort into MWC in recent years. We'd be surprised to see anything more than just some new Vive demos on the show floor. On the other hand, ZTE will likely show off its Blade V9, with its 5.7-inch, 18:9 display and a Snapdragon 450 processor. And Vivo, a Chinese phone maker you probably haven't heard of, will spend some more time showing off devices a fingerprint sensor built into their displays.There's obviously going to be a lot of news coming out of Barcelona next week, and even with all this information, there's bound to be a few surprises. Be sure to stick around to get the full scoop on everything once MWC is officially underway.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",MWC 2018: What to expect from the world’s biggest phone show
9653,3691864,2018-02-22 17:52:45,"Airbnb is rolling out a new tier aimed at higher-end travelers0Airbnb today is rolling out a few new additions to their home-booking system, including new tiers that are aimed at higher-end customers, called Airbnb Plus and Beyond by Airbnb.The new Airbnb Plus tier has homes that are verified for “quality and comfort,” rolling out with around 2,000 homes to start that have been inspected against a thorough checklist. The luxury tier, called Beyond by Airbnb, is built around whole trips including hospitality, custom experience and — of course — higher-end homes. Each appears to be targeting verticals within the travel space that make sense based on the typical hospitality industry, but weren’t specifically singled out on Airbnb.Rather than just searching for a home on Airbnb and flipping through the photos or reviews, the new tiers may offer customers willing to spend more an opportunity to tap into the same expectations they’d get from a luxury resort — or, at least, a higher-end one aiming to hit the status of something like the Sofitel in Bangkok, Thailand. Frequent business travelers often have accumulated massive piles of reward points (and maybe have bigger travel budgets) and may be accustomed to this, and there’s also plenty of opportunity to target the same kinds of experiences that something like an Atlantis full-service resort in the Bahamas might offer (with water slides and all).In addition to those tiers, which are being announced at an event at San Francisco this morning (I overheard there may be more than 1,000 employees at this event, as well as a few dozen “super-hosts”), the company also is adding new ways to search for property on its service. The new brackets are vacation home, unique space, B&B and Boutique. All these basically existed on the platform, but that segmentation wasn’t there yet. Hosts are going to have an opportunity to have a more granular way of classifying their homes. As Airbnb pushes itself to be a community-driven product, it’s not surprising that a huge event would show up at a huge venue including those same super-hosts.In fact, you’ll often find these kinds of home setups already in place when you’re searching for places to stay abroad. You might find several rental units in the same building, or of the same configuration, all by the same host. That’s almost like a kind of faux hotel system, but this will offer an opportunity for Airbnb hosts to differentiate themselves for users that are looking for something more specific. Or you might find a small house on the beach where the hosts offer plenty of amenities and maybe even food. Either way, these granular options aim to not only help users dig more deeply into specific experiences, but also help hosts raise their hands to identify that their homes are catered around those experiences.As Airbnb has tried to begin turning itself into a kind of experience machine with the launch of Experiences, it’s trying to position itself as an alternative and more robust option in the hospitality industry. Rather than just booking a hotel at a luxury resort you find on TripAdvisor, Airbnb is trying to build the credibility that it can offer unique experiences you won’t find in those hotel environments — whether that’s a tiny apartment in the middle of Shinjuku, Japan or a beautiful condo in the middle of Ponta Delgada in the Azores. All this comes back to the hosts, which now have an opportunity to try to craft those experiences and, in the end, pick up an additional revenue stream on the property they might already own.While Airbnb was already a kind of intent-driven service — users probably have a general idea of what they’re looking for when they come — this could really go two ways. It could make Airbnb more of a hub for discovery for people looking to travel abroad with customized experiences and a more robust array of options for homes. Or it could force those to be even more granular and silo users even further. Either way, the company’s Experiences product seems to be doing fine (and they are not bad from my experience trying it, as well), so it seems like the end result is going to be up in the air until we see how this plays out for a few months.",Airbnb is rolling out a new tier aimed at higher-end travelers
9654,3691866,2018-02-22 15:46:47,"Robinhood rolls out zero-fee crypto trading as it hits 4M users0Coinbase has some serious competition. Today, Robinhood starts rolling out its no-commission cryptocurrency trading feature in California, Massachusetts, Missouri, Montana and New Hampshire. Users there can buy and sell Bitcoin and Ethereum with no extra fees, and everyone can track those and 14 other coins in its sleek app. That’s compared to paying 1.5 to 4 percent fees in the U.S. on Coinbase. Users can sign up on the Robinhood Crypto site to waitlist for access.Robinhood has a chance to usurp Coinbase as the de facto crypto trading site app by vastly undercutting its fees. When people are buying thousands of dollars of cryptocurrencies at a time, Coinbase’s 1.5 to 4 percent fees in the U.S. can quickly add up.But Robinhood sees giving away the service for free as a powerful play to gain users for its existing service that lets people trade stocks, ETFs and options without additional charges. Its stylish, retro-future Tron interface is also a super easy way to check on pricing and news about 16 coins: Bitcoin, Ethereum, Bitcoin Cash, Litecoin, Ripple, Ethereum Classic, Zcash, Monero, Dash, Stellar, Qtum, Bitcoin Gold, OmiseGo, NEO, Lisk and Dogecoin. Tracking is now available for everyone, with trading coming to waitlisted users and more states soon.Robinhood Crypto first announced the feature last month, with one million people signing up in just the first four days. That interest has driven Robinhood’s total registered user count to more than 4 million, up from 3 million in November. Those users have transacted more than $100 billion to date, saving $1 billion in commission fees.On most stock trading services like E*Trade and Scottrade, customers pay around $7 per trade to cover these companies’ marketing, physical branches and sales reps. Founded in 2013, Robinhood ditches those fees by running a lean operation centered around engineers and its app. It makes money on the interest of cash its customers keep with it, or by selling monthly Robinhood Gold subscriptions that let users borrow money to trade with.That business has allowed the startup to raise $176 million, most recently at a $1.3 billion valuation. And with its free crypto trading, it may have found a way to luring in a fresh class of amateur investors. Keeping security locked tight will be critical, especially given disastrous breaches at other crypto companies. But as crypto draws a new generation into the world of finance, Robinhood wants to help them play the market, day or night.",Robinhood rolls out zero-fee crypto trading as it hits 4M users
9655,3691867,2018-02-22 15:02:21,"The beauty company Glossier just closed on a whopping $52 million in fresh funding0Glossier, the nearly four-year-old, direct-to-consumer beauty company, has landed $52 million in Series C funding in what it describes as a heavily oversubscribed round. The financing was led by earlier investors IVP and Index Ventures.The New York-based company — which evolved out of the popular blog Into the Gloss by founder and CEO Emily Weiss — began selling its own make-up at the outset but has more recently added body and fragrance products, too, bringing its total number of offerings to 22. One of the company’s most popular products is a mascara-like eyebrow filler called Boy Brow. Among its newest: a solid version of its fragrance, You.The company says it launches something new every six weeks, on average, and that it spends a lot of time working on its formulas. (A spokesperson tells us Glossier spent 15 months developing a new $24 exfoliant.) But Glossier is just as well-known for brand-building and its ability to grow consumer awareness in a highly crowded global beauty industry that’s expected to grow from $433 billion today to $750 billion by 2024, according to one estimate.The 150-person company doesn’t disclosed its revenue numbers, but it tells us Glossier saw three times as much revenue last year as in 2016. It also opened offices in London and Montreal, after quietly acquiring a small Canadian digital strategy studio called Dynamo. (One of Dynamo’s cofounders, Bryan Mahoney, is now Glossier’s CTO.)And Glossier opened up a sixth-floor showroom in the same SoHo building where its offices are located. Even without a street-level window, it generates more sales revenue per square foot than the average Apple store, Weiss told New York Magazine in a glowing cover story about Glossier that ran last month. (The story also referred to Weiss as the “millennials’ Estée Lauder.”)Much of the company’s magic appears to be rooted in simplicity. Instead of offering 15 shades of lipstick, for example, it tells its customers it has developed the most universally flattering red lipstick in order to make it simpler for them to choose a shade.The company also prides itself on the numerous ways it keeps communication channels to its consumers wide open. Beyond its popular blog, Glossier uses Instagram as well as any brand, and in late 2016, it began testing a representative program to start paying the most powerful of its “Glossier Girls” — formerly unpaid brand evangelists — which it says hasgrown tremendously.The reps receive advance notice of new products; they’re also often asked to participate in feedback sessions, they have direct access to Glossier’s team, and they are invited to and host their own community events, as well as talk about Glossier’s products on social media. (In addition to access to Glossier goodies, they each have their own landing pages on Glossier.com where followers can learn more about them.)Weiss spoke at an event hosted by this editor last year, where she noted that in this day and age, not only is the customer always right, but thanks to social media — whether it’s a product review on Amazon or an Instagram post — that customer “has a microphone and she’s reaching 50, 500, 5,000 or 500,000 of her nearest and dearest friends and is able to talk about her preferences.”It’s why Glossier is so laser focused on “transparency” and “voice,” Weiss had explained.“We like to think that whenever we talk to [our customer] through captions on Instagram or through email or through copy on the site, that we’re writing text messages to a friend.” For Glossier, “staying true to that voice has created a lot of loyalty and trust with our customers.”It apparently has Glossier’s investors feeling very loyal to the company, too. With its newest round, Glossier has now raised $86 million from them altogether.",The beauty company Glossier just closed on a whopping $52 million in fresh funding
9656,3691944,2018-02-22 19:58:19,"Will AI enslave the human race? Probably not, but it might jack you at the ATM.AI is changing everything. The healthcare industry is in the middle of a revolution, social media is getting smarter, and the era of drone-wielding super villains is right around the corner.Earlier this week seven of the world’s most prominent organizations in the field of futurism published a report predicting the dangers posed by AI.The document is called “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation.” You can read the full version here. It’s 100 pages long and utterly terrifying. The only thing that could make it scarier is if Samuel L. Jackson were holding you at gunpoint and screaming it at you.It was put together in a collaboration between OpenAI, The Future of Humanity Institute, University of Oxford, University of Cambridge, Center for the Study of Existential Risk, Center for a New American Security, and Electronic Frontier Foundation.For perspective, those groups and universities represent notable figures like Elon Musk and Nick Bostrom, and contain members from each of the big US tech companies. This study wasn’t conducted by a vague market research group, but by working AI experts.One section in particular is troubling: “Physical Security.” It’s broken down into five distinct threats, each of which could be combined with any or all of the others to become even more frightening. Let’s grab some popcorn and dive in.First up: “Terrorist repurposing of commercial AI systems.”When a report on the threat AI poses to our physical security starts with the word “terrorist” we’re off to a bad start. Or maybe it’s a good start because the people who warn us about stuff like this are doing their jobs. When AI is used in this way, according to the study:Commercial systems are used in harmful and unintended ways, such as using drones or autonomous vehicles to deliver explosives and cause crashes.The fear here is that terrorists would gain control of autonomous vehicles and crash them into buildings, basically 9/11 with computers instead of hijackers. According to the researchers this falls under the danger of “expansion of existing threats.”The second section is called: “Endowing low-skill individuals with previously high-skill attack capabilities.”This threat is an extension of the cyber threat that AI poses. Just a decade ago hackers were considered computer experts, now all it takes to be a hacker is to download the right software on the darknet.It’s likely AI will transform physical crime in much the same way. Car thieves, for example, could let a computer figure out how to override the ECM on a vehicle, while AI-powered image recognition walked them through cutting wires or defeating alarm systems.Next up, bigger is always better: “Increased scale of attacks.”Human-machine teaming using autonomous systems increase the amount of damage that individuals or small groups can do: e.g. one person launching an attack with many weaponized autonomous drones.With this kind of attack we’re seriously getting into Marvel Comics bad guy territory. What if Elon Musk is really building underground tunnels all over the world to house a robot army instead of hyperloops? It would explain why he knows how World War III will start.And for those of you who’ve watched Netflix’s Black Mirror, there’s section four: “Swarming attacks.”In this case the researchers are talking about criminals using AI to attack multiple systems at once. While law enforcement trains for these conflagration attacks, the threat here is that a dozen different systems ranging from traffic lights to bank security could be compromised instantly.In Hollywood movies this is accomplished by the “brain” of the operation hiring a series of specialized expert criminals. But in the future it could be accomplished by a few idiots with iPhones and algorithms.If terrorists using drones to attack us or the rise AI-powered super villains doesn’t get your blood pumping, perhaps the idea of getting mugged remotely will. The final section is called: “Attacks further removed in time and space.”Physical attacks are further removed from the actor initiating the attack as a result of autonomous operation, including in environments where remote communication with the system is not possible.Imagine getting into an argument with someone on social media who later tracks you down using AI and sends a drone to smash your car windshield while you’re travelling down the highway at 100 k/mh. Or even worse, getting robbed at gun point by an autonomous machine that’s set to self destruct if it’s caught.Welcome to a future where even petty criminals can phone it in and work from home.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.","Will AI enslave the human race? Probably not, but it might jack you at the ATM."
9657,3691945,2018-02-22 19:42:45,"TNW SitesCryptocurrencies and their effects on emotion are bigger than you may thinkCryptocurrencies, like most other emerging technologies, are often called “disrupting” for a number of different reasons.Its advocates believe that they will change financial transactions and affect the social order, while its opponents believe that it will create a huge economic bubble and further distrust for financial institutions, for example.Whatever view one might subscribe to, it is hard to ignore the impact that cryptocurrencies have had on our world, particularly when it comes to the way we understand money and how that has affected our behaviour and feelings towards it.Money is a hard-to-grasp concept with huge psychological implicationsAll types of currencies, and money in a more general term, are not entirely unnatural but they are still incredibly hard to define and grasp from any kind of standpoint.While we will not go into the different definitions proposed by experts in the field, let us assume that money is simply a tool through which we can effectively trade goods and services on any kind of scale and in essentially any kind of situation.Of course, such a definition is very limited and does not at all cover things like wealth inequality, the political realities that constantly shape financial institutions, the subtle differences between trade, exchange, “the market”, and so forth.What we can understand, however, is that money affects every single person on the planet, in one way or another. We talk about it, we work for it, we spend it for a variety of different reasons and we try to accumulate it for others.This, as one might expect, has huge implications on human psychology. The wealthy, for instance, are often seen as less compassionate than their less advantaged counterparts with studies to back it up.Cryptocurrencies create both different and similar effectsAs their name suggests, cryptocurrencies are still a form of currency despite how different they are to the more traditional forms that everyone is used to by now.What that means on the behavioural and psychological side of things is that they tend to have similar effects on human behaviour, emotion, and thought like traditional currencies and money do.Things like accumulating wealth and maximising one’s capital are still huge for those who are deeply invested in cryptocurrencies. Those who have spent fortunes and have accumulated more in return in the cryptocurrency game have a lot to win and lose by the rise and fall of currencies like Bitcoin and Ethereum, for instance.However, cryptocurrencies are also distinctly different. Their very conception was somewhat meant to be a disruption of the current social order and the way financial institutions are run, a sort of “by the people, for the people” currency that would remove bad qualities of the institutions we know of such as corruption and greed.Cryptocurrencies are affected by human psychology tooAs cryptocurrencies affect our psychology and behaviour, they are also affected in return. As research by Gaea Solutions has shown us, there are numerous psychological effects that contribute to the cryptocurrency market’s movements.For example, fear of missing out often causes people to invest into institutions that may or may not actually offer something valuable. It is the same concept that drives a considerable percentage of social media too.In the digital age, people are particularly invested in being the best version of themselves and in drawing in as much information as possible from as many sources as they can.When it comes to cryptocurrencies, a single article that may report a positive change can be a huge influence towards new investments in a particular coin or a particular company, contributing to its rise in the matter of hours.Similarly, the cryptocurrency market can and has failed dramatically in the past simply due to a couple of statements made by powerful individuals or institutions.It is obvious then that human beings affected and are affected in return by anything that has to do with cryptocurrencies in often the same ways as it happens with more traditional forms of money.Then, it is not unreasonable to assume that cryptocurrencies will continue to affect us in ways we may not be able to understand yet. For instance, some suggest that cryptocurrencies actually make us more empathic instead of less so as it happens with other currencies.The truth is that the effects of cryptocurrency are yet to be understoodThe amazing thing with disruptive technologies is that we often don’t know exactly how they will disrupt the world even if they have already been launched.Cryptocurrencies have shown us a different way of conducting financial transactions and their disruption on finance is already obvious but their disruption on human psychology and behaviour is still ongoing.The only thing we can know for certain is that cryptocurrencies are here to stay and that their effects on human beings will continue to be interesting, especially as more research is being conducted.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Cryptocurrencies and their effects on emotion are bigger than you may think
9658,3693993,2018-02-22 20:15:00,"'Alto's Odyssey' took three years to make, and that's all rightIt's been three years since Snowman, a tiny independent studio based in Toronto, launched Alto's Adventure on iOS devices. Back then, Snowman was three people -- Ryan Cash, Harry Nesbitt and Jordan Rosenberg -- and Alto's Adventure was their first real game. They didn't know what to expect when they published it in the App Store for $2.99 in Feb. 2015, but the team hoped for the best.That launch changed everything.Alto's Adventure was a huge success. Players devoured the serene, soothing experience set on the slopes of a snowy mountain range -- it was an endless-runner snowboarding and llama-herding game, and its only mechanic involved tapping the screen to jump. Though gameplay was simple, the atmosphere and art told a larger story about comfort, risk and the warmth of home. Apple users ate it up, and the next year, Android players got a taste as well.It's been three years since Alto's Adventure debuted on mobile devices, which means it's been about three years since players have been asking Snowman about a sequel. This week, they got their wish as Alto's Odyssey landed in the App Store for $4.99.""The big elephant in the room is, this game's taking quite a long time to release since the first one came out,"" producer Eli Cymet said. ""And that's because we really wanted to pursue a feeling that was more natural to us.""The Snowman team has flourished since 2015, and the studio now acts as a quasi-publisher for other independent developers, collaborating on projects like Where Cards Fall with Los Angeles studio The Game Band. Cymet joined Team Alto (the group within Snowman that's dedicated to this particular franchise) after the launch of Alto's Adventure but before Nesbitt and the crew added a consequence-free Zen Mode in June 2016.This is one reason it took three years -- and one high-profile delay -- for Alto's Odyssey to see the light of day: Developers were busy supporting the game they had already launched. This meant bringing Alto's Adventure to Android and Windows, ensuring it was compatible with new hardware and software updates, and listening to player feedback to improve the game. That's how Zen Mode happened, after all.""As much as there's been a focus on creating the right game, it's also about having the right workflow and the right process, and working with the right people,"" Nesbitt said. ""So as the team has expanded to accommodate the new ambitions that we've got, then also there's a lot to learn for how to work in that way and essentially just find a stable rhythm so that it's not just a one-off situation. We want to keep on making games that people love and that excite us.""From the first line of code to the last, Team Alto has been actively working on Alto's Odyssey for just one and a half years rather than the full three. The rest of that time has been spent building a stable business and keeping existing players happy.""Alto's Adventure is our first foray into gaming,"" Cash, the series director, said. ""We're kind of figuring a lot of this out on the way. Alto's Odyssey will sort of only be our second step into this world. We were kind of, I don't want to say blindly, hoping for the best. We're kind of just making it up as we go. I think we're feeling pretty good about it now, but we're also pretty young in the industry.""Snowman might be a young studio, but developers are taking a measured approach to their post-Alto's Adventure projects. There's hustle but no rush. No one on the team wants to push out a soulless game just because players expect it or, even worse, just to make a quick buck.""We would get a lot of letters and sometimes physical mail telling us that Alto helped people relax at night and cope with illness or loss of a loved one,"" Cymet said. ""What that really taught us is that Alto as a series is strongest when it is about capturing these meaningful feelings and connecting with players on an emotional level.""Alto's Odyssey means a lot to Team Alto, and developers have poured their personal demons and triumphs into this deceptively emotional endless-running experience. Cymet, for instance, moved his family 2,000 miles from Vancouver to Toronto to join the Snowman team, and the feeling of displacement helped him develop a deeper relationship with the idea of ""home"" -- one of the major themes in the Alto series.""I think what I've learned, and what Harry and I have talked a lot about personally as we've reminisced during development and late nights and things, is that home isn't necessarily a place,"" Cymet said. ""It's the people you're with and the people that make you feel supported and loved.""The sequel also holds up a mirror to Snowman's growth as a studio. While Alto's Adventure encapsulated the comfort of home, Alto's Odyssey is about letting go of familiar things and exploring new worlds. Rather than being confined to the slopes of snow-capped mountains, Alto's Odyssey puts players in an array of desert biomes with dangerous new obstacles to conquer in gorgeous, sand-drenched settings.""Odyssey, as the name suggests, is about going outside of that [comfort zone] and challenging yourself,"" Nesbitt said. ""And I think that's sort of a parallel with a lot of our personal experiences, having made the game and then suddenly finding ourselves in this whole new world that we have to either grab it by the horns or let it steamroll over you. I think we can all relate to that.""Speaking of change, Alto's Odyssey is hitting a vastly different App Store than Alto's Adventure. Nowadays, there are options to pre-order iOS games, and Apple highlights developer stories in the Today tab -- two features that didn't exist in 2015. In three years, the mobile market has shifted from offering premium $10 games to having an influx of $0.99 titles to finally being overrun with free games packed with in-app purchases. Not that the industry is frozen in place today: There's been a recent newfound appreciation for premium titles as many players realize the actual cost of freemium games.Cash says the premium mobile market is alive and well -- and it could have a major impact on the video game industry overall.""If you make a game for PS4, you're making a game for people who play video games,"" he said. ""And I think, with mobile, it's a sort of unique opportunity where you can reach almost everyone on the planet, so to speak, who has a phone. If you're able to make a game for that platform, you have the potential opportunity to reach someone who may not consider themselves a gamer. You have a chance to be someone's first foray into gaming.""Jessica earned her BA in journalism from ASU's Walter Cronkite School in 2011, and she's written for online outlets since 2008, with four years as senior reporter at Joystiq. She specializes in covering independent video games and esports, and she strives to tell human stories within the broader tech industry. Jessica is also a sci-fi novelist with a completed manuscript floating through the mysterious ether of potential publishers.","'Alto's Odyssey' took three years to make, and that's all right"
9659,3694087,2018-02-22 16:00:00,"China's Xinjiang surveillance is the dystopian future nobody wantsMonitoring tech pioneered in the province is spreading across China and the world.In July 2009, deadly riots broke out in Urumqi, the capital of Xinjiang, China. Nearly 200 people died, the majority ethnic Han Chinese, and thousands of Chinese troops were brought in to quell the riots. An information battle soon followed, as mobile phone and internet service was cut off in the entire province. For the next 10 months, web access would be almost nonexistent in Xinjiang, a vast region larger than Texas with a population of more than 20 million. It was one of the most widespread, longest internet shutdowns ever.That event, which followed similar unrest in neighboring Chinese-ruled Tibet in 2008, was the sign of a new phase in the Chinese state's quest to control its restive outer regions. The 2009 shutdown was the first large-scale sign of a shift in tactics: the use of technology to control information.""Xinjiang has gotten little attention, but this is where we're really seeing the coming together of multiple streams of technology [for surveillance] that just hasn't happened in other contexts before,"" said Steven Feldstein, fellow in the Democracy and Rule of Law Program at the Carnegie Endowment for International Peace.Nine years later, Xinjiang has seen the widespread implementation of sophisticated high-tech surveillance and monitoring technology, what BuzzFeed called ""a 21st century police state."" But what happens in Xinjiang does not stay in Xinjiang. The technologies piloted there are already spreading across all of China, and there are even early signs that Chinese companies are beginning to sell some of this technology to other authoritarian-minded countries. If this trend continues, the future of technology, particularly for those in the Global South, could more resemble what's happening in Xinjiang than developments in Silicon Valley.Xinjiang is the home to the Uyghurs, a Turkic people who mostly follow Islam and have a distinct culture and language. Not surprisingly, the region has a tenuous relationship with Beijing, which is more than 1,400 miles away. Protests, riots and even terrorist attacks have been connected to the Uyghur struggle, which gives cover to Chinese authorities to implement the harshest strategies there.""Abuses are most apparent in Xinjiang because of the lack of privacy protections but also because the power imbalance between the people there and the police is the greatest in China,"" said Maya Wang, China researcher at Human Rights Watch.""The power imbalance between the people there and the police is the greatest in China.""That is why security investment in Xinjiang skyrocketed after the riots. According to Adrian Zenz, a lecturer at the European School of Culture and Theology who has written extensively about the police presence in Xinjiang and Tibet, the region's security forces doubled between 2009 and 2011 to more than 11,000 people. And it kept growing: In 2017, he documented more than 65,000 public job advertisements for security-related positions in Xinjiang, and last year Amnesty International estimated that there were 90,000 security staff in the region, the highest ratio of people to security in any province in China.Several new tools and tactics accompanied this rise in security personnel, most notably the implementation of ""convenience police stations,"" a dense network of street corner, village or neighborhood police stations designed to keep an eye out everywhere and rapidly respond to any threat, perceived or real. But there were also corresponding investments in security technology on a globally unprecedented scale. It started with a drive to put up security cameras in the aftermath of the 2009 riots before evolving into something far more sophisticated, as Xinjiang turned into a place for state-connected companies to test all of their surveillance innovations.""The rule of law doesn't exist,"" said William Nee, China researcher at Amnesty International. ""They are able to pioneer new methods of control that, if successful, they could use elsewhere in China.""Today, Xinjiang has both a massive security presence and ubiquitous surveillance technology: facial-recognition cameras; iris and body scanners at checkpoints, gas stations and government facilities; the collection of DNA samples for a massive database; mandatory apps that monitor messages and data flow on Uyghurs' smartphones; drones to monitor the borders. While there's some debate over how advanced the system tying these technologies together is, it's clear that China's plan is for a fully integrated system that uses artificial intelligence to rapidly process massive amounts of information for use by the similarly massive numbers of police in convenience stations.""[Xinjiang] represents a very new frontier and approach when it comes to online surveillance and oppression.""For Uyghurs, it means that wherever they go, whomever they talk to and even whatever they read online are all being monitored by the Chinese government. According to The New York Times, ""When Uighurs buy a kitchen knife, their ID data is etched on the blade as a QR code."" BuzzFeed documented stories of family members too scared to speak openly to relatives abroad. And the combination of all of these tools through increasingly powerful AI and data processing means absolute control and little freedom.""It's one thing to have GPS tracking. It's another thing to monitor social media usage of large populations,"" said Feldstein. ""But to do that in combination with a large DNA database of up to 40 million people and to integrate those methods with other modes of surveillance and intrusion -- that represents a very new frontier and approach when it comes to online surveillance and oppression.""The result, at least for China, is a massive success. Violence in the region has fallen as riots, protests and attacks are now rare in Xinjiang. Part of that is due to the presence of the state, but it's also related to a rise in fear, as no one is sure how pervasive the Chinese surveillance apparatus is.""People can never be sure if they are free from monitoring,"" said Nicole Morgret, project coordinator at the Uyghur Human Rights Project. ""The fear is such that even if the surveillance is not complete, people behave as if it is. The technology is being rolled out so quickly.""That is because access to the actual platforms being used by the Chinese authorities is limited, and much of the knowledge about surveillance technology comes from observations by the few journalists who can report from Xinjiang or through looking at public tender and budget documents. Or, increasingly, the knowledge comes from observing how other regions in China are being monitored and how Chinese tech companies abroad are deploying or marketing similar tools.“The fear is such that even if the surveillance is not complete, people behave as if it is.”Nicole Morgret, Uyghur Human Rights ProjectWhile the Xinjiang model may be extreme even for China, it is starting to influence policing across the country. The advent of the surveillance state in Xinjiang has come alongside China's increasingly tightening control over national information flows, including the blocking or removal from app stores of many foreign apps, VPNs and platforms, most recently Skype.""The question a lot of people have [is] ... to what extent is this going to be rolled [out] across the rest of China and packaged and sold to other repressive governments around the world?"" said Morgret. ""You can definitely see parts of it being implemented in China proper, such as the police database and collecting DNA samples from certain people. I certainly suspect the government has ambitions to create this type of total surveillance across the country.""The government has a powerful tool at its disposal, as last year, a new cybersecurity law went into effect that greatly broadens the power of the state to further control information. It requires foreign companies to maintain data centers in China, something Apple, for example, is complying with, leading the nonprofit watchdog group Reporters Without Borders to warn journalists working in China not to use iCloud anymore to store data. WeChat, China's do-everything app, is already suspected of sharing user data with the state.There are other signs that Xinjiang's policing innovations are entering the rest of China. The country is planning to integrate footage from its estimated 176 million surveillance cameras into a ""police cloud"" system, linked to national identity cards, making it possible that in the near future, everyone in China could be tracked anywhere. A model of this was demonstrated earlier this month when news reports emerged that new facial-recognition glasses are being used by police in train stations and airports across the country, tracking travelers ahead of the Lunar New Year.Considering all of this, it's no surprise that China is already the world's biggest market for surveillance software and hardware, estimated by industry researcher IHS Markit at $6.4 billion in 2016, a figure expected to triple by 2020. China's tech giants Baidu, Alibaba and Tencent are also jumping in, investing heavily in surveillance technology to take advantage of this boom.These companies are starting to sell some of these tools abroad as well. In Ecuador, a Chinese ECU911 Integrated Security Service system, the development of which was connected to the state-owned China National Electronics Import and Export Corporation, was deployed in 2016 and credited with a 24 percent drop in crime. A more worrisome case was uncovered by Human Rights Watch, which found evidence that the Ethiopian government was using telecom-surveillance technology provided by the Chinese telecom giant ZTE to monitor the political opposition, activists and journalists.""China wants to become a world leader in AI, and that includes a lot of these security applications that are already earmarked for exporting.""Other companies are following ZTE's path. Yitu Technology, an AI facial-recognition company, has already set up offices in several African countries and is looking to expand to Europe, where it sees potential due to recent terrorist attacks -- the same rationale initially used to expand the surveillance state in Xinjiang. These examples are few and not yet a sign that the Xinjiang model is having a big global impact, but even if the overseas market for Chinese surveillance technology remains limited for now, many observers think that could quickly change.""Now that China is delving into this new technology realm and is repressing very successfully and effectively, it is by nature that other dictatorial regimes would try to emulate this,"" said Feldstein.""I think we're on the threshold of this exploding,"" said Zenz. ""China wants to become a world leader in AI, and that includes a lot of these security applications that are already earmarked for exporting.""While the technology itself is not necessarily harmful, the concern is that in the wrong hands, it could empower repressive governments around the world to further abuse human rights. And the number of these regimes is growing, as recently released reports from the Economist Intelligence Unit and Freedom House show that around the world, free speech and democracy are falling and censorship, authoritarianism and autocracy are rising.""The Chinese government is leading on thinking around mass surveillance, and it has the impact of influencing other countries to think, 'Well, we could have an authoritarian government but look outwardly stable by putting in these systems to make sure that even if people are discontented, we can still keep them down by ensuring that every move is monitored,'"" said Wang. ""As this technology becomes cheaper, that reality might become more possible even for countries without massive resources like the Chinese government.""Id Kah Mosque in Kashgar, XinjiangIn Xinjiang, there are no signs that the massive buildup in both police presence and surveillance technology will recede anytime soon, despite the perceived success in limiting violence and protests thus far. If anything, it looks like things will get a lot worse. More and more Uyghurs, perhaps as many as 120,000, are being rounded up and sent to reeducation camps for minor offenses. Increasingly, any outward expression of religion or cultural expression is being seen as subversive, with even elderly intellectuals facing arrests, like the 82-year-old Islamic scholar Muhammad Salih Hajim, who died earlier this year in a reeducation camp. Now Uyghurs are also being forced to hand over DNA samples and put spyware on their phones. Meanwhile, spending on both technology and human-security presence is expected to rise even further.""It is going to crazy heights and there are no sign of it abating ... quite to the contrary, the state officials are really into intelligent, big data processing, networking of information, storing all the information and linking it up, applying AI and predictive policing for it,"" said Zenz.At least one facet of the Xinjiang model has gone global. Internet shutdowns, like what happened in Xinjiang in 2009, are now common around the world. Just this past year, there were widespread internet shutdowns in Indian-controlled Kashmir, the English-speaking region of Cameroon, Ethiopia, Kenya and more than 30 other countries. Often the causes are similar to what took place in Xinjiang -- ethnic tensions, riots or political events such as elections.""It's an increase around the world,"" said Melody Patry, a spokesperson with Access Now. ""Moreover, the phenomenon of repeat offenders is on the rise. ... When a government issues a first internet shutdown, they are more likely to issue others.""But China has moved on, and internet shutdowns are now rare. According to Access Now, there was only one documented shutdown in China in all of 2016. While uninformed observers could see this as a sign of progress, in actuality it shows that the next frontier of digital surveillance and state control is not blocking information access but harvesting it with a purpose.""You don't need these blackout shutdowns anymore when you have much more fine-grained mechanisms of control ... that can very early on detect potential issues and problems, and in turn promote self-policing, self-censorship,"" said Zenz. ""Because people know what consequences there are.""The shift in China is that the internet, which was initially seen as a threat due to its ability to allow users to access information, is now being perceived differently. What was back in 2009 blamed for the riots is now the source of information empowering the Chinese government to preemptively arrest and detain not only Uyghurs but also, increasingly, Chinese human rights lawyers, feminist activists and journalists around the country before they can post something inflammatory on a website or share sensitive content on WeChat.""The internet ... has become a great source of information that can be intelligently processed at capacity and speed that was not possible 10 years ago,"" said Zenz. ""What we see is a moving from a mere firewall that just blocks or an instant response, like the deletion of messages, to proactive self-censorship.""The global rise in shutdowns, which Access Now notes are getting more sophisticated and fine-tuned, shows that China's Xinjiang model has a market in an increasingly technological, authoritarian world. How quickly other countries follow China's move toward more total, personalized and data-driven control depends on both the need and the availability of the tools pioneered in Xinjiang on the global marketplace.Nithin Coca is a freelance journalist who focuses on social and economic issues in developing countries, and has specific expertise in South and Southeast Asia.Coca's feature and news pieces have appeared in global media outlets including Al Jazeera, Quartz, Forbes Asia, SciDev.Net, Southeast Asia Globe, The Diplomat, Vice and numerous regional publications in Asia and the United States.",China's Xinjiang surveillance is the dystopian future nobody wants
9660,3694331,2018-02-22 20:14:01,"TNW SitesWho needs Half-Life 3 when you have Final-Half-Fantasy-Life XV?Square Enix will soon release a new version of Final Fantasy XV for PC — because when a game takes ten years to come out, they’d better milk it for all its worth. But the promotional extras for the game seem more like a cruel joke on the PC faithful.The Steam version of FFXV comes with the Half-Life Pack, a cosmetic add-on that lets the player dress up as Gordon Freeman, complete with crowbar, both in single-player and multi-player. Given how long players have been waiting for the mythical third installment in the Half-Life series, a beautiful HD version of Gordon’s suit is tantalizing.I mean, who needs a continuation of a classic series that stopped midway through on a cliffhanger when I can just have a character in a totally unrelated game cosplay as the main character? 10/10, take my bottle caps already!I’m teasing, of course. Costumes and skins are a standard consolation prize to throw at buyers of one particular version of the game over another. The Final Fantasy series could probably do well for itself by pulling more costumes from older games, if only to cobble together some dignity for its consistently overdressed protagonists.If you must play as Noctis Freeman, the Windows Edition comes out on March 16, along with the Half-Life pack.",Who needs Half-Life 3 when you have Final Half Fantasy Life XV instead
9661,3696384,2018-02-22 18:20:00,"Arecibo Observatory, which is the second-largest radio telescope in the world, is under new management. A group led by the University of Central Florida will take over the operations of the telescope from the National Science Foundation, which was considering shutting down the observatory.The telescope's fate had previously been uncertain. Back in 2016, the National Science Foundation announced that it was exploring different options in regard to Arecibo. There wasn't enough funding to continue supporting the telescope, so the NSF was looking at partnering with other organizations, scaling back or shutting down Arecibo entirely. That same year, the observatory was the first to capture repeating cosmic radio bursts, which have helped us understand the nature of our galaxy and the universe around it.But now, this new agreement ensures that Arecibo Observatory will remain open. It is scheduled to take effect on April 1st. UCF and its partners, Universidad Metropolitana in San Juan and Yang Enterprises, Inc. in Oviedo, also plan to expand the operations of the telescope. It's good news for the scientific community, and also for Puerto Rico.",Puerto Rico's Arecibo Observatory saved from uncertain fate
9662,3696385,2018-02-22 22:17:00,"The FCC's order to overturn net neutrality protections was officially published in the Federal Register today and soon thereafter, the attorneys general of 22 states and Washington DC filed a lawsuit challenging the FCC's order. The coalition filed a suit earlier this year, but agreed last week to withdraw it until the FCC published the order, Reuters reports. ""Today, the FCC made official its illegal rollback of net neutrality -- and, as promised, our coalition of attorneys general is filing suit,"" New York Attorney General Eric Schneiderman said in a statement. ""Consumers and businesses in New York and across the country have the right to a free and open internet, and our coalition of attorneys general won't stop fighting to protect that right.""Other efforts aimed at blocking the FCC's decision include a Senate challenge to the order that is currently one vote shy as well as a Day of Action aimed at convincing one more Senator to join the cause. And three states -- New York, Montana and New Jersey -- have enacted policies aimed at encouraging ISPs in the state to uphold net neutrality. Now that the order is in the Federal Register, legislators have 60 days to overturn the decision.The attorneys general say in their complaint that the FCC's order was ""arbitrary, capricious and an abuse of discretion within the meaning of the Administrative Procedure Act."" They also say it violates federal law and conflicts with the notice-and-comment rulemaking requirements. They're asking the court to vacate the order.",23 attorneys general refile challenge to FCC net neutrality repeal
9663,3696476,2018-02-15 00:00:00,"Who's Missing From America's Colleges? Rural High School GraduatesFebruary 15, 20186:00 AM ETJon MarcusMatt KrupnickFrom The Hechinger ReportWhen Dustin Gordon arrived at the University of Iowa, he found himself taking lecture classes with more people in them than his entire hometown of Sharpsburg, Iowa, population 89. Ben Smith/The Hechinger Report hide captiontoggle captionBen Smith/The Hechinger ReportWhen Dustin Gordon's high school invited juniors and seniors to meet with recruiters from colleges and universities, a handful of students showed up.A few were serious about the prospect of continuing their educations, he said, ""But I think some of them went just to get out of class.""In his sparsely settled community in the agricultural countryside of southern Iowa, ""there's just no motivation for people to go"" to college, says Gordon, who's now a senior at the University of Iowa.""When they're ready to be done with high school, they think, 'That's all the school I need, and I'm just going to go and find a job.' "" That job, Gordon explains, might be on the family farm or at the egg-packaging plant or the factory that makes pulleys and conveyor belts, or driving trucks that haul grain.Variations of this mindset, among many other reasons, have given rise to a reality that has gotten lost in the impassioned debate over who gets to go to college, which often focuses on racial and ethnic minorities and students from low-income families: The high school graduates who head off to campus in the lowest proportions in America are the ones from rural places.Overall, 59 percent of rural high school grads — white and nonwhite, at every income level — go to college the subsequent fall. That's a lower proportion than the 62 percent of urban and 67 percent of suburban graduates, according to the National Student Clearinghouse, which tracks this.Understanding and addressing this ""is critical to our future, not just for employment but for civil discourse and kids feeling like they can contribute and achieve and not feeling lost and ignored,"" says Jeff Hawkins, executive director of the Kentucky Valley Educational Cooperative. The nonprofit group works to encourage students in that state's coal-mining southeast corner to go on to college.It's not that rural students aren't academically prepared. They score better on the National Assessment of Educational Progress than urban students, and they graduate from high school at a higher percentage than the national average, the U.S. Department of Education reports.A farm in rural Iowa. The high school graduates who head off to college in the lowest proportions in America are the ones from rural places. Brad Covington/Flickr hide captiontoggle captionBrad Covington/FlickrA farm in rural Iowa. The high school graduates who head off to college in the lowest proportions in America are the ones from rural places.Brad Covington/FlickrMany are historic. Rural students live in places where it once was possible to make a decent living from farming, mining and timber-harvesting, said Charles Fluharty, president and CEO of the Rural Policy Research Institute at the University of Iowa. None of those required college educations. Then manufacturing began to leave, agriculture became increasingly automated, and mines closed. While cities were generally able to economically diversify after the decline of industries such as auto manufacturing, rural areas haven't.""You could go to ag[ricultural] school, but you didn't have to,"" said Fluharty, who was raised on a farm in the Appalachian foothills of Ohio that has been in his family for five generations. ""You could get those jobs, so why should you go to college?""A resulting sense of hopelessness in places where jobs became sparse, Fluharty says, meant that rural students lost interest in their high schools' field trips to technical colleges or public universities, or in those visits from recruiters.The same malaise apparently affects their parents. A third of rural whites, and 40 percent of rural white men, are resigned to believing that their children will grow up with a lower standard of living than they did, a survey by the Pew Research Center found. That's a higher proportion than people who live in cities (23 percent) or suburbs (28 percent).This disaffection has been widely cited as a reason anti-establishment candidate Donald Trump won 62 percent of the rural vote in the 2016 presidential election, compared with Hillary Clinton's 34 percent — a much wider margin than in suburbs. In cities, Trump lost to Clinton by a wide margin.Dustin Gordon bucked the trend. Though neither of his parents finished college, they insisted that he go.""That's why I did it, I guess. They kind of pushed it,"" he said.""When I think of my [high school] classmates, the kids that went to college, their parents had better jobs or had gone to college.""But because of their histories, rural places have fewer such people than urban and suburban areas. Fewer than 1 in 5 rural adults ages 25 and older have college degrees, says the U.S. Department of Agriculture. That compares with the national average of nearly half, according to the Lumina Foundation, which is pushing for an increase in college completion among all Americans. (Lumina is among the funders of The Hechinger Report, which produced this story.)Council Bluffs, Iowa, Jan. 31, 2016: Donald Trump at a middle school rally of about 1,500 in western Iowa. Trump won 62 percent of the rural vote in the 2016 presidential election. Robert Daemmrich/Getty Images hide captiontoggle captionRobert Daemmrich/Getty ImagesCouncil Bluffs, Iowa, Jan. 31, 2016: Donald Trump at a middle school rally of about 1,500 in western Iowa. Trump won 62 percent of the rural vote in the 2016 presidential election.Robert Daemmrich/Getty Images""Because we don't have a diverse set of vocations kids can look at or try on or have an example of someone in their community that they aspire to be like, they're kind of pushed into a position of 'I have a choice of becoming a coal miner or working in retail or health care,' "" says Jeff Hawkins. ""They can see a coal miner or a cashier, but they rarely, if anywhere except on television, encounter lawyers or doctors or astrophysicists.""Rural areas are far from homogenous — contrast Marion County, Tenn., and its tiny population of 30,000 with California's sprawling Kern County, which has nearly 890,000 residents spread across a wide-open patchwork of farms, small towns and larger cities such as Bakersfield. But their challenges are largely the same.The Tennessee-based National Rural Education Association notes that, in addition to other problems, rural areas contend with drug and mental-health issues, poverty and a lack of high-speed access to the Internet, for instance.Some remote areas can't attract enough teachers to offer college-preparatory classes. In Marion County, where Alabama, Georgia and Tennessee meet, the county school district struggled to find enough math teachers for this academic year, said Mark Griffith, director of the Marion County School System. Finding teachers for other subjects was also a challenge.""There aren't any applicants out there,"" Griffith said. ""We've got one physics teacher in the county for three high schools.""Dustin Gordon grew up in a small town where everyone knew each other. His high school graduating class had 29 students. When he got to the University of Iowa, he says, ""I literally knew nobody on campus."" Ben Smith/The Hechinger Report hide captiontoggle captionBen Smith/The Hechinger ReportNor is there widespread confidence in rural places that going to college is worth it.""This has become a cultural phenomenon. It's not an educational phenomenon,"" Fluharty said. Encouraging a rural student to go to college instead of doing the same work as the adults in a community, he said, is like ""suggesting that that child should not do what I have done, should not be where I have been, should not value all that I have raised them to honor, whether that's going to the mill or turning on the tractor at 6 a.m.""Hawkins' program to encourage college-going tries to overcome this by connecting its students with people who are already enrolled in college. ""They strike up a relationship that could extend online and they begin to create a future support group for when they get to college,"" he said.Such a support system is important because rural students who do make it to campus are more likely to drop out between the first and second year than their urban and suburban classmates, the National Student Clearinghouse reports.One reason is cost. For others, the problem is culture shock. ""They go from 80 or 90 kids in their entire graduating class and now they're on campus with 20,000 kids,"" said Hawkins. In rural towns, ""we grow up knowing our neighbors, going to church with them, shopping at the Dollar General store. There is more of a familiarity.""Dustin Gordon felt that keenly. The regional school he attended houses all 12 grades in the same building. There were 29 students in his graduating class. But when, after first enrolling at community college, he transferred to the University of Iowa, he found himself in lecture classes with more people in them than his entire hometown of Sharpsburg, population 89.""Coming from a rural community, everybody knows who you are,"" says Gordon, who quarterbacked his high school football team, played baseball and ran track and field. When he got to the University of Iowa — with more than 33,000 students from around the country and around the world — ""I literally knew nobody on campus,"" he says. ""It's just kind of intimidating.""Gordon knows that most of his high school classmates and teammates ""are going to stay in rural Iowa and not really get out to see much of the world.""As for him, he says he's hoping to become a financial planner when he graduates in May, and has his eye on moving to Des Moines.""Since I've started going to the University of Iowa,"" he said, ""I almost don't like going home. I've kind of changed. I probably won't end up back in Lenox, Iowa.""This story was produced by The Hechinger Report, a nonprofit, independent news organization focused on inequality and innovation in education. It originally appeared in The Atlantic.We've been to school. We know how education works. Right? In fact, many aspects of learning — in homes, at schools, at work and elsewhere — are evolving rapidly, along with our understanding of learning. Join us as we explore how learning happens.",Who's Missing From America's Colleges? Rural High School Graduates
9664,3696722,2018-02-22 23:59:41,"About TNWTNW SitesKylie Jenner’s tweet hurt Snapchat. Its users are destroying it.Kylie Jenner yesterday tweeted that she wasn’t using Snapchat anymore, and today the company’s stock took a major hit. But it wasn’t just Kylie who’s falling out of love with the app — users are already on their way out due to a very unpopular new update.The tweet, by itself, doesn’t sound too damning:sooo does anyone else not open Snapchat anymore? Or is it just me… ugh this is so sad.Considering Kylie recently gave birth to her daughter Stormi, I initially took this to mean that she wasn’t able to keep up her previous social media schedule. However, couple this with a tweet from earlier this month, and it looks like more like an indictment of the app:Mm just saw the new Snapchat.. I don’t know how i feel about it! What do you guys think?Today, Snap shares on the stock market dropped by 7 percent, and Snap lost as much as $1 billion in market value. It certainly sounds like Kylie was the one who sent Wall Street into a panic, but she didn’t do it by herself.While I have no doubt Kylie holds enough sway to get a few social media addicts to avoid the app, I don’t think the people who are migrating away from Snapchat are doing so strictly because of her. The schism in Snapchat started weeks ago, and it’s plain to see for anyone who’s watching the company’s fortunes closely.The main reason people have been complaining about Snapchat lately — and have they ever been complaining — is due to the app’s redesigned layout. Reviews on both the App Store and Google Play are littered with harsh words about the unintuitive new design. There’s even a Change.org petition asking Snap to kill the update. It’s been signed by a whopping 1.2 million people at the time of this writing — a non-insubstantial portion of Snapchat’s userbase.Perhaps the most infuriating part of it, for those who want to see Snapchat changed, is that the company has made it clear they have no intention of complying. Last week, a spokesperson told Variety that Snap is essentially waiting for the users to get used to the changes: “Updates as big as this one can take a little getting used to, but we hope the community will enjoy it once they settle in.”Snap responded to the petition directly two days ago, saying the app’s new changes “will adapt to you the more that you use it.” The comments replying to this are decidedly frosty.So the writing was on the wall long before Kylie admitted her own move away from Snapchat. Sure, her tweets may have been the catalyst for the stock market panic — considering her popularity, she makes a good barometer for public opinion about Snapchat. But she’s not the reason people are losing interest in the app.For her part, Kylie doesn’t seem to be burning her bridges too badly. The last thing she had to say on the topic was more congenial:","No, Kylie Jenner didn't sink Snapchat -- not by herself, anyway."
9665,3698874,2018-02-20 16:55:56,"Why Writing a Linked List in (safe) Rust is So Damned HardBefore I start this post, let me preface it by saying that I’m not an experienced Rustacean by any means. The excellent foks at /r/rust gave a lot of helpful feedback and cleared up some misconceptions I had. Futher errata and corrections are appreciated. This post is aimed at helping other fledgling rust-learners avoid my mistake. First, by helping Rust learners pick good introductory projects that will fit naturally in idiomatic rust. Second, by helping Rust learners start building Rust-friendly design intuition.I’d heard about Rust and it’s inscrutable borrow checker for years, but after reading a few blog posts about compiler error improvements, I figured it might be user-friendly enough to give it a try. I read a few chapters of the book and then set about my first project: I wanted to build an x-fast trie. Specifically, I was curious about the its performance in practice (hence the choice of a low-level language). I’ll save the details of what is a really cool data structure for a later post, and cut to the chase: an x-fast trie is a trie with values at the leaves. To enable \(O(1)\) predecessor and successor searches once you have a leaf node, the values are stored in a doubly linked list.I was coding along fairly successfully with a few small hiccups until I hit the doubly linked list. But when I started implementing it, the compiler let me know, in no uncertain terms that what I was doing wasn’t ok. I tried and tried poking and prodding in different ways before I realized that I was trying to make Rust do something it fundamentally would not let me do.To explain why, consider a simple Node class for a doubly-linked list:Each node has a 64-bit value, and optional next and prev nodes. Before I get into the parts of Rust that make this impossible, let me talk about the parts that make Rust awesome.next and prev must be Optional because there is no such thing as a null pointer in Rust. As the witnesser of many a segfault, this is awesome.next and prev recursively refer to Node, so we can’t put them directly into the struct.1Box, the simplest of Rust’s “smart pointers” will heap allocate its contents when Box::new() is called.So far so good. The compiler happily accepts our struct. The problems start if we try to actually use it.Rust and OwnershipTo understand intuitively why this won’t work, we have to understand ownership. Rust’s safety features don’t come for free, and ownership is one of the costs. Rust has 3 simple rules of ownership with complex and sweeping implications:Each value in Rust has a variable that’s called its owner.There can only be one owner at a time.When the owner goes out of scope, the value will be dropped.I won’t attempt to describe these implications here – there are too many and I don’t understand most of them. The one that is important to note, however, is that if you pass a value directly to a function, the function (and the return value of the function, if there is one) now owns your value. You can’t use it anymore.Our first try at implementing a linked list based on the struct above will hit problems immediately.// must be mut so we can modify it letmut head = Node { value: 5, next: None, prev: None, }; let next = Node { value: 6, next: None, // next takes ownership of head!!! prev: Some(Box::new(head)), }; // I actually don't understand why the line below compiles. // Since `head` was moved into the box, I'm not sure why I can mutate it. head.next = Some(Box::new(next));Of course! We can’t use head anymore because it was moved into prev. What about printing next?error[E0382]: use of moved value: `next`--> src/lib.rs:27:26|26| head.next = Some(Box::new(next)); |---- value moved here 27| println!(""{:?}"", next); |^^^^ value used here after move|= note: move occurs because `next` has type`ll::Node`, which does not implement the `Copy`traitSame problem! When we set head.next = next, head took ownership, and we don’t have it anymore. You can try to sidestep this problem in a couple of ways:Making Node keep borrowed boxes instead of valued boxes:Node { // ... next: Option<&Box<Node>> }This helps until you want to mutate them. Borrowing is like a read-write lock for mutation. You can borrow immutably in multiple places, but if you want to borrow mutably, all your immutable borrowers must return the item. This isn’t going to work since our list is composed of immutable borrows.Rust has Rc, a ref-counted pointer which seems like it could do the trick, but it prohibits mutation. This is obvious in hindsight: how could Rust let you have multiple references to something but also let you mutate them?There are 3 solutions I’m aware of:Use RefCell, a runtime-checked borrow system (which still only works on nightly if you want to mutate). RefCell::borrow_mut allows mutable borrowing of the cell’s contents.Eschew safe Rust altogether and wade into the magical swamp of unsafe Rust. This isn’t nearly as big a deal as I initially thought – even things like Vec<> are built on unsafe Rust.2 While it’s not ideal, unsafe Rust is really just “normal mode” in most other programming languages.Keep pointers as indices into a Vec<> instead of pointers, using something like the indextree crateConclusion and TakeawaysIn hindsight and with a deeper understanding, it’s not surprising why a doubly linked list is so problematic. Each variable can only have 1 owner. If prev and next both hold pointers to a middle node, which is the owner? They can’t both be the owner. If one is the owner, another can borrow. But Rust won’t let you mutate while someone else is borrowing, so neither could actually modify the list! In general, if you have loops in your object graph you’re out of luck.I find a bit of solace in the fact that implementing a data structure like this in a non-garbage collected language without Rust is also quite tricky; you need to carefully keep track of when a node has no more references and can be freed. In the end, I gave up on Rust for this project and implemented it in Go instead. Never has coding in Go felt so effortless ;-). In the mean time, I’m getting started on my next learn-rust project, a port of my Sumoshell tool. Since I ran into tons of race conditions in the initial implementation, I’m hopeful that Rust will help me get it right the first time.I post about once a week on topics like databases, language internals and algorithms. Do you want to hire me? I’m available for engagements from 1 week to a few months. Email me at *@*.me, where * = rcoh.To stack allocate it, Rust would need to know exactly how much space it would take. Since it’s a recursive datastructure, there is no way to know! [return]",Why Writing a Linked List in (safe) Rust is So Damned Hard
9666,3698876,2018-02-21 17:27:00,"We use cookies and browser capability checks to help us deliver our online services, including to learn if you enabled Flash for video or ad blocking. By using our website or by closing this message box, you agree to our use of browser capability checks, and to our use of cookies as described in our Cookie Policy.","Sorry, Collectors, Nobody Wants Your Beanie Babies Anymore"
9667,3699041,2018-02-22 22:09:52,"Snap CEO Evan Spiegel got a $637 million bonus last year0Snap’s stock investors haven’t made much money since the company went public last year, but CEO Evan Spiegel still got a hefty payday.According to an SEC filing, he was granted an RSU of 37,447,817, which vested at the time of the IPO. In other words, that was worth nearly $636.6 million. His salary for the year was $98,078, and he had over $1 million in other benefits, so all in all, Spiegel made $638 million in 2017 alone.That’s a lot of money. But the Snapchat co-founder also started the whole thing.And he wasn’t the only executive to get a big payday. Chief Strategy Officer Imran Khan netted over $100 million and the company’s new general counsel Michael O’Sullivan earned almost $17 million, for less than half a year’s work. Former general counsel Chris Handman earned over $54 million for the first seven months of last year.",Snap CEO Evan Spiegel got a $637 million bonus last year
9668,3699121,2018-02-23 00:16:37,"TNW SitesSamsung’s HMD Odyssey is the best way to experience Microsoft’s Mixed RealityWhen Microsoft announced its entrance into the VR headset market, its partner products were attractive and relatively inexpensive, but didn’t live up to the Oculus Rift and HTC Vive in terms of pure image quality and specs. Enter Samsung’s HMD Odyssey, a $499 headset that can compete with the best of them.I’ve been living with the Odyssey for the past couple of months, spending many an hour immersed in Microsoft’s mixed reality world.The Odyssey’s combination of some of the highest resolutions around and OLED technology makes it one of the more pleasant VR experiences I’ve tried. For comparison, the Oculus Rift and HTC Vive both have 2160 x 1200 resolutions, while the Odyssey runs at 2880 x 1600. There’s a small amount of black smearing during dark scenes with a lot of motion, but that’s a problem OLED tech hasn’t been able to completely work out; I’ll compromise for the improved colors, sharpness and black levels.The headset itself is super comfortable for short bursts, although the use of pleather means it can get warm during extended play sessions. I also took some issue with the headphones. They sound solid, but I wish I could remove them to more easily use my own headset. More annoyingly, it was difficult to get them to sit comfortably on my ears without finagling for the perfect fit.Caveats aside, I’ve tried all of the current mixed reality headsets, and though some have advantages in the comfort and price department, the sheer improvement in image quality on Samsung’s headset make it worth the extra cost.Of course, this would all be naught if the VR experience weren’t up to snuff, but Microsoft’s Mixed Reality portal is a glimpse of the future I want to live in. A somewhat unfinished and occasionally buggy glimpse, but the future nonetheless.The good starts with the setup experience. As soon as I plugged in the HMD, Microsoft’s Mixed Reality Portal opened. I was prompted to pair the controllers, downloaded a few updates, then calibrated the headset to my room.The whole process took less than 10 minutes and was surprisingly easy. Because the headset uses 3D mapping technically borrowed from the HoloLens, there was no need for setting up external cameras and extra wires – something I seriously appreciated in my cramped studio apartment.Like all of Microsoft’s HMD headsets, the Odyssey uses cameras built into the headset itself to detect the position of the controllers.And if you don’t need the room scale environment – or if you don’t have enough free space – you can ignore the mapping step altogether. The experience was largely similar and I was still able to move around with my feet. I just wasn’t warned when I was about to slam into a wall.In VR, the mixed reality home replaces the desktop. In the same way the 2D desktop was supposed to be an analog for a physical desk where you might keep documents and folders, the home expands that potential into 3D space.You can stick mixed reality apps onto any wall, place 3D models all around your home. You can resize these or reorient them to your liking, although I wasn’t able to find a way to stick a screen onto the ceiling. You can even access your desktop within mixed reality, like using an IMAX sized monitor, albeit a very blurry one.Thanks to a partnership with Steam, you can access SteamVR using a Windows Mixed Reality headset, immediately giving you access to a myriad of titles not available on the Windows Store. It’s in beta, so the experience is still kind of buggy – sometimes it wouldn’t load at all until I restarted my computer. I also noticed sometimes the same titles performed worse on steam than they did with the Mixed Reality portal.Still, it’s good to see Microsoft working with partners to prevent increasing fragmentation in the VR market. The heart of modern VR is still in gaming, and it’s where the mixed reality experience shines best at the moment.That being said, using Mixed Reality at regular intervals the past few months helped me appreciate Microsoft’s vision for the future of computing. I work from a small apartment but with mixed reality, I can pretend to work in a giant loft. The possibilities within that virtual space feel endless. There’s just not enough to fill it up with.It’s almost frustrating to see so much potential stifled by the limits of modern technology. The usual VR caveats still apply: the headset is bulky, resolution is far lower than our eye’s visual fidelity, youre isolated from the rest of the world, and you need to be tethered to a PC.I want to be able to use a virtual 200-foot monitor for work and watching movies, but the image is too blurry for extended use. I wish I could edit my photos in a virtual environment, but the app support isn’t there. Then there are the occasional disconnects and bugginess from being a first-gen product.Still, with Samsung’s Odyssey and Microsoft Mixed Reality, I see a glimmer of hope for future. The HMD Odyssey has the best viewing experience I’ve tried so far, and Microsoft took a step forward for accessibility in terms of price and ease-of-use. I might not be able to use my virtual home as a real work and entertainment environment, but for once, that future feels tangible.",Samsung's HMD Odyssey is the best way to experience Microsoft's Mixed Reality
9669,3701217,2018-02-21 13:00:46,"Worst Roommate EverShareAlex Miller’s spare room had been on Craigslist for two weeks when, last March, she got the call she’d been waiting for. The man at the other end identified himself as Jed Creek. Creek was a lawyer from New York, but he had grown up just outside Philadelphia, only a few minutes’ drive from Miller’s apartment in the city’s well-to-do neighborhood of Chestnut Hill. Creek explained that he needed a place to stay while he tended to family matters — his mother was old and frail and his older brother was suffering from complications with hepatitis C, he said — and he’d been looking for a place without much luck. “I find Philadelphians to be very difficult,” he said. “A lot of flaky people.”“I’m not flaky,” Miller, then 31, assured him, “so you’re off to a good start.”Creek was tall, slim, and handsome, with hair as black as squid’s ink. Though he was 60, he looked to be in his late 40s. When he came to visit the apartment, he brought his dog, a 13-year-old Border-collie mix named Zachary, so that he could meet Miller’s arthritic black Lab, Cosimo.To Miller, Creek’s arrival felt like a godsend. She was dealing with the sudden departure of a roommate, a looming lease renewal, a bank account kept precariously afloat by part-time work at a juice bar and at a nearby law firm filing paperwork. Here was a courtly gentleman, Miller thought, as she walked Creek through her cluttered apartment, an experienced lawyer who’d lived in Europe and the Middle East. At the end of the tour, they settled on her couch and fell into a deeper conversation. Creek shared his interest in Buddhist meditation; Miller told him about recent romantic troubles and Creek offered advice. The sky outside was turning dusky blue when Creek said, “I like the place, and I like you. If you like me, I could just do this now”—move in, he meant.His abruptness surprised Miller, but Creek said he could pay her on the spot. He pulled a check from his pocket and made it out for $800. Miller noticed that the upper-left corner of the check was blank, and in the space where his name and address should have appeared, Creek wrote “219 E. Willow Grove Ave” — her own address now made his. He did not write his name. He signed the check in a messy scrawl, the only discernible letter an enormous, looping J. Then he and Zachary hailed an Uber, with a promise to return that evening. Miller asked if he needed any furniture. “No,” Creek said. “I have everything I need.”Everything Creek needed, Miller saw when he returned, fit inside six Rubbermaid bins and a cat carrier. (It turned out that along with Zachary, he had a desperately shy tabby named Abigail.) He brought no mattress: For a bed, he dropped a heap of comforters on the bedroom floor. It struck Miller that someone who slept like this might not have much in the way of a proper bank account. But the following afternoon, she deposited his rent check and it cleared.The two quickly fell into a comfortable routine. Creek rose early in the morning and took the dog for a run. He tended carefully to his pets. He spoke to Zachary exclusively in Dutch, which he said he’d learned while living in the Netherlands in the 1980s. He fed the animals well: for Zachary, brand-name kibble; for Abigail, a mix of dry food and organic chicken, which he diced with a serrated silver knife. They spent the nights together on the couch, drinking wine and watching The Rachel Maddow Show, one of Creek’s favorites. One evening, an old hookup overstayed his welcome, refusing to leave despite Miller’s requests. Creek barged into the room and said, “Buddy, I’m living here too. She’s asked you to go, I’m asking you to go. I’ll ask you one more time, or I’ll remove you myself.” The guy left.Then, on April 5, their 11th day of living together, Miller showed Creek the utility bills and asked for his half, $140.80. Creek refused. The bills, he noted, covered a period before he’d moved in. When Miller pressed him, he texted, “We can handle this in court if you would prefer.” At first the escalation in tone jarred Miller. Looking at the dates, however, she second-guessed herself: Maybe Creek was right.Strange things began to happen. One evening, Miller came home to find the living-room lights wouldn’t turn on — Creek had taken the bulbs and screwed them into lamps in his bedroom. A few days later, the six chairs at the kitchen table disappeared. Miller knocked on Creek’s door, and when he opened it she saw he’d fashioned them into a desk. Miller had assumed Creek spent his days in court, but neighbors said they saw him loitering on the property throughout the afternoon. He began sprinkling his speech with legalese. When they argued, he accused her of breaking “the covenant of quiet enjoyment,” a technical phrase Miller recognized from her days working for a real-estate agent. When he found a cigarette butt in the toilet bowl one afternoon, he told her flatly that he would not be paying the next month’s rent. As a paralegal, “you should know about the warranty of habitability,” he texted her.Hearing about Creek’s behavior, Alex’s mother asked her daughter for his phone number, then plugged it into Google. She found two articles and didn’t finish reading them before picking up the phone and calling her daughter. “Alex, we have a big problem,” she said. “Jed Creek is not who he says he is.”Creek’s legal name was Jamison Bachman. In 2012, Bachman had shown up at the home of a woman across town named Melissa Frost, claiming to be a New Yorker whose home had been destroyed in Hurricane Sandy. Overcome with pity, Frost let him in — and nearly lost her house. In an expensive and frightening ordeal that dragged on for months, Bachman slowly laid claim to the space, using his intricate knowledge of tenancy laws to stay one step ahead of her. He scuffed up the floors, kicked down the doors, and clogged the toilets with cat litter. “He went from being this cordial, polite person who understood he was a guest in my house,” Frost said in one of the articles, “to someone who was approaching me aggressively and flat-out saying, ‘This is my house now.’ ”Alex Miller.Photo: Brian FinkeI reached out to Frost this past summer, having read about her encounter with Bachman. Over the years, she told me, other roommates had written to her; working with them and with public records, I soon identified a dozen victims of Bachman’s, spread up and down the East Coast. Bachman, these stories made clear, was a serial squatter operating on a virtuosic scale, driving roommate after roommate into court and often from their home. But Bachman wasn’t a typical squatter in that he did not appear especially interested in strong-arming his way to free rent (although he often granted himself that privilege); instead, he seemed to relish the anguish of those who had taken him in without realizing that they would soon be pulled into a terrifying battle for their home. Nothing they did could satisfy or appease him, because the objective was not material gain but, seemingly, the sadistic pleasure of watching them squirm as he displaced them.The roommates’ stories often start with a desperate arrival: Some emergency pushes Bachman to their doorstep; without a place to spend the night, he and his pets would be wandering the streets. For Frost, it was the hurricane, and for Miller, a sickly relative in need of Bachman’s aid. But for others, an alcoholic roommate or a sudden change in employment did the trick. Brash and confident, Bachman swaggers into their home, sizing up the place. He notes his education at Georgetown University (a master’s in history) and the University of Miami (where he got his law degree); he describes how he makes a living “doing litigation” and tutoring “youngsters” online; he promises that he’s clean and respectful and requires nothing more than a quiet room and a fast internet connection.Bachman’s rent and security deposit, made out on a check curiously without a return address.Photo: Courtesy of Alex MillerThe bullets Miller found in Bachman’s room.Photo: William BrennanSome roommates took pity; others were desperate themselves. When Sonia Acevedo, a 49-year-old vet tech from Brooklyn, saw Bachman’s U-Haul pull up outside her beachside condo in Rockaway Beach, Queens, in the spring of 2012, she prayed to God in thanks: To a woman struggling to make mortgage payments, the $1,400 check Bachman wrote on the spot looked like salvation.Things started off smoothly, as they always did. “Those first three months were perfect,” Acevedo recalled. She and Bachman ate breakfast together while the sun rose over the beach, talking about little things — errands, pets, politics. It even seemed, early on, like a friendship might be forming. Shortly after Bachman moved in, one of Acevedo’s cats died. When Acevedo returned from the vet that afternoon, Bachman met her at her car, tears welling in his eyes. “I’m so, so sorry,” he said, pulling her into a hug. Acevedo was struck by the tenderness of this moment. Eventually, she grew comfortable enough to invite Bachman to the beach at Jacob Riis Park, where she sunbathed topless. “He was very respectful.”Often, the first signs of trouble were easy to downplay: In many cases, roommates came home to find a chandelier removed, a bookshelf filled with unfamiliar books, a couch or potted plant shifted slightly this way or that. These incursions, almost imperceptible, seemed calculated to unsettle. Suspecting Bachman was entering her room while she was at work, Acevedo once placed an empty wine bottle behind her bedroom door, so anyone going in would knock it over; when she returned, she opened the door without thinking and then braced herself, but the bottle did not fall, having been moved several inches away. Michael Oberhauser, a 31-year-old composer and music theorist living in northwest D.C., welcomed Bachman into his apartment in the fall of 2016. Almost immediately, tensions arose around a red bath mat of Oberhauser’s, which Bachman picked up and tossed away in the corner every time he used the bathroom. “I asked him about it, and he said, ‘Oh, yeah, I was going to clean it,’ ” Oberhauser told me. “So I put it back, and he kept on throwing it out.” Eventually, Oberhauser duct-taped the mat to the floor. Beneath it, he placed a note reading, simply: WHY?If Bachman’s intentions were at first unclear, in most cases, by the time the second month’s rent came due, they became unmistakable. Time and again, Bachman’s roommates were informed that some minor discomfort they’d inflicted upon him (a dirty living room, a dish left in the sink) had voided their lease — and meant that Bachman wouldn’t be paying his rent. They considered him a guest in their home, but he made it clear that he saw it the other way around. “The effort he put into doing this was life-consuming,” Frost told me. “When things got bad between us, he stopped leaving the house, because he thought I might change the locks.” To her, Bachman appeared to function as if he were “at war.”One Saturday, she told me, she unplugged his microwave and brought it upstairs to his room, telling him he couldn’t keep his things in the common areas. Bachman shouted that she had “no right to touch his things,” she said, and used the microwave to push her slowly backward, until she was teetering on the edge of the staircase. A friend intervened, and Frost called the police. Sometime after the cops arrived, a calico cat of Bachman’s named Emma disappeared, and Bachman wrote to Frost in fury: “YOU ARE THE PROXIMATE CAUSE OF MY CAT’S DISAPPEARANCE AND PRESUMED DEATH … DO NOT COMMUNICATE WITH ME AGAIN UNLESS IT IS THROUGH YOUR ATTORNEY.”Yet even after all of this, Frost approached him to try to negotiate a peaceful exit. She offered to return the money he’d paid in November and to help him find a new place to stay. Hearing her entreaties, Bachman just laughed. When Frost burst into tears, Bachman pretended to comfort her, she said. “He goes, ‘You’ve got your whole life in front of you. You’re pretty, and you’re talented, and you’ve got this house — well, you don’t have this house anymore. This house is my house.’ It was like something out of a movie.”Jamison Bachman's former roommates share their horror stories.It would not be accurate to call Bachman a con artist; his tactics involved relatively little artifice and even less artistry. What Bachman craved was a fight; the goal was to get his roommates to sue him. “I’m happy to have her file an eviction notice,” he told a reporter in 2013, while squatting in Frost’s home. “She files the filing fee, and then I piggyback on the filing fee and hit her with the counterclaim. That’s just tactics.”Bachman’s legal training came late in life, after he’d returned from those years abroad. He got his law degree at the age of 45, and his instructors at Georgetown and the University of Miami remembered him as a “remarkable” student with “extraordinary talents,” a star researcher whose contrarian style made classroom discussions lively. “In 20 years of university teaching,” one Georgetown professor wrote in a letter of recommendation, “I have encountered very few people of his caliber.”The original Craigslist ad from Frost that first brought her into contact with Bachman.Photo: Courtesy of Melissa FrostBachman’s first payment to Frost.Photo: Courtesy of Melissa FrostBachman may have started his legal education late, but he wasted no time putting it to use, tangling with at least three people in housing disputes before he’d even graduated. Yet, bizarrely, he never took the final step in legitimizing his career: In 2003, he failed the bar exam on his first try and never bothered to take it again. His legal skills were thus limited to a single client — himself.In his disputes with roommates, he cited precedent exhaustively and leaned confidently on legal shibboleths, but often undercut his own claims with personal jibes and snide remarks — wanting to demonstrate mastery and authority but also to bully. One woman, suing for the repayment of more than $36,000 in debts, became, in Bachman’s words, “bitter and a woman scorned”; alleging she’d given him herpes, Bachman countersued her for the “tortious transmission of an incurable venereal disease.” Another target, having pointed out Bachman’s tendency to clog his roommates’ toilets with cat litter, elicited the statement: “Correct me if I’m wrong, as I only have two graduate degrees, but my understanding was that the proper place for shit is in a toilet.”On the day in 2015 when he faced off against Jill Weatherford, a South Carolina Realtor whose tenants had taken him in, he showed up in a sweat-drenched suit, having walked the four miles to the courthouse in the Charleston sun. He had somehow compiled a list of her past tenants and began rattling off the names, falsely accusing Weatherford of being a slumlord. “I said, ‘I’ve never met this man in my life,’ ” Weatherford told me. “I’ve been doing this for 33 years and never seen anything like it.” When he stepped before Judge Marvin Williams in Philadelphia, to accuse Melissa Frost of destroying his property, Williams told him, “I find you to be totally incredible. I don’t believe a word you say — and, frankly, you’re frightening.”In most instances, the counterclaims and self-defenses Bachman advanced failed resoundingly — but the result seems to have been almost beside the point. He would eventually disappear, but never before the acrimony reached a crescendo. In 2005, he was hired to teach at the Thornton-Donovan School, a private school in New Rochelle. The headmaster offered him an apartment in a beautiful home on a peaceful street near campus. According to one former roommate, Bachman began boasting about how much he’d impressed the school — so much, he said, that they were already considering making him the school’s next headmaster. (On the website Rate My Teachers, the only student who left a review of Bachman wrote: “He scares me …”) In the spring, when the school informed him that his contract would not be renewed, he withheld his rent in protest and refused to move out of the faculty apartment, until, after two months, the school evicted him.Arleen Hairabedian, a 43-year-old professional dog walker living in Queens, allowed Bachman to stay with her in June 2006, after his eviction from the school. At the time she took him in, Hairabedian and Bachman were casually dating, and Hairabedian lived in a railroad apartment above a hobby shop in Richmond Hill, in a building so close to the elevated tracks that the passing trains rattled the windows. Hairabedian made Bachman guarantee he’d stay no longer than two months, she told me, “and he promised.” But two months became six, six months became a year, one year became four. Bachman only ever paid one month’s rent to Hairabedian, but she was trapped by her own conscience: She knew that if she moved out, she’d be foisting Bachman on her landlord.Bachman had an odd habit of dumping cat litter into the toilet.Photo: Courtesy of Melissa FrostSo she stayed, the tension slowly rising. In October 2010, more than four years after Bachman had moved in, she opened up the bills and “just lost it,” she told me. She turned to Bachman and demanded he pay for the cable. Bachman told her he wouldn’t. “I’m not a violent person,” Hairabedian said, but rage overcame her and she slapped him. In response, Bachman grabbed her throat; she pulled herself free and ran out to the street for help. Although they lived in the same home, they acquired protection orders against each other — which legally required them to remain 100 yards apart. The only way to satisfy that demand, Hairabedian decided, was to finally file for his eviction, so on a November morning she and her landlord went to the Queens County Civil Court and put in the paperwork. When Bachman learned what she’d done, he retaliated, filing a police report that claimed she’d come at him with a knife and persuading the police to arrest her (Hairabedian says she never attacked him). Hairabedian was forbidden to go near the apartment — which meant Bachman now had full possession of her home. Taking advantage, he began dropping off her cats at kill shelters.Bachman’s outbursts were becoming scarier. Mark Gainer, a former principal oboist of the Charleston Symphony Orchestra, told me that Bachman moved into his home in the spring of 2015 and promptly began walking around with a baseball bat over his shoulder. On January 10, 2017, Bachman arrived at the home of Neville Henry, a 40-year-old Bermudan immigrant living in South Philly. According to Henry, Bachman sent pictures of himself ahead of time, but when he showed up on Henry’s doorstep, “I didn’t even recognize who he was. I said, ‘Can I help you?’ Then he said he was in a relationship with someone for years and they took everything from him and he wanted a fresh start.” Henry let him in. A week later, Bachman came after him with the broken leg of a coffee table. Then Bachman abandoned the house and later sued Henry, trying to recoup his rent. Two and a half months after that, Jed Creek moved into Alex Miller’s apartment.Victim: Melissa Frost.Photo: Brian FinkeBachman’s roommates described him as a man whose life had gone awry — and, in fact, it had. As a kid, Bachman had been groomed for greatness. His parents raised him in Elkins Park, an old, elegant neighborhood of close-clustered homes on the northern border of Philadelphia. His father owned a construction company, and his mother stayed at home; his brother, Harry, four years Jamison’s senior, was handsome and multitalented, juggling the varsity soccer team and the school productions of Camelot and Brigadoon. Where Harry was outgoing and humble, Jamison was ostentatiously self-confident. “He was the cockiest kid you ever met,” said Bob Friedman, one of Jamison’s closest childhood friends. Jamison harbored no doubts about his own abilities: He earned high marks, excelled at tennis, and spent his free time devouring books on the history of Western civilization. Unlike other students’, his high-school yearbook entry records just a two-line quote, attributed to Bismarck, that appears in retrospect like a mission statement: “Fools say that they learn by experience. I prefer to profit by others’ experiences.”According to Friedman, Bachman had an almost unquenchable competitive streak and very little interest in a fair fight. Weekend after weekend, he forced Friedman to play an obscure, multi-hour board game called Midway, which simulated the 1942 Pacific Theater battle in which American airmen devastated the Imperial Japanese Navy. “The game was slanted toward the Americans winning,” Friedman said, “and he was always the Americans.”Although by any reasonable comparison, his older brother would turn out to be the star, Jamison was supposed to be the golden boy. “His parents made him think he was the Christ child, that he could do no wrong,” Friedman told me. He remembered Jamison’s mother, Joan, as “a Carol Channing type,” an “ebullient” woman who pinched her son’s cheeks; his father, Jim, was more taciturn but “nothing but upbeat” when it came to his younger son’s prospects. “They doted on him. It was always ‘You’re doing great, champ’ or ‘You’re the best,’ ” said Friedman. It got to “the point where it was almost phony — it was over-the-top. It felt like they were both performing a role.”For his part, Jamison had one model: his maternal grandfather, Abraham J. Brem Levy, a prominent attorney in the city. In the 1950s, Levy had co-founded a criminal-defense firm with Samuel Dash, who would go on to serve as chief counsel to the Senate Watergate Commission in 1973. As a trial lawyer, Levy became a fixture in the local papers, orchestrating theatrical rhetorical feats like arranging for an actress to walk into the room at the emotional high point of his closing statement in a murder trial. Bachman boasted throughout his youth of his grandfather’s success to anyone who would listen. Decades later, on his personal website, Bachman would note that his passion for the law extended back to childhood. “My grandfather started taking me to his murder trials when I was just a boy,” he wrote.Bachman enrolled at Tulane University in the fall of 1975, but his time there was rocky and brief, ruptured by a horrific incident in January at the Sigma Chi house, just off campus. Although Bachman was not a member of the frat, he told Friedman he’d been hanging around the house with a friend from Elkins Park, a boy a year older named Ken Gutzeit. Suddenly, a man had appeared with a knife and slashed Gutzeit’s throat. “The word Jamison used was beheaded,” Friedman told me. According to news reports, Gutzeit was killed by a 25-year-old student librarian named Randell Vidrine. The two were said to have been feuding since the previous fall, after Vidrine called campus police on Gutzeit for eating a cheese sandwich among the stacks. (“I know it sounds incredible, but from what we understand they never argued about anything else,” a police spokesperson told a reporter at the time. “It was always about the sandwich.”) Gutzeit stumbled onto the frat-house steps and bled to death, surrounded by Bachman and some two dozen other witnesses. (A grand jury declined to indict Vidrine.)After Bachman returned home in the summer of 1976, family and friends found him shaken. He appeared oddly paranoid. He ranted to Friedman about a rising tide of anti-Semitism and threats to the State of Israel. Those close to him worried that Gutzeit’s murder had served as a mental breaking point. (A therapist who later evaluated Bachman noted that he was “excessively dependent on the world” to take care of him and wondered if this indicated a personality disorder, but eventually concluded only that Bachman was depressed.) Clearly distraught, Bachman spent the summer getting high. And then he dropped off the map.Jamison Bachman.Photo: Montgomery County DAFriedman didn’t see him again for 20 years, when Bachman called him out of the blue and said he was living in D.C. The two met at a bookstore in Crystal City, “and it was like finding a long-lost brother,” Friedman said. “We got very close, very quickly.” Bachman said he had been living in Israel, where, he claimed, he had served in the Israel Defense Forces. There, he had fallen in love with a Dutch woman, whom he had followed home. In the Netherlands, he had studied Japanese at Leiden University, a school that catered to international students. (The early 1980s marked the zenith of the Dutch squatting movement.) Shortly after the reunion with Friedman, Bachman broke up with his girlfriend, and Friedman invited him to stay at his family’s home in the suburbs out by Dulles airport. “He was never a problem, perfectly well behaved, a great guest,” he said, though after a while Bachman started to make Friedman’s wife uncomfortable.In the two decades since that paranoid summer in Philadelphia, Friedman and Bachman had notably diverged. After a career in journalism covering the White House for PBS, Friedman had worked with Lee Atwater to launch an international barbecue-restaurant franchise called Red, Hot & Blue; Bachman said he was stocking books and writing copy part-time for a news program (sometimes he mowed lawns). Pitying Bachman, Friedman hired him as a manager at a local Red, Hot & Blue. Bachman showed up for his first day of work in a suit and tie, telling employees he’d been brought on as a consultant to turn the business around. Friedman fired Bachman and, not long after, asked him to move out. Friedman couldn’t help but feel perverse satisfaction: “I was the one supposed to turn out like him, and he was supposed to turn out like me — I was doing well and he was not, and it wasn’t supposed to be that way.” Bachman, he added, “was very jealous of the fact I turned out well. Proud but jealous. He would say, ‘You’ve done well for yourself; you’ve got a wonderful wife, wonderful children, you’re playing tennis …’ ”Bachman’s brother’s success offered perhaps a sharper contrast still. Harry had earned a degree in architecture from Cornell, married a psychologist from Paris, and raised two daughters in a quaint, Colonial-style home on a quiet street in Elkins Park — a sort of suburban fairy tale of making good and sticking close to your roots. But Bachman’s parents, according to Friedman, were ashamed that their younger son hadn’t similarly flourished.When Jamison talked about his family, it was often with resentment; sometimes he noted what he perceived as his parents’ better treatment of his brother, as if it explained his failure to launch properly in adulthood. He told one interlocutor that his father, Jim, had paid for Harry’s college education but had refused to do the same when he wanted to go back to school around age 40 — a sign, in Jamison’s eyes, of open and unforgivable favoritism. School-based status was a running concern for him: “He clearly had a competitive thing with me,” Frost said. “The fact that I had gone to UPenn was a point that he consistently brought up when he was trying to tear me down. He would say, ‘Oh, your Ivy League degree won’t help you with this, will it?’ ”Bachman’s resentment toward his father festered so much that, as Jim lay dying of cancer in the mid-aughts, his son declared he would skip the funeral and had “no regrets” about it. And several roommates told me that Bachman expressed a deep-seated hatred for his mother. “Jamison would say, ‘At least you had a mother. I didn’t have a mother after the age of 8, because I had a mind of my own and she didn’t like that,’ ” Hairabedian told me. “I guess that’s when their relationship started to go downward.”As he said he would, Bachman sat out his father’s funeral in New York. But shortly afterward, he returned home, where the fate of his father’s estate was being determined, expecting to receive a portion of the money. But, he told Acevedo, his mother claimed it for herself. “He was furious,” Acevedo told me. “He said, ‘She didn’t leave us one cent. Not one cent.’ ”By the time he arrived at Alex Miller’s home in March 2017, the only consistent presences in Bachman’s life were his pets, Zachary and Abigail, whom he called his “children.” A few days after Alex and her mother, Susan, discovered his true identity, Susan Miller let herself into Alex’s apartment unannounced and Bachman came roaring out at her. “What are you doing in my home?” he said. “This is my daughter’s home, Jamison,” Susan said. Bachman’s face went pale: It was the first time either of the Millers had acknowledged they knew his true name.Bachman had brushed off Alex’s demands that he leave with the mantra “I’ll see you in court.” So on April 26, Alex took letterhead from the lawyer she worked for and typed out a notice of demand. “Local police authorities have been alerted as to your previously recorded disputes as a tenant in sufferance,” she wrote. Bachman ignored the letter. Alex put out a listing for a new roommate, but when she brought one woman by to see the room, Bachman refused to open the door.By May 1, Miller had a plan. That night, a dozen friends, her mother, and a few neighbors arrived for a party that she described on Facebook as “a send-off … for the Serial Squatter Jamison Bachman,” meant to reclaim the space and signal he wasn’t welcome. She knew he started the online tutoring sessions he led to support himself in the evenings, so she told everyone to arrive at seven o’clock, prime time. She handed out mixed drinks made with Jameson whiskey. She blasted rap — which Bachman hated — from her stereo. She went online and found photos of Frost, which she printed and (“to psychologically fuck with him”) taped up in the bathroom above votive candles, so Bachman would see them. “I wanted him to know I knew his past,” she said, “and to have to face the people he’d harmed.”The partyers could hear Bachman in his room, shouting into his computer. Around 11 o’clock, Bachman emerged with a box of cat litter and dumped its contents into the toilet. Then he huffed out of the apartment with a backpack slung over his shoulder, Zachary slinking along behind him. Emboldened by Bachman’s absence and whiskey, Miller’s friend took a drill to his bedroom door and removed the knob.As the party wound down, friends implored Miller to stay with them for the night — Frost had warned the Millers about provoking Bachman. But Miller refused. She went to bed with her door open and slept poorly.Before dawn the following morning, Miller heard Bachman rise unusually early and leave the house. She crossed the hall into the bathroom and was brushing her teeth, thinking she might be able to slip out to work while he was gone, when the front door opened. Bachman barreled down the narrow hallway and, with a fist, slammed the bathroom door open. He pushed her against the wall, his hand at her throat, but when she screamed, he retreated. She followed him to his bedroom.Standing half in the doorway, she shouted, “Who the fuck do you think you are?” Bachman sat on his heap of quilts, dicing the cat food with the serrated knife — and then he was coming back at her, the knife in hand. He leaned against the door to shut it, and as she pulled back, her leg got stuck between the door and the frame. “You’ve made a grave mistake,” Bachman growled, jabbing the knife toward her through the opening. It sliced her thigh. Blood smeared the door. When it opened wide enough, Miller pulled her leg back and ran to her room to hide.That morning, two police officers arrived from the 14th District. According to their report, they found Bachman polite, cooperative, and apologetic. But when they saw the cuts on Miller’s legs, they arrested him. Bachman was charged with aggravated assault and other felonies and sent to jail, and Miller obtained a protection order.Without Bachman around, Zachary wandered the house aimlessly. Abigail, who had hidden in Bachman’s blankets since the day they moved in, emerged for the first time, her legs stiff, and took up a spot on Miller’s bed. Inside Bachman’s room, the heap of comforters still lay on the floor, and Bachman’s computer sat upon his makeshift desk of kitchen chairs. In folders, Miller found hundreds of pages of court filings against previous roommates, which she and her mother would use to track down other victims, and, in the back of his closet, she came across a blue box: a cleaning kit for a .380 caliber pistol and a box of bullets. Alex and Susan turned the house inside out looking for the gun. They cleaned out cabinets, peered into the air-conditioning vents, rented a metal detector and scoured the lawn. But the gun was nowhere to be found.Harry Bachman, Jamison’s brother.At one point, Harry and his wife, Caroline, had taken Jamison into their home in Elkins Park, only to learn what living with him could be like. Harry was not keen to experience that again, but it hurt him to know his younger brother was holed up in a jail cell. On June 17, Harry bailed him out. A few weeks later, the Millers arranged to meet Bachman at the local precinct to return his belongings. The morning of the exchange, Bachman stood outside the station, filming the Millers with his phone and narrating their arrival. As police observers hovered nearby, they handed him the Rubbermaid bins and Abigail. But Bachman was enraged when they declined to give back Zachary — they had sent him to live with a woman in the suburbs, and the judge had permitted her to keep him. As the Millers left the station, Bachman pulled up alongside them in a rented car and rolled down his window. “You’re dead, bitch,” he said, before speeding off. She turned around and reported him for violating the protection order, and a few weeks later he was rearrested.Imprisoned again, Bachman grew frantic about his cat, which had been left behind in an Airbnb after his arrest. He called Harry, concerned about getting bailed out immediately so that he could get back the cat, but it had been fostered out to someone by an animal shelter while he was in jail.On October 28, Harry bailed him out a second time. Jamison asked to stay at Harry’s house in Elkins Park, but Harry refused. Caroline Bachman was out of town to see their newborn grandchild, with plans to have Harry meet her the following week. But even from afar she feared what kind of argument might ensue if Jamison, now free, made an appearance at the house, and she asked Harry to stay elsewhere.Shortly before seven o’clock on the evening of November 3, Harry stopped home on his way out of town. As he pulled up in Caroline’s red Ford Escape, an unwelcome sight confronted him. “Guess who just showed up just as I drove in,” he texted Caroline. “No, don’t guess.” It was Jamison.Harry had been scheduled to arrive in upstate New York later that night, but he never made it. When Caroline called the police, an officer went to canvass the home and, at first, seeing that the red Ford Escape was gone, assumed he had left. But when police returned later that day they noticed a trail of blood leading from the sidewalk to the front door. When they entered the home, they found it in gory disarray: The dining room was blood-spattered, a “fresh” hole had been made in the wall, and shards of a shattered serving dish littered the floor. They followed “bloody drag marks” to the basement door, which had been blocked by a box. They opened the door and there, on the stairway, lay the body of Harry Bachman.Soon, police discovered a red Ford Escape in the parking lot of a hotel up the road, bloody towels inside the car. Jamison had checked into Room 102 the night before, under the name Harry G. Bachman. Around 10:30 p.m., a SWAT team broke down the door, and what happened next is unclear: The police originally reported that Bachman submitted “without incident,” but an affidavit filed a few weeks later claims he rushed the swat team, swinging a lime-green campfire ax at them “in a figure-X pattern.” As the police hauled him before the camera for his mug shot later that night, his face swollen and his shoulders hunched, Jamison focused his eyes in a dazed squint. A thin streak of blood slithered down his left cheek.Bachman’s preliminary hearing was set for the morning of December 11. At nine o’clock, I entered a low-slung district court building in Elkins Park, a few minutes away from the little stone house where Harry had died. The courtroom comprised just a few cheap stackable chairs and two wooden tables. The space was tiny, and it struck me how close Bachman would be — no more than a few feet away from me. After months of talking to those forced to live with him, I felt oddly nervous.The room, however, remained empty. After five minutes, a clerk came in. “Are you here for the nine o’clock?” she asked. “It’s been canceled.” Jamison Bachman was dead. A few days earlier, he’d hanged himself in his cell at the Montgomery County prison.The news of Bachman’s death rocked his former roommates. When I told Arleen Hairabedian, she burst into tears. “I wanted him to suffer,” she said. But she also sounded like someone who’d spent a lot of time trying to navigate around Bachman’s anger. “What if he just wanted somewhere to stay and he showed up and his brother said he didn’t want him there and it escalated? What if he was desperate? Here I am, making excuses.”The deaths of the Bachman brothers left Alex Miller wracked by guilt. “I feel responsible for all of it,” she told me not long after. But along with guilt came relief: The trial was canceled, and she would no longer have to face him in court. There would be no more looking over her shoulder when she left the house, no more worrying about the gun. She was ready to move on.In August, Miller and a friend — someone she’s known for half her life — finally settled into another apartment, in a quiet neighborhood just across the city line. On the January morning I last visited her there, the air was warm, and from the bushes issued the frenzied song of the common house sparrow. Inside, Miller was busy on the phone: Her roommate, she explained, had moved out a few weeks earlier, and she was fielding calls from potential replacements. I could hear her vetting them: “How old are you? What was your situation before this? What do you do for work? Do you have three references?” So far, only one visitor of four — a “no drama” restaurant manager who worked long hours — had caught her interest. And there had been a scare. “One woman showed up on a Sunday after church, and she had her purse, a miniature poodle, and a duffel bag,” Miller told me, as if to say, Can you believe it? As the woman stood on the front step, eyeing her expectantly, Alex shut the door.*This article appears in the February 19, 2018, issue of New York Magazine. Subscribe Now!",This Is the Worst Roommate Story You’ll Ever Read
9670,3703835,2018-02-23 04:34:32,"TNW SitesSpaceX’s satellites for beaming internet access to earth are now in orbitSpaceX is now one step closer to delivering broadband access via satellite, as it’s successfully launched a Falcon 9 rocket (seen above) that delivered two Starlink demo satellites into low-earth-orbit.The company’s been working on this for some time now, and the two Starlink satellites will help it test its systems for beaming internet access down to earth before more of them are deployed into space. The launch was slated for last weekend and then delayed to allow for more stringent testing of the updated fairing.As Motherboard notes, traditional satellite internet services are expensive and can be laggy, owing to the way geostationary satellites work. But with a constellation of thousands of smaller satellites, SpaceX can overcome the obstacles presented by the distance between them, and hopefully, offer a faster and more affordable service than existing ones.To that end, SpaceX has plans to put thousands of such satellites into orbit between 2019 and 2024, in the hopes that the revenue generated by the service it offers will fund missions to Mars. Is 2018 turning out to be a good year for CEO Elon Musk or what?The Next Web’s 2018 conference is just a few months away, and it’ll be 💥💥. Find out all about our tracks here.",SpaceX's satellites for beaming internet access to earth are now in orbit
9671,3703836,2018-02-23 01:45:20,"TNW SitesResearchers are wirelessly charging devices by blasting them with lasersEven the most futuristic of devices start to feel dated when you plug them into charge. Sure, there’s wireless charging, but it’s just plugging in your device without actually plugging it in, right? I mean, you’re still plugging in the mat that it sits on to charge.You know what sounds futuristic? Lasers.Researchers think so too. A team of scientists recently discovered a way to charge our favorite devices using laser beams. Using a narrow, invisible beam from a laser emitter, the researchers discovered they could successfully charge a smartphone from across the room — and potentially just as quickly as using a standard USB cable.The team mounted a thin power cell to the back of a smartphone. The power cell harnesses the power of the laser, and charges the phone through the standard charging input.To avoid a potential micro-sized Death Star situation, the team added a metal heatsink ti dissipate excess heat from the laser, as well as a reflector mechanism that shuts the beam off if someone moves into its path.“Safety was our focus in designing this system,” said co-author Shyam Gollakota, an associate professor in the UW’s Paul G. Allen School of Computer Science & Engineering. “We have designed, constructed and tested this laser-based charging system with a rapid-response safety mechanism, which ensures that the laser emitter will terminate the charging beam before a person comes into the path of the laser.”Currently, the beam is only capable of about 2 watts per hour on a 9 square centimeter (15 square inch) area from about 4.3 meters (14 feet). According to the research paper, however, this is modifiable to expand the charging beams radius to around 100 square centimeters, from a distance of 12 meters.I wouldn’t expect to see laser charging in the near future — especially with the current device market moving slower than molasses in accepting wireless charging — but, fingers crossed, it could be what’s next. You know, assuming nobody blows up.",Researchers found a way to charge your mobile device with lasers
9672,3706154,2018-02-23 05:33:37,"Apple is reportedly adding hands-free Siri and water resistance to its next AirPodsWe heard back in December 2017 that Apple had plans for new AirPods; this year, they’re slated to get a new wireless chip for improved wireless chip. That’ll allow for access to Siri without having to physically tap the earphones; you’ll be able to invoke the assistant with just a voice command.Next year, they’ll also be water resistant, so they won’t go kaput if you’re caught in the rain.That’s good news for Apple fans and people who are already comfortable using Siri. Google Assistant only recently became available on headphones, starting with the QC35 II set from Bose; it also features in the smaller Pixel buds from Google (which also manage real-time translation). And while Amazon’s Alexa has made it into headphones, it’s yet to be baked into wireless earbuds. Apparently, it’s not easy for third-party manufacturers to pull off without significantly affecting battery life.While I’m more interested in higher sound fidelity and battery life in upcoming wireless audio gear, access to assistants for things like help with navigation wouldn’t hurt. Oh, and wireless charging would be nice: Apple is slated to launch an AirPods case for that this year too, so it might have a truly compelling offering on its hands by 2019.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Apple is reportedly adding hands-free Siri and water resistance to its next AirPods
9673,3708240,2018-02-01 22:26:15,"How Much Money Do People Have?What’s your net worth? Are you saving enough for retirement? Is your cash flow enough to manage your student and personal loans? How do you compare to others like you?It can be difficult to gauge your financial progress in life without benchmarks for comparison. Data can be useful to see how you’re doing. For young professionals in particular, about whom the following data is most relevant, it can add some context to broader national statistics about savings and debt.In a new data study from Earnest, we analyzed cash, investment, and debt balances based on anonymized data from in tens of thousands of applications for student loan refinancing.In our analysis, we focused on three major categories of financial accounts: cash, including checking and savings and miscellaneous banking accounts; investments, including retirement accounts and/or regular accounts; and debt, including credit cards, personal loans, and student loan balances.We then used this data to calculate net worth, or liabilities (debt) subtracted from assets (cash, investments, and other equity). A limitation of our data is that we did not include assets and liabilities related to home or company stock, both of which would contribute to net worth for an individual.Earnest’s data is based on our applicant pool and does not represent the national picture but rather a subset of people with student loans. Through the following charts, you can view our findings about net worth by income level, higher education degree, age, and gender.Key TakeawaysNet worth, cash, and investment account balances generally trend upward as income and age increase, though debt grows accordingly.MBAs have the highest net worth of all higher educational degrees, while doctors and dentists have the most cash (and debt).Men and women have similar cash and debt balances.On average, people in every income bracket have 1-2 months of monthly salary in cash.Men have more than double the investment account balances, leading to much higher net worth.How Does Your Net Worth Compare?Overall, net worth trends upward as income increases, reaching a peak in the $120-$150,000 income level. Applicants in the lower income levels struggle to save more than they owe, resulting in negative average net worth for those making less than $60,000.The highest income bracket, earning more than $150,000 per year, takes a hit to their net worth by accruing significantly more debt than lower income groups. This is likely due to the fact that within our data pool, these high-earners tend to be those with heavy debt loads from medical or dentistry school.While those making $120-$150,000 have the highest net worth, who has the most liquid cash?It’s simple: higher earners have more cash.Those making more than $150,000 per year have the highest average balances in their checking, savings and banking accounts. Those making less than $40,000 per year have the lowest balances, averaging less than $5,000.The encouraging news?Across the board, applicants keep one to two months’ worth of income as cash ($5,000 for someone making $40,000; $11,000 for someone making $95,000). Financial planners typically advise people to keep between two and six months worth of income in an emergency fund.Do applicants keep their investments as proportional to their income as they do their cash on hand?While investment account balances also increase with income, the trend is steeper. Compounding interest in retirement and investment accounts help bigger balances grow faster than smaller balances, creating a fast-widening gap between earning groups.Interestingly, applicants earning between $120-$150,000 have larger retirement accounts on average than those making more than $150,000. This could be a byproduct of the length of time in school for the highest-earners who tend to hold MDs and DDS degrees—and when they are not contributing to an employer-sponsored retirement plan.Is debt proportional to income as well? Or do lower incomes take on more debt out of necessity?Surprisingly, income does not have as big of an impact on debt as it does on cash and investment account balances. Debt increases as income rise, but the trend is slight.Applicants in income groups under $90,000 have credit card debt ranging from $1,000 to $2,000 on average, and student and personal loan debt averaging between $17,000-$21,000.The highest income group averages double that—with $41,000 on average—in student and personal loans, potentially from securing an advanced degree or starting a business or private practice.Your Money, Your EducationNext, we looked at net worth by degree type. The result is a peek into the complex financial picture associated with pursuing certain degrees.Applicants with Doctor of Osteopathic Medicine (D.O.M.) and Doctor of Dental Surgery (D.D.S.) degrees have the lowest net worth, while applicants with Masters of Business Administration (M.B.A.) degrees have the highest average net worth, on average.Why is that? Osteopathic doctors and dentists likely have the lowest net worths despite healthy cash and investment balances due to high levels of debt. The data showed that those with Doctor of Medicine (M.D.) degrees balance out high levels of debt with high cash and investment amounts resulting in moderate net worth.Now, let’s get a closer look at cash by degree.Medical doctors have the highest cash balances, averaging more than $32,000 between checking, savings, and banking accounts. Despite their low net worth, dentists and osteopathic doctors also have high cash balances, averaging about $29,000 and $25,000 respectively. Applicants with associate degrees have the lowest balances, averaging less than $6,000 cash on hand.Who’s saving best for retirement? Next, we looked at investment account balances by degree.While doctors and dentists had the most cash, MBAs have the most socked away for retirement. One explanation may be that earning an MBA typically only typically only takes two years and degree-holders are not out of the workforce for long. They may also be working at larger companies with 401(k) match and their own familiarity with financial products could also help.Older, Wiser, RicherNext, we focused on age groups to get a better look into the role of age on financial balances. Note: In our data analysis, we didn’t splice by specific age, but created age deciles clustered by median age.Overall, net worth trends upward with age, starting at -$7,849 for the youngest age group and peaking at $38,072 in the oldest age group in our sample. A slight dip occurs for the group with a median age of 35, with an uptick in debt and subsequent decline in cash.Let’s look more closely at cash balances by age decile.While the overall trend shows cash on hand increasing with age, something is happening after the median age of 35 that causes cash to trend downward. The youngest applicants have about $6,000 in cash, while those around age 35 have an average of $13,347 in cash. By age 38, our applicants averaged $9,704 in cash account balances.Are people putting more money into retirement accounts after age 35? We looked into investment account balances by age decile to find out.There is a clear, and steep, increase in investment account balances as age increases, particularly after age 38.The youngest applicants, under age 25, have an average of $6,799 in investment accounts, and only $3,529 in their retirement accounts. The oldest age group, age 38 and older, have more than $59,000 invested, with an average of $48,112 socked away for retirement.This still doesn’t explain the net worth drop around age 35. Debt may play a role in that trend, so we honed in on debt by age decile.Debt from the ages 25 through 34, stays fairly constant around at around $21,000-$24,000. The average debt balance for those with a median age 35 jumps up to $32,125. One explanation is that this is when many people make the leap to buy their first home. While our data study did not include mortgage debt, there may be some overflow costs with personal debt. As a side note, another of our recent studies found that people outgrow IKEA around age 34 when many become homeowners.Minding the Investment GapWhile income, age group, and higher education degree clearly play a role in the financial outcome, is there a difference with cash, investment and debt balances by gender? We split our data out into male and female groups to find out.On average, young professional men and women both have positive net worth. However, there is a large gap in net worth between men and women. Men have more than double the net worth of women, averaging $12,188 compared to the female average of $5,541.Overall, men and women have similar cash balances: $9,512 for women and $9,565 for men. Women hold more money in their savings accounts than men, who keep more in their checking accounts. Are women more conservative with their cash?Women have slightly less debt, on average than men: $23,511 versus $24,095. While their student and personal loans are similar, men hold higher debt in their credit card balances than women.While men and women have similar average debt and cash balances, the big difference comes in the form of investment balances.On average, women have investment account balances totaling $19,541 compared to the male average of $26,717. They have significantly lower balances in their investment and retirement accounts. This is particularly troublesome: over time, the investment gap will grow exponentially.This investment gap could be explained by a few factors: lower starting salaries, a persistent wage gap, behavioral bias toward risk-taking, and so on. Whatever the reasons underpinning this gap, our data provides more evidence that overall economic picture for women starts to lag behind men starting in their 20s. While today, the gap is $7,177. Fast forward 30 years and that same gap, invested in a fund earning 5% annually and no further contributions), that gap will more than quadruple to $32,161.***We set out to paint a picture of net worth for a variety of groups. We found that net worth generally increases with age and income, though debt does as well. Doctors and dentists had the most cash on hand – and debt – of all higher education degrees, while applicants with MBAs had the strongest investment accounts and net worth. On average, men have higher net worth than women, primarily due to larger investment account balances.","Ranking Net Worth by Age, Income, Degree, and Gender"
9674,3708476,2018-02-23 06:26:11,"Next, a source at Google told Variety that the company intends to carry out its plan of bringing ARCore to 100 million phones by March. That means it’ll need to enable support for the framework on a wider range of devices (only Pixel handsets are natively supported at the moment).Samsung lineup of high-end Android phones seems most likely to gain ARCore support first, as its Galaxy S8 was made eligible to run a developer preview version of the framework.The other major change to help with adoption is that ARCore now works not just with Android Oreo 8.0, but also with the older Nougat versions. That makes sense, given that only a small number of handsets are presently running the latest version of the Android platform. I’m also interpreting this as a reduction in the need for additional depth-sensing hardware components, which Google previously required for the now-defunct Project Tango that preceded ARCore.It’ll be interesting to see what that spells for users and developers. While Apple fans have been able to access a wide range of AR experiences thanks to the company’s rapid rollout of ARKit, ARCore content has largely been restricted to animated stickers that you can place in real-world scenes seen through your phone’s camera.Stay tuned to TNW for more MWC coverage next week.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Google is one step closer to bringing ARCore to 100 million phones
9675,3711499,2018-02-23 08:18:47,"TNW SitesBlizzard revives Warcraft III after 15 years with major update and tournamentAlthough it’s been focused on its smash hit multiplayer shooter Overwatch lately, Blizzard Entertainment has managed to keep its blockbuster real-time strategy game Warcraft III up-to-date since 2002 with annual patches. Now, it’s set to make waves again with a tournament.The first-ever ‘Warcraft III Invitational’ will see well-known players from around the world go up against each other at Blizzard’s headquarters this weekend. It’ll be streamed live on Twitch, and hosted by casters from the Back2Warcraft site that continues to air and encourage competitive Warcraft III matches.In addition, the title is getting a major update with balance adjustments for several heroes in the game, as well as 16:9 widescreen support. These are already available to those in the Public Test Realm, which opened up last fall.Naturally, this news is stoking rumors about a possible Warcraft III remaster – but Blizzard hasn’t said anything about that yet.",Warcraft III is making a comeback after 15 years
9676,3711500,2018-02-07 00:29:49,"About TNWTNW Sites15 steps to protect your company from a cyberattackWith last year reporting the highest level of cyberattacks yet, this year is a year of prevention. Companies are taking the time to understand how to keep their customer information safe from a cyberthreat and putting in place solid measures that can thwart any attack to their data.As a business owner, you need to consider the importance of protecting your customers’ confidential information to prevent it from getting into the wrong hands. To understand how your company can assess its cybersecurity and repair any potential sources of risk, entrepreneurs fromYEC weighed in.Name one step business owners can take to accurately assess their company’s level of cybersecurity risk and begin to tackle it.Their best answers are below:1. Identify basic threats.Hackers and anyone who poses a cybersecurity threat feed off a lack of information on your part. You should have a plan for basic threats like unauthorized access before you even start, and plan to keep learning about cybersecurity your whole career. – Michael Dash, Parallel HR Solutions, Inc.2. Operate like you expect an attack.Be aware of the data that is leaving your business and whether it would be attractive to cyberattackers. Make sure your team has the resources to conduct risk assessments regularly and develop a multi-layered strategy that includes solutions, such as privilege management. – Blair Thomas, eMerchantBroker3. Look at employee behavior.Start with employees, including the remote team, and how they connect and use your network. Also, consider how they use passwords and view security threats. Often, the risk comes from staff that don’t realize the threat. – Peter Daisyme, Calendar4. Use two-factor authentication.We hear of a new data breach every day. Assume your passwords will be leaked at some point in the future and plan for it. Require all employees use two-factor authentication when accessing any important accounts. This is the simplest way to minimize risk. – Francois de Lame, Policygenius Inc.5. Commit to an audit.If you’ve gotten to the point that you are certain you want to validate your security risks, you need to commit to getting an audit done. There are lots of good cybersecurity consultants that can do this. Another option is to consider hiring a chief information security officer. – Nicole Munoz, Start Ranking Now6. Have a strong offboarding policy.It’s important to have a strong offboarding policy when employees leave, to mitigate the risk of a potential cybersecurity threat. Offboarding should include returning ID badges, company credit cards, mobile devices and laptops, as well as deleting their email address and changing passwords if not encrypted. – Syed Balkhi, OptinMonster7. Start with what’s important.What would be the most vulnerable thing to have leak out or lose? Start with that. Check how it’s accessed by staff and that it can’t be accessed by outside sources. Audit your procedures around storing that information to make sure you’re protected where it matters. – Jürgen Himmelmann, The Global Work & Travel Co.8. Conduct a risk assessment.It’s simple. Most businesses should be conducting a cybersecurity risk assessment once every couple of years. This will ensure important risk mitigation is in place, and priorities can be made and tasks completed. Risk management should be an ongoing activity and fall into a standard cadence to ensure the business is not exposed. – Baruch Labunski, Rank Secure9. Hire a professional.Hire a ‘white hat’ hacker who will do their best to try and breach your systems. They will try everything from code vulnerabilities and brute-forcing passwords, to manipulating your employees for sensitive data. They’ll tell you the results and probably also help you fix them. – Karl Kangur, MRR Media10. Consult the FINRA checklist.The Financial Industry Regulatory Authority (FINRA) has created a valuable cybersecurity checklist for small businesses. It’s available for download on their website. It’s a four-part list that focuses on prevention, detection, planning for a possible security breach and recovery of lost or stolen assets in the event of a security incident. It’s an excellent place to start. – Thomas Smale, FE International11. Calculate your risk rating.Calculating your risk rating for each identified threat will help you determine which areas you need to focus on first. Each threat will receive its own value and risk calculation of low, elevated or severe. – Jared Atchison, WPForms12. Don’t rely on ad-hoc security processes.Security depends on knowledge — a business has to understand where it may be exposed to risk before it can implement effective security policies. First step: systematically audit your networks. Build a comprehensive overview of the hardware and software you have in play. Next, use the results of your audit to implement security policies and processes that reduce the risks you have identified. – Justin Blanchard, ServerMania Inc.13. Use decentralized blockchain applications.I recommend leveraging blockchain decentralized applications. There are various blockchain applications offerings and different blockchain solutions for better security compared to a centralized model. – Matthew Capala, Alphametic14. Inquire about a cyber insurance policy.Certain companies now offer insurance policies for cyber attacks. This may or may not be a good investment for your own business. However, simply inquiring about such policies and getting an estimate will help you better understand your level of risk. As with any type of insurance, the cost of the policy depends on your risk factors. – Kalin Kassabov, ProTexting15. Check for impersonators on social media.My recommendation is to look for duplicate profiles on social media, if you are someone who is well known around the web. Also, as far as websites are concerned, you need SSL certificates and also security from your hosting provider to keep things secure. But if you see any suspicious activity, report it to hosting providers right away. Also, use password vaults to store passwords. – Rafi Chowdhury, Chowdhury’s DigitalThis post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",15 steps to protect your company from a cyberattack
9677,3713899,2018-02-21 22:35:34,"A Practical Introduction to Container TerminologyYou might think containers seem like a pretty straightforward concept, so why do I need to read about container terminology? In my work as a container technology evangelist, I’ve encountered misuse of container terminology that causes people to stumble on the road to mastering containers. Terms like containers and images are used interchangeably, but there are important conceptual differences. In the world of containers, repository has a different meaning than what you’d expect. Additionally, the landscape for container technologies is larger than just docker. Without a good handle on the terminology, It can be difficult to grasp the key differences between docker and (pick your favorites, CRI-O, rkt, lxc/lxd) or understand what the Open Container Initiative is doing to standardize container technology.BackgroundIt is deceptively simple to get started with Linux Containers. It takes only a few minutes to install a container engine like docker and run your first commands. Within another few minutes, you are building your first container image and sharing it. Next, you begin the familiar process of architecting a production-like container environment, and have the epiphany that it’s necessary to understand a lot of terminology and technology behind the scenes. Worse, many of the following terms are used interchangeably… often causing quite a bit of confusion for newcomers.ContainerImageContainer ImageImage LayerRegistryRepositoryTagBase ImagePlatform ImageLayerUnderstanding the terminology laid out in this technical dictionary will provide you a deeper understanding of the underlying technologies. This will help you and your teams speak the same language and also provide insight into how to better architect your container environment for the goals you have. As an industry and wider community, this deeper understanding will enable us to build new architectures and solutions. Note, this technical dictionary assumes that the reader already has an understanding of how to run containers. If you need a primer, try starting with A Practical Introduction to Docker Containers on the Red Hat Developer Blog.Containers 101To understand container terminology, it’s important to understand exactly what a container is – with technical precision. A container is really two different things. Like a normal Linux program, containers really have two states – rest and running. When at rest, a container is a file (or set of files) that is saved on disk. This is referred to as a Container Image or Container Repository. When you type the command to start a container, the Container Engine unpacks the required files and meta-data, then hands them off to the the Linux kernel. Starting a container is very similar to starting a normal Linux process and requires making an API call to the Linux kernel. This API call typically initiates extra isolation and mounts a copy of the files that were in the container image. Once running, Containers are just a Linux process. The process for starting containers, as well as the image format on disk, are defined and governed by standards.There are several competing Container Image formats (Docker, Appc, LXD), but the industry is moving forward with a standard governed under the Open Container Initiative – sometimes referred to simply as Open Containers or the OCI. The scope of the OCI includes a Container Image Format Specification, which defines the on-disk format for container images as well as the meta-data which defines things like hardware architecture and the operating system (Linux, Windows, etc). An industry wide container image format enables ecosystems of software to flourish – different individual contributors, projects, and vendors are able to build images and tooling, which are interoperable. Users want interoperability between tools for signing, scanning, building, running, moving and managing container images.Tools which target the OCI Container Image Format Specification and Container Runtime Specification ensure portability between a broad ecosystem of container platforms, container engines, and supporting tools across cloud providers and on premise architectures. Understanding the nomenclature, container standards, and the architecture of the building blocks of containers, will ensure that you can communicate with other architects to build scalable & supportable containerized applications and environments to productively run containers for years to come.Basic VocabularyContainer ImageThe container community uses “container image” quite a bit, but this nomenclature can be quite confusing. Docker, RKT, and even LXD, operate on the concept of pulling remote files and running them as a Container. Each of these technologies treats container images in different ways. LXD pulls a single container image (single layer), while docker and RKT use OCI-based images which can be made up of multiple layers.Container Image FormatHistorically, each image-based Container Engine has its own format for container images. LXD, RKT, and Docker all had their own image formats. Some are made up of a single layer, while others are made up of a bunch of layers in a tree structure.The industry is getting behind the Open Container Initiative (OCI) with the goal of unifying behind a single container image format that will be used by a wide ecosystem of container engines, cloud providers and tools providers (security scanning, signing, building and moving).Container EngineA container engine (aka container runtime) is the piece of software that consumes a Container Image and turns it into a Container. There are many container engines, including docker, RKT, CRI-O, and LXD. Also, many cloud providers, Platforms as a Service (PaaS), and Container Platforms have their own built-in container engines which consume docker and/or OCI compliant Container Images. Having an industry standard Container Image Format allows interoperability between all of these different platforms.Container HostThe container host is the system that runs the containerized processes, often simply called containers. This could be, for example, RHEL Atomic Host running in a VM, as an instance in the public cloud, or on bare metal in your data center. Once a container image (aka repository) is pulled from a Registry Server to the local container host, it is said to be in the local cache.Determining which repositories are synchronized to the local cache can be determined with the following command:Registry ServerA registry server is essentially a fancy file server that is used to store docker repositories. Typically, the registry server is specified as a normal DNS name and optionally a port number to connect to. Much of the value in the docker ecosystem comes from the ability to push and pull repositories from registry servers.When a docker daemon does not have a locally cached copy of a repository, it will automatically pull it from a registry server. Most Linux distributions have the docker daemon configured to pull from docker.io but it is configurable on some Linux distributions. For example, Red Hat Enterprise Linux is configured to pull repositories from registry.access.redhat.com first, then it will try docker.io (Docker Hub).It is important to stress, that there is implicit trust in the registry server. You must determine how much you trust the content provided by the registry and you may want to allow or block certain registries. In addition to security, there are other concerns such as users having access to licensed software and compliance issues. The simplicity with which docker allows users to pull software makes it critical that you trust upstream content.In Red Hat Enterprise Linux, the default docker registry is configurable. Specific registry servers can be added or blocked in RHEL7 and RHEL7 Atomic bymodifying the configuration file:vi /etc/sysconfig/dockerIn RHEL7 and RHEL 7 Atomic, Red Hat’s registry server is configured out of the box:ADD_REGISTRY='--add-registry registry.access.redhat.com'As a matter of security, it may be useful to block public docker repositories such as DockerHub:Advanced VocabularyImage LayerRepositories are often referred to as images or container images, but actually they are made up of one or more layers. Image layers in a repository are connected together in a parent-child relationship. Each image layer represents changes between itself and the parent layer.Below, we are going to inspect the layers of a repository on the local container host. SinceDocker 1.7, there is no native tooling to inspect image layers in a local repository (there are tools for online registries). With the help of a tool called Dockviz, you can quickly inspect all of the layers. Notice that each layer has tag and aUniversally Unique Identifier (UUID). The following command will returned shortened versions of the UUID that are typically unique enough to work with on a single machine. If you need to the full UUID, use the –no-trunc option.Notice, that the “docker.io/registry” repository is actually made up of many images layers. More importantly, notice that a user could potentially “run” a container based off of any one of these layers. The following command is perfectly valid, though not guaranteed to have been tested or actually even work correctly. Typically, an image builder will tag (create a name for) specific layers that you should use:docker run -it 45b3c59b9130 bashRepositories are constructed this way because whenever an image builder creates a new image, the differences are saved as a layer. There are two main ways that new layers are created in a repository. First, ifbuilding an image manually, each “commit” creates a new layer. If the image builder isbuilding an image with a Dockerfile, each directive in the file creates a new layer. It is useful to have visibility into what has changed in a container repository between each layer.TagEven though a user can run a container from any of the image layers, they shouldn’t necessarily do that. When an image builder creates a new repository, they will typically label the best image layers to use. These are called tags and typically map to versions of software contained in the repository.To remotely view the tags available in a repository, run the following command (thejq utility makes the output a lot more readable):RepositoryWhen using the docker command, a repository is what is specified on the command line, not an image. In the following command, “rhel7” is the repository.docker pull rhel7This is actually expanded automatically to:docker pull registry.access.redhat.com/rhel7:latestThis can be confusing, and many people refer to this as an image or a container image. In fact, the docker images sub-command is what is used to list the locally available repositories. Conceptually, these repositories can be thought of as container images, but it’s important to realize that these repositories are actually made up of layers:When we specify the repository on the command line, the Container Engine is doing some extra work for you. In this case, the docker daemon (not the client tool) is configured with a list of servers to search. In our example above, the daemon will search for the “rhel7” repository on each of the configured servers.In the above command, only the repository name was specified, but it’s also possible to specify a full URL with the docker client. To highlight this, let’s start with dissecting a full URL.Another way you will often see this specified is:REGISTRY/NAMESPACE/REPOSITORY[:TAG]The full URL is made up of a standard server name, a namespace, and optionally a tag. There are actually many permutations of how to specify a URL and as you explore the docker ecosystem, you will find that many pieces are optional. The following commands are all valid and all pull some permutation of the same repository:NamespaceA namespace is a tool for separating groups of repositories. On the public DockerHub, the namespace is typically the username of the person sharing the image, but can also be a group name, or a logical name.Red Hat uses the namespace to separate groups of repositories based on products listed on theRed Hat Federated Registry server. Here are some example results returned by registry.access.redhat.com. Notice, the last result is actually listed on another registry server. This is because Red Hat works to also list repositories on our partner’s registry servers:Notice, that sometimes the full URL does not need to be specified. In this case, there is a default repository for a given namespace. If a user only specifies the fedora namespace, the latest tag from the default repository will be pulled to the local server. So, running the following commands is essentially the same, each one more specific:Kernel NamespaceA kernel namespace is completely different than the namespace we are referring to when discussing Repositories and Registry Servers. When discussing containers, Kernel namespaces are perhaps the most important data structure, because they enable containers as we know them today. Kernel namespaces enable each container to have it’s own mount points, network interfaces, user identifiers, process identifiers, etc.When you type a command in a Bash terminal and hit enter, Bash makes a request to the kernel to create a normal Linux process using a version of the exec() system call. A container is special because when you send a request to a container engine like docker, the docker daemon makes a request to the kernel to create a containerized process using a different system call called clone(). This clone() system call is special because it can create a process with its own virtual mount points, process ids, user ids, network interfaces, hostname, etcWhile, technically, there is no single data structure in Linux that represents a container, kernel namespaces and the clone() system call are as close as it comes.Graph DriverWhen the end user specifies the Tag of a container image to run – by default this is the latest Tag – the graph driver unpacks all of the dependent Image Layers necessary to construct the data in the selected Tag. Thegraph driver is the piece of software that maps the necessary image layers in the Repository to a piece of local storage. The container image layers can be mapped to a directory using a driver like Overlay2 or in block storage using a driver like Device Mapper. Drivers include: aufs, devicemapper, btrfs, zfs, and overlayfs.When the container is started, the image layers are mounted read-only with a kernel namespace. The Image Layers from the Repository are always mounted read only but by default, a separate copy-on-write layer is also set up. This allows the containerized process to write data within the container. When data is written, it is stored in the copy-on-write layer, on the underlying host. This copy-on-write layer can be disabled by running the container with an option such as –readonly.The docker daemon has it’s own set of Graph Drivers and there are other open source libraries which provide Graph Drivers such as containers/images which is used in tools like CRI-O, Skopeo and other container engines.Determining which graph driver you are using can be done with the docker info command:Container Use CasesThere are many types of Container design patterns forming. Since containers are the run time version of a container image, the way it is built is tightly coupled to how it is run.Some Container Images are designed to be run without privilege, while others are more specialized and require root-like privileges. There are many dimensions in which patterns can be evaluated and often users will see multiple patterns or use cases tackled together in one container image/container.This section will delve into some of the common use cases that users are tackling with containers.Application ContainersApplication containers are the most popular form of container. These are what developers and application owner’s care about. Application containers contain the code that developers work on. They also include things like MySQL, Apache, MongoDB, and or Node.js.There is a great ecosystem of application containers forming. Projects like Software Collections are providing secure and supportable applications container images for use with Red Hat Enterprise Linux. At the same time, Red Hat community members are driving some great cutting edge applications containers.Red Hat believes that Application Containers should not typically require special privileges to run their workloads. That said, production container environments typically require much more than just non-privileged application containers to provide other supporting services.Operating System ContainersSee also System ContainersOS Containers are containers which are treated more like a full virtual operating system. OS Containers still share a host kernel, but run a full init system which allows them to easily run multiple processes. LXC and LXD are examples of OS Containers because they are treated much like a full virtual machine.It is also possible to approximate an OS Container with docker/OCI based containers, but requires running systemd inside the container. This allows an end user to install software like they normally would and treat the container much more like a full operating system.yum install mysqlsystemctl enable mysqlThis makes it easier to migrate existing applications. Red Hat is working hard to make OS Containers easier by enabling systemd to run inside a container and by enabling management with machined. While many customers aren’t (yet) ready to adopt micro-services, they can still gain benefits from adopting image based containers as a software delivery model.Pet ContainersThis is a common pattern that not many people talk about publicly. The Pet Container use case is common when starting out, when developing tools, and even when integrating into an existing production environment where the end users do not want to change their workflow.Pet containers provide users with the portability and convenience of a standardized container infrastructure relying on registry servers, container images, and standard container hosts for infrastructure, but provide the flexibility of a traditional environment.Super Privileged ContainersWhen building container infrastructure on dedicated container hosts such as Red Hat Enterprise Linux Atomic Host, systems administrators still need to perform administrative tasks. Whether used with distributed systems, such as Kubernetes or OpenShift or standalone container hosts, Super Privileged Containers (SPC) are a powerful tool. SPCs can even do things like load specialized kernel modules, such as with systemtap.In an infrastructure that is built to run containers, administrators will most likely need SPCs to do things like monitoring, backups, etc. It’s important to realize that there is typically a tighter coupling between SPCs and the host kernel, so administrators need to choose a rock solid container host and standardize on it, especially in a large clustered/distributed environment where things are more difficult to troubleshoot. They then need to select a user space in the SPC that is compatible with the host kernel.Tools & Operating System SoftwareLinux distributions have always provided users with system software such as Rsyslogd, SSSD, sadc, etc. Historically, these pieces of system software were installed through RPM or DEB packages. But with the advent of containers as a packaging format, it has become both convenient and easy to install system software through containers images. Red Hat provides some pre-packaged containers for things like the Red Hat Virtualization tools, rsyslog, sssd, and sadc.Architecture of ContainersNew design patterns are forming as more and more people deliver software with containers. Red Hat engineering is leveraging and driving many of these patterns in the community. The goal of this section is to help highlight and define some of these patterns.The way a container is saved on disk (i.e. its image format) can have a dramatic affect on how it is run. For example, a container which is designed to run sssd (needs to have special privileges whenever it’s run, or it can’t do its job. The following is a short list of patterns that are forming in the container community:Application ImagesThese images are what end users consume. Use cases range from databases and web servers, to applications and services buses. These can be built in house or delivered to a customer from an ISV. Often end users will investigate and care about what bits were used to create a standalone image. Standalone images are the easiest kind of image to consume, but the hardest to design, build, and patch.Base ImagesA base image is one of the simplest types of images, but you will find a lot of definitions. Sometimes users will refer to corporate standard build, or even an application image as the “base image.” Technically this is not a base image. These are Intermediate images.Simply put, a base image is an image that has no parent layer. Typically, a base image contains a fresh copy of an operating system. Base images normally include the tools (yum, rpm, apt-get, dnf, microdnf) necessary to install packages / make updates to the image over time. While base images can be “hand crafted”, in practice they are typically produced and published by open source projects (like Debian, Fedora or CentOS) and vendors (like Red Hat). The provenance of base images is critical for security. In short, the sole purpose of a base image is to provide a starting place for creating your derivative images. When using a dockerfile, the choice of which base image you are using is explicit:FROM registry.access.redhat.com/rhel7-atomicBuilder ImagesThese are a specialized form of container image which produce application container images as offspring. They include everything but a developer’s source code. Builder images include operating system libraries, language runtimes, middleware, and the source-to-image tooling.When a builder image is run, it injects the developers source code and produces a ready-to-run offspring application container image. This newly created application container image can then be run in development or production.For example, if a developer has PHP code and they want to run it in a container, they can use a PHP builder image to produce a ready to run application container image. The developer passes the GitHub URL where the code is stored and the builder image does the rest of the work for them. The output of a Builder container is an Application container image which includes Red Hat Enterprise Linux, PHP from Software Collections, and the developer’s code, all together, ready to run.Builder images provide a powerful way to go from code to container quickly and easily, building off of trusted components.Containerized ComponentsA container is meant to be deployed as part of a larger software system, not on its own. Two major trends are driving this.First, microservices are driving the use of best of breed components – this is also driving the use of more components combined together to build a single application. Containerized components are meeting the need to deploy an expanding quantity of complex software more quickly and easily.Second, not all pieces of software are easy to deploy as containers. Sometimes, it makes sense to containerize only certain components which are easier to move to containers or provide more value to the overall project. With multi-service application, some services may be deployed as containers, while others may be deployed through traditional a traditional methodology such as an RPM or installer script.It’s important to understand that containerized components are not designed to function on their own. They provide value to a larger piece of software, but provide very little value on their own.For example, when OpenShift Enterprise 3.0 was released, most of the core code was deployed using RPMs, but after installation administrators deployed the router and registry as containers. With the release of OpenShift 3.1 an option was added to the installer to deploy the master, node, openvswitch and etcd components as containers – after installation, administrators were given the option to deploy elasticsearch, fluentd, and kibana as containers.While the OpenShift installer still makes modifications to a server’s file system, all of the major software components can now be installed using container images. What makes these containerized components is that, for example, an instance of the etcd image built into OpenShift should and would never be used to store data for your customer facing application code, because it is a containerized component designed to be run as part of OpenShift Container Platform.With the latest releases of OpenShift, there is a trend towards more and more containerized components. The containerized component pattern is becoming more and more common and other software vendors are seeing an advantage to deploying as containerized components.Deployer ImagesA deployer image is a specialized kind of container which, when run, deploys or manages other containers. This pattern enables sophisticated deployment techniques such as mandating the start order of containers, or first run logic such as populating schema or data.As an example, the “image/container type” pattern is used to deploy the logging and metrics in OpenShift. Deploying these components with a deployer container allows the OpenShift engineering team to control start order of the different components and make sure they are all up and running together.Intermediate ImagesAn Intermediate image is any container image that relies on a base image. Typically, core builds, middleware and language runtimes are built as layers on “top of” a base image. These images are then referenced in the FROM directive of another image. These images are not used on their own, they are typically used as a building block to build a standalone image.It is common to have different teams of specialists own different layers of an image. Systems administrators may own the core build layer, while “developer experience” may own the middleware layer. Intermediate Images are built to be consumed by other teams building images, but can sometimes be ran standalone too, especially for testing.Intermodal Container ImagesIntermodal container images are images that have hybrid architectures. For example, many Red Hat Software Collections images can be used in two ways.First, they can be used as simple Application Containers running a fully contained Ruby on Rails and Apache server.Second, they can be used as Builder Images inside of OpenShift Container Platform. In this case, the output child images which contain Ruby on Rails, Apache, and the application code which the source to image process was pointed towards during the build phase.The intermodal pattern is becoming more and more common to solve two business problems with one container image.System ContainersWhen system software is distributed as a container, it often needs to run super privileged. To make this deployment easier, and to allow these containers to start before the container runtime or orchestration, Red Hat developed a special container pattern called System Containers. System Containers start early in the boot process and rely on the atomic command and systemd to be started independent of any container runtime or orchestration. Red Hat provides System Containers for many pieces of software including rsyslog, cockpit, etcd, and flanneld. In the future, Red Hat will expand the list.This design pattern will make it easier for administrators to add these services to Red Hat Enterprise Linux and Atomic Host in a modular way.ConclusionContainers are quite easy to consume, but when building a production container environment, it shifts the complexity behind the scenes. To be able to discuss architectures, and how you will build your environment, it’s important to have shared nomenclature. There are a lot of pitfalls as you dig deeper into building and architecting your environment. We leave you with a couple of critical ones to remember.People often use the words container image and repository interchangeably and the docker sub-commands don’t make a distinction between an image and a repository. The commands are quite easy to use, but once architecture discussions start, it’s important to understand that a repository is really the central data structure.It’s also quite easy to misunderstand the difference between a namespace, repository, image layer, and tag. Each of these has an architectural purpose. While different vendors, and users are using them for different purposes, they are tools in our toolbox.The goal of this article is to leave you with the ability to command this nomenclature so that more sophisticated architectures can be created. For example, imagine that you have just been charged with building an infrastructure that limits, based on role, which namespaces, repositories, and even which image layers and tags can be pushed and pulled based on business rules. Finally, remember that how a container image is built will have profound effect on how it is to be run (orchestrated, privileged, etc).",A Practical Introduction to Container Terminology - RHD Blog
9678,3714064,2018-02-22 15:54:25,"Get Assassin's Creed: Origins on Xbox One for the lowest price everBayek, we've never seen it for lessSharesUbisoft's Assassin's Creed: Origins is the grand return to form we'd been hoping for and the first essential title in the series since Black Flag. Critical acclaim and strong sales are the worst enemies of discounts though, so cheap offers have been slim.We have some excellent news though if you have an Xbox One or Xbox One X as CDKeys has just come up with the lowest price yet in the UK, US and Australia with this digital edition. Better yet (sort of), you also get a free download code for Assassin's Creed: Unity on Xbox One too, which is a handy 'gift' for any family member or friend you're not especially keen on right now.Assassin's Creed: Origins on Xbox One XIf you own Microsoft's new and improved console, or are considering picking up an Xbox One X bundle in the near future, Assassin's Creed: Origins is a must-buy. Don't get us wrong, it looks gorgeous on a regular Xbox One and 1080p screen, but ancient Egypt's beautiful scenes are at another level when viewed with the full power of 4K High Dynamic Range visuals too.",Get Assassin's Creed: Origins on Xbox One for the lowest price ever
9679,3714139,2018-02-23 09:31:39,"TNW SitesThis app teaches your kids how to save money (because you suck at it)This interview is part of our series ofGrowth Stories. We interviewed the founders and CEOs of 20 of the fastest growing startups in Europe. We asked them about their companies, their companies’ culture, and their lives, trying to understand how these three factors played a role in the achievement of such impressive growth.The two most important things in the life of a parent are probably kids and family finance. There’s a startup that combined both those things and created quite a fortunate product.The startup is called GoHenry, and it’s a British company that provides pocket money solutions for children.With GoHenry, parents can give their little money-suckers VISA payment cards as soon as they turn six. But this doesn’t mean that the family will go bankrupt before the kids are seven. Parents retain total control over the card. They can even decide where the children are allowed to use it.GoHenry is not just a virtual piggy bank to stash Tooth Fairy money. The app boasts a bunch of other nifty features. For example, kids can earn extra pennies doing chores. This way, they supposedly learn the relationship between what they do and what they can buy. Capitalism in a baby bottle.GoHenry’s interfaceGoHenry is a “fun-fintech” startup, meaning that it’s at the crossroads of fun learning and fintech. BUX — the investment app that we featured earlier in this series— is an another example of this burgeoning trend.The raised capital fueled a remarkable growth in revenue — 1316 percent over the past three years. To know more about this company, I reached out to its CEO, Alex Zivoder.Alex ZivoderHow did it all start?We started seven years ago. The three founders were parents who saw a gap in the market: There were no pocket money apps for kids. That’s what got them started. And — believe it or not — the first transaction was made in November 2012 by a kid named Henry.Why should parents pay £ 2,99 per month to GoHenry?Because we offer a unique solution. With GoHenry, parents can at the same time handle their kids pocket money and teach them a couple of things about finance management. Traditional banks don’t offer anything like this. In addition, they often lack the credibility and the tone of voice. It may sound like a cliche but GoHenry is really a product built for parents by parents. Are you a parent?Not to my knowledge.Well, when you become one you’ll see that at age seven your kids start to develop some kind of consciousness about money. From then on, you can teach them how to manage it. And the earlier you start, the better it is.How can GoHenry teach about money?In many ways. For example, parents can set up tasks for their children and reward them with small sums of money. This way, kids learn that there is a relationship between what they do and what they can buy. But we’re also developing in-house content. That’s also really relevant for us.The unboxing of GoHenry cards is a Youtube sub-genre in its own regard.I guess that word-of-mouth marketing plays a significant role for GoHenry. I can picture endless discussions in nightmarish WhatsApp parent groups…Of course. Facebook, WhatsApp, and all other social media are extremely useful tools for us. Parents like to chat about their kids and to recommend new products.We’re referring to pounds and pennies. Are you present outside of the United Kingdom?Not yet, but we’re thinking to expand to the United States and afterwards to Europe.Speaking of the EU, Magnus Hultman (CEO Strossle) asked: how can the European Union be so stupid 70 years after we founded it?After Brexit, we don’t have any say in that. Anyway, there are 50 million children in Europe. For us, it’s a giant market. But it’s a really difficult one. Because of different policies and regulations but also different languages. That’s why we’ll expand in the US first.Niccolo Maisto (CEO FaceIT) wanted to knowhow you retain the right people in your company. I think he meant developers.It’s difficult for us as well. You see a lot of movement nowadays. Developers contract more and more. Some kind of shift is happening in the labor market. We’ll adapt to the changes.And what is a question you’d like to ask other CEOs and founders in this series of Growth Stories?I’d ask the famous Peter Thiel’s interview question: “Tell me something that’s true but nobody agrees on.” It’s probably the most important thing you could ask a CEO.To conclude, a question that seems apt for a family-oriented app: what about job-life balance?Oh, that’s really, really difficult to maintain. The toughest challenge. One of my daughters is still really young and I sometimes wonder: Am I spending enough time with her? It breaks my heart.",This app teaches your kids how to save money (because you suck at it)
9680,3716464,2018-02-23 11:43:45,"TNW SitesTired of spammy ads and blatant banners? Blockchain might fix thatThe online advertising model is far from perfect, in fact, it is broken. From a consumer’s point of view, they get served way too many ads on overcrowded banners or in their spam box. From a business’s point of view, the ads are too pricey.Businesses currently pay for ads on an “impressions, views and click” basis. This means that large online advertising companies like Google, Facebook and Amazon are not motivated to optimise for a conversion, which occurs when a customer buys the product. They are instead driven to maximize their own profits by increasing the price of desirable ad placements and click-through rates. However, what would justify a return on investment for the business would be paying for it on a ‘conversions’ basis.Furthermore, there is no guarantee that the traffic to the ads are genuine because nearly 50 percent of all advertising traffic are generated by bots. The result is that ads have low conversion rates, hardly justifying a return on investment. This makes up an ad network model that defeats the point of advertising because the businesses spend significant portions of their marketing budgets on ad impressions, page views, and click-throughs which only loosely translates to business sales or customer value.Getting rid of the middlemenBitClave argues that an advertising model comprising large online advertising companies as “middlemen” is not only unnecessary but potentially detrimental to business value and reputation. It would be more beneficial for customers and businesses to directly connect in mutually beneficial market activity throughout the entire promotion-to-purchase value chain.For example, a business could only choose to advertise discounts to new customers who are interested in buying their products. The business benefits by not wasting advertising dollars on uninterested customers while the interested customers benefit by getting a cheaper product.To enable this to happen, the next logical evolution in advertising is to use blockchain technology. Blockchain is a powerful emerging technology that enables mutually distrusting parties like businesses and customers to engage in mutually beneficial co-enterprise without the requirement of a central authority, or a middleman. BitClave believes that a blockchain-based system is well suited for enabling an ad model that redirects promotions to reach customers who are genuinely interested in the products being sold.BitClave offers this service through their BitClave Active Search Ecosystem (BASE). BASE offers a decentralized search solution which enables big and small companies to participate in an open ecosystem. Decentralized search is achieved by supporting distributed, customer-driven data like customer profiles, search preferences and interests collected in an anonymized activity ledger.Activity anonymization is done in a way to that allows only authorized parties to attribute multiple activities to the same customer. For all other parties, the data is not attributable to specific individuals (or linkable beyond certain lengths of times). This gives the customer control over what and who they share their data with.BASE also cuts out traditional hurdles found in digital advertising like privacy infringement, untrusted sources of data and expensive third-party ad networks.There are several search use cases that would particularly benefit from BASE.# Lawyers and legal advisorsEach legal case can widely differ from another so sharing personal data can be critical in finding the right professional for the job. As each individual case is full of nuanced information, it’s challenging for prospective customers to communicate all the important details. It’s also difficult for the lawyer to have enough information to make informed decisions, especially in the early stages. In these situations, personal data can help legal professionals determine if they could be a good match for the potential client.Since BASE supports the ability for a user to post personal information and transaction history in a protected way, the user can later choose to share selected information with a potential legal professional. If particular user data would be useful to the legal professional, the user can choose to reveal it to them, without revealing it publicly. This allows legal professionals to target more relevant clients and personalize offers to them.# Educational degree programsEducational institutions operate in a crowded marketplace where they seem to promote similar features and benefits. Therefore, it can be difficult for a potential student to discover which school is the best fit. In the current advertising model, the largest universities with significant advertising budgets can outbid smaller institutions for advertising space. As a result, these smaller schools lose opportunities to connect with students who might be a perfect match for their community.Let’s take the example of a student who is about to graduate from high school and is interested in a Biology program at a university near their home town. The traditional ad model will show them universities that spend the most money on advertising. On the other hand, BASE well use information already stored privately in their BASE profile to guide potential university admissions coordinators to respond to their search with details of the relevant Biology program. Unlike third-party networks, BASE creates more relevant connections between the student and degree program based on the ability to selectively share the most relevant information for the search.# Job searchSimilar to the use case of matching students seeking to further their education, BASE can be used as a job search platform. Job seekers can use the BASE decentralized search platform to search for employers, while employers can find potential employees with particular skill sets.BASE can also be used as a sort of “reverse job search”. Job seekers can anonymously post their job interests and core skill sets, and subsequently allow employers to target particular job seekers with a custom-designed position tailored to fit both the needs of the company and the unique skills of the employee. This is particularly useful in highly technical jobs as traditional job search boards in this sector have generic job positions such as “Software engineer” which can lead to high turnover rates within the company.Introducing the consumer activity token (CAT)On top of this distributed activity data collection system, BitClave is introducing a token, called the BitClave Consumer Activity Token (CAT), to be used internally within the BASE ecosystem among the participating parties. CAT tokens will be used to facilitate rewards within the system for a variety of services available in BASE.For example, a business may be offering a product that two customers, X and Y are potentially interested in. If the business asks for additional information, and only X responds, the business may choose to reward X with a CAT token as an offer as they find that X is more likely to convert into a sale.In summary, BitClave tries to fix the broken ad model with a blockchain powered platform that enables businesses and customers to interact without the need for intermediaries. This makes online advertising more relevant, effective and cost efficient.This post is brought to you by The Cointelegraphand shouldn't be considered investment advice by TNW. Yes, TNW sells ads. But we sell ads that don’t suck.",Tired of spammy ads and blatant banners? Blockchain might fix that
9681,3719239,2018-02-20 13:00:00,"The Poison We PickShareIt is a beautiful, hardy flower,Papaver somniferum, a poppy that grows up to four feet in height and arrives in a multitude of colors. It thrives in temperate climates, needs no fertilizer, attracts few pests, and is as tough as many weeds. The blooms last only a few days and then the petals fall, revealing a matte, greenish-gray pod fringed with flutes. The seeds are nutritious and have no psychotropic effects. No one knows when the first curious human learned to crush this bulblike pod and mix it with water, creating a substance that has an oddly calming and euphoric effect on the human brain. Nor do we know who first found out that if you cut the pod with a small knife, capture its milky sap, and leave that to harden in the air, you’ll get a smokable nugget that provides an even more intense experience. We do know, from Neolithic ruins in Europe, that the cultivation of this plant goes back as far as 6,000 years, probably farther. Homer called it a “wondrous substance.” Those who consumed it, he marveled, “did not shed a tear all day long, even if their mother or father had died, even if a brother or beloved son was killed before their own eyes.” For millennia, it has salved pain, suspended grief, and seduced humans with its intimations of the divine. It was a medicine before there was such a thing as medicine. Every attempt to banish it, destroy it, or prohibit it has failed.The poppy’s power, in fact, is greater than ever. The molecules derived from it have effectively conquered contemporary America. Opium, heroin, morphine, and a universe of synthetic opioids, including the superpowerful painkiller fentanyl, are its proliferating offspring. More than 2 million Americans are now hooked on some kind of opioid, and drug overdoses — from heroin and fentanyl in particular — claimed more American lives last year than were lost in the entire Vietnam War. Overdose deaths are higher than in the peak year of AIDS and far higher than fatalities from car crashes. The poppy, through its many offshoots, has now been responsible for a decline in life spans in America for two years in a row, a decline that isn’t happening in any other developed nation. According to the best estimates, opioids will kill another 52,000 Americans this year alone — and up to half a million in the next decade.We look at this number and have become almost numb to it. But of all the many social indicators flashing red in contemporary America, this is surely the brightest. Most of the ways we come to terms with this wave of mass death — by casting the pharmaceutical companies as the villains, or doctors as enablers, or blaming the Obama or Trump administrations or our policies of drug prohibition or our own collapse in morality and self-control or the economic stress the country is enduring — miss a deeper American story. It is a story of pain and the search for an end to it. It is a story of how the most ancient painkiller known to humanity has emerged to numb the agonies of the world’s most highly evolved liberal democracy. Just as LSD helps explain the 1960s, cocaine the 1980s, and crack the 1990s, so opium defines this new era. I say era, because this trend will, in all probability, last a very long time. The scale and darkness of this phenomenon is a sign of a civilization in a more acute crisis than we knew, a nation overwhelmed by a warp-speed, postindustrial world, a culture yearning to give up, indifferent to life and death, enraptured by withdrawal and nothingness. America, having pioneered the modern way of life, is now in the midst of trying to escape it.How Marketing — and Medicine — Spurred the Opioid CrisisHow does an opioid make you feel? We tend to avoid this subject in discussing recreational drugs, because no one wants to encourage experimentation, let alone addiction. And it’s easy to believe that weak people take drugs for inexplicable, reckless, or simply immoral reasons. What few are prepared to acknowledge in public is that drugs alter consciousness in specific and distinct ways that seem to make people at least temporarily happy, even if the consequences can be dire. Fewer still are willing to concede that there is a significant difference between these various forms of drug-induced “happiness” — that the draw of crack, say, is vastly different than that of heroin. But unless you understand what users get out of an illicit substance, it’s impossible to understand its appeal, or why an epidemic takes off, or what purpose it is serving in so many people’s lives. And it is significant, it seems to me, that the drugs now conquering America are downers: They are not the means to engage in life more vividly but to seek a respite from its ordeals.Related StoriesThe alkaloids that opioids contain have a large effect on the human brain because they tap into our natural “mu-opioid” receptors. The oxytocin we experience from love or friendship or orgasm is chemically replicated by the molecules derived from the poppy plant. It’s a shortcut — and an instant intensification — of the happiness we might ordinarily experience in a good and fruitful communal life. It ends not just physical pain but psychological, emotional, even existential pain. And it can easily become a lifelong entanglement for anyone it seduces, a love affair in which the passion is more powerful than even the fear of extinction.Perhaps the best descriptions of the poppy’s appeal come to us from the gifted writers who have embraced and struggled with it. Many of the Romantic luminaries of the early-19th century — including the poets Coleridge, Byron, Shelley, Keats, and Baudelaire, and the novelist Walter Scott — were as infused with opium as the late Beatles were with LSD. And the earliest and in many ways most poignant account of what opium and its derivatives feel like is provided by the classic memoir Confessions of an English Opium-Eater, published in 1821 by the writer Thomas De Quincey.De Quincey suffered trauma in childhood, losing his sister when he was 6 and his father a year later. Throughout his life, he experienced bouts of acute stomach pain, as well as obvious depression, and at the age of 19 he endured 20 consecutive days of what he called “excruciating rheumatic pains of the head and face.” As his pain drove him mad, he finally went into an apothecary and bought some opium (which was legal at the time, as it was across the West until the war on drugs began a century ago).An hour after he took it, his physical pain had vanished. But he was no longer even occupied by such mundane concerns. Instead, he was overwhelmed with what he called the “abyss of divine enjoyment” that overcame him: “What an upheaving from its lowest depths, of the inner spirit! … here was the secret of happiness, about which philosophers had disputed for many ages.” The sensation from opium was steadier than alcohol, he reported, and calmer. “I stood at a distance, and aloof from the uproar of life,” he wrote. “Here were the hopes which blossom in the paths of life, reconciled with the peace which is in the grave.” A century later, the French writer Jean Cocteau described the experience in similar ways: “Opium remains unique and the euphoria it induces superior to health. I owe it my perfect hours.”The metaphors used are often of lightness, of floating: “Rising even as it falls, a feather,” as William Brewer, America’s poet laureate of the opioid crisis, describes it. “And then, within a fog that knows what I’m going to do, before I do — weightlessness.” Unlike cannabis, opium does not make you want to share your experience with others, or make you giggly or hungry or paranoid. It seduces you into solitude and serenity and provokes a profound indifference to food. Unlike cocaine or crack or meth, it doesn’t rev you up or boost your sex drive. It makes you drowsy — somniferum means “sleep-inducing” — and lays waste to the libido. Once the high hits, your head begins to nod and your eyelids close.When we see the addicted stumbling around like drunk ghosts, or collapsed on sidewalks or in restrooms, their faces pale, their skin riddled with infection, their eyes dead to the world, we often see only misery. What we do not see is what they see: In those moments, they feel beyond gravity, entranced away from pain and sadness. In the addict’s eyes, it is those who are sober who are asleep. That is why the police and EMS workers who rescue those slipping toward death by administering blasts of naloxone — a powerful antidote, without which death rates would be even higher — are almost never thanked. They are hated. They ruined the high. And some part of being free from all pain makes you indifferent to death itself. Death is, after all, the greatest of existential pains. “Everything one achieves in life, even love, occurs in an express train racing toward death,” Cocteau observed. “To smoke opium is to get out of the train while it is still moving. It is to concern oneself with something other than life or death.”This terrifyingly dark side of the poppy reveals itself the moment one tries to break free. The withdrawal from opioids is unlike any other. The waking nightmares, hideous stomach cramps, fevers, and psychic agony last for weeks, until the body chemically cleanses itself. “A silence,” Cocteau wrote, “equivalent to the crying of thousands of children whose mothers do not return to give them the breast.” Among the symptoms: an involuntary and constant agitation of the legs (whence the term “kicking the habit”). The addict becomes ashamed as his life disintegrates. He wants to quit, but, as De Quincey put it, he lies instead “under the weight of incubus and nightmare … he would lay down his life if he might get up and walk; but he is powerless as an infant, and cannot even attempt to rise.”The poppy’s paradox is a profoundly human one: If you want to bring Heaven to Earth, you must also bring Hell. In the words of Lenny Bruce, “I’ll die young, but it’s like kissing God.”Photo-Illustration: Joe DarrowNo other developed country is as devoted to the poppy as America. We consume 99 percent of the world’s hydrocodone and 81 percent of its oxycodone. We use an estimated 30 times more opioids than is medically necessary for a population our size. And this love affair has been with us from the start. The drug was ubiquitous among both the British and American forces in the War of Independence as an indispensable medicine for the pain of battlefield injuries. Thomas Jefferson planted poppies at Monticello, and they became part of the place’s legend (until the DEA raided his garden in 1987 and tore them out of the ground). Benjamin Franklin was reputed to be an addict in later life, as many were at the time. William Wilberforce, the evangelical who abolished the British slave trade, was a daily enthusiast. As Martin Booth explains in his classic history of the drug, poppies proliferated in America, and the use of opioids in over-the-counter drugs was commonplace. A wide range of household remedies were based on the poppy’s fruit; among the most popular was an elixir called laudanum — the word literally means “praiseworthy” — which took off in England as early as the 17th century.Mixed with wine or licorice, or anything else to disguise the bitter taste, opiates were for much of the 19th century the primary treatment for diarrhea or any physical pain. Mothers gave them to squalling infants as a “soothing syrup.” A huge boom was kick-started by the Civil War, when many states cultivated poppies in order to treat not only the excruciating pain of horrific injuries but endemic dysentery. Booth notes that 10 million opium pills and 2 million ounces of opiates in powder or tinctures were distributed by Union forces. Subsequently, vast numbers of veterans became addicted — the condition became known as “Soldier’s Disease” — and their high became more intense with the developments of morphine and the hypodermic needle. They were joined by millions of wives, sisters, and mothers who, consumed by postwar grief, sought refuge in the obliviating joy that opiates offered.Based on contemporary accounts, it appears that the epidemic of the late 1860s and 1870s was probably more widespread, if far less intense, than today’s — a response to the way in which the war tore up settled ways of life, as industrialization transformed the landscape, and as huge social change generated acute emotional distress. This aspect of the epidemic — as a response to mass social and cultural dislocation — was also clear among the working classes in the earlier part of the 19th century in Britain. As small armies of human beings were lured from their accustomed rural environments, with traditions and seasons and community, and thrown into vast new industrialized cities, the psychic stress gave opium an allure not even alcohol could match. Some historians estimate that as much as 10 percent of a working family’s income in industrializing Britain was spent on opium. By 1870, opium was more available in the United States than tobacco was in 1970. It was as if the shift toward modernity and a wholly different kind of life for humanity necessitated for most working people some kind of relief — some way of getting out of the train while it was still moving.It is tempting to wonder if, in the future, today’s crisis will be seen as generated from the same kind of trauma, this time in reverse.If industrialization caused an opium epidemic, deindustrialization is no small part of what’s fueling our opioid surge. It’s telling that the drug has not taken off as intensely among all Americans — especially not among the engaged, multiethnic, urban-dwelling, financially successful inhabitants of the coasts. The poppy has instead found a home in those places left behind — towns and small cities that owed their success to a particular industry, whose civic life was built around a factory or a mine. Unlike in Europe, where cities and towns existed long before industrialization, much of America’s heartland has no remaining preindustrial history, given the destruction of Native American societies. The gutting of that industrial backbone — especially as globalization intensified in a country where market forces are least restrained — has been not just an economic fact but a cultural, even spiritual devastation. The pain was exacerbated by the Great Recession and has barely receded in the years since. And to meet that pain, America’s uniquely market-driven health-care system was more than ready.The great dream of the medical profession, which has been fascinated by opioids over the centuries, was to create an experience that captured the drug’s miraculous pain relief but somehow managed to eliminate its intoxicating hook. The attempt to refine opium into a pain reliever without addictive properties produced morphine and later heroin — each generated by perfectly legal pharmaceutical and medical specialists for the most enlightened of reasons. (The word heroin was coined from the German word Heroisch, meaning “heroic,” by the drug company Bayer.) In the mid-1990s, OxyContin emerged as the latest innovation: A slow timed release would prevent sudden highs or lows, which, researchers hoped, would remove craving and thereby addiction. Relying on a single study based on a mere 38 subjects, scientists concluded that the vast majority of hospital inpatients who underwent pain treatment with strong opioids did not go on to develop an addiction, spurring the drug to be administered more widely.This reassuring research coincided with a social and cultural revolution in medicine: In the wake of the AIDS epidemic, patients were becoming much more assertive in managing their own treatment — and those suffering from debilitating pain began to demand the relief that the new opioids promised. The industry moved quickly to cash in on the opportunity: aggressively marketing the new drugs to doctors via sales reps, coupons, and countless luxurious conferences, while waging innovative video campaigns designed to be played in doctors’ waiting rooms. As Sam Quinones explains in his indispensable account of the epidemic, Dreamland, all this happened at the same time that doctors were being pressured to become much more efficient under the new regime of “managed care.” It was a fateful combination: Patients began to come into doctors’ offices demanding pain relief, and doctors needed to process patients faster. A “pain” diagnosis was often the most difficult and time-consuming to resolve, so it became far easier just to write a quick prescription to abolish the discomfort rather than attempt to isolate its cause. The more expensive and laborious methods for treating pain — physical and psychological therapy — were abandoned almost overnight in favor of the magic pills.A huge new supply and a burgeoning demand thereby created a massive new population of opioid users. Getting your opioid fix no longer meant a visit to a terrifying shooting alley in a ravaged city; now it just required a legitimate prescription and a bottle of pills that looked as bland as a statin or an SSRI. But as time went on, doctors and scientists began to realize that they were indeed creating addicts. Much of the initial, hopeful research had been taken from patients who had undergone opioid treatment as inpatients, under strict supervision. No one had examined the addictive potential of opioids for outpatients, handed bottles and bottles of pills, in doses that could be easily abused. Doctors and scientists also missed something only recently revealed about OxyContin itself: Its effects actually declined after a few hours, not 12 — thus subjecting most patients to daily highs and lows and the increased craving this created. Patients whose pain hadn’t gone away entirely were kept on opioids for longer periods of time and at higher dosages. And OxyContin had not removed the agonies of withdrawal: Someone on painkillers for three months would often find, as her prescription ran out, that she started vomiting or was convulsed with fever. The quickest and simplest solution was a return to the doctor.Add to this the federal government’s move in the mid-1980s to replace welfare payments for the poor with disability benefits — which covered opioids for pain — and unscrupulous doctors, often in poorer areas, found a way to make a literal killing from shady pill mills. So did many patients. A Medicaid co-pay of $3 for a bottle of pills, as Quinones discovered, could yield $10,000 on the streets — an economic arbitrage that enticed countless middle-class Americans to become drug dealers. One study has found that 75 percent of those addicted to opioids in the United States began with prescription painkillers given to them by a friend, family member, or dealer. As a result, the social and cultural profile of opioid users shifted as well: The old stereotype of a heroin junkie — a dropout or a hippie or a Vietnam vet — disappeared in the younger generation, especially in high schools. Football players were given opioids to mask injuries and keep them on the field; they shared them with cheerleaders and other popular peers; and their elevated social status rebranded the addiction. Now opiates came wrapped in the bodies and minds of some of the most promising, physically fit, and capable young men and women of their generation. Courtesy of their doctors and coaches.It’s hard to convey the sheer magnitude of what happened. Between 2007 and 2012, for example, 780 million hydrocodone and oxycodone pills were delivered to West Virginia, a state with a mere 1.8 million residents. In one town, population 2,900, more than 20 million opioid prescriptions were processed in the past decade. Nationwide, between 1999 and 2011, oxycodone prescriptions increased sixfold. National per capita consumption of oxycodone went from around 10 milligrams in 1995 to almost 250 milligrams by 2012.The quantum leap in opioid use arrived by stealth. Most previous drug epidemics were accompanied by waves of crime and violence, which prompted others, outside the drug circles, to take notice and action. But the opioid scourge was accompanied, during its first decade, by a record drop in both. Drug users were not out on the streets causing mayhem or havoc. They were inside, mostly alone, and deadly quiet. There were no crack houses to raid or gangs to monitor. Overdose deaths began to climb, but they were often obscured by a variety of dry terms used in coroners’ reports to hide what was really happening. When the cause of death was inescapable — young corpses discovered in bedrooms or fast-food restrooms — it was also, frequently, too shameful to share. Parents of dead teenagers were unlikely to advertise their agony.In time, of course, doctors realized the scale of their error. Between 2010 and 2015, opioid prescriptions declined by 18 percent. But if it was a huge, well-intended mistake to create this army of addicts, it was an even bigger one to cut them off from their supply. That is when the addicted were forced to turn to black-market pills and street heroin. Here again, the illegal supply channel broke with previous patterns. It was no longer controlled by the established cartels in the big cities that had historically been the main source of narcotics. This time, the heroin — particularly cheap, black-tar heroin from Mexico — came from small drug-dealing operations that avoided major urban areas, instead following the trail of methadone clinics and pill mills into the American heartland.A recent shipment of fentanyl seized in New Jersey fit into the trunk of single car, yet contained more potential death than a dirty bomb or a small nuke.Their innovation, Quinones discovered, was to pay the dealers a flat salary, rather than a cut from the heroin itself. This removed the incentives to weaken the product, by cutting it with baking soda or other additives, and so made the new drug much more predictable in its power and reliable in its dosage. And rather than setting up a central location to sell the drugs — like a conventional shooting gallery or crack house — the new heroin marketers delivered it by car. Outside methadone clinics or pill mills, they handed out cards bearing only a telephone number. Call them and they would arrange to meet you near your house, in a suburban parking lot. They were routinely polite and punctual.Buying heroin became as easy in the suburbs and rural areas as buying weed in the cities. No violence, low risk, familiar surroundings: an entire system specifically designed to provide a clean-cut, friendly, middle-class high. America was returning to the norm of the 19th century, when opiates were a routine medicine, but it was consuming compounds far more potent, addictive, and deadly than any 19th-century tincture enthusiast could have imagined. The country resembled someone who had once been accustomed to opium, who had spent a long time in recovery, whose tolerance for the drug had collapsed, and who was then offered a hit of the most powerful new variety.The iron law of prohibition, as first stipulated by activist Richard Cowan in 1986, is that the more intense the crackdown, “the more potent the drugs will become.” In other words, the harder the enforcement, the harder the drugs. The legal risks associated with manufacturing and transporting a drug increase exponentially under prohibition, which pushes the cost of supplying the drug higher, which incentivizes traffickers to minimize the size of the product, which leads to innovations in higher potency. That’s why, during the prohibition of alcohol, much of the production and trafficking was in hard liquor, not beer or wine; why amphetamines evolved into crystal meth; why today’s cannabis is much more potent than in the late-20th century. Heroin, rather than old-fashioned opium, became the opioid of the streets.Then came fentanyl, a massively concentrated opioid that delivers up to 50 times the strength of heroin. Developed in 1959, it is now one of the most widely used opioids in global medicine, its miraculous pain relief delivered through transdermal patches, or lozenges, that have revolutionized surgery and recovery and helped save countless lives. But in its raw form, it is one of the most dangerous drugs ever created by human beings. A recent shipment of fentanyl seized in New Jersey fit into the trunk of a single car yet contained enough poison to wipe out the entire population of New Jersey and New York City combined. That’s more potential death than a dirty bomb or a small nuke. That’s also what makes it a dream for traffickers. A kilo of heroin can yield $500,000; a kilo of fentanyl is worth as much as $1.2 million.Related StoriesThe problem with fentanyl, as it pertains to traffickers, is that it is close to impossible to dose correctly. To be injected at all, fentanyl’s microscopic form requires it to be cut with various other substances, and that cutting is playing with fire. Just the equivalent of a few grains of salt can send you into sudden paroxysms of heaven; a few more grains will kill you. It is obviously not in the interests of drug dealers to kill their entire customer base, but keeping most of their clients alive appears beyond their skill. The way heroin kills you is simple: The drug dramatically slows the respiratory system, suffocating users as they drift to sleep. Increase the potency by a factor of 50 and it is no surprise that you can die from ingesting just a half a milligram of the stuff.Fentanyl comes from labs in China; you can find it, if you try, on the dark web. It’s so small in size and so valuable that it’s close to impossible to prevent it coming into the country. Last year, 500 million packages of all kinds entered the United States through the regular mail — making them virtually impossible to monitor with the Postal Service’s current technology. And so, over the past few years, the impact of opioids has gone from mass intoxication to mass death. In the last heroin epidemic, as Vietnam vets brought the addiction back home, the overdose rate was 1.5 per 10,000 Americans. Now, it’s 10.5. Three years ago in New Jersey, 2 percent of all seized heroin contained fentanyl. Today, it’s a third. Since 2013, overdose deaths from fentanyl and other synthetic opioids have increased sixfold, outstripping those from every other drug.If the war on drugs is seen as a century-long game of chess between the law and the drugs, it seems pretty obvious that fentanyl, by massively concentrating the most pleasurable substance ever known to mankind, is checkmate.Watching as this catastrophe unfolded these past few years, I began to notice how closely it resembles the last epidemic that dramatically reduced life-spans in America: AIDS. It took a while for anyone to really notice what was happening there, too. AIDS occurred in a population that was often hidden and therefore distant from the cultural elite (or closeted within it). To everyone else, the deaths were abstract, and relatively tolerable, especially as they were associated with an activity most people disapproved of. By the time the epidemic was exposed and understood, so much damage had been done that tens of thousands of deaths were already inevitable.Today, once more, the cultural and political elites find it possible to ignore the scale of the crisis because it is so often invisible in their — our — own lives. The polarized nature of our society only makes this worse: A plague that is killing the other tribe is easier to look away from. Occasionally, members of the elite discover their own children with the disease, and it suddenly becomes more urgent. A celebrity death — Rock Hudson in 1985, Prince in 2016 — begins to break down some of the denial. Those within the vortex of death get radicalized by the failure of government to tackle the problem. The dying gay men who joined ACT UP in the 1980s share one thing with the opioid-ridden communities who voted for Donald Trump in unexpected numbers: a desperate sense of powerlessness, of living through a plague that others are choosing not to see.At some point, the sheer numbers of the dead become unmissable. With AIDS, the government, along with pharmaceutical companies, eventually developed a plan of action: prevention, education, and research for a viable treatment and cure. Some of this is happening with opioids. The widespread distribution of Narcan sprays — which contain the antidote naloxone — has already saved countless lives. The use of alternative, less-dangerous opioid drugs such as methadone and buprenorphine to wean people off heroin or cushion them through withdrawal has helped. Some harm-reduction centers have established needle-exchange programs. But none of this comes close to stopping the current onslaught. With HIV and AIDS, after all, there was a clear scientific goal: to find drugs that would prevent HIV from replicating. With opioid addiction, there is no such potential cure in the foreseeable future. When we see the toll from opioids exceed that of peak AIDS deaths, it’s important to remember that after that peak came a sudden decline. After the latest fentanyl peak, no such decline looks probable. On the contrary, the deaths continue to mount.Over time, AIDS worked its way through the political system.More than anything else, it destroyed the closet and massively accelerated our culture’s acceptance of the dignity and humanity of homosexuals. Marriage equality and open military service were the fruits of this transformation. But with the opioid crisis, our politics has remained curiously unmoved. The Trump administration, despite overwhelming support from many of the communities most afflicted, hasn’t appointed anyone with sufficient clout and expertise to corral the federal government to respond adequately.The critical Office of National Drug Control Policy has spent a year without a permanent director. Its budget is slated to be slashed by 95 percent, and until a few weeks ago, its deputy chief of staff was a 24-year-old former campaign intern. Kellyanne Conway — Trump’s “opioid czar” — has no expertise in government, let alone in drug control. Although Trump plans to increase spending on treating addiction, the overall emphasis is on an even more intense form of prohibition, plus an advertising campaign. Attorney General Jeff Sessions even recently opined that he believes marijuana is really the key gateway to heroin — a view so detached from reality it beggars belief. It seems clear that in the future, Trump’s record on opioids will be as tainted as Reagan’s was on AIDS. But the human toll could be even higher.One of the few proven ways to reduce overdose deaths is to establish supervised injection sites that eventually wean users off the hard stuff while steering them into counseling, safe housing, and job training.After the first injection site in North America opened in Vancouver, deaths from heroin overdoses plunged by 35 percent. In Switzerland, where such sites operate nationwide, overdose deaths have been cut in half. By treating the addicted as human beings with dignity rather than as losers and criminals who have ostracized themselves, these programs have coaxed many away from the cliff face of extinction toward a more productive life.But for such success to be replicated in the United States, we would have to contemplate actually providing heroin to addicts in some cases, and we’d have to shift much of the current spending on prohibition, criminalization, and incarceration into a huge program of opioid rehabilitation. We would, in short, have to end the war on drugs. We are nowhere near prepared to do that. And in the meantime, the comparison to act up is exceedingly depressing, as the only politics that opioids appear to generate is nihilistic and self-defeating. The drug itself saps initiative and generates social withdrawal. A few small activist groups have sprung up, but it is hardly a national movement of any heft or urgency.And so we wait to see what amount of death will be tolerable in America as the price of retaining prohibition. Is it 100,000 deaths a year? More? At what point does a medical emergency actually provoke a government response that takes mass death seriously? Imagine a terror attack that killed over 40,000 people. Imagine a new virus that threatened to kill 52,000 Americans this year. Wouldn’t any government make it the top priority before any other?In some ways, the spread of fentanyl — now beginning to infiltrate cocaine, fake Adderall, and meth, which is also seeing a spike in use — might best be thought of as a mass poisoning. It has infected often nonfatal drugs and turned them into instant killers. Think back to the poison discovered in a handful of tainted Tylenol pills in 1982. Every bottle of Tylenol in America was immediately recalled; in Chicago, police went into neighborhoods with loudspeakers to warn residents of the danger. That was in response to a scare that killed, in total, seven people. In 2016, 20,000 people died from overdosing on synthetic opioids, a form of poison in the illicit drug market. Some lives, it would appear, are several degrees of magnitude more valuable than others. Some lives are not worth saving at all.One of the more vivid images that Americans have of drug abuse is of a rat in a cage, tapping a cocaine-infused water bottle again and again until the rodent expires. Years later, as recounted in Johann Hari’s epic history of the drug war, Chasing the Scream, a curious scientist replicated the experiment. But this time he added a control group. In one cage sat a rat and a water dispenser serving diluted morphine. In another cage, with another rat and an identical dispenser, he added something else: wheels to run in, colored balls to play with, lots of food to eat, and other rats for the junkie rodent to play or have sex with. Call it rat park. And the rats in rat park consumed just one-fifth of the morphine water of the rat in the cage. One reason for pathological addiction, it turns out, is the environment. If you were trapped in solitary confinement, with only morphine to pass the time, you’d die of your addiction pretty swiftly too. Take away the stimulus of community and all the oxytocin it naturally generates, and an artificial variety of the substance becomes much more compelling.One way of thinking of postindustrial America is to imagine it as a former rat park, slowly converting into a rat cage. Market capitalism and revolutionary technology in the past couple of decades have transformed our economic and cultural reality, most intensely for those without college degrees. The dignity that many working-class men retained by providing for their families through physical labor has been greatly reduced by automation. Stable family life has collapsed, and the number of children without two parents in the home has risen among the white working and middle classes. The internet has ravaged local retail stores, flattening the uniqueness of many communities. Smartphones have eviscerated those moments of oxytocin-friendly actual human interaction. Meaning — once effortlessly provided by a more unified and often religious culture shared, at least nominally, by others — is harder to find, and the proportion of Americans who identify as “nones,” with no religious affiliation, has risen to record levels. Even as we near peak employment and record-high median household income, a sense of permanent economic insecurity and spiritual emptiness has become widespread. Some of that emptiness was once assuaged by a constantly rising standard of living, generation to generation.But that has now evaporated for most Americans.New Hampshire, Ohio, Kentucky, and Pennsylvania have overtaken the big cities in heroin use and abuse, and rural addiction has spread swiftly to the suburbs. Now, in the latest twist, opioids have reemerged in that other, more familiar place without hope: the black inner city, where overdose deaths among African-Americans, mostly from fentanyl, are suddenly soaring. To make matters worse, political and cultural tribalism has deeply weakened the glue of a unifying patriotism to give a broader meaning to people’s lives — large numbers of whites and blacks both feel like strangers in their own land. Mass immigration has, for many whites, intensified the sense of cultural abandonment. Somewhere increasingly feels like nowhere.It’s been several decades since Daniel Bell wrote The Cultural Contradictions of Capitalism, but his insights have proven prescient. Ever-more-powerful market forces actually undermine the foundations of social stability, wreaking havoc on tradition, religion, and robust civil associations, destroying what conservatives value the most. They create a less human world. They make us less happy. They generate pain.This was always a worry about the American experiment in capitalist liberal democracy. The pace of change, the ethos of individualism, the relentless dehumanization that capitalism abets, the constant moving and disruption, combined with a relatively small government and the absence of official religion, risked the construction of an overly atomized society, where everyone has to create his or her own meaning, and everyone feels alone. The American project always left an empty center of collective meaning, but for a long time Americans filled it with their own extraordinary work ethic, an unprecedented web of associations and clubs and communal or ethnic ties far surpassing Europe’s, and such a plethora of religious options that almost no one was left without a purpose or some kind of easily available meaning to their lives. Tocqueville marveled at this American exceptionalism as the key to democratic success, but he worried that it might not endure forever.And it hasn’t. What has happened in the past few decades is an accelerated waning of all these traditional American supports for a meaningful, collective life, and their replacement with various forms of cheap distraction. Addiction — to work, to food, to phones, to TV, to video games, to porn, to news, and to drugs — is all around us. The core habit of bourgeois life — deferred gratification — has lost its grip on the American soul. We seek the instant, easy highs, and it’s hard not to see this as the broader context for the opioid wave. This was not originally a conscious choice for most of those caught up in it: Most were introduced to the poppy’s joys by their own family members and friends, the last link in a chain that included the medical establishment and began with the pharmaceutical companies. It may be best to think of this wave therefore not as a function of miserable people turning to drugs en masse but of people who didn’t realize how miserable they were until they found out what life without misery could be. To return to their previous lives became unthinkable. For so many, it still is.If Marx posited that religion is the opiate of the people, then we have reached a new, more clarifying moment in the history of the West: Opiates are now the religion of the people. A verse by the poet William Brewer sums up this new world:Where once was faith,there are sirens: red lights spinningdoor to door, a record twenty-fourin one day, all the bodiesat the morgue filled with light.It is easy to dismiss or pity those trapped or dead for whom opiates have filled this emptiness. But it’s not quite so easy for the tens of millions of us on antidepressants, or Xanax, or some benzo-drug to keep less acute anxieties at bay. In the same period that opioids have spread like wildfire, so has the use of cannabis — another downer nowhere near as strong as opiates but suddenly popular among many who are the success stories of our times. Is it any wonder that something more powerful is used by the failures? There’s a passage in one of Brewer’s poems that tears at me all the time. It’s about an opioid-addicted father and his son. The father tells us:Times my simple son will shake me to,syringe still hanging like a feather from my arm.What are you always doing, he asks.Flying, I say. Show me how, he begs.And finally, I do. You’d thinkthe sun had gotten lost inside his head,the way he smiled.To see this epidemic as simply a pharmaceutical or chemically addictive problem is to miss something: the despair that currently makes so many want to fly away. Opioids are just one of the ways Americans are trying to cope with an inhuman new world where everything is flat, where communication is virtual, and where those core elements of human happiness — faith, family, community — seem to elude so many. Until we resolve these deeper social, cultural, and psychological problems, until we discover a new meaning or reimagine our old religion or reinvent our way of life, the poppy will flourish.We have seen this story before — in America and elsewhere.The allure of opiates’ joys are filling a hole in the human heart and soul today as they have since the dawn of civilization. But this time, the drugs are not merely laced with danger and addiction. In a way never experienced by humanity before, the pharmaceutically sophisticated and ever more intense bastard children of the sturdy little flower bring mass death in their wake. This time, they are agents of an eternal and enveloping darkness. And there is a long, long path ahead, and many more bodies to count, before we will see any light.*This article appears in the February 19, 2018, issue of New York Magazine. Subscribe Now!",Americans Invented Modern Life. Now We’re Using Opioids to Escape It.
9682,3719242,2018-02-22 20:35:34,"QuickBASIC Lives On with QB64When I got my first computer, a second hand 386 running MS-DOS 6.22, I didn’t have an Internet connection. But I did have QuickBASIC installed and a stack of programming magazines the local library was throwing out, so I had plenty to keep myself busy. At the time, I thought QuickBASIC was more or less indistinguishable from magic. I could write simple code and compile it into an .exe, put it on a floppy, and give it to somebody else to run on their own machine. It seemed too good to be true, how could this technology possibly be improved upon?Of course, that was many years ago, and things are very different now. The programming languages du jour are worlds more capable than the plodding BASIC variants of the 80’s and 90’s. But still, when I found a floppy full of programs I wrote decades ago, I couldn’t help but wonder about getting them running again. With something like DOSBox I reasoned I should be able to install the QuickBASIC IDE and run them like I was back on my trusty 386.Unfortunately, that was not to be. Maybe I’m just not well versed enough in DOSBox, but I couldn’t get the IDE to actually run any of the source code I pulled off the floppy. This was disappointing, but then it occured to me that modern BASIC interpreters are probably being developed in some corner of the Internet, and perhaps I could find a way to run my nearly 30 year old code without having to rely on 30 year old software to do it.The QB64 ProjectAfter searching around a bit, I found the very cool QB64 project (alternate site: QB64.org) This is an open source QuickBASIC development environment that is not only completely compatible with existing programs, but adds in functions and capabilities that were unthinkable back on my 386. Displaying a PNG, loading TTF fonts, or playing an MP3 in the background can be accomplished with just one or two commands.Such things were possible with the original QuickBASIC, but existed more in the realm of tech demos than anything else. Oh the games I could have made back in the day with software like this! I had to be content with bleeps and bloops, and even that required you to figure out the timing for the tones yourself.Even better, QB64 is cross-platform and supports compiling into native binaries for Linux, Windows, and Mac OS. That meant that not only could I run my old code within the IDE, but I could actually compile it into a binary for my Linux desktop. I don’t own a Windows computer anymore, but with WINE I was able to run the Windows version of QB64 and compile an .exe that I could give to my friends who are still living in the dark ages.Conjuring Black MagicThis might be lost on those who never wrote BASIC code on a vintage machine, but the following code creates a 800×600 screen, puts a full screen PNG up, plays an MP3, and writes a message using a TrueType font.Revisiting a ProjectIn my edgy teenage days, I created a graphical version of the “Drugwars” style game. You moved a little stick man around a pixelated environment, buying and selling substances that I had heard about in movies but certainly had never seen in person. It was terrible. But it was part of my youth and I thought it would be fun to see if I could shoehorn in some modern flash using QB64.As it turns out, transparent PNGs and the ability to display proper fonts makes things a lot easier. Being able to play music and ambient sound effects in the background makes even sloppily done games seem a lot better. The following screenshots are of the main menu of my little teenage crime fantasy, before and after the application of QB64. Note that the core source code itself is more or less the same, I’m just interleaving it with the ability to load and display external files.Should You Be Using QuickBasic?No, you definitely should not. I didn’t write this to try and convince anyone to jump on a programming language that peaked before many of our readers were even born. QuickBASIC is an antiquated language, stuck with outdated methods and limitations that are confounding to the modern programmer. But QB64 does do an excellent job of modernizing this classic language, if only to a relatively small degree in the grand scheme of things, for those of us who cut our teeth on it.Being able to take a disk with BASIC code I wrote on a DOS 386 in the early 90’s and turn it into a Linux binary in 2018 is a pretty neat accomplishment, and I salute the QB64 development team for making it possible. I won’t be writing any new code in the language, and I don’t suggest you do either, but it was a lot of fun being able to revisit this period in my life and drag it kicking and screaming into the modern era.104 thoughts on “QuickBASIC Lives On with QB64”“Of course, that was many years ago, and things are very different now. The programming languages du jour are worlds more capable than the plodding BASIC variants of the 80’s and 90’s.”The programming languages of the 80’s and 90’s were much more capable than the plodding BASIC variants of the 80’s and 90’s. Basic has always been a toy language to learn basic programming concepts on.I’m a bit older than the writer, and I learned on an older version of basic than QBasic. I did love having QBasic on old Dos and early windows machines and definitely missed it when it went away. Even back then BASIC was a toy.Does the writer even know that the “B” in BASIC stand for Beginner’s? Clearly not judging from his “Should you be using qucikbasic” conclusion. It never was a serious choice.Many corporate products are written in some variant of Basic. Large behemoths of code, written completely in visual fox pro or VB. Largely because industries which adopted computing early on typically pushed the “coding” on some operations department.I currently work in travel, the majority of code which talks to airlines, does QC, handles the back office, etc is written in VB. Typically this code was written by travel agents who filled a support type role already. This is not unique to travel however, for a long time the medical field was also filled to the brim with silly code written by people with no business writing such important systems.If you want to talk hacks, let’s talk about huge systems written over the last 20 years by non programmers.Though the purpose of Basic may be to educate and reduce the barrier of entry, a pretty lucrative and easy retirement plan is converting VB systems to more robust solutions.I also had Microsoft Basic (not quickbasic) back in the 80’s, the “Pro” version, which was a complete oxymoron. And I did do some large apps at work in Visual Basic 3 back in the windows 3.11 era. But as you say, that was exactly because of companies pushing “coding” on other departments (in the VB3 case, the department responsible for tech support and running the WAN).My reply to the author that QBasic is hardly an example of the state of programming languages in the 80’s and 90’s is still valid.Most of that legacy BASIC code can be traced directly back to the “Learn Computer Programming From the Comfort of Your Own Home” scams which were so prevalent during the early 80’s.I got really, really tired of employment agency staff saying “But he’s got BASIC”, or “He’s got BASIC, isn’t that the same thing?”, or some other variation thereof, while trying palm an obviously unsuitable candidate off on to me.The one which really took the biscuit though, was the guy who had no programming skills whatsoever which didn’t matter because “You’d really like him if you met him”.Oh really? Maybe you could refresh my memory: what C compilers came bundled with MS-DOS? Better yet, which ones could I have installed on my TRS-80?More advanced languages existed in those days, but they might as well have been on the Moon for the access we had to them as home-gamers. If you’ve got dialup (at BEST) and couldn’t afford to buy a boxed copy of Borland C/Pascal/Whatever, what were you supposed to do? You used the tools you had.If anything, I think you are the one who might have the twisted view of BASIC. For most people, it was the only programming language they had access to. We call it a toy now, but back then it was the only game in town. You would by a computer and you would get dumped right into a blinking BASIC prompt. Have fun, there’s your computer.“By comparison, most CP/M machines came with an assembler and a C compiler.”Not that I recall.The assembler, yes, but not a “C compiler”…There was CBASIC, a very popular compiled BASIC used to create business applications, and DBase. Many burgeoning professionals cut their teeth writing so-called vertical applications using CBASIC and DBase applications on CP/M machines.Agreed. I learned QBasic as my first programming language, and my father was horrified and told me I was wasting my time. He insisted that I learn VisualBasic instead. My preteen rebellion was to learn C++ instead.I first learned basic and then learned qbasic. It was well understood that they weren’t professional languages, and they weren’t designed to be. With the release of qb 4.5, one could compile executables which was a pretty big deal since I could give my programs to any IBM compatibles to run without batch starting basic. It also ran way faster assembled. Other languages were out there and I learned C, did well with that but c++ ended up giving me the death kiss. Learning object oriented programming when you are 14 and grew up mastering qbasic, was quite fruitless. I understand better these days but I ended up not pursuing my potential as a programmer. I always think what could have been had I put my head down and stuck with it. These days I mostly program micro controllers because it makes sense for what I need and wiring language is mostly c.Abstraction is a great tool for people “too stupid” to program bare-metal hardware, or deal directly with memory by using “hard” things like pointers, malloc, or just people who do stupid things and need a nice guarded sandbox to play in.“BASIC” Allowed direct access to hardware, the OS, and other dangerous things. Most people are OK with JAVA,since it protects them from their own stupidity.Abstraction – like Wirth’s ‘unspeakably evil goto’ – is a tool just like any other. It’s up to you to learn how to use it properly.BTW: Peek and Poke – that direct hardware access you so eloquently referred to above – were not part of the original language specification. They were added later by one of the early ‘home computer’ vendors because BASIC lacked graphic and sound functions.““BASIC” Allowed direct access to hardware, the OS, and other dangerous things. Most people are OK with JAVA,since it protects them from their own stupidity.”“Out”, “Peek” and “Poke” were microcomputer-specific extensions, they were not part of the BASIC language definition, they were added by Microsoft to better support the machines they ported their software to.Back the day, when I wanted something fast and tiny, I turned to ‘ASIC’ (Short for ‘almost BASIC’), which was a much larger pain in the butt to actually use because it was so pared down, but it was almost as good as assembly in its speed. I recall using it to write libraries for my QB & PDS programs. I also remember embedding said tiny programs in batch files as DEBUG input strings. The batch file would pipe /itself/ as input to DEBUG, which would then create the program that the rest of the batch file would use. VIrus scanners just don’t expect you to do things like that. :grin:If you are a little bit older and BASICA was your thing before QuickBASIC then you want Blassic. http://blassic.net/I have an old EE textbook from the 80s that I bought at a garage sale some time in the 90s. It has BASIC excercises in it. I started working through this book a couple years ago. Yes, I could have bought something newer but I tried and failed as a kid so I wanted to turn that around. Anyway, of course I could have ignored the fact that the book talks about BASIC and just went with Python or some other modern language but.. I felt like keeping with the period of the book. Besides, a language that relies on whitespace to convey meaning can’t be any better than one which relies on GOTO/GOSUB right? Anyway, I looked at QB64 but went with Blassic because QB64 was too modern.LOL I used to use MBASIC in a native 32 bit XP cml/dos box to parse CSV’s of my daily share trading dumps and offer a 8 x 8 matrix of “best fit” guides for next day’s trading over several stocks !It was fun to fit that into only 62K of usable space, it was damn fast though and sometimes had half a dozen windows open. Getting it to grab the odd html/java output from some of the share trading apps on windows was a bit trickier though. Maybe now I can go a bit better. might be busy this weekend, :shrug:Hmm interesting Tom Nardi – didn’t know QBasic had “graduated” a bit, offers a few lines of progress to maybe revitalise commerce with my investor group who might be happy to get dirty hands again from early Qbasic experience,Basic may be for beginners as some jibe but, the more learned & mature who are in the business at the coal face of productive programming efficiency have seemingly offered it some more fruit. The time impost and documentation hurdles in just trying to do simple utilities with the more conventional (modern) packages is surprisingly onerous. I say surprising as most younger programmers might proclaim newer is better but, they take ages to do rather simple things, well stuff I so often consider simple ;-)So anything that can advance the gestaltic like conjunction of straightforward documentation with structure to improve efficiency has my attention and especially so if there is as little variance necessary to replicate functionality across platforms.So I especially like the idea of potential migration to Android as we (my meek little enterprise) already has momentum in that direction on “all sorts of stuff” :-)Nice article Tom Nardi a a good primmer for my style of progress over a few gaps re current utility decisions I have in mind which can also maybe fill other gaps…That’s a good point, and maybe BASIC is worth revisiting again to challenge those assertions! I remember BASIC it being damn easy to make something go, but I was a kid then, and I guess I didn’t try to “go” very far. Nowadays when I see a “beginner language” (i.e. it tries to keep you from shooting yourself in the foot), the wordiness and convoluted methods to do something outside the fence are very off-putting.Would I be bothered by the verbosity of QBasic now? I mean, knowing Perl and C and a bunch of other stuff, maybe I would think BASIC was as tedious as reading COBOL?I think taking ages to do simple things is probably down to the programmer, not the language.I managed to learn enough python to write some simple linux scripts in a couple of days. It might have quicker but I kept tripping over the damn spaces/tabs crap. WTF was van Rossum smoking when he made THAT decision?The TI-BASIC built into the TI-99/4 and TI-99/4A were terribly slow. Not because of BASIC but because the BASIC code was first translated to GPL (Graphics Programming Language) *then* to 9900 machine code. TI Extended BASIC was coded in a much more efficient way, including things like tokenizing the common commands internally to reduce program size.I wish I’d kept copies of the TI programs I’d written, but I traded my stash of TI hardware and software for four new 1 megabyte 30 pin SIMMs way back when. I may still have a printout or my hand written code for a role playing game character generator. With that I could re-create it using an emulator. In Extended BASIC it could spit out character sheets as fast as I could poke Enter.I’m one of those weirdos that still appreciates VBA in Excel. I wrote a small program to batch proces exam analisys data and email each student a pdf with a personalized report. Havent got the time to learn how to donit in a proper language and it works on all school computers…Having cut my teeth on Fortran and Basic (in that order, oddly enough), I was always turning out small code projects to crunch lab data using Qbasic/BasicA/Level1 Basic/PET Basic. Why? Because the campus “computer centers” had all the other resources locked down under layers of bureaucracy. You use the tools you can actually get at.Imagine my dismay when I first lit up Visual Basic – possibly all the tools in the universe but incapable of whacking together code in 20 minutes for a single purpose unless you were willing to tolerate a month(s)-long learning curve and many unnecessary layers. Perhaps there was a “simple” mode that I never found.Hello, Matlab!For what it’s worth, MS has issued “Small Basic” that seems to have gone mostly back to the educational roots (though it compiles rather than interprets) and seems like a better place to go if you’ve an urge to work with this.i do not know, but there seems to be a collection of programming “smart asses” that think that whatever programming language that is on vogue right now is the way to go, and feel the urge to go back and reconstruct the world using that and their newfound all encompassing “power”.i say BS. coming from an engineering background, i think you need to use whatever gets you to your goal the easiest way possible and you feel comfortable. also, knowing when to stop is a welcome attribute, specially when you have a time limited budget (budgets are limited by money by definition :))Must say I agree with you on that, use what works for you, I found Euphoria in the early nineties, still use it for my projects, bit like basic but I can go a year without needing it and then get back into it quickly.AS a software dev, I totally agree about your observation of the programming “smart asses”. The amount of crap frameworks and oddball libraries in general that has been introduced over the years by software devs who love anything shiny and new (we call these resume stuffers, because they look great on a resume) is my daily headache. The amount of abstraction for the sake of abstraction… the amount of generalisation for something that will be used for one specific purpose… it is such a mess!I still have all my qb programs done so many years ago on my hard drive! Survived through all the countless hard disk and computer upgrades. Just downloaded qb64 and ran a old sieve benchmark program. Probably one I typed in from a old Byte or Dr Dobbs magazine. This brings back so many memories… THANKSI agree about the line numbers, but forced myself off of line numbering while working on a QB64 program for work. The program read data coming in from a fire panel RS232 port and email it out. Did it in about 200 lines of QB64 code. It’s been running on 10 laptops 24/7 for over 2 years!TRS-80 BASIC was good enough to start my first business back in the 80’s selling a few games and even a paper route management program via mail order. Sometimes I miss those simpler days when everything didn’t require a GUI and web services. Granted, the cassette tapes we shipped the software on were not as reliable as some of the more modern media formats…QBasic was mostly “used” in addition with ASM inline code. It makes QBasic quite versatile in fact. No Operating system in view, but i remember realmode madness, which led me to C later in my childhood. 320*200*256 resolution for INT13H interruption, etc… Also the B8000 knowledge which was mandatory in order to create cool ASCII TEXT shits. It was really quite something back in the old days.These days computer hardware is cheap. Programmers time is expensive. Anything you can do to reduce the amount of work they must do, and make it easier to maintain the code after it is written, is worth balancing against the extra hardware needed to do it. I’ve written code in everything from assembler, through C/C++, assorted varieties of BASIC, PASCAL/Delphi, Java and C#. I can’t say Java is my favourite language, but it’s productive and you can find plenty of developers who can read and maintain code written in it. The hardware needed to run it is neither here nor there (most ARM SBCs have enough oomph to run it).All that talk about “real programmers” with (almost) bare metal, pointers, manual memory allocation and so on. It’s all fun and games until you have a deadline to meet. Not delivering working code on time is a great way to get fired.You DO know that most Microsoft code is written in C/C++? Many of the errors that they are famous for are directly attributable to this fact. “Real Programmers” are aware of the strengths and limitations of the language they have chosen and work accordingly.“I had to be content with bleeps and bloops, and even that required you to figure out the timing for the tones yourself.”QBasic had functions to play music in the background; I forget the commands, but you basically made an array with “notes” written down as “ABDEFG…” in some weird syntax and then called the function, which would start playing the music in the background.It also had commands to call custom machine code functions stored in memory, so you could “in-line” assembly code and achieve things like proper frame buffers and get 320×200 framebuffered graphics with sprites. I made an prototype of an isometric game engine that way and tested it to run at a good clip, something over 60 fps on a 486DX2Very cool! Back in the early and mid 90s my primary use for my amazingly fast 386DX33 was to program in QuickBASIC. I still have most of my old .BAS files, but I had no way to read them. I think I’ll be installing QB64 today and taking a look at a past segment of my life!I don’t know why People just don’t load a USB stick with 6.22 DOS or setup dual boot. All you need is a hard drive formatted in FAT and you can get the files from the Windows environment and run native mode. Funny listening to everyone talk about the old days. I did Basic+2 on a PDP 11/70 in the 70’s (also Fortran/Watfor and Cobol), along with multiple versions under CPM, MPM, DOS, etc – CBASIC, MBASIC, QBASIC, ZBASIC. Back in the days very few People had the money to buy things like compilers. I actually have a copy of BASCOM for DOS, a basic compiler, and about 400 diskettes full of software that runs under .386 DOS (now converted into sub-directories under windows), including a ton of assembly I had written at the time. Also things like Norton Guides which allowed you to write online manuals you could pull up with shift keys – including all of the DOS calls/processor instructions. When Windows came out it was near impossible to address things like printing directly. Until some of the more advanced interface books came out, it was use a control under VB3 to do it. Even under DOS until “Undocumented DOS” came out nobody knew what the insides of DOS looked like, like the SDA DOS swappable areas. And yes, back then it was a given that you needed to know 2f8,3f8,2e8,3e8 along with b800:0 and b000:0 video areas. In the end under DOS it was assembly using OPTASM/MASM and OPTLINK/LINK for me. Those days were fun, and they ended roughly in 1994 when VB was starting to take hold. And for historical sake – VB was chosen by businesses because they could produce screens for business and technical project analysis quickly. The morons managing never understood that screens didn’t equate to real code. Still, for business applications VB was a fast development environment to code in, easily debugged (since it didn’t allow direct memory pointers until later), and easy to maintain if some standards were used during development.Before you might complain qb64 does not have all the same debugging tools as qb45 did. Look for VWATCH and use it too. Since the board was reset it might be awhile before VWATCH shows up. QB64 is so much more than a BAS to C spewer. All the extensions added to QB64 make a worth while second look. Hell you can even make windows system calls. Like file attributes, mod date/archive/create date/flags.BTW, latest version of QB64 will take qb45 tokenized files and load them. No need to save them as TEXT.Dosbox, nah. Ran qb45 directly in windows7 command prompt the other day, appears to work. I used to be quite good with it many years ago. Still good with batch but it’s surprising how much of the qb45 syntax has leaked out of my memory over the years. Practice me thinks.My first dive into programming was with Qbasic 1.1 back in 1998. I was 12… I didn’t really know any better, and as far as I was concerned, it might as well be magic. Being able to draw circles and lines and manipulating ASCII text in 16 colours (and later on 256 colours when a friend ‘discovered’ SCREEN 13 and hurriedly called me on the phone one day).The first program my friend showed me (which got me hooked) is as follows:Hmm Anthony, My initial understanding is the underscore provides clarify in respect of long function names to improve readability – especially efficient for those professional programmers who thought ahead acquiring the skills of speed reading in tight commercial environments for the most optimum level of productivity.ie. The space normally used in sentences gave way to the underscore when variable & function names could be long to offer as much useful context as possible as the space offers delimiting functionality in most languages.So now the underscore is added as an arbitrary marker instead of some other reserved character in that context, couldn’t this add a later of confusion for the novice when exposed to high level code at the earliest, rhetorical question for most part ;-)You had me at: “This might be lost on those who never wrote BASIC code on a vintage machine, but the following code creates a 800×600 screen, puts a full screen PNG up, plays an MP3, and writes a message using a TrueType font.”You lost me at: “No, you definitely should not. I didn’t write this to try and convince anyone to jump on a programming language that peaked before many of our readers were even born. ‘The implication seems pretty clear. If you learned programming on a BASIC variant, or even have some QB code you want to revisit, then the new functions offered in QB64 are a welcome addition.But if it’s 2018 and you’ve never written a line of code, BASIC isn’t where you want to start.To me it seems a little bit like restoring a classic car. Yeah it’s cool to look at, and we can all appreciate the work that went into keeping it up and running. But few people would seriously suggest you should hop into a 50 year old car and tool around in it as a daily driver.",QuickBASIC Lives On with QB64
9683,3719246,2018-02-23 04:31:26,"The need for a backup bee has become critical, particularly in almond orchards. Almonds are California’s second-largest crop, injecting an estimated $21 billion annually into the state’s economy. In 2016 California’s almond growers needed nearly 1.9 million honeybee colonies—almost three-quarters of all the commercial colonies in the country—to pollinate their 940,000 acres....Annual colony losses in the U.S. for the past 11 years have ranged between 29 and 45 percent. Add in the ever-expanding almond acreage—from 570,000 acres in 2004 to more than a million today—and the entire system is stretched....[The Wonderful Company, the largest almond grower in the world] chose to develop Osmia lignaria, a native mason bee known as the blue orchard bee, or BOB....In 2017 Wonderful needed about 76,000 honeybee colonies to pollinate its almonds (at two colonies per acre). But that number will diminish by 320 this spring because [Gordon Wardell, director of bee biology for the Wonderful Company] will put 128,000 female BOBs into the orchards—the largest deployment ever. If Wardell’s experiment succeeds, the results could have far-reaching implications for the almond industry as well as a host of other early-blooming crops—from apples and cherries to apricots and peaches. All told, more than a million and a half acres could benefit from having BOBs as a backup—if they prove worthy this year.","Backup pollinators: California almond, fruit producers hope blue orchard bees can reinforce honeybees | Genetic Literacy Project"
9684,3719483,2018-02-23 13:29:15,"But not all throwbacks are good. One thing that’s recently made a comeback (especially in the wake of the tragic massacre at Stoneman Douglas High School, in Parkland, Florida) is the purported link between video game violence and real-life acts of violence.In the wake of this real-world tragedy, politicians are looking for their culprit in the virtual world.Just two days after Nikolas Cruz’s rampage, which left seventeen dead, Kentucky’s governor Matt Bevin decried America’s “cultural problem,” saying:“You look at the ‘culture of death’ that is being celebrated. There are video games, that yes, are listed for mature audiences, but kids play them and everybody knows it and there’s nothing to prevent the child from playing them, that celebrate the slaughtering of people.”A few days later, Donald Trump fired off his own volley of nonsensical bullshit. If you need to take an Ibuprofen after reading this, I completely understand:“I’m hearing more and more people seeing the level of violence in video games is really shaping young people’s thoughts. And then you go the further step, and that’s the movies. You see these movies, and they’re so violent a kid is able to see the movie if sex isn’t involved, but killing is involved, and maybe we need to put a rating system for that. The fact is that you are having movies come out, that are so violent, the killing and everything else, and we may have to think about that.”There’s a lot to unpack there. Let’s ignore the fact that rating systems for games and movies already exist, and retailers in the US voluntarily (and routinely) prevent minors from buying age-inappropriate content. Fuck knows what Trump is banging on about there.Instead, let’s talk about how “the level of violence in video games is really shaping young people’s thoughts.”Seriously? I thought we were past this.Gaming is (arguably) bigger than Hollywood. In dollar terms, video game sales have long eclipsed box office receipts. No longer is it the preserve of nerdy blokes in their bedrooms, as was the case in the mid-1990’s. Gaming is mainstream, and has been for a really long time.As a result, a lot of the critical voices have quietened.In the 1990’s and 2000’s, video game violence was tabloid fodder. I remember how you could barely go a week without the Daily Mail crusading impotently against the likes of Doom, Manhunt, and Carmageddon.A real headline in the Daily Mail.That just isn’t really a thing any more.Anyone remember Jack Thompson? In the 2000’s, the (now disbarred) attorney made a name for himself by advocating against the games industry, suing the likes of Sony, Rockstar, and Take-Two interactive. Since then, he’s barely been heard from.I’d argue that’s because people are no longer convinced by these arguments. People have (broadly) realized that gaming is big, and if it really did cause people to commit acts of mass violence, there’d be a bigger body count, particularly in the countries where game consumption is highest. Countries like Sweden, Japan, Germany, and South Korea.Those still making this argument look inherently anachronistic. But the optics don’t matter, and it doesn’t matter who is making the argument. All that matters is the science.Put simply, there’s no proven link between gaming and violent behavior in the real world. In fact, the evidence says otherwise.A study from the University of York, published in January of this year, looked at 3,000 gamers and how violent video games influenced their behavior. It found that games don’t prepare — or ‘prime’ — people to behave in a certain way, and that realism in gaming doesn’t translate into increased aggression.Discover Magazine’s Christopher J. Ferguson recently published a sterling article on this matter, which looked at decades of research. The most pertinent line from the article is this:Any claims that there is consistent evidence that violent video games encourage aggression are simply false.Simply false.So, why the renewed backlash against games?Well, perhaps it’s because addressing America’s gun violence problem requires people to ask hard questions about the availability and role of firearms in U.S. society.Hard questions are just that. Hard. Sometimes, it’s preferable to look for simpler answers, even if they aren’t actually right.Rather than addressing the inherent public safety issues associated with widespread gun ownership, we’re looking for easy scapegoats.And yeah, going after games is a simple solution that’ll be palatable to many, but it won’t help stop the next high school massacre.",We're blaming video games for real-world violence? Again?
9685,3719484,2018-02-23 13:26:23,"About TNWTNW SitesFrom Boris: Inflection pointWe live in remarkable times. Never before in history have we had itso good, despite what you read in the news. We are healthier, richer, and safer than we’ve ever been. Still, we tend to focus on what’s right in front of us and ignore the past or immediate future.Humans are very well equipped to deal with the now and not the future. A paper cut can ruin our day while we have already forgotten that broken toe from last year, and still merrily commit to a risky adventure where we might break a leg.It is, therefore, no surprise that we are equally bad at predicting what’s next. Even though all the signs are there.If you are reading this you might now realize I’m not talking to you specifically. You and I are in a business that is training itself to look ahead. But even in our business, we tend to overlook the obvious signs that things are about to change. We knew Blockchain and Bitcoin were going to change the world, but most of us didn’t buy Bitcoin at 20 cents.We know self-driving cars are going to change the world, and pretty soon too, but we haven’t really considered how disruptive this is going to be to parking garages, trains, gas stations, or the design of cities. If you look out the window now there’s a good chance you’ll see a bunch of cars. Now, imagine your view without any cars at all, except for a few self-driving ones. Now, redesign your city without parking spaces and parked cars. That’s just one example of the changes we can now predict we will see in our lifetime.For years we’ve talked about the revolution smartphones would bring, and lived through some of it. The same for artificial intelligence, home automation, and a bunch of other exciting developments. All these technologies have been maturing over the past decade. We’ve seen the early signs, we’ve played with the prototypes and early products, but I firmly believe we are now at an inflection point where all these technologies will really break through and start making dramatic changes.A few months ago, I looked at the state of technology and it reminded me of the moment that Steve Jobs stood on stage and demoed the first Apple laptop with built-in Wi-Fi. Wi-Fi had been available for years. I had Wi-Fi in my laptop, but it was a Cisco card that required special drivers, was difficult to manage, and expensive. You needed to be a geek, and know how to hack your computer, to get it to work. The moment Apple introduced Wi-Fi I realized this was an inflection point; the technology would cease to be seen as ‘a technology’ and become commonplace. That would, in turn, have an effect on how people work and move around, and that would impact how we organize our offices.We are at an inflection point now. All the signs are there, and it is incredibly exciting to imagine what is going to change next.Every year people ask me why they should visit our conference in Amsterdam. This year my answer is ‘Inflection point’.If you want to know what is going to happen next, what the signs are and what this will mean for your business, your environment, and for you personally, then you need tojoin me in Amsterdamand listen to the experts on stage, the 15.000 attendees, and the hundreds of companies who will attend.",From Boris: Inflection point
9686,3719485,2018-02-23 12:10:00,"TNW SitesVitalik Buterin has made it clear that designing more efficient scaling solutions is one of the biggest challenges for the Ethereum blockchain – and it seems the company is gearing up take on this hurdle head-on. Buterin has indicated the company will soon begin testing its new scaling tech better known as sharding.Talking about the cryptocurrency scamepidemic on Twitter, the young co-founder strongly hinted that Ethereum is close to launching its sharding testnet. While Buterin did not offer a clear timeline, he said that Leeroy – a decentralized Twitter rival – will be a suitable candidate for the experiment.“I hope they can participate in the sharding testnet soon when it’s ready,” the founder responded to a comment asking about a verified Twitter on the blockchain.Buterin has since purged the tweet, but you can still see the full response here:Buterin and lead developer Nick Johnson have often stressed the central role sharding could play in Ethereum’s plan to move from proof-of-work to proof-of-stake system for verifying transactions on its network.During a talk in Taipei in November last year, Buterin laid out the company’s intention to reach Visa levels of scalability within the next three to five years, as reported by Trustnodes. Sharding was painted as a crucial part towards accomplishing this goal.For context, Visa processes upward of 1,500 transactions per second, while Ethereum roughly handles between 10 and 30.The way Ethereum is currently setup requires that a transaction is verified by every single validator on the blockchain, which makes the network inherently more secure but also significantly slower. Sharding basically splits the network into small partitions known as shards, with each piece containing its own independent state and transaction history.“In this system, certain nodes would process transactions only for certain shards, allowing the throughput of transactions processed in total across all shards to be much higher,” as explained by blockchain developer Raul Jordan explains in a blog post.Back in September last year, Johnson also suggested that Ethereum is gradually working out its vision for sharding, calling the technology “a series of ideas under development – not a deployed system.”It remains unclear how long it will take to rollout a functional version of Ethereum powered by sharding, but the implementation could go a long way in improving transaction efficiency, fending off network congestion and keeping the blockchain alive when the next CryptoKitties sensation arrives.We’ve reached out to Buterin for further clarification and will update this piece accordingly if we hear back.","Ethereum gears up to test sharding scaling solution, Vitalik Buterin hints"
9687,3721791,2018-02-23 04:36:00,"Paul Ryan moves to replace election security officialThe White House and House Speaker Paul Ryan are looking to replace a federal official who's been working to protect election systems from possible Russian cyber attacks. According to Reuters, Matthew Masterson, who holds a seat picked by the House Speaker and formally nominated by the president, won't be re-appointed as a commissioner for the US Election Assistance Commission. Masterson made cybersecurity his priority when he became the commission's chairman in February 2017 in response to what happened during the 2016 Presidential Elections.On its website, the EAC says it ""accredits testing laboratories and certifies voting systems,"" among other responsibilities. When Masterson wrote an opinion piece for The Hill, he said the EAC would support and empower state and local governments to purchase ""new and innovative election"" machines. He also said that his office will ""keep turning to state and local election officials to listen and respond"" to election cybersecurity issues.The US intelligence community believes Russian authorities launched a cyber attack on US election infrastructure on top of hacking the DNC and Hillary Clinton herself before and during the 2016 elections. In addition, the voting machines the country has been using for the past decade are already obsolete and vulnerable to infiltration. At least a couple of states are already planning to replace them with more secure models, though in Pennsylvania's case, it doesn't have the budget to do so.It's unclear why the Speaker isn't nominating Masterson for a second four-year term. Ryan's spokesperson AshLee Strong only told Reuters: ""The appointment expired in December and we are going in a different direction for our nomination. We nominate people for a variety of positions and generally speaking choose our own folks.""Privacy advocates are already worried about the impact of Masterson's removal from the EAC, since it could mean that the meaningful security changes he's been fighting for won't see the light of day before the midterm elections take place in November. Especially since intelligence agencies believe that Russia will try to attack again. Center for Democracy & Technology chief technologist Joseph Lorenzo Hall told Politico: ""This is insanity. [Masterson] is extremely capable and has been a champion of more secure and better elections the entire time he's been on the EAC."" Colorado election chief Judd Choate echoed that sentiment, telling Reuters that ""It is pretty remarkable that in this environment, given the importance of this issue, that the speaker would choose this moment to not reappoint the person doing the most work in this area.""While Strong didn't explain why Ryan won't be re-appointing Masterson, it's worth noting that a Republican-led House panel passed a bill last year to terminate the EAC. Masterson has been fighting to keep the agency alive. As for who'll replace him in the role of chairman, Politico reporter Eric Geller says it could be Republican appointee Christy McCormick, who's not entirely convinced that election security is an urgent issue.Masterson has made cybersecurity a major focus of his chairmanship, which began last February (he had been a commissioner since 2014). Unclear why the White House and Speaker Ryan want to replace him. https://t.co/slar706QkC",Paul Ryan moves to replace election security official
9688,3721792,2018-02-23 01:14:00,"‘A Normal Lost Phone’ arrives on Nintendo Switch March 1stA Normal Lost Phoneis the kind of indie game that seizes a tricky niche (mobile gaming) and leans into the form factor for a unique experience -- a Gone Home or Life Is Strange that takes place entirely within a mock phone interface. The game will lose that skeuomorphic novelty when it arrives on Nintendo Switch on March 1st, but will be able to reach players on a console increasingly known for indie titles.Like other character-driven games, A Normal Lost Phone is best explored without spoilers. But some critics felt the central conceit -- picking up a stranger's phone, poking around their texts and photos and then sending personal information to other characters -- presented ethical challenges.The game aims to deliver a unique LGBTQ narrative, and the team received developmental input from members of that community. But it was criticized for forcing the player to expose details about its queer central character without their permission, a serious breach of privacy during a deeply personal process.But if you want to explore ""A relatable story that helps build empathy with the characters, which allows difficult topics to be explored,"" as the game is described, it will arrive in the Nintendo eShop on March 1st.",‘A Normal Lost Phone’ arrives on Nintendo Switch March 1st
9689,3721793,2018-02-23 00:23:00,"Robinhood's commission-free cryptocurrency trading is liveThe zero-fee stock trading app, Robinhood, announced its plan to enable users to buy and sell Bitcoin and Ethereum last month. Now the company is making good on its promises. Starting today, Robinhood is rolling out access to trade the two cryptocurrencies in California, Massachusetts, Missouri, Montana and New Hampshire, with plans to expand to many more states later.If you're not in one of the states covered, you can still monitor and track market data for all 16 cryptocurrencies in the Robinhood app. You'll also have access to a new feed, which gives users a way to talk about cryptocurrency, news and market swings in real time with other folks on the system. Robinhood Feed is only available to a limited number of people, however, so you'll need to update the app to see if you're one of them.",Robinhood's commission-free cryptocurrency trading is live
9690,3721794,2018-02-23 12:02:00,"Nissan's self-driving taxi is ready for passengersNissan will start testing its self-driving taxi service Easy Ride in a few days in hopes of launching it in time for the 2020 Summer Olympics in Tokyo. The automaker and Tokyo-based mobile developer DeNA will begin ferrying passengers in Yokohama on March 5th. Nissan's autonomous cars will only be able to drive them along a set route, a 2.8-mile-long stretch of road between Nissan's HQ and the Yokohama World Porters shopping center. But they'll at least be able to give the Easy Ride app's features a try during their trip.Passengers will be able to tell the app via text or voice what activity they want to do while in the area. The car's built-in tablet screen will then show them recommendations, including places of interests and event. Sure, you could look all those up on your own, but the Easy Ride system can hook you up with coupons from the retailers and restaurants it recommended. You'll also be asked to rate your experience and how much you'll be willing to pay for a ride like it in the future.When Nissan first announced its plans to test the service this year, it said the initial trial period will only last for a couple of weeks. Nissan and DeNA won't be able to gather tons of feedback within that time, but any survey response they get will be used to develop future field tests and Easy Ride itself. To be able to offer rides to Olympics tourists and Japan's aging population, they're planning to expand their routes, offer multilingual support and fine-tune their pick-up and drop-off processes in the next couple of years. While it's not exactly clear if the initial tests will have a human driver behind the wheel, Nissan says the companies have set up a remote monitoring center for customers' peace of mind.",Nissan's self-driving taxi is ready for passengers
9691,3721795,2018-02-23 09:15:00,"Watch Netflix's first trailer for hip-hop biopic 'Roxanne, Roxanne'Roxanne, Roxanne, the Lolita ""Roxanne Shanté"" Gooden biopic that premiered at Sundance last year, is making its way to Netflix on March 23rd -- and you can watch its first official trailer below the fold. Shanté rose to prominence as one of the most fearsome battle rappers in Queens, New York back in the 80s when she was just 14 years old. The biopic has some big names attached to it, including Oscar-winner Mahershala Ali (Moonlight, Luke Cage). Chanté Adams, the new actress who plays Roxanne, held her own though and won Sundance's Special Jury Award for Breakthrough Performance.Shanté's journey began when she wrote a track in response to rap trio U.T.F.O.'s Roxanne, Roxanne about a girl who wouldn't respond to their advances. In addition to showing how Shanté became a feared battle rapper in Queens, the Netflix original also explores the difficulties she faced to support her family.","Watch Netflix's first trailer for hip-hop biopic 'Roxanne, Roxanne'"
9692,3722058,2018-02-22 08:00:39,"Land Rover Explore is 'the toughest phone in the world'A phone to get your engine running?SharesIt's not every day that a car manufacturer launches a smartphone, but that's exactly what British firm Land Rover has done as it's just introduced a rugged handset.The Land Rover Explore is being billed as ""the toughest phone in the world"" and features IP68 water and dust resistance - the same as the Samsung Galaxy S8 - is drop proof to a height of 1.8 meters, and comes with a factory fitted screen protector.It can also survive extreme temperature changes, including ""freezing cold to blistering heat, thermal shock, intense humidity and vibration exposure.""A mammoth 4,000mAh battery is found inside to keep you going in remote locations, which Land Rover claims will give you two days of usage with the screen constantly on.There's also the option of a 'Adventure Pack' that sticks magnetically to the rear of the handset to extend battery life with an additional 3.600mAh, as well as improving the Explore's GPS and mapping capabilities.The 3,600mAh additional battery attaches to the rear of the phoneSending out a SOSA compass and SOS light have also been built into the Land Rover Explore, and the touchscreen display can be used when wet or when wearing gloves - two scenarios traditional smartphones struggle with.",Land Rover Explore is 'the toughest phone in the world'
9693,3722059,2018-02-22 12:54:33,"Concocted in tandem with Microsoft and its Windows 10 S operating system, these laptops and tablets – of which there are three so far: the Asus NovaGo, HP Envy x2 and Lenovo Miix 630 – use the unique power-management capabilities and built-in LTE connectivity of Snapdragon processors to create truly always-on and always-connected Windows 10 computers.Let’s just say that this major move by Qualcomm has the laptop world talking, and we’ve asked all the major manufacturers what they think.If you ask those who have bought into Qualcomm’s mission, i.e. the aforementioned device-makers, that’s exactly what these chips do. Of course, those who have yet to dive in either have serious doubts or have their toes in the water, so to speak.“The industry has tried – and given up – a couple times on always-on, always-connected PCs,” Acer's senior product marketing and brand manager Eric Ackerson says. “I think, with products like our Swift 7, we're trying it again and we're going to see what happens.”However, the most recent Acer Swift 7 is not a Qualcomm Snapdragon 835 laptop, but rather one with a 7th-generation Intel Core i7 processor and Intel LTE cellular modem inside – it claims up to 10 hours of battery life compared to Qualcomm’s offering of 20.“We don't have a solution with the Snapdragon 835,” Ackerson says, “[but] that's not to say that we won't. We are evaluating, and we'll see what happens. I think, personally, it's a very interesting idea.”Acer clearly wants to see whether it can go toe-to-toe with these Qualcomm laptops with a more tried-and-true offering before taking the plunge. Meanwhile, Lenovo is wasting no time picking up what Qualcomm is putting out.“When we first conceived the Lenovo Miix 630,” Lenovo VP of global consumer marketing Matt Bereda says, “we really wanted to create a computing tool for the mobile user – a segment of the market that has constant connectivity through their smartphones, but needs much more computing power when they’re on the go. It’s with this in mind that we designed the Lenovo Miix 630 to emulate the convenient connectivity of smartphones, while retaining the productivity of a laptop with a full-sized keyboard.”It’s clear that Lenovo believes Snapdragon is the way forward to this type of computing experience, and so does Asus.“That Qualcomm SOC has really proven that it can handle a really wide variety of things, and we're able to do a lot [with it on the] Android platform,” Philip Tamaki, product marketing manager for Asus, says. “Of course, at the same time Asus has always kind of led the way with our Windows PC products.”There’s a clear thread here between the creators of the first three Snapdragon laptops and existing relationships with Qualcomm through their phone businesses. Both Asus and Lenovo (and Motorola) have been releasing Android phones with Qualcomm’s chips inside for years, while HP has recently got into the game with phones like the Elite x3.However, in HP’s case, it clearly feels more comfortable testing out the Snapdragon chipset within an existing product before dedicating significant design resources to the cause, repurposing its latest Envy x2 Windows tablet. (In fact, HP even has an Envy x2 model with an Intel processor inside.)“What I love is [that] we keep the full-travel keyboard [and] great touchpad,” HP VP of customer experience and portfolio strategy Mike Nash says. “[The Envy x2 has] got a phenomenal experience whether you're using the thing in a laptop – or attached – mode or detached in a tablet mode.”“We're really interested to see how customers respond to the LTE support that's built in as a standard on this product as well [as that], right now, basically all of the major carriers in the US [and] around the world,” Nash adds.“It's kind of an amazing experience [to] just know that, when I'm in the office, I use Wi-Fi, [but] when I'm on the plane about to take off or in some other public place, I don't worry about logging on to whatever Wi-Fi – LTE is an amazing [and] transformational experience.”HP's Envy x2 running on SnapdragonSnapdragon still faces a rather large fenceThat said, not every laptop maker is necessarily gushing about this new wave of mobile Windows 10 machines. Some companies, like LG and Dell in particular, seem to have their feet firmly planted on the other side of the fence.“LG currently has no plans to include Snapdragon in its lineup of premium ultra-light LG gram laptops,” Hyukki ‘HK’ Kim, director of IT and product management for LG, tells us. “When comparing our LG gram lineup with the Intel processor to other previously launched devices, we did not find any with longer battery life,” Kim adds, referring to the 13-inch LG gram laptop’s claim to 22 hours of battery life on a charge.""We're definitely looking at Windows on Snapdragon again to potentially target a more value consumer.”Samsung's Shoneel KolhatkarMeanwhile, Dell seems to think that it’s achieved Qualcomm’s vision of ‘always-on, always-connected’ mobile PCs just fine on its own (and with Intel).“It’s important to find the right balance of performance and battery life, which we believe is the case with our current portfolio of consumer and commercial PCs,” Jay Parker, Dell’s president of its Client Product Group, says. “Combine that with LTE connectivity, which we offer today on many products, and that constitutes ‘always on’ in the customer’s mind.”Samsung, one of Qualcomm’s biggest partners and customers worldwide through its Galaxy phone business, is surprisingly hesitant integrate Snapdragon into its laptops. But, the firm’s restraint to fully buy into Qualcomm’s vision seems to have more to do with Microsoft’s baggage regarding Windows on ARM-based processors than anything.“We want to make sure that it is perfect,” Samsung senior director of mobile computing product marketing Shoneel Kolhatkar says. “There are some historical elements of Windows RT etcetera, but we’ve seen some good signs with the new Windows [10 S] on Snapdragon.”“We feel that the Notebook 9 Pen, Notebook 9 and Notebook 7 Spin meets the needs of the consumer,” Kolhatkar adds, “especially the Notebook 9 which has a 75 watt-hour battery that’s getting about 20 hours. We believe that a consumer has a choice today and we're definitely looking at Windows on Snapdragon again to potentially target a more value consumer.”Lenovo's Miix 630 running on Snapdragon.Will Snapdragon’s ripples turn into waves?While it’s still early days for Snapdragon-based Windows laptops and tablets, one can’t help but wonder what effect these devices will have on the laptop scene at large. For the longest time, almost every Windows laptop and tablet runs on an Intel chip, save for the odd AMD option.Will that paradigm be truly shifted, thanks to Qualcomm?“[Snapdragon is] not going to be a kind of revolution to the industry – more [like] a progression,” Asus’s Tamaki says. “People have phones all the time, and they're used to being always connected on the phone, but a lot of times people simply demand [to be] connected on something that has a bit more screen real estate and for more professional or purpose-built use cases.”“I think it's great to have choice, but at the same time it can be overwhelming for end-users that maybe aren't as in tune to the space or the individual technologies ... right?”Acer's Eric AckersonIn particular, Tamaki sees this breed of laptops and tablets being particularly popular with professionals like journalists and real-estate agents – people whose careers see them constantly on the move. On the other hand, Lenovo sees a wider impact in the tea leaves, but Snapdragon’s success in the PC world rides quite heavily on distribution and carrier data plan pricing.“We’re seeing a growing desire from consumers for computing devices that are powerful, portable and truly mobile,” Lenovo’s Bereda tells us. “The Lenovo Miix 630 is a compelling offering that checks all the boxes, but carrier support for distribution and compelling data plans will play a big part of the category’s success.”Of course, despite that, we’re told that Lenovo is confident in a ‘bright future’ for the ‘always-connected’ PC and that it’s working hard on the network operator piece of the puzzle.If you ask Samsung, the baggage of two previously failed attempts at Windows on ARM could weigh heavily upon the masses, but an even bigger factor will be whether power management or performance are considerably compromised in the endeavor.“Let's see the reactions to the initial [products],” Samsung’s Kolhatkar says. “Is there any baggage that consumers have in terms of perception? More importantly, it has to be [that] the performance, the power and [the] portability have to balance each other. There has to be absolutely no compromise.”In this case, HP is simply following what it believes in ‘based on insights from customers,’ we’re told, that folks certainly want power and the flexibility of a 360 design from their laptops and tablets. But, as HP’s consumer insight data tells it, there’s an increasing amount of people that are doing more on their computers on the go than ever before, hence the draw of the LTE-equipped, always-connected PC.To Acer, it all comes down to brand positioning and messaging, which at this point requires quite a few parties – Qualcomm, Microsoft, the device makers and the carriers – to all work together.“I think it's great to have choice, but at the same time it can be overwhelming for end-users that maybe aren't as in tune to the space or the individual technologies as you and I might be, right?” Acer’s Ackerson asks. “On the upper end of the performance and price spectrum, those are a little more knowledgeable users. They'll do some research, they'll read reviews that you're going to write and they'll use that information to guide them.”While there’s plenty to be excited about regarding Qualcomm’s move into the PC world – laptops that last more than 20 hours?! – whether Snapdragon inside mobile Windows PCs will be the norm this time next year depends on several factors. Of course, the most important of which is how well these devices deliver on the promises of ‘always-on’ and ‘always-connected.’",Qualcomm Snapdragon 835 on Windows: what the world’s laptop experts think
9694,3722060,2018-02-22 15:22:25,"How to use a keyboard and mouse on Xbox OneCan you? Yes. Should you? That's up to youSharesFor around three years now Microsoft has said that official keyboard and mouse support is coming to Xbox One. Unfortunately, it’s yet to happen and we don’t currently have a date for when it will. No matter, though – if you simply can’t wait there’s a way to make it happen for yourself and we can tell you how.Now, there is some question as to the fairness of using a keyboard and mouse when others might be using a controller as it’s commonly accepted that those using a controller are at a disadvantage. However, Microsoft has been steadily bringing the Xbox and Windows 10 platforms closer together over the years and now that cross-play is becoming more common, it’s great to give players a choice.Microsoft’s official stance is that for reasons of developer freedom, player choice, and accessibility, it won’t be putting a stop to keyboard and mouse use on Xbox One, even though it could. Just try not to use these in competitive games or you might hit some consternation.How is it done?If you’re not troubled by wires and you’re happy to sit close to your console, you can actually plug a wired keyboard into the USB port on your Xbox console. This has been possible for a while but it’s pretty limited and isn’t great for full-on gaming. It is pretty useful for sending messages or participating in stream chats you might be watching on your console, though.If, however, you want a full keyboard and mouse gaming experience you’re going to need a little something extra - an adapter and the patience to set it up. Using one of these isn't a simple matter of plug and play – they’ll require some set up, some wires and some software configuration.You have a few options when it comes to adapters and they vary in price. Below, we’ve listed some of the top suggestions and how they’ll help you.All of these devices essentially manipulate the signal from your controller, making the console think that the inputs from your mouse and keyboard are from a traditional console pad.CronusMaxCronusMax is one of the more affordable options on the market but it does have its limitations. Essentially it’s a simple USB device that will allow your console to recognize the keyboard and mouse as an input. It’s worth pointing out that you will need a PC close to your console to make this work as it needs to be plugged into both devices.As well as this, it only has one USB port so you’ll have to make sure your mouse and keyboard don’t have their own separate USB dongles and can connect to the one.XIM 4XIM 4 is a more expensive option but it’s probably a more popular one and it offers a higher quality experience. The XIM 4 has next to no lag and it has two USB ports so no need to worry about your keyboard and mouse situation. It also has a free iOS and Android companion app so there's no need to have everything linked up to your PC to play or switch game profiles.XIM 4 is a little hard to find at the moment, but its successor, Apex, is expected in this first quarter of this year.Titan OneTitan One is a third option and it’s another good one that’s quite similar to the CronusMax. It’ll also require that you keep your console and PC pretty close together and you'll have to download an additional MaxAim DI plugin in order to map controls.While these devices will work with many popular games, it's worth noting that they don't work with every single game out there so it's best to check the game you want to play is supported first. The same goes for the mouse and keyboard you'd like to use – make sure they're supported before purchasing.",How to use a keyboard and mouse on Xbox One
9695,3722131,2018-02-23 14:54:17,"About TNWTNW SitesLooks like Google is finally building dark mode straight into AndroidOne thing Android users have consistently asked for over the years is a dedicated dark mode for browsing at night – and it seems Google has finally decided to accommodate this request.Responding to a thread asking to include the feature in the next iteration of the mobile operating system, a Google employee confirmed that its engineering team has added the functionality.Responding to a thread asking to include the feature in the next iteration of the mobile operating system, a Google employee confirmed that its engineering department has added the functionality.“Our engineering team has added [dark mode],” the employee said. “It will be available in a future Android release.” Unfortunately, the message makes no mention precisely in which iteration the new feature will land.But from the looks of it, it will likely be included in the upcomming Android P.Among other advantages, dark mode makes browsing at night less heavy on the eyes and also reduces battery drain.For the record, the Big G has been toying around with the idea of including dark mode in its OS since at least the release of Android M. More recently, the feature was spotted in the developer version of Android N, but ultimately did not make it to the commercial release.Indeed, my colleague Napier Lopez highlighted the lack of dedicated black mode as one of the reasons why custom-built Android skins are swiftly catching up with stock Android – especially when it comes to user experience and easy of use.It’s not clear whether dark mode will work in third-party software, but the good thing is that a number of apps have already taken the initiative to build their own modes for browsing at night. The list includes Twitter, YouTube and Samsung’s Internet browser.In the meantime, you can always resort to third-party apps like Substratum to enable dark mode – or just try one of these tricks.",Google is finally building dark mode straight into Android
9696,3724507,2018-02-23 15:00:00,"Samsung Galaxy S9: What to expect from Unpacked 2018Samsung is primed to unveil its latest flagship, the Galaxy S9. Fortunately for anyone desperate to hear what's coming on February 25th, there's been no shortage of leaks and renders before the big day. It's not good news for the secret-keepers at Samsung but gives us plenty of threads to pull at ahead of the big reveal in Barcelona. How will the Galaxy series fare against the latest trio of iPhones? Can it best the talking-poop emoji?Say hello to the Galaxy S9 and the Galaxy S9 PlusMultiple rumors (and common sense) point to a repeat of last year, namely two new phones, once again. According to specs from WinFuture, the main difference between the incoming S9 and the S9 Plus will still be screen size. However, it may not be the only thing separating the 5.8-inch Galaxy S9 from the 6.2-inch Plus version. The bigger S9 Plus will apparently pack dual 12-megapixel rear cameras -- a first for the Galaxy S family, even if the Note 8 picked up a second camera last year.Judging by the leaked images from Evan Blass (AKA @evleaks), Samsung's designers aren't making any drastic stylistic changes.The new Galaxy S9 will apparently be all-screen, with just a lil' bit of bezel. We're not knocking it: Samsung's 2017 flagships set a new standard for stylish smartphones, so we're happy to see the same design. Early renders offered a silhouette with oddly thicker sides than last year's S8, although last week's blitz of leaks suggest ""infinity displays"" that once again stretch right from edge to edge, if not quite to infinity.The latest shots also suggest slightly slimmer borders above and below the screen, which sounds good to us. And not a notch in sight. That said, there does appear to be a busier sensor and camera array above the screen, and it's not the only thing that suggests the new Galaxy S phones will offer more in the way of security options and facial-tracking tricks.Animated emojiAt the start of the year, Samsung's chip division announced its latest Exynos mobile processor, and it made a big deal about how its new chip would improve future smartphones' AI performance, face detection and image recognition. Real-time 3D scanning of your face, then, would be very possible. As the press release put it: ""Hybrid face detection enables realistic face-tracking filters as well as stronger security when unlocking a device with one's face.""We know more than that, too. ETNews sources believe the S9 will pack a ""3D emoji"" ready to go head-to-head with Apple's Animoji on the iPhone X. 3D faces will map your expressions to lil' emoji -- and we may have already glimpsed the animation through Samsung's latest S9 preview video.If this is a sign of what's coming, we might be able to pin animated emoji to objects beyond your own grinning, karaoke-prone face.New camera tricksLet's home in on those purported dual cameras. Beside the depth of field skills we've seen on other companies' camera phones, Samsung might be planning something a little more remarkable: a rear camera that can switch between two different aperture modes.Like Samsung's Asia-only W2018 from last year, the camera may tap into an f/1.5 aperture for low-light shooting or macro photography, but also use an f/2.4 aperture to ensure more objects are in focus. F/1.5 is also an extraordinarily low f-stop for a smartphone -- and means more light hitting the imaging sensor, more detail and less noise. A very early leak, involving a possible retail box for the S9, suggests this variable aperture camera will make it to both the new phones.If you were intrigued by Samsung's DeX, the phone dock that turned the S8 into a desktop PC of sorts, then you can happily expect an updated model to go with your new Galaxy flagship. Sure, it looks like a Sega Genesis, but there's nothing wrong with that. The phone will apparently nestle flat on this new device, rather than be propped -- perhaps that touchscreen could come in useful?Any other businessLast but not least, it looks like Samsung will fix one of our biggest issues with the Galaxy S8 -- that terrible fingerprint sensor placement. Now, it's graciously moved away from being right next to the camera sensor(s), looking a lot more like rival Android phones, at least on the back. We'll discover which rumors turned out to be true in a few days. There's a conspicuous absence of Bixby news, but we're certain Samsung will have more to say about its beleaguered virtual assistant as it comes up to its one-year anniversary. Rest assured, we'll be giving it a thorough hands-on testing as soon as we can.Mat once failed an audition to be the Milkybar Kid, an advert creation that pushed white chocolate on gluttonous British children. Two decades later, having repressed that early rejection, he completed a three-year teaching stint in Japan with help from world-class internet and a raft of bizarre DS titles. After a few years heading up Engadget's coverage from Japan, covering high-tech toilets and robot restaurants, he heads up our UK bureau in London.",Samsung Galaxy S9: What to expect from Unpacked 2018
9697,3724508,2018-02-23 14:58:00,"Google Assistant will get support for Routines 'in the coming weeks'Today's Google Assistant is much, much more capable than the version that first debuted on the original Pixel and Pixel XL. Don't expect that progress to slow anytime soon, either: Google laid out some new plans to improve the Assistant just in time for Mobile World Congress, and they extend far beyond just teaching it more languages.Most importantly, Google confirmed it has been working with smartphone makers on ways to weave Assistant more elegantly into our smartphones. That work is being formalized in the new Assistant Mobile OEM program, and Google's list of accomplishments with its partners is nothing to sneeze at: it helped make Assistant compatible with certain kinds of mobile AI coprocessor and worked to make sure devices can listen for the right wake-words even when their screens are off. It won't be long before you start to see device-specific Google Assistant commands, either -- LG touted a list of 23 new commands for its updated V30, and Google also cited close working relationships with companies like Sony and Xiaomi.Google Assistant is also finally getting support for Routines, a feature first announced last year. Long story short, you'll be able to string together multiple actions with a single command; saying ""OK Google, goodnight,"" for instance, could dim your Philips lights, dial down the temperature on your Nest thermostat and lower the volume on your Google Home Max. Routine support is expected to go live within the next few weeks, as will location-based reminders through Assistant-powered speakers. (Yes, you could do this through a phone already, but parity between different flavors of Google Assistant is always a good thing.)Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",Google Assistant will get support for Routines 'in the coming weeks'
9698,3724601,2018-02-23 15:46:18,"Benchmarking Google’s new TPUv2For most of us, deep learning still happens on Nvidia GPUs. There is currently no alternative with practical relevance. Google’s Tensor Processing Unit (TPU), a custom-developed chip for deep learning, promises to change that.Nine months after the initial announcement, Google last week finally released TPUv2 to early beta users on the Google Cloud Platform. At RiseML, we got our hands on them and ran a couple of quick benchmarks. Below, we’d like to share our experience and preliminary results.More competition in the market for deep learning hardware has been long sought after and has the potential of breaking up Nvidia’s monopoly on hardware for deep learning. Along with that, this will define what the deep learning infrastructure of the future will look like.Keep in mind that TPUs are still in early beta — as unmistakingly communicated by Google in many places — so some of the things we discuss might change in the future.TPUs on the Google CloudWhile the first generation of chips, TPUv1, were geared towards inference, the second and current generation is focused on speeding up learning in the first place. At the core of the TPUv2, a systolic array is responsible for performing matrix multiplications, which are used heavily in deep learning. According to Jeff Dean’s slides, each Cloud TPU device consists of four “TPUv2 Chips”. Each chip has 16GB of memory with two cores, each with two matrix multiplication units. Together, the two cores provide 45 TFLOPs, totalling 180 TFLOPs and 64GB of memory for the whole TPU device. To put this into perspective, the current generation of Nvidia V100 GPUs provides 125 TFLOPs and 16GB of memory.To use TPUs on the Google Cloud Platform, you need to start a Cloud TPU (after obtaining quota to do so). There is no need (or way) to assign a Cloud TPU to a specific VM instance. Instead, discovery of the Cloud TPU from your instance happens via network. Each Cloud TPU is assigned a name and gets an IP address that you need to provide to your TensorFlow code.Creating a new Cloud TPU. Note that a Cloud TPU has an IP address.TPUs are only supported by TensorFlow version 1.6, which is available as a release candidate. Besides that, you don’t need any drivers on your VM instance since all of the required code for communicating with the TPU is provided by TensorFlow itself. Code that is executed on the TPU is optimized and just-in-time compiled by XLA, which is also part of TensorFlow.In order to efficiently use TPUs, your code should build on the high-level Estimator abstraction. You can then drop in a TPUEstimator which performs a lot of the necessary tasks for making efficient use of the TPU, e.g., it sets up data-queueing to the TPU and parallelizes the computation across its different cores. There is certainly a way around using the TPUEstimator, but we are currently unaware of an example or documentation.Once you’ve set up everything, run your TensorFlow code as usual and the TPU will be discovered during start-up and the computation graph is compiled and transferred to the TPU. Interestingly, the TPU can also directly read and write from cloud storage to store checkpoints or event summaries. To allow this, you need to provide the service account behind the Cloud TPU write access to your cloud storage.BenchmarksThe interesting part is, of course, how fast TPUs really are. TensorFlow has a GitHup repository of models for TPUs that are known to work well. Below, we report on experiments with ResNet and Inception. We were also keen to see how a model that is not yet optimized for TPUs performs, so we adapted a model for text classification using LSTMs to run on TPUs. In general, Google recommends to use larger models (see when to use TPUs). This is a smaller model, so it was especially interesting to see if TPUs could still provide a benefit.For all models, we compared training speed on a single Cloud TPU to a single Nvidia P100 and V100 GPU. We note that a thorough comparison should also include final quality and convergence of the model in addition to mere throughput. Our experiments are meant as a first peek and we will leave an in-detail analysis to future work.Experiments for TPUs and P100 were run on Google Cloud Platform on n1-standard-16 instances (16 vCPUs Intel Haswell, 60 GB memory). For the V100 GPU, we used p3.2xlarge (8 vCPUs, 60 GB memory) instances on AWS. All systems were running Ubuntu 16.04. For TPUs, we installed TensorFlow 1.6.0-rc1 from the PyPi repository. GPU experiments were run using nvidia-docker using TensorFlow 1.5 images (tensorflow:1.5.0-gpu-py3) that include CUDA 9.0 and cuDNN 7.0.TPU-optimized ModelsLet’s first look at the performance of models that are officially optimized for TPUs. Below, you can see the performance in terms of images per second.Batch sizes were 1024 for TPU and 128 for GPUs. For GPUs, we used the implementations from the TensorFlow benchmarks repository. Training data was the fake Imagenet dataset provided by Google stored on cloud storage (for TPUs) and on local disks (for GPUs).On ResNet-50, a single Cloud TPU (containing 8 cores and 64GB of RAM) is ~8.4 faster than a single P100 and ~5.1 times faster than a V100. For InceptionV3, the speedup is almost the same (~8.4 and ~4.8, respectively). With smaller precision (fp16), the V100 gains a lot of speed.Clearly, beyond just speed, one has to take price into account. The table shows the performance normalized for on-demand pricing with per-second billing. The TPU still comes out clearly ahead.Custom LSTM ModelOur custom model is a bi-directional LSTM for text classification with 1024 hidden units. LSTMs are a basic building block in NLP nowadays so this nicely contrasts the official models, which are all computer vision based.The original code was already using the Estimator framework, so adapting it to use TPUEstimator was very straightforward. There is one big disclaimer though: on TPUs we couldn’t get the model to converge whereas the same model (batch size, etc.) on GPUs worked fine. We think this is due to a bug that will be fixed — either in our code (if you find one, please let us know!) or in TensorFlow.It turns out that the TPU is even faster on the LSTM model (21402 examples/s): ~12.9 times faster than a P100 (1658 examples/s) and ~7.7 times faster than a V100 (2778 examples/s)! Given that the model is comparably small and was not tuned in any way this is a very promising speedup. As long as the bug is not fixed please consider the results preliminary.ConclusionOn the models we tested, TPUs compare very well, both, performance-wise and economically, to the latest generations of GPUs. This stands in contrast to previous reports. While Google promotes TPUs for being optimal for scaling larger models, our preliminary results for a small model also look very promising. Overall, the experience of using TPUs and adapting TensorFlow code is already pretty good for a beta.We think that once TPUs are available to a larger audience, they could become a real alternative to Nvidia GPUs.",Benchmarking Google’s new TPUv2
9699,3724766,2018-02-23 16:01:25,"0After playing around in the experimental phase, Google is bringing its ARCore augmented reality platform to its 1.0 release with availability on over 100 million Android devices.If you have a new Pixel phone, you may have been able to play with Google’s AR Stickers that brought Star Wars and Stranger Things into the real world. With ARCore’s launch today, developers are now able to put their own creations into the Play Store and users with phones left out of the preview are able to take a first shot at phone augmented reality.Google has already been working with companies like Snap, Sony, Wayfare, Porsche and others to bring ARCore app experiences to life.To make use of the AR functionality you’ll need one of the following phones: Pixel, Pixel XL, Pixel 2, Pixel 2 XL, Galaxy S8, S8+, Note8, S7 and S7 edge, LGE’s V30, V30+ (Android O only), ASUS Zenfone AR and OnePlus’s OnePlus 5. That’s obviously just a small subset of all of the Android phones out there today, but Google says they did hit their target with that bunch of pushing ARCore onto 100 million phones at launch.Google also specifically detailed that they are working with “Samsung, Huawei, LGE, Motorola, ASUS, Xiaomi, HMD/Nokia, ZTE, Sony Mobile, and Vivo” to bring ARCore onto soon-to-be-launched devices, so you can likely expect that the Galaxy S9 will have ARCore support ready-to-go for instance.In terms of functionality compared to Apple’s ARKit, Google is launching something pretty similar. The one difference might be that Google offers a bit more flexibility when it comes to the size of surfaces that you’re able to project digital models onto, ARCore 1.0 will launch with “improved environmental understanding that enables users to place virtual assets on any textured surfaces.”While ARKit focuses on large horizontal and vertical planes — think floors, tables and walls — ARCore is just thinking about surfaces in general so if you want to put an AR creation into the palm of your hand or onto the side of a pillow you can, while you’re of course also able to put something onto the wall or floor.While ARCore will be exiting its preview and hitting 1.0 status, Google is still working with its preview of its Lens computer vision tool, though it says it will be bringing it to more users going forward. The functionality will soon be available to users of the Google Photos app running Android or iOS 9 and above. This will allow features to use Lens features after a photo has been taken, the live functionality inside Google Assistant will be coming to some upcoming devices from Samsung, Huawei, LG, Motorola, Sony, and HMD/Nokia.","Google publicly launches ARCore 1.0 on 13 phones, will begin expanding Lens preview"
9700,3724767,2018-02-23 12:21:29,"1Password bolts on a ‘pwned password’ check0Password management service 1Password has a neat new feature that lets users check whether a password they’re thinking of using has already been breached. At which point it will suggest they pick another.This is in addition to the more usual password strength indicator bar that tries to encourage web users to improve their security practices. The pwnage check builds on that by further reducing the risk of password reuse because it’s verifying if the specific password has appeared in a number of known data breaches.Here’s a video of the new feature in action:To power the feature, 1Password is leaning on Pnwed Passwords, a service launched by Troy Hunt last summer, and updated this month with a chunk more password data. It now contains around half a billion downloadable passwords, harvested by Hunt from various online dumps resulting from all sorts of different data breaches. The passwords in the database have been hashed by Hunt with SHA-1.Hunt is best known for creating the Have I Been Pwned? breach notification service. And indeed it was through running that free online check, which lets people sign up to be informed if/when their email address surfaces in a data breach, that the idea for Pwned Passwords came about — as he says one of the most common reactions to people being informed their email had been found in a breach was to ask if they could also check whether their password had been breached.Thing is, knowing your data has been found among millions of breached credentials, which you’re told includes emails and passwords, but not knowing exactly what was compromised in your case can feel frustrating. Although changing your password is always the sensible thing to do in such a situation.And while Hunt has always resisted calls to make breached plain text passwords searchable (for obvious security and privacy reasons), the size of modern data breaches — which can almost routinely involve multi-millions of users these days — has demonstrably ramped up pressure on Have I Been Pwned? to also offer some sort of check for pwned passwords too.Although, to be clear, Hunt’s Pwned Passwords service is not intended for people to check their actual passwords. Because no one should be typing actual passwords into another third party service, even one run by a such a demonstrably good guy.(Hunt himself makes this point, writing: “[D]on’t enter a password you currently use into any third-party service like this! I don’t explicitly log them and I’m a trustworthy guy but yeah, don’t. The point of the web-based service is so that people who have been guilty of using sloppy passwords have a means of independent verification that it’s not one they should be using any more.”)But he’s has done something much more useful and interesting than simply providing an amusing way to find out that “password” has been used as a password more than 3.3 million times in this database. Or that “123456” has been used over 20.7M times. (Which can itself provide a handy ‘security 101’ lesson if you need to help, for example, a less tech-savvy relative get up to speed on password risks.)Because Hunt has made the pwned passwords downloadable and queryable via an API — in a way that does not entail the sharing of full passwords with third parties.And this is what 1Password is using to power its new pwnage check.Cloudflare gets some credit here too. After Hunt created the password database, he says he was contacted by a Cloudflare developer, Junade Ali, who wanted to make use of the database to improve password security but also wanted to incorporate an anonymity model to enable validation of leaked passwords without risking passwords being leaked in the process.Ali has blogged here about the approach he took, using a mathematical property called k-anonymity — and both Hunt and 1Password are using this method to enable password checks against Pwned Passwords that don’t share the full hash of the password being checked (which would be a bad idea because it could create a breach risk).“[O]ur approach adds an additional layer of security by utilising a mathematical property known as k-Anonymity and applying it to password hashes in the form of range queries,” writes Ali. “As such, the Pwned Passwords API service never gains enough information about a non-breached password hash to be able to breach it later.”Only the first five characters of the 40 character hash of the password to be validated are sent to the server hosting the password database, which then returns a list of leaked password hashes that contain the same five initial characters. After that it’s just a trivial local comparison between the hashed password and the list to see whether or not there’s a match.Of course even if there is no match found during a pwnage check it does not absolutely guarantee the password you want to use hasn’t been breached or compromised in some way. But it’s at very least a way of weeding out passwords that absolutely have been breached — and nudging users away from reusing insecure credentials. A horrible practice which, er, has sometimes even caught out some very techie people.1Password says the password check service is available now to everyone with a 1Password membership. To check their passwords users need to sign into their account on 1Password.com, then click “Open Vault” to view their items and then click an item to see its details.After that it says they need to enter keyboard sequence Shift-Control-Option-C (or Shift+Ctrl+Alt+C on Windows) to unlock the proof of concept, and then they can click the new “Check Password” button which appears next to the password.Hunt has flagged a number of other services which have also incorporated the “first generation of Pwned Passwords” on his blog, including some which will entirely block password reuse, adding: “My hope is that they inspire others to build on top of this data set and ultimately, make a positive difference to web security for everyone.”",1Password bolts on a ‘pwned password’ check
9701,3724769,2018-02-22 21:07:09,"Visualizing the slave insurance industry0Similar to the way people insure their cars, houses and lives, slave-owners would sometimes insure their slaves.Fearful of not getting their money’s worth from their slaves, owners would sometimes take out insurance policies on them.In the 1800s, for example, some slave-owners who rented out their slaves would insure them so that, in the event their slaves died or were severely injured in the hands of someone else, the owners would not suffer too much of an economic loss, according to The Treasury of Weary Souls.The Treasury of Weary Souls, created by New York University professor Michael Ralph and engineers from Resilient Coders, examines what slavery looked like after the trade was outlawed in 1808, but continued to exist via the smuggling, breeding and renting of slaves within the United States. Ralph spent a solid seven years researching slave policies, and when it came time to sharing it with the world, he hired a handful of coders from Resilient Coders.Resilient Coders is a nonprofit organization that trains people of color for jobs in the tech industry as software engineers. The bootcamp trains people of color for 14 weeks in semantically structured HTML, responsive CSS, JavaScript, jQuery, git and more.For the last year or so, a handful of Resilient Coders worked with Ralph through the concept, design and the ultimate creation of the project, which utilized JavaScript data visualization tool D3.js. The project was spearheaded by Resilient Labs’ Muigai Unaki, a senior at Northwestern University pursuing a dual BFA degree in graphic design and interactive media.Map of insurance policies taken out on slaves (Source: The Treasury of Weary Souls)As Ralph’s research shows, slave owners sometimes relied on financial firms like Aetna, AIG and New York Life Insurance to insure the slaves whose skills were highly valued. In Alabama, New York Life, which was known as The Nautilus Insurance Company at the time, was the largest slave insurer, according to the Treasury of Weary Souls. Between 1845 and 1848, NYLI sold policies to slaveholders to insure their slaves against damages or death.By the 1840s, the number of slaves insured in the South was about the same as the number of free whites with life insurance, according to Ralph. Of that number, however, Ralph said he never saw a plantation slave insured in his seven years researching 1,300 policies.“Even though plantation slaves were valuable in the marketplace, they were never insured,” Ralph said. “They were viewed more as livestock. They enhanced the value of the plantation but their skills weren’t seen as valuable or premium.”Instead, slave owners would insure coal miners, blacksmiths, carpenters, railroad workers and other slaves with valued skills. Miners, for example, made up 15.4 percent of the insured slave workforce, according to the project. Steamboat workers accounted for 12.6 percent of those insured and domestic workers accounted for 14.6 percent of the insured slave population, according to the ledger.In January 1855, a slave owner by the name of Thomas Doswell insured seven slaves to work in the coal pits in what is now West Virginia. Two of the older slaves were insured for $500 each and the younger ones were insured for $700 each. In 1855, the average price for a slave was $600, according to Ralph’s research.Ralph says the Treasury of Weary Souls is the most comprehensive ledger of skilled slaves who built America. In his research, Ralph said he found these slaves had more mobility than plantation slaves. Sometimes they were able to negotiate their work schedules and other terms of forced labor, he said. The market for slave insurance was mostly in urban areas.“On one hand, this is about a new tier of slaves who emerged in the last decades of legalized slavery who were more elite,” Ralph said. “There were ways in which they were more valuable even though they were less free than free white people.”Ralph’s research is based on data from the California Department of Insurance, Slavery Era Insurance Registry, Illinois Department of Insurance, Slavery Era Insurance Policies Registry and other sources. The archives are incomplete, but Ralph says his evidence suggests at least 85 percent of policy records may have been lost.In his upcoming book, Life: Slavery and Insurance in US History, Ralph explores how the life insurance industry took off as slavery was drawing to a close. He argues the slave insurance industry helped establish the life insurance industry for free citizens. At the time, Americans were skeptical of placing monetary value on a life, he said.“With urbanization, more people started to become open to the idea of finding some way to insure against potential loss of income if they happen to pass away,” he said. “The industry of slave insurance created the appreciation for what that mechanism can do.”",Visualizing the slave insurance industry
9702,3724846,2018-02-23 15:14:40,"About TNWTNW SitesHere’s proof notifications don’t have to suckNotifications suck. I mean they really, really suck. Consider what my desktop workspace looks like at any given time.I mean, yes, I did create those reminders and yes, I do need to do all of those things, but displaying notifications over the right side of the screen blocks, well,the whole top-right corner of the screen!Do you realize how much stuff takes place over there?When someone calls me on Teams, I can’t accept the phone call.There’s a phone call back there and you can’t even see it through my wall of procrastination. And I sure as hell can’t answer it. I can’t even see who it is!I can’t get to any of my Chrome Extensions.I can’t search Apple Music because the search bar is blocked. Yes — I’m the one guy using Apple Music.And it’s not any better on mobile. I have almost all of my notifications turned off for this very reason. So much is happening all the time. Tweets, emails, reminders, texts, phone calls, IMs — notifications STRESS ME OUT. Even the Headspace notification stresses me out!No Headspace. I can’t have an open mind because it’s currently full of notifications!I realize at this point that many of you are likely searching for the comment box so you can tell me about how Android solves all of these problems and if only I would step into the light, all of my problems would go away.Well, I can’t. I already told you I’m using Apple Music. And I have an Apple Watch. And a Mac. Apple has me in a headlock — more like a Full Nelson really. You can’t just walk away from a Full Nelson. Have you ever even been in a Full Nelson?Besides, I don’t believe you anyway.I’ve written about this before and called notifications a DoS attack on your brain.“Notifications are a DoS attack on your brain”Me. I don’t remember when. Probably because of notifications.But Idowant to know what is going on. I just don’t want it to be so invasive as to be like a small child nagging me constantly.“Are we there yet?”“Are we there yet?”“Are we there yet?”I SWEAR TO GOD I WILL PULL THIS CAR OVER…We Can Do BetterI still want to know when things are happening, but it shouldn’t irritate me to the point of drinking. To be honest, the drinking is going to happen anyway, I would just prefer to be less stressed out when I get there.First, let’s define what I do NOT want…Don’t Interrupt — Do NOT put things on my screen; mobile or otherwise. I don’t care if they are at the top or the right. I don’t care if they disappear after a while. You have no right to stick things in my face. Ever. Unless it’s beer. Or cheese sticks.No Noise — Noise is almost worse. I keep Skype closed because I cannot STAND that sound it makes every time someone comes online or goes offline. Yes, I’m sure you can turn it off. I’m just too angry at Skype to invest that much time in configuring it.To be fair, doing notifications on a device is hard. If you can’t make noise and you can’t show anything on the screen, what can you do?Let me propose an alternative: what if we don’t use the device at all. What if the notification takes place via something else in the environment. Something like, say, a lamp?Introducing Twitter BulbTwitter Bulb is an automated service that flashes a light in my office gentle colors of blue whenever notifications come in on Twitter. This is what it looks like in action…Let’s look at how to build a Twitter Bulb. But before we can build the service for Twitter Bulb, we need the bulb! And For that, we’re going to start at a product called LIFX.LIFXADORBSLIFX makes some pretty cool connected lights. More importantly, the LIFX API is wide open and well documented. I bought the LIFX Mini which retails for around ~40$ on Amazon. It is Wifi enabled and supports like 80 bagillion colors or something like that. I don’t know. However many colors is a lot. Also, it comes in a way cute box!LIFX has a mobile app that pulls in a resounding 2.5 stars in the iOS App Store. However, it worked fine for me and I think maybe someone has a vendetta here.The first step is to connect the light to your Wifi. You do this by turning the light on and then looking for its Wifi signal on your device. You then join the bulb where you can set its Wifi in the app. This is exactly how pretty much every other connected device works.The second thing you need to do isclaimyour bulb. This is basically where you hit a button on the app that says “that’s my bulb”. It sends you a validation code and then boom — you’ve claimed it. I don’t get that part at all. I set the thing up on my Wifi, didn’t I? It’s my freaking bulb. However, the API won’t work if you don’t claim your bulb and it won’t tell you that either. Which is nice.Using The LIFX APIThe API on the LIFX is open, simple and RESTful — whatever that even means anymore. It doesn’t take much to authenticate. In fact, you can just register for a token and then pass that on your requests to the API if you don’t want to go through the OAuth rigamarole. I amnot a fan of OAuth. Or trying to figure out how to spell “rigamarole”.Tokens are app based — not user based. You can see I’ve created one for the Twitter Bulb service.Now that we’ve got a token, we’re ready to build out an endpoint that we can hit which will control out bulb. This way our token stays private and we can add any further logic that we want. Instead of creating a whole web application to do that, we can use Serverless.Creating A Serverless EndpointI created a Serverless function usingAzure Functions. My preferred method of doing that is to use theCLI. This has the added benefit of being able to debug Serverless locally with Visual Studio Code. I also use theAzure Functions Extension for VS Codebecause I came for the tooling.Assuming you have both the CLI and the Functions Extension installed, from within VS Code, you can create a new function app. Then I created a function called “breathe”, which is the name of the effect on the LIFX bulb that causes that gentle pulsing. There is also a method called “pulse”, but it does not in fact pulse. Breathe pulses. Still with me?Good. Let’s build the function.Building an Azure function is just like building any Node application. You export one main function that gets called. That function gets a context object and a request object passed to it. Inside of that function, you can do the same things you do in Node. You just need to set theresobject on thecontextwhen you are done and then calledcontext.done. Theresis the response that is returned to the user and calling done tells the runtime that “We’re done here”.First, install thelifx-http-apipackage. That’s right — there is a Node package for this lightbulb. This is why you love, JavaScript.I also use thedotenvpackage which allows me to easily set local environment variables.This is what the full code looks like when we’re done. Note that we’re using thereq.queryobject to see what color was passed in. If no color is passed in, we return an error.const lifx = require('lifx-http-api'); require('dotenv').config();module.exports = function(context, req) {// if a color was passed on the query string if (req.query.color) { let client = new lifx({ bearerToken: process.env.LIFX_TOKEN });And that’s it. We can now run this locally and, if we’ve done everything right, it should trigger a breathe effect with the desired color. WHICH IT DOES.I push my functions to production using Github hooks into Azure. That way whenever I make code changes, the app gets redeployed automatically.Once the function is published to Azure, you can send the URL to all your friends and let them change the color of your lightbulb. I did it in a Twitch stream for a while and it was fun. Although I had never used Twitch before so I felt more than a tad stupid.Now we’re ready to bring the Twitter logic into this project. The easiest way to do this is with anAzure Logic App.Azure Logic AppsAzure Logic Apps are kinda like IFTTT. There are a bunch of connectors to listen for actions and then execute triggers.Logic Apps have a built-in connector for Twitter. You just authenticate and then select what you want to search for. I chose my own name because this will pull in any action on Twitter which involves yours truly. Which honestly isn’t that much. It’s a sad state of affairs.Logic Apps have connectors right into Azure Functions.It took me a while to figure out how to pass the color parameter to my function from Logic Apps, but I finally realized it’s in a JSON format in the “Queries” field.And that’s it! We’re done. Click the “Run” button at the top of the screen and sit back and enjoy your new calming notifications with the LIFX lightbulb. Watch as your quality of living improves and you begin to understand the true meaning of life — which of course is to get Twitter notifications with a light bulb.What Else Can We Do?If you look through all of the other action types in the Logic Apps, you will see connectors to all sorts of services. What other things can we build with our connected lightbulb? There are connectors for Outlook, Github, Dropbox, Salesforce. We could basically use this lightbulb to notify us of everything that ever happens on the internet. We could even use theLUISservice to determine whether or not they are WRONG on the internet!That’s the Twitter Bulb. And Serverless and a bonus Logic App. I really like the LIFX bulb andBrian Clarkand I have some plans to build even more ridiculous projects with it. You’re welcome.This story is republished from Hacker Noon: how hackers start their afternoons. Like them on Facebook here and follow them down here:",Notifications suck. Here’s how to build one that sucks less
9703,3724847,2018-02-20 12:12:19,"About TNWTNW SitesAeon Timeline 2 can get everybody on the same project management pageBig projects can become pretty darn overwhelming pretty darn quick. Once you start digging into research, laying out events, and piecing together all the moving parts that have to come together for a successful product, it can be daunting enough to actually paralyze you a bit.Loads of details and an epic scope don’t have to be a bad thing. In fact, Aeon Timeline 2 ($19.99, 60 percent off from TNW Deals) can help you get a grip on any project, big or small, and lay out a tight, effective plan for reaching the triumphant conclusion you’re looking for.Aeon Timeline 2 started its life as a creative writing tool, helping writers gather notes, chart timelines, and visually represent and track a story’s progress from start to finish. But even for non-writers, Aeon Timeline 2 is still chock full of time and resource management features to focus your efforts and make sure nothing slips through the cracks.Create a template for your project, then start filling in your details. Craft a timeline, lay in your milestones, attach all your references and documentation…before you know it, you’ll have assembled every piece of your project in one cohesive, interconnected web.Timeline allows any number of team members access to the document, so everyone can add to and work off the document at a moment’s notice.Get your project on track — and keep it there — with Aeon Timeline 2, now at the discounted price of just $19.99 thanks to this limited time deal.",Aeon Timeline 2 can get everybody on the same project management page
9704,3727151,2018-02-23 17:00:00,"Super NtGet more infoEngadgetCriticUsersThe 16-bit aesthetic is the new vinyl. It taps into a growing vein of '90s nostalgia, and it also reflects a longing for a tactile past world that just predates full-scale digitization. Fat, colorful sprites represent an era when technology was still analogue and full of exciting possibilities. The Super Nintendo is as much an emblem of this retro near-futurism as it is a game machine. But boy, is it also a great game machine.Hence, nostalgia for the Super Nintendo is currently at its absolute peak. There are half a dozen or so clone consoles on the market and advanced emulators such as Higan that run with near-cycle perfection on high-end PCs. Then there's Nintendo's own incredibly popular SNES Mini. In short, there is no shortage of ways to play these classic games right now. All options have their strengths and drawbacks, but Analogue's new Super NT retro console easily blows them all out of the water, delivering sprites with pixel-perfect accuracy, zero lag and considerable polish.Engadget ScorePoorUninspiringGoodExcellentKeyProsConsCosts can quickly rise if you don't already have cartsNo way to output to a CRT monitor yetSummaryThe Super NT is a retro console that doesn't get lost in nostalgia. It's a sleekly designed box that offers near-perfect emulation while supporting HDMI output and wireless controllers. Nintendo's own Classic Mini SNES may be cheaper, but it doesn't come close to matching the quality of Analogue's effort. If you want a deep dive into the Super Nintendo's excellent library, you can't do better than this.For years, Analogue has been delivering high-end retro consoles that run original carts and deliver pixel perfection seamlessly to modern televisions. The Super NT is the culmination of these efforts: a 99-percent-accurate recreation of the Super Nintendo in a consumer-friendly package. Analogue achieves this magic via field-programmable gate array (FPGA). Nearly all purpose-built consoles, including Nintendo's own SNES Mini, run retro games through software emulation, which means they're using a program to simulate the original Super Nintendo. But FPGA is an integrated circuit that is coded to recreate the architecture of the original Super Nintendo hardware itself.To look at it another way, software emulation is like streaming a recording of a classic song while FPGA is playing the original vinyl on a modern turntable with digital outputs. For most consumers looking for a quick hit of nostalgia, software emulation is just fine. Many emulators deliver a mostly accurate and quality experience. But for enthusiasts seeking purity, software emulators just can't compare to what the Super NT's FPGA chip delivers.To be clear, the Super NT is not the ""perfect"" SNES experience for purists. The absolute gold standard is to locate an original Super Nintendo 1CHIP console (a rarer revision of the hardware that outputs video over RGB) and either wire it into a high-end CRT monitor or run it through an expensive Framemeister or Open Source Scan Converter in order to hook it up to a modern TV. It's a deeply nerdy experience that is time-consuming and space prohibitive, and it requires a strong knowledge of how old consoles work. But the Super NT is about as close to this gold standard as you could possibly get -- plus, it is a pretty piece of hardware, and it plugs in with only an HDMI cable. For anyone who has spent weeks tracking down a good CRT or tweaking the settings on a Framemeister, the act of plugging the Super NT into a 4K TV and receiving pixel perfection with the click of a button is a magical experience.As simple as it is to get started, the Super NT has a deep well of options that you can modify on the fly through a menu that has been cleverly programmed to run on the Super Nintendo CPU. You can adjust the resolution, the refresh rate and the aspect ratio. In my experience, 1080p output delivers the sharpest pixel-perfect image, and 720p is better suited for turning on scan lines (there are several options to customize your scan line thickness and improve brightness, which dulls when using scan line effects). Buffer-mode adjustments are available. Normally, the Super Nintendo runs at 60.09Hz, but with the zero-delay buffer option, you can adjust this to 60Hz, which removes any latency in the gaming experience (or there are options that more closely replicate the original experience).Other adjustments available to tinkerers include interlacing options; pseudo high-res blending, which simulates certain techniques that made use of CRT monitors to create special ""high-res"" effects; and 64-sprite mode, which enables more sprites on-screen and fixes flicker in certain classically problematic games. These modes will give hobbyists plenty to play with, though they only really apply to select games. In my experience, 64-sprite mode eliminated a lot of the notorious flicker from R-Type III. I suspect that in time, retroheads will discover all kinds of new applications of these features, and recommended-settings regimens for each game will begin circulating.It also works with all Super Nintendo peripherals (except light guns, which will require a CRT), such as Super Game Boy. And you can toggle regions and frequency to play PAL carts (though I didn't have any on hand to verify this). In short, as Analogue founder and CEO Christopher Taber told me, the Super NT was ""literally designed to be the end all, be all.""The range of visual options are stunning (and a tad overwhelming), but for me the real ""wow"" moment came from the sound. Strictly in auditory terms, switching between the SNES Mini and the Super NT is night and day. SNES audio has always been notoriously difficult to emulate on the software level, but the FPGA chip's hardware mimicry achieves the best achievable sound quality from games. It hits the difficult notes in Yoshi's Island or Final Fantasy III perfectly.The Super NT is suffused with other quality touches. The interface's font and colors are customizable, and even the dang LED power light is adjustable (rainbow throb is my jam)! In terms of product design, the console is wonderfully tactile. Made with a compact plastic shell a tad thicker than an original Super Nintendo and with a weighted base and rubber mat on the bottom, the console has heft and presence. I tested the black model, which felt perfectly at home with all the other sleek black boxes that live under a modern TV. The console feels good to touch, and touch it you will: Swapping carts is a tactile pleasure in its own right.And the carts are probably what will make your mind up here, as you need Super Nintendo carts to play on this console. While the SNES Mini comes with 21 games pre-installed (including many of the best), you will need to head to eBay, Craigslist or your mom's basement to hunt down your own carts to use with the Super NT. This will deter the casual nostalgia seeker. (However, the console does come with two pre-installed titles, Super Turrican: Director's Cut -- an extended edition of the original -- and Super Turrican 2. Both are wonderfully frenetic action games.)This fact belies the hidden costs of the console, one of its bigger drawbacks. You will likely need to buy not only carts but also a controller. You can plug in an original pad, but Analogue recommends the 8Bitdo SN30 wireless Bluetooth controller ($25) and Retro Receiver ($20), which are available in matching colors. The 8Bitdo feels very much like the original SNES controller and can be synced with other devices, such as PCs, Macs and even the Nintendo Switch -- it's a good standalone product in its own right but works extremely well with the Super NT.These hidden costs add up. The console itself is $190, but add on one or two 8Bitdo pads and shipping fees and you're looking at around $250 to $300. If you also need to build a library of carts, your total expenditure can swiftly hit $500. Stack this up against the SNES Mini, which delivers 21 top-shelf games and two controllers for $79, and it's clearly the premium option. But if you want to track down a SNES 1CHIP, that'll run you about $175 on eBay, and if you're going that far, you'll want to get a high-end CRT monitor. A 20-inch Sony PVM will set you back around $200. Considering this, the Super NT is the more affordable option.There's one other glaring drawback: The console currently only supports HDMI, so there is no way to output to a CRT monitor. Analogue says it will release an adapter that converts the digital signal out to RGB, component, S-Video and all those geeky options ""in a month or two."" For a device that is built on retro fidelity, the lack of analog outputs out of the gate is disappointing.So who is this console for? If you're still here and you're still interested, then chances are it's for you. It's for the hipster geek who loves retro games but not quite enough to track down a Sony PVM and a bunch of cables and rig up a 1CHIP SNES. Surprisingly, this is a bigger market than one would think. Demand has already far outstripped Analogue's expectations: Pre-orders sold out before release, and the next run of pre-orders may follow suit, so keep a close eye on the stock if you're on the fence.Personally, the Super NT has focused my attention on Super Nintendo games in a way that the SNES Mini simply couldn't. The play experience is far more accurate, but more importantly, the possibilities are far greater. Though I only tested the Super NT with about a dozen SNES games, Analogue says that it's tested it with practically all 2,200 SNES and Super Famicom games ever made.The SNES library is deep and contains some of the best games of all time. That library is well represented on the SNES Mini, but what grabs my interest are the games that for whatever reason didn't make Nintendo's cut. Chrono Trigger, Demon's Crest, Blackthorne, R-Type, even Uniracers -- a game that literally haunted my childhood with fever-dream nightmares of disembodied unicycles -- are all on the menu. For me, the Super NT represents the possibilities of a deep dive into the richest and most varied video gaming catalog in existence.The Super NT is a retro machine that somehow does not wallow in nostalgia -- it looks and feels like it belongs under a flat panel in 2018. The design is thoughtful, tasteful and unobtrusive. Analogue has miraculously achieved a rare alchemy: 99-percent-perfect Super Nintendo accuracy, with zero hassle. The nerdy stuff about cabling and Framemeisters is gone, and the distracting intangible factors that come along with nostalgic consoles have been nullified too. Who touched these controllers before, and why are they so gummy now? Did this yellowing console come from a smoke-filled home, and what was that like? Why is life so much more complicated now anyway? With the Super NT, these questions are moot, leaving the games to speak for themselves.",In search of pixel perfection with the Analogue Super NT
9705,3727153,2018-02-23 16:43:00,"Over the last few months, Google has been introducing lightweight ""Go"" editions of its popular apps, such as Gmail and Google Assistant. It's a concerted effort and catering to a traditionally underserved market in tech: Those who have older phones and who have purchased newer, but cheaper, phones. It's especially important in developing areas, where people might not be able to afford the newest, most powerful phones. Now Google will give those users hardware that is dedicated to and optimized for Android Oreo (Go Edition).",Android Go phones will be available soon
9706,3727245,2018-02-23 15:15:34,"There are two types of people in this world: those who know how to convert PDFs into Word documents and those who are indicted for money laundering. Former Trump campaign chairman Paul Manafort is the second kind of person.Back in October, a grand jury indictment charged Manafort and his business associate Rick Gates with a variety of crimes, including conspiring “to defraud the United States.” On Thursday, special counsel Robert Mueller filed a new indictment against the pair, substantially expanding the charges. As one former federal prosecutor told the Washington Post, Manafort and Gates’ methods appear to have been “extensive and bold and greedy with a capital ‘G,’ but … not all that sophisticated.”One new detail from the indictment, however, points to just how unsophisticated Manafort seems to have been. Here’s the relevant passage from the indictment. I’ve bolded the most important bits:Manafort and Gates made numerous false and fraudulent representations to secure the loans. For example, Manafort provided the bank with doctored [profit and loss statements] for [Davis Manafort Inc.] for both 2015 and 2016, overstating its income by millions of dollars. The doctored 2015 DMI P&L submitted to Lender D was the same false statement previously submitted to Lender C, which overstated DMI’s income by more than $4 million. The doctored 2016 DMI P&L was inflated by Manafort by more than $3.5 million. To create the false 2016 P&L, on or about October 21, 2016, Manafort emailed Gates a .pdf version of the real 2016 DMI P&L, which showed a loss of more than $600,000. Gates converted that .pdf into a “Word” document so that it could be edited, which Gates sent back to Manafort. Manafort altered that “Word” document by adding more than $3.5 million in income. He then sent this falsified P&L to Gates and asked that the “Word” document be converted back to a .pdf, which Gates did and returned to Manafort. Manafort then sent the falsified 2016 DMI P&L .pdf to Lender D.So here’s the essence of what went wrong for Manafort and Gates, according to Mueller’s investigation: Manafort allegedly wanted to falsify his company’s income, but he couldn’t figure out how to edit the PDF. He therefore had Gates turn it into a Microsoft Word document for him, which led the two to bounce the documents back-and-forth over email. As attorney and blogger Susan Simpson notes on Twitter, Manafort’s inability to complete a basic task on his own seems to have effectively “created an incriminating paper trail.”Y'all I'm dying, Manafort created an incriminating paper trail because he needed someone to help him convert a Word doc to PDF. pic.twitter.com/CTE4oV7zj0In Manafort’s defense, converting documents to and from Word could be easier. Not having tried it for a while, I attempted to transform my Word draft of this blog post into a PDF. I confess that I did fumble a bit at first (it’s been a while), but I eventually managed to get the job done. According to my stopwatch, the full ordeal took me 42 seconds. It involves a few steps, but there are plenty of accessible tutorials out there if you get lost.Changing PDFs back to editable Word documents, meanwhile, does get a little more complicated. Try it in Adobe Acrobat (via the “Save as Other” command under “File” on a Mac) and you’ll quickly be redirected to Adobe’s website and presented with a handful of subscription packages that will allow you to transform your documents. For as little as $2 a month, Adobe will allow you to convert PDF files to Word, Excel, and rich text formats. If this feels extortionate, there are also plenty of services online that promise to let you do the same thing for free, but—and, to be clear, I’m no financial genius—even people who are allegedly misreporting millions of dollars in income can almost certainly afford the budget option. Indeed, it’s probably a little safer, all things considered.What have we learned from all this? If you’re going to engage in some kind of complicated conspiracy, it’s probably a good idea to bone up on some basic computer skills first.",Manafort Left an Incriminating Paper Trail Because He Couldn’t Figure Out How to Convert PDFs to Word Files
9707,3727252,2018-02-16 19:45:00,"How to Sell a $300 Chocolate BarFollow the wine playbook.Email This ArticlePlease separate multiple addresses with commas. We won't share addresses with third parties.MessageSubscribe me to theAtlas Obscura NewsletterRemember to use the tongs to prevent melting any precious product. To’ak ChocolateIf you imagine an expensive bottle of wine, what comes to mind? You may have visions of a vineyard in France or a dusty cellar where centuries-old bottles are removed with care. Or perhaps you think of sloshing liquid poured in a rounded wine glass, where it is swirled, sniffed, and tasted. Whatever association comes to mind, most people would agree there’s no rush to consume an expensive wine.That’s an idea the wine world wants to instill in customers. Learning tidbits about a wine’s place of origin—or holding it up to the light—are crucial to building up the prestige of a high-end product. It’s with this sense of ritual and backstory that Jerry Toth, co-founder of To’ak Chocolate, hopes to elevate a traditionally cheap, sweet treat into the luxury market, with chocolate bars starting at $295.Several hundred dollars may seem like a lot to pay for one bar of chocolate. But according to Christopher Olivola, assistant professor of marketing at Carnegie Mellon University, a $295 price tag is also a selling point. Most customers believe that if something is expensive, it’s bound to be a better product (better ingredients; more attention to detail). In contrast, it’s impossible to convince someone that a one-dollar chocolate bar or five-dollar wine is the world’s best. This phenomenon is known as the price heuristic.But a high price is not enough, of course. Selling a luxury product—including Toth’s To’ak chocolates—requires good storytelling.Toth says you should ideally let the chocolate melt in your mouth rather than chew it. To’ak ChocolateWhen he presents his bars to customers, Toth describes the chocolate’s route from Ecuador to their hands. The story, which Toth printed in a 116-page booklet, begins with 5,300-year-old cocoa trees in Piedra de Plata, the valley in Ecuador where Toth sources the chocolate. He compares it to Bordeaux, France. The narrative then highlights 14 growers who hand-sort and shell the beans, which then go through no less than six phases of quality control.“The more effort and time it takes to hand-make something,” says Olivola, “the more people are willing to add a value to it.” Olivola points to Starbucks’s conversion of coffee as a ho-hum beverage to artisanal commodity as one example of this effect, which is known in psychology as the effort heuristic.Toth himself has a pretty good backstory. A graduate of Cornell University, he escaped the world of New York investment banking to explore Central and South America. He worked odd jobs for five years, until a job as foreign correspondent in Ecuador focused his attention on unsustainable development. He became convinced that politics wasn’t going to address those issues.A year later, he co-founded Third Millennium Alliance, a rainforest conservation foundation, with an Ecuadorian sustainability expert and an American ecologist. They created the Jama-Coaque Ecological Reserve, and there, they discovered semi-wild cacao trees whose fruits the group began making into chocolate—inside a bamboo house the group built by hand. The homemade bars led to the creation of To’ak and a tangible product Toth could export to promote his mission of saving the rainforest.Presentation matters. To’ak ChocolateWhether intentional or not, Toth leverages the effort heuristic when he discusses (and sells) the founding story of To’ak Chocolate. He doesn’t quote the price and walk away. He emphasizes the care put into each bar (weeks of work by hand). He provides historical background on cacao, which lets him weave in the message that his chocolate is rare. He keeps the storyline top of mind for customers who purchase To’ak by placing a single, roasted cacao bean in the center of each bar in altar-like fashion. And, of course, he holds tastings and educates people on how to appreciate and savor $300 bars of chocolate.Similar to champagne and wine, To’ak Chocolate is meant to be smelled, savored, and meditated on. Toth’s team recommends a quiet, smell-free room. Ideally, tasters should open the packaging, breath in the bar’s aroma, and pick it up with a provided pair of tongs that prevent the melting of any precious material. From there, Toth suggests placing a square of chocolate into your mouth, breaking it into small pieces with your teeth, and letting it melt. No chewing.Rituals like these are common among artisanal products, and almost anything can be pushed into this category, says Olivola. “Rituals bring people together. Throughout culture, people partake in fun or painful rituals. It makes them feel like they can control the outcome by taking part.”If this all seems like a bit much—or not even to sell hundred-dollar chocolate—consider wine and wine tasting, which evolved from humble roots.Note the bean placed in the center of the bar. To’ak Chocolate“The criteria for a good wine was [once] dramatically different than it is today,” says Kolleen Guy, associate professor of history at University of Texas at San Antonio. Initially, wine was judged using elements of Galenic medicine (hot, cold, wet, dry) to balance foods, and, into the 18th century, wine tasting meant a worker in a warehouse or cellar engaging in quality control. Poetic language such as green, cherry blossoms, and citrus only arose in the 19th century as the Industrial Revolution created a growing middle class and more emphasis on good taste.Similarly, Rachel ​Speckan, an advanced sommelier, points out that the rise of wine as an intellectual pursuit only began in earnest in the 1970s. She cites the founding of the Wine & Spirits Education Trust at the time, as well as the first master sommelier examinations by the United Kingdom’s Court of Master Sommeliers. Both remain leading institutions. And while visiting a winery is now common, such intense interest in a wine’s origins and the viticulturists is relatively modern—a contrast to the traditional business-to-business nature of the wine industry.Chocolate is arguably headed on the same path: While the industry was once mostly milk chocolate marketed to kids, it’s increasingly common for adults to buy expensive truffles and visit artisanal chocolate factories. To’ak just represents a (hopeful) leap into the future.“The tasting procedure is something that has been around for a long time, and originated in the wine industry,” says Toth, who once aspired to become a sommelier and openly concedes to following the industry’s playbook. “[Chocolate tasting] might be a new concept, but we’re not re-inventing the wheel.”The book explaining the chocolate’s story is over 100 pages. To’ak ChocolateBut an important part of that playbook is authenticity, and the feeling that the stories and the process is not cynical. The success of To’ak Chocolate will likely be determined not by the effort heuristic and rituals, but by the sincerity of Toth and his team.“What we’re trying to do is bring respect back to [the cacao bean], which was once so highly regarded that it was used as currency by civilizations around the world,” says Toth. “Theobroma, the Latin name given to cacao by early Spanish explorers, means ‘Food of the Gods.’”Toth describes To’ak Chocolate as a way to educate the world about the rich history of cacao, to pay farmers the right price for their work, and to rescue chocolate from being relegated to a cheap, check-out impulse buy. And when you think of it that way, $300 seems like a small price to pay to help restore glory to Ecuador’s cacao tree.Stay in Touch!No purchase necessary. Winner will be selected at random on 03/01/2018. Offer available only in the U.S. (including Puerto Rico). Offer subject to change without notice. See contest rules for full details.Add Some Wonder to Your InboxEvery weekday we compile our most wondrous stories and deliver them straight to you.",How to Sell a $300 Chocolate Bar
9708,3727490,2018-02-23 17:53:22,"About TNWTNW SitesGmail now has analytics, and it’s awesomeI’m going to venture a guess and say you have a Gmail account. Chances are, you have more than one. I say this with confidence knowing Gmail currently sports more than 1 billion monthly active users, up from a relatively paltry 425 million back in 2012. It’s free, it’s fast, it’s reliable, it offers ample storage for most users, and it’s relatively easy to organize, so it’s a prime choice for both business and individual users.But there are some things that keep Gmail from being a perfect system. While there’s an intuitive search feature (that can be modified with search operators) to help you find your long-lost messages, there’s no real way to gauge how you’re using Gmail – even simple things like how many emails you sent and received yesterday are difficult to figure out. That is, until now.The recently released Gmail Metrics provides visualized analytics to help users understand email usage, with hundreds of data points and customizable visualizations that help users interpret and react to that data.I signed up for the free trial to check it out, and found the following data points particularly interesting:Emails sent and receivedThe tool shows how many emails I send and receive each day, or within any given date range, so I calculate how much of my time and attention (or my employees’) is spent on email.Email breakdown by day and timeThe tool provides a visualized breakdown of hourly and daily use of email. Interestingly, I found that I send the most emails on Tuesdays, but receive the most emails on Wednesdays. As is typical, my email activity hits a valley around the weekend.Top senders and recipientsThe tool breaks down your top senders and recipients, helping you clearly see the distribution of your inbound and outbound messages. I found this helpful in identifying clients who take up the most of my time.Stats on conversations and threadsChances are, you have a few running conversations currently sitting in your inbox. The tool helps you understand how those conversations typically unfold; for example, how many conversations do you initiate, and how many are you roped into? How many emails are exchanged in a typical email thread?Word countsInterestingly, my average sent email contains 34 words, and the average email I receive contains 77 words. This is useful for gauging how my responses compare to other people’s responses.Response timesThe app also tracks how long it takes you to respond to inbound emails, and how quickly people respond to yours.How email analytics data can be usedSo how can typical Gmail users leverage email analytics data?These are some of the most common applications:Employee tracking and monitoring — In some ways, Gmail Metrics belongs in the same ranks as Toggl and TimeCamp, apps that actively monitor employee time expenditure and performance. The app enables users to identify top email performers, as well as problem workers who don’t use email as efficiently as they should.Personal productivity improvement — Some users will rely on the visualized insights for their own personal use, measuring how much time they spend on various email-related activities so they can improve in the future.Project ROI — Still others will use the information to determine how much time they spend on each project relative to how much money they have coming in from each one. If one client needs twice as much attention as another, but pays the same money, you have an asymmetrical ROI that needs to be corrected.As this is just the first iteration of the tool, it’s likely that there’s more to come. Google Analytics, a similar tool for measuring and analyzing web traffic, has undergone significant evolution over the years.With email being such a critical function for modern businesses (and individuals), this tool has the power to reshape our professional lives and potentially save hours of work each week. Perhaps soon, every method of communication we use on a daily basis can be analyzed so objectively.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.","Gmail now has analytics, and it's awesome"
9709,3727491,2018-02-23 17:31:12,"About TNWTNW SitesNew Twitch fund offers grants to female streamersTwitch today announced a collaboration with nonprofit organization 1,000 Dreams Fund (1DF) to offer a special grant for female streamers, as well as a series of of charity streams to raise money for the nonprofit in question.The BroadcastHER grant is designed to support female Twitch streamers, is worth between $500 to $2,000, and is awarded to two people every academic semester. To be eligible, applicants must be enrolled in high school or college, and have a demonstrable financial need.According to 1DF, the grant is intended to pay for “travel to gaming conventions, educational conferences, hardware upgrades, and instructional programs related to creative or artistic pursuits.”As part of its new collaboration and to celebrate Women’s History Month, Twitch is also partnering with ten streamers for a month of charity streams called Stream for Dreams, which begins on March 1.1DF offers grants to high school- and college-aged women who want to pursue education or interests that school costs don’t allow for. In the case of According to its website:There are more than 20 million high school and college-age girls in the U.S., each of whom has her own hopes and dreams. But with parents’ limited expendable income and the high cost of college, these dreams can be out of reach. The 1,000 Dreams Fund’s model is to empower YOU to change that.”Twitch, for all its virtues, isn’t always the most welcoming place for female streamers. Between sexual harassment and hostility towards any display of skin, it can be a little tricky to be a woman on Twitch.1DF CEO Christie Garton expressed hope that the campaign will help young women expland their opportunities on Twitch:As an organization committed to all dreams, we are excited to partner with members of the Twitch community on this amazing new campaign. The initiative will not only raise critical funds, but will also boost awareness around the shortage of support for these creative young women in the digital broadcasting space.All funds raised through the event will fund the BroadcastHER grant, with Twitch offering merchandise to supplement donations. The complete list of the broadcasters, all popular female streamers — will be announced later this month.TNW Conference 2018 features a track dedicated to this kind of self-improvement and growth through tech called Project YOU. For more information, check out our event page.",Twitch collaborates with nonprofit to offer grants to female streamers
9710,3727492,2018-02-23 17:20:01,"About TNWTNW SitesCryptocurrency News Feb 23 – It’s the weekend babyYou’re in a desert walking along in the sand when all of the sudden you look down, and you see a Bitcoin block, with a mining pool having trouble mining it. You reach down, you tell them you won’t help them mine the block. The miners still can’t successfully find the block solution, their oily backs baking in the hot sun, posting curse words on Discord desperately trying to mine that block, but they can’t, not without your help. But you’re not helping. Why is that?What is a Bitcoin?Describe in single words, only the good things that come into your mind about your cryptocurrency.Cryptocurrency…I’ll tell you about my cryptocurrencySo Bitcoin fees are down, which is good news for anyone pretending to use Bitcoin for transactions in place of fiat currency. However it’s not necessarily good news for the value of Bitcoin because there are roughly half the amount of transactions. The lord giveth and the lord taketh away. SegWit will allegedly come along and help people pretend that Bitcoin can replace fiat currency soon, I’m sure of it.All these Bitcoins will be lost…like Raiblocks…on BitGrail…If you die your Bitcoins die with you, or at least they’ll be stuck in a wallet because you decided to use 8-factor authentication that involves someone cuckolding you to access your 40 BTC horde. When I die? I’m gonna write my private keys on 24 pigs, and then release two more pigs as well as all those pigs at my funeral. Who will be the lucky person to catch every pig, write down the phrases and receive my 0.0005 BTC? Find out soon!Fork the systemTurns out there are all sorts of forks right now, like something called Bitcoin Private (?) and ZClassic (?) or whatever. There’s also in this article something called Onology forking from Ripple that I can literally find nothing about. Also Monero is forking? Ethereum Classic? This is all getting a bit much for me. I know they all mean well but I have to wonder what they’re doing. I mean it takes 40 years to send one transaction on Bitcoin I guess. Oh well, get that money guys.",Cryptocurrency News Feb 23 - It's the weekend baby
9711,3729796,2018-02-23 18:00:00,"Not even IBM is sure where its quantum computer experiments will leadDespite the hype and hoopla surrounding the burgeoning field of quantum computing, the technology is still in its infancy. Just a few years ago, researchers were making headlines with rudimentary machines that housed less than a dozen qubits -- the quantum version of a classical computer's binary bit. At IBM's inaugural Index Developer Conference held in San Francisco this week, the company showed off its latest prototype: a quantum computing rig housing 50 qubits, one of the most advanced machines currently in existence.Quantum computing -- with its ability to calculate and solve algorithms in parallel, at speeds far faster than conventional computers -- promises to revolutionize fields from chemistry and logistics to finance and physics. The thing is, while quantum computing is a technology for the world of tomorrow, it hasn't yet advanced far enough for anyone to know what that world will actually look like.""People aren't going to just wake up in three or four years, and say, 'Oh okay, now I'm ready to use quantum, what do I have to learn,'"" Bob Sutor, VP of IBM Q Strategy and Ecosystem at IBM Research, told Engadget.These systems rely on the ""spooky"" properties of quantum physics, as Einstein put it, and their operation is radically different from how today's computers work. ""What you're basically doing is you're replacing the notion of bits with something called qubits,"" Sutor said. ""Ultimately when you measure a qubit it's zero or one, but before that there's a realm of freedom of what that can actually be. It's not zero and one at the same time or anything like this, it just takes on values from a much, much larger mathematical space.""The basic logic gates [AND, OR, NOT, NOR, etc], those gates are different for quantum,"" he continued. ""The way the different qubits work together to get to a solution is completely different from the way the bits within your general memory works."" Rather than tackling problems in sequence, as classical computers do, quantum rigs attempt to solve them in parallel. This enables quantum computers to solve certain equations, such as modeling complex molecules, far more efficiently.This efficiency, however, is tempered by the system's frailty. Currently, a qubit's coherence time tops out at 90 milliseconds before decaying. That is, if a qubit is designated as a 1, it'll only remain a 1 for 0.09 seconds. ""After that all bets are off. You've got a certain amount of time in which to actually use this thing reliably,"" Sutor said. ""Any computations you're going to do with a qubit have to come within that period.""As such quantum computers are highly sensitive to interference from temperature, microwaves, photons, even the electricity running the machine itself. Sutor said, ""With heat you've got lots of electrons moving around, bumping into each other,"" which can lead to the qubit's decoherence. That's why these rigs have to be cooled to near absolute zero on order to operate.""Outer space in the shade is between two and three degree Kelvin,"" Sutor explained. ""Outer space is much too warm to do these types of calculations."" Instead, the lowest levels of a quantum computer rig, where the calculations themselves take place, exist at a frosty 10 millikelvin -- a tenth of a degree above absolute zero. So no, Sutor assured Engadget, we probably shouldn't expect desktop quantum computers running at room temperature to exist within the next few decades -- perhaps even within our lifetimes.Surprisingly, these systems are fairly energy efficient. Aside from the energy needed to sufficiently cool the system for operation (a process that takes around 36 hours) IBM's 50-qubit rig only draws 10 to 15 kilowatts of power -- roughly equivalent to a standard microwave oven.So now that IBM has developed a number of quantum computer systems ranging from 5 to 50 qubits, the next challenge is figuring out what to do with them. And that's where the company's Q network comes in. Last December, IBM announced that it's partnering with a number of Fortune 500 companies and research institutes -- including JPMorgan Chase, Samsung, Honda, Japan's Keio University, Oak Ridge National Lab and Oxford University -- to suss out potential practical applications for the technology.Learning centers like Keio University also act as localized hubs. ""We in IBM research, while we have a large team on this, we can't work with everybody in the world who wants to work on quantum computing,"" Sutor explained. These hubs, however, ""can work with local companies, local colleges, whomever to do whatever. They would get their quantum computer power from us, but they would be at the front lines."" The same is true for Oakridge National Lab, Oxford University and the University of Melbourne.What's more, the company has also launched the IBM Q experience which allows anyone -- businesses, universities, even private citizens -- to write and submit their own quantum application or experiment to be run on the company's publicly available quantum computing rig. It's essentially a cloud service for quantum computations. So far more than 75,000 people have taken advantage of the service, running more than 2.5 million calculations which have resulted in more than two dozen published research papers on subjects ranging from quantum phase space measurement to homomorphic encryption.But while the public's interest in this technology is piqued, there is a significant knowledge gap that must be overcome before we start to see quantum applications proliferate the way classical programs did in the 1970s and '80s. ""Let's say in the future you're running investment house types of calculations [similar to the financial risk applications that JP Morgan is currently developing],"" Sutor points out, ""there are big questions as to what those would be, and what the algorithms would be. We're way too early to have anything determined like that, even to the extent of knowing how well [quantum computing] will be applicable in some of these other areas.""The entry point for writing programs is a challenge too. For classical computers, it's as simple as running a compiler. But there's not yet such a function for quantum computers. ""What does it mean to optimize a quantum program knowing that this completely different from the model that's in your phone?"" he queried.Another challenge that must be overcome is how to scale these machines. As Sutor points out, it's a simple enough task to add qubits to silicon chips, but every component added, increases the amount of heat generated and the amount of energy needed to keep the system within its operational temperature boundaries.So rather than simply packing in more and more qubits and setting off a quantum version of Moore's Law, Sutor believes that the next major step forward for this technology is quality over quantity. ""Having 50 great qubits is much more powerful than having 2,000 lousy ones,"" he quipped. ""You don't want something very noisy that you're going to have to fix,"" but instead research should focus on improving the system's fidelity over increasing the qubit count.Gallery: IBM Q | 12 PhotosBut even as quantum technologies continue to improve, there will still be a place in the world of tomorrow for classical computers. ""Don't think of quantum as a wholesale replacement for anything you do,"" Sutor warned. ""The theory says that you could run any classical algorithm on a quantum rig but it would be so glacially slow because it's not designed to run those types of products.""Instead, Sutor prefers to think of the current crop of quantum technologies as an accelerator. ""It does certain things very quickly, it does some things we don't know how to do well classically... and so it'll work hand in hand that way.""And if you're waiting for today's quantum computers to be able to compete with modern supercomputers anytime soon, you shouldn't hold your breath. ""We need to get several orders of magnitude better than we are now to probably move into that period where we're solving the really super hard problems,"" he said.""Just to be very clear,"" Sutor concluded, ""this is a play for the 21st century... this is I think going to be one of the most critical computing technologies for the remainder of the century, and major breakthroughs will occur all along the line. Many of which we can't even imagine right now.""Andrew has lived in San Francisco since 1982 and has been writing clever things about technology since 2011. When not arguing the finer points of portable vaporizers and military defense systems with strangers on the internet, he enjoys tooling around his garden, knitting and binge watching anime.",Not even IBM is sure where its quantum computer experiments will lead
9712,3730055,2018-02-23 15:41:58,"‘Annihilation’ is an unsettling science fiction fever dream0Writer-director Alex Garland has said that his adaptation of Annihilation isn’t a straightforward retelling of the book — instead, he said, it’s “true to my subjective response to the novel.”That’s a fair warning: The movie’s details don’t really match the book, which was written by Jeff VanderMeer. What carries over, however, is a sense of dread and unease; readers of the novel and watchers of the film will both feel a pervasive wrongness that they can’t quite put their finger on.As the movie begins, a biologist played by Natalie Portman has returned from Area X, a mysterious patch of wilderness hidden behind a barrier called The Shimmer. The expedition’s other members have not returned, and the biologist seems uncertain about whether any of them might still be alive.The rest of the film (which opens in US theaters today, before going live on Netflix internationally) fills in the details of what transpired beyond The Shimmer, and of why Portman’s character felt compelled to join the expedition in the first place.Latest Crunch ReportIn some ways, Garland’s plot is more straightforward than VanderMeer’s. The film’s scientists follow a more coherent plan, with more forward motion, than they do in the book, and the monsters that hunt them are more conventionally scary.But very little about the film feels conventional. The storyline seems far less important than the long conversations about self-destruction and change, the flashbacks to Portman’s marriage to a soldier played by Oscar Isaac, and the eerie, prismatic quality of the light.I’ll admit that Garland has never been a particularly subtle writer. Characters in 28 Days Later, Sunshine and Dredd (all films he wrote) tend to argue about the movie’s big ideas quite openly, just as they do in his directorial debut, Ex Machina. That’s true in Annihilation as well, though there’s less argument here and more brooding.What makes the movie feel exciting, even daring, is the visual form that Garland and his team have found for those ideas. The plants and animals of Area X have been transformed, different species and features mixing together, sometimes beautifully, sometimes disgustingly, often both — Annihilation has its fair share of monsters and corpses, and they’re some of the best-looking monsters and corpses I’ve ever seen.Without giving too much away, I can say that by the film’s end, we do get some explanations for the weird stuff we’ve been seeing. The explanations are fine, but the way we discover them —walking past a grove of crystalline trees, through a tangle of pale white branches and diving into a dark hole, where things get really weird — is far more memorable.CrunchbaseOverviewNetflix is an online platform that enables user to watch TV shows and movies on smart TVs, gaming consoles, PCs, Macs, mobiles, tablets, and so on. It provides its services under three segments: international streaming, domestic streaming, and domestic DVD. The network enables members to access and view more than one billion hours of TV shows and movies per month, including Netflix original series. …Not everyone will enjoy Annihilation, and not just because it’s a strange movie. It also has some real flaws.For example, even in the book, a couple of the expedition members felt rather sketched in, but I still wish they’d been more memorable here, and that the scheming psychologist who leads the team had made the transition with more of her story intact. Isaac, meanwhile, gives his character a convincingly haunted quality — but for some reason, he’s trying to pull off an erratic Southern accent that becomes a huge distraction. And ultimately, I’m not sure that the revelations in the film’s very last scene felt justified.But I wasn’t thinking about any of that as I left my screening. Instead, for at least a few minutes, the world around me felt genuinely unsettled. The light seemed a little off, and I worried about strange mutations lurking under the skin of my fellow subway passengers. Area X was closer than I’d realized.",‘Annihilation’ is an unsettling science fiction fever dream
9713,3730134,2018-02-23 18:34:46,"TNW SitesAre Asimov’s Laws of Robotics still good enough in 2018?It’s been 76 years since renowned science fiction author Isaac Asimov penned his Laws of Robotics. At the time, they must have seemed future-proof. But just how well do those rules hold up in a world where AI has permeated society so deeply we don’t even see it anymore?Originally published in the short story Runaround, Asimov’s laws are:A robot may not injure a human being or, through inaction, allow a human being to come to harm.A robot must obey the orders given to it by human beings, except where such orders would conflict with the First Law.A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.For nearly a century now Asimov’s Laws seemed like a good place to start when it comes to regulating robots — Will Smith even made a movie about it. But according to the experts, they simply don’t apply to today’s modern AI.In fairness to Mr. Asimov, nobody saw Google and Facebook coming back in the 1940s. Everyone was thinking about robots with arms and lasers, not social media advertising and search engine algorithms.Yet, here we are on the verge of normalizing artificial intelligence to the point of making it seem dull — at least until the singularity. And this means stopping robots from murdering us is probably the least of our worries.In lieu of sentience, the next stop on the artificial intelligence hype-train is regulation-ville. Politicians around the world are calling upon the world’s leading experts to advise them on the impending automation takeover.So, what should rules for artificial intelligence look like in the non-fiction world?According to a report published this week by Cambridge Consultants, titled “AI: Understanding And Harnessing The Potential,” there are five key areas that rules for AI should address:Regardless of the way in which rules are set and who imposes them, we think the following principles identified by various groups above are the important ones to capture in law and working practices:Responsibility: There needs to be a specific person responsible for the effects of an autonomous system’s behaviour. This is not just for legal redress but also for providing feedback, monitoring outcomes and implementing changes.Explainability: It needs to be possible to explain to people impacted (often laypeople) why the behaviour is what it is.Accuracy: Sources of error need to be identified, monitored, evaluated and if appropriate mitigated against or removed.Transparency: It needs to be possible to test, review (publicly or privately), criticise and challenge the outcomes produced by an autonomous system. The results of audits and evaluation should be available publicly and explained.Fairness: The way in which data is used should be reasonable and respect privacy. This will help remove biases and prevent other problematic behaviour becoming embedded.You’ll notice there’s no mention of AI refraining from the willful destruction of humans. This is likely because, at the time of this writing, machines aren’t capable of making those decisions for themselves.Common sense rules for the development of all AI needs to address real-world concerns. The chances of the algorithms powering Apple’s Face ID murdering you are slim, but an unethical programmer could certainly design AI that invades privacy using a smartphone camera.This is why any set of rules for AI should focus on predicting harm, mitigating risk, and ensuring safety is a priority. Google, for example, has guidelines set for dealing with machines that learn:We’ve outlined five problems we think will be very important as we apply AI in more general circumstances. These are all forward thinking, long-term research questions — minor issues today, but important to address for future systems:Avoiding Negative Side Effects: How can we ensure that an AI system will not disturb its environment in negative ways while pursuing its goals, e.g. a cleaning robot knocking over a vase because it can clean faster by doing so?Avoiding Reward Hacking: How can we avoid gaming of the reward function? For example, we don’t want this cleaning robot simply covering over messes with materials it can’t see through.Scalable Oversight: How can we efficiently ensure that a given AI system respects aspects of the objective that are too expensive to be frequently evaluated during training? For example, if an AI system gets human feedback as it performs a task, it needs to use that feedback efficiently because asking too often would be annoying.Safe Exploration: How do we ensure that an AI system doesn’t make exploratory moves with very negative repercussions? For example, maybe a cleaning robot should experiment with mopping strategies, but clearly it shouldn’t try putting a wet mop in an electrical outlet.Robustness to Distributional Shift: How do we ensure that an AI system recognizes, and behaves robustly, when it’s in an environment very different from its training environment? For example, heuristics learned for a factory workfloor may not be safe enough for an office.The future of AI isn’t just a problem for companies like Google and Cambridge Consultants though, as machine learning becomes a part of more and more devices — including the majority of smartphones and computers — its effects will be exacerbated. Unethical codes could propagate in the wild, especially since we know that AI can be developed to create better algorithms than people can.It’s clear that the regulatory and ethical problems in the AI space have little to do with killer robots, with the exception of purpose-built machines of war. Instead governments should focus on the dangers AI could pose to individuals.Of course, “don’t kill humans” is a good rule for all people and machines whether they’re intelligent or not.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",Are Asimov's Laws of Robotics still good enough in 2018?
9714,3732821,2018-02-23 20:00:00,"The best smart speakers for music fansIf you're a music fan, the first wave of smart speakers was probably a disappointment. While Alexa and Google Assistant have definitively proven they have a place in the home, the first Echo and Google Home devices were unimpressive when it came to actually playing music. They did the job in a pinch, and being able to command Spotify with your voice is a killer feature, but many longed for better-quality audio.Fortunately, that call has been answered. In the past six months, Sonos, Google and Apple have all released music-first speakers with voice assistants built in. There's no doubt that the Google Home Max, Alexa-powered Sonos One and Apple HomePod all sound far better than just about any other voice-powered speakers out there. If you value audio quality above all else (and have about $400 to spend), what's the right smart speaker for you? Let's break it down.How we testedFor the purposes of this story, we compared a single HomePod ($350), two Sonos One speakers ($350 total) paired in stereo and a single Google Home Max ($399). While the prices are in the same range, you'll get different speaker components with each set. Each Sonos One includes a single tweeter and one mid-woofer to cover both mid-range frequencies and bass. The Home Max answers with two tweeters and two large 4.5-inch woofers. The HomePod has perhaps the most unusual arrangement: seven total tweeters (each with its own amplifier), along with a single woofer that points straight up.To compare the speakers, we set them up in the same room, made sure each was running the most current version of its software and, in the case of the Sonos One, ran the TruePlay tuning software, which adjusts audio quality for the room and placement of the speakers. The HomePod and Home Max both tune themselves automatically; there's no way to control that part of the audio equation. In all cases, we left EQ settings on default. Finally, we used an Apple Music subscription for the HomePod and Sonos One, while Google Play Music provided audio to the Home Max. Song choices trended toward modern rock, indie and pop, although we sampled songs from every decade since the 1960s.Lastly, although I did most of the listening, a number of other Engadget editors lent their ears and feedback to this review.Audio qualityThere wasn't a clear, unequivocal winner among the three speakers tested; rather, each speaker showed its prowess in different situations. Chris Ip, our features editor, did a comparison listen between all three speakers and preferred the HomePod's rendition of Beyoncé's ""Countdown."" Specifically, Ip is a fan of prominent bass, and the HomePod's woofer elevated it over the Sonos One. The Home Max also provides strong bass, but it didn't reproduce mids and highs with the same precision.That was the story across the board with the Home Max. Its strengths come from two places: bass and volume. At louder volumes (think 75 percent or higher), it sounds better than a pair of Sonos One speakers; the lack of bass means the One ends up sounding shriller than the Home Max when you crank it up. But both the HomePod and Sonos One offered better overall clarity across the range of songs we tested. The bass doesn't feel as well regulated on the Home Max; while the HomePod does a great job of adjusting bass levels to match the room and the speaker's overall volume level, the Max's overall sonic picture is muddier and less distinct than the competition.As mentioned, the Sonos One is the speaker most lacking in bass here; pairing two of them together helps a bit, but it just doesn't reproduce the physical, table-rattling, chest-thumping sounds you can coax from the Home Max and HomePod. However, spending $350 on a pair of Sonos One speakers opens up a number of audio experiences that just aren't possible with the other speakers we looked at. By virtue of being two separate speakers you can place anywhere you want, the pair of Ones naturally did a far better job of reproducing a wide stereo soundstage. The Home Max and Home Pod do a better job of this than they should, but it's still not the same as having true stereo separation. And if you're more interested in multi-room audio, two One speakers will give you that option for a lot less money than getting multiple Home Maxes or HomePods (which don't even have stereo pairing or multi-room audio capabilities yet, though Apple says its speaker will this year).There wasn't a definitive winner between the paired Sonos One speakers and the HomePod. When listening to Metallica's ""Master of Puppets,"" I preferred how the HomePod added a nice low end to the frantically chugging guitars; the One felt like it lacked the power needed to make the song hit home, particularly at higher volumes. But Sonos did a great job at reproducing the psychedelic soundscape of Pink Floyd's ""Shine On You Crazy Diamond."" For plenty of other songs, it was truly difficult to pick a winner. Executive editor Dana Wollman did a blind test and couldn't decide which speaker did a better job playing back John Coltrane's version of ""My Favorite Things,"" while senior editor Kris Naudus was similarly torn picking between the HomePod and Sonos when listening to ""Cherry Tulips"" by Headlights.Along with senior editor Chris Velazco (who wrote our HomePod review), I spent hours bouncing songs back and forth between the speakers -- everything from the swirling trip of Beck's ""Colors"" and the continually shifting soundscape of the Radiohead classic ""Paranoid Android"" to Carly Rae Jepsen's over-the-top hit ""Cut to the Feeling"" and the electronic glitches of ""Such Great Heights"" by the Postal Service. Throughout our listening, we were never able to fully write off either the HomePod or the Sonos One.Ultimately, though, I give a slight edge to the HomePod -- it has significantly better bass reproduction, but I wouldn't describe it as an overly bass-heavy speaker. It just makes it a lot easier to hear every aspect of the song the way it was intended to be heard, bass included. The Sonos One does a lot with lesser specs, and pairing the speakers up in stereo makes for a truly great listening experience. But to my ears, the HomePod more often had the edge.Everything elseIf music were our only focus, we'd be done -- but even if sound quality is your foremost concern, there are still other things to consider before making a decision. Specifically, each comes with a voice assistant that can be used to control audio playback, but Siri, the Google Assistant and Alexa can do much more than that. Additionally, each speaker supports different music services in different ways, so that's another thing to take into account.Plenty has been said about Siri's limitations as a personal assistant on the HomePod. Ditto for the device's reliance on Apple Music. But I think Siri holds its own in a number of situations, specifically finding music. Telling it to just play some music brings up a station populated primarily with music in your library, ensuring you start off with tunes you'll recognize and enjoy. Siri does a good job of fielding queries like ""Play me some rock music from the '70s,"" ""Play popular songs from the last decade"" or something as simple as ""Play me new music I'll like.""Initially, the Google Assistant did a horrible job of answering the same questions on the Home Max using Google Play Music. When I asked it to play rock music from the '60s, it played a compilation album called Rock and Roll Instrumentals: 50's–60's that was populated with terrible covers of songs from that period. Eventually, I figured out that I could ask it to play Google's custom radio stations -- just saying something like ""Play a station featuring songs from the '80s"" would do the trick. Most users aren't going to realize that, initially.Combined, Alexa and Spotify did a solid job answering such requests by pulling up playlists from the service's massive collection. But Siri still gets the edge with music commands; it add songs to playlists or your library as a whole, and you can tell it that you ""like"" whatever song is playing to improve Apple Music's recommendations. Neither the Google Assistant nor Alexa was able to add songs to specific playlists or my music library for either service, though you can tell Google to add a thumbs-up or thumbs-down rating to songs while using Play Music. It's possible Amazon Music is better supported through Alexa, but far more people are likely to use Alexa and the Sonos One with Spotify.Siri's advantage dwindled once we moved on to other tasks. Overall, the Google Assistant was best at answering general knowledge queries, and it also does a better job of recognizing context in follow-up questions. For example, I can ask who the starting quarterback for the New England Patriots is and then ask where ""he"" went to college, and it'll know I'm still talking about Tom Brady. Siri and Alexa could also answer these questions, but they didn't have as much context awareness.Setting alarms, adding reminders and checking the weather all worked equally well, but Siri falls down when setting timers, because you can set only one at a time. Siri also can't add things to a calendar, even though it can do so on iOS devices. Both the Google Assistant and Alexa support calendars, but Alexa ultimately wins because it supports multiple Google calendars (including Google's own G Suite) as well as Microsoft Outlook and Apple iCloud. Strangely enough, Siri is the only assistant that can take notes; it adds them to the Mac/iOS notes app.Google wins when it comes to using the Home Max as a speakerphone; you can initiate calls with anyone in your Google address book just by asking. The HomePod can take only calls that you start on an iPhone, while Alexa on the Sonos One can't handle calls or messages at all; those features are restricted to Amazon's own hardware for now. Google can't send text messages either, but Siri can. Of course, all of these Siri features are limited to one account, and anyone with access to the speaker can send messages or have the speaker read your texts back to them. Apple really needs to implement some better security features around that, and quickly.All three assistants work with a pretty huge range of smart home devices. If you're thinking of using these devices to control lights, thermostats and more, just do the research first and make sure they're supported by Alexa, Google Assistant or Apple's HomeKit. But at this point, just about all the major players work across all three platforms.Ultimately, Siri's issues weren't entirely a deal-breaker for me; I don't have any smart home devices yet, and I don't use voice assistants for much beyond basic interactions. But it's hard to ignore Google and Amazon's lead here. And if you can't choose which virtual servant you prefer, Sonos has promised it'll add support for Google Assistant on the One. Right now, Alexa is the only option, but if you prefer Google you'll be able to just switch over. Additionally, the One will work with AirPlay 2 when Apple releases it. So while you can't currently control Apple Music on the Sonos One with your voice, AirPlay 2 should enable owners to talk to Siri on an iPhone to play music over the One.While the Sonos/Alexa combo supports only a handful of music services with voice control, Sonos' own controller app lets you use virtually any music service you can think of. With the Home Max, you're limited to a small set of services with voice control; after that, you can send music via Google's Cast technology through supported apps, or just use Bluetooth or a 3.5mm audio cable. Finally, the HomePod works with Apple Music or iTunes purchases almost exclusively, as we've all heard. However, in all cases, you can use basic voice commands to get some additional functionality out of services that aren't supported by these speakers.You can AirPlay songs from Spotify and other services to the HomePod, and then use voice commands to skip tracks, play, pause and change volume. With the Sonos One, you can start playing songs from any service using the controller app on your phone or computer, and then use Alexa voice commands to play, pause, adjust the volume or skip tracks in the queue. And the Home Max offers the same commands when you're casting audio from other apps.Wrap-upThere's a lot to like about all of these speakers, even Apple's locked-in HomePod. For most potential buyers, any of these will likely be a huge upgrade over using your laptop, pack-in headphones or a battery-powered Bluetooth speaker to listen to tunes. Of the three options we tested, a pair of Sonos One speakers is the best choice for most people. Despite a relative lack of bass, the One consistently reproduces songs clearly, and having two speakers makes for a much better stereo experience. I personally prefer the Google Assistant, but Alexa is a strong voice option. And besides, the One will support Google before long anyway.That said, if you're a fan of bass, both the HomePod and the Home Max are worth considering. If you're deep in the Google ecosystem and use Google's own services for music, the Home Max won't disappoint. And the millions of people paying for Apple Music will appreciate the HomePod's excellent fidelity and Siri's general smarts when it comes to finding music, even if Siri isn't as good an assistant as Google and Alexa.But for the millions of Spotify users who want to talk to their speakers and get excellent audio quality, the Sonos One delivers. It sounds great now, and Sonos promises to keep improving it over time. Whether that's more music services or the forthcoming Google Assistant support, buyers should feel confident the speaker will actually get better over time. And they're a great first piece in a bigger, multi-room audio setup as well. A pair of Sonos Ones should delight all but the most die-hard Apple or Google fans -- and even those people would probably find a lot to like with these speakers.Nathan is the deputy managing editor at Engadget, keeping track of the site's daily news operations and covering Google, Apple, gaming, apps and weird internet culture. He now lives in Philadelphia after stints in Boston and San Francisco.",The best smart speakers for music fans
9715,3732917,2018-02-23 09:43:47,"How Exercise May Help the Memory Grow StrongerExercise may help the brain to build durable memories, through good times and bad.Stress and adversity weaken the brain’s ability to learn and retain information, earlier research has found. But according to a remarkable new neurological study in mice, regular exercise can counteract those effects by bolstering communication between brain cells.Memory has long been considered a biological enigma, a medley of mental ephemera that has some basis in material existence. Memories are coded into brain cells in the hippocampus, the brain’s memory center. If our memories were not written into those cells, they would not be available for later, long-term recall, and every brain would be like that of Dory, the memory-challenged fish in “Finding Nemo.”But representations of experience are extremely complex, and aspects of most memories must be spread across multiple brain cells, neuroscientists have determined.These cells must be able to connect with one another, so that the memory, as a whole, stays intact.The connections between neurons, known as synapses, are composed of electrical and chemical signals that move from cell to cell, like notes passed in class. The signals can be relatively weak and sporadic or flow with vigor and frequency. In general, the stronger the messages between neurons, the sturdier and more permanent the memories they hold.Neuroscientists have known for some time that the potency of our synapses depends to some degree on how we live our lives. Lack of sleep, alcohol, diet and other aspects of our lifestyles, especially stress, may dampen the flow of messages between brain cells, while practice fortifies it. Repeat an action and the signals between the cells maintaining the memory of that action can strengthen. That is learning.There also have been hints that exercise might affect synapses in the hippocampus. Exercise has been shown in many studies to improve learning and memory. But only a few past animal studies have closely tracked changes to synapses after exercise and none looked simultaneously at stress, leaving the results unrepresentative of actual life, which always contains some amount of stress.They divided the animals into groups. Some, as a control, continued with their normal rodent lives. Others began running at will on wheels in their cages; mice seem to enjoy running and these eagerly covered about three miles a day.After a month, some of the sedentary animals were exposed to three days of stressful experiences. These mostly involved some type of mild restraint, which makes prey animals like mice understandably anxious.The researchers were trying to simulate relatively chronic stress with the animals, somewhat like what most of us might feel with ongoing work pressures or other anxieties.Some of the runners were also restrained and stressed.Then, to see if any changes to the animals’ synapses would be reflected in their lived experience, the researchers had some mice from each group learn a maze with a treat in one hidden corner.An error has occurred. Please try again later.You are already subscribed to this email.Finally, the researchers looked microscopically at the operations of the synapses joining the neurons in the animals’ hippocampi. By electrically stimulating some of the isolated cells, they could see how many and what types of messages jumped between them.It was immediately clear that three days of chronic stress had reduced the effectiveness of the synapses in the stressed-out, sedentary animals, compared to those from the control mice. Their intracellular connections were much weaker.The unstressed runners, on the other hand, now had the strongest, busiest synapses, suggesting that their ability to learn and remember would be higher than in the other animals.Perhaps most interesting, the animals that had run and also experienced chronic stress had synapses that resembled those from the normal, unstressed control group. They were not as strong as those from the never-stressed runners but much stronger than those from the animals that had been stressed but not exercised.Behaviorally, the runners, stressed or not, also learned the location of the treats in the maze more quickly than the sedentary animals did, and remembered it more rapidly and accurately several weeks later.Over all, it seems that exercise had improved the animals’ memories, even in the face of stress, by bulking up their synapses and buffering the negative effects that stress otherwise would have had on those neural connections, says Roxanne Miller, who led the study as part of her doctoral research at B.Y.U.It is not yet clear, though, she says, how exercise changed the animals’ synapses at a molecular level. She and her colleagues did find increases in the activity of certain genes and the levels of some proteins in the brains of the runners that could have contributed to the changes in their synapses, she says.But far more research is needed into that issue, as well as into whether other forms of exercise, such as resistance training or gentle walking, would have comparable effects, she says.And of course, mice are not people, and it is impossible to know if the same changes occur in our synapses when we exercise.But even so, the results do seem to offer one more reason being physically active “is a very good idea,” Dr. Miller says.",How Exercise May Help the Memory Grow Stronger
9716,3732920,2018-02-23 18:18:02,"Citi to Refund $330 Million to Credit Card Customers It OverchargedCitigroup said it would issue refunds averaging $190 to customers who were overcharged.Credit Shin Woong-jae for The New York TimesCitigroup is preparing to issue $330 million in refunds after the bank discovered it had overcharged nearly two million credit card accounts on their annual interest rates, a spokeswoman said on Friday.The bank, which has about 150 million credit card accounts, said it caught the error in a routine internal review mandated under federal law. Citigroup said it is had notified regulators about the mistake.“We sincerely apologize to our customers and are taking every action to provide refunds as quickly as possible,” the bank spokeswoman, Elizabeth Fogarty, said in a statement.Citigroup disclosed the error in its annual report, filed Friday with the Securities and Exchange Commission. The bank said it had discovered “methodological issues” in its calculations of some customers’ annual interest rates.Citigroup discovered the error as it conducted a review required by the CARD Act, which took effect in 2011. The federal law, enacted after the 2008 financial crisis, requires credit card issuers to review the accounts of customers whose interest rates have risen after incidents like missed payments or bounced checks. Issuers must then determine whether the customers have improved their financial standing enough to qualify for lower rates.Correction: February 23, 2018An earlier version of this article mischaracterized the extent of an overcharge to credit card accounts by Citigroup. The bank said that 1.75 million accounts, not customers, were affected.",Citi to Refund $330 Million to Credit Card Customers It Overcharged
9717,3732922,2018-02-23 14:00:00,"Over the past year and a half, the Google Assistant has grown from being available on just one device in one language to across many types of devices, including speakers, phones, Android Auto and TVs, in many languages all around the world. We’ve been focused on making the Assistant useful throughout all parts of your day, and earlier this year we showed the latest features we’re bringing to the Assistant in homes and in cars.As we head into Mobile World Congress, the mobile industry’s largest trade show, we're sharing more about how we’re working closely with the mobile ecosystem to bring the Assistant to more people around the world. Similar to Android, where we've partnered closely with mobile carriers and device makers to build great products for people everywhere, we’re taking an ecosystem approach to the Assistant on mobile. Here's a look at what's coming.Bringing the Assistant to more than 30 languagesAndroid users are all around the world, so from the start, our goal has been to bring the Assistant to as many people, languages, and locations as possible. The Assistant is already available in eight languages, and by the end of the year it will be available in more than 30 languages, reaching 95 percent of all eligible Android phones worldwide. In the next few months, we’ll bring the Assistant to Danish, Dutch, Hindi, Indonesian, Norwegian, Swedish and Thai on Android phones and iPhones, and we’ll add more languages on more devices throughout the year.We’re also making the Assistant multilingual later this year, so families or individuals that speak more than one language can speak naturally to the Assistant. With this new feature, the Assistant will be able to understand you in multiple languages fluently. If you prefer to speak German at work, but French at home, your Assistant is right there with you. Multilingual will first be available in English, French and German, with support for more languages coming over time.Building a great Assistant for phonesSince MWC last year, we've been working closely with device makers (OEMs) to bring all the capabilities of the Assistant to life on Android phones. This year, we’re bringing these efforts together as the Assistant Mobile OEM program, which will enable OEMs to build deeper integrations between the Assistant and device features, using natural language understanding and the conversational interfaces of the Assistant. We’ve already been working with OEMs for more than a year and continue to work together so they can build device-specific commands with the Assistant, develop integration with hardware-based AI chips, ensure “Ok Google"" and ""Hey Google"" work when the screen is off, and build other custom integrations. Coming soon, we’ll also have new integrations from LG, Sony Mobile and Xiaomi.Mobile carriers also play a critical role in delivering great mobile experiences to people through their networks and services. Our Assistant Carrier program helps mobile carriers use capabilities in the Assistant to give their customers more insight and control over their service. This includes helping people learn more about their plan, add new services (like international data roaming), get customer support and more. This gives carriers a new way to support their customers while reducing response time. Carriers Sprint, Koodo, Telus and Vodafone are already developing integrations with the Assistant, with more coming soon.A better experience across devicesThe Assistant can already help you keep track of your day, control your smart home devices, make calls, find recipes and more. Starting over the next week, we’re adding two new features that help you use the Assistant across all the devices in your life.Routines: We first announced Routines last year, which help you get multiple things done with just a single command. In the coming weeks in the U.S., you’ll be able to use six routines that help with your morning, commutes to and from work, and evening at home. For example, say “Hey Google, I’m home” and the Assistant on your Google Home or phone can turn on the lights, share any home reminders, play your favorite music and more, all with just four words.Location-based reminders: You can already set reminders based on a location with the Assistant on your phone. In the coming weeks, you’ll also be able to set them with your speaker. Want to make sure you pick up the milk at the grocery store? All you have to do is ask the Assistant on your smart speaker, like Google Home—and when you get to the store, the Assistant on your phone will remind you.With more languages, more features and closer integrations with phone makers and carriers, the Assistant is getting better for you.",The Google Assistant is going global
9718,3733079,2018-02-23 16:42:42,"It will allow offices and business to apply for a Wall Connector to be installed in the company garage or parking lot. And, except for the actual electricity used by employees charging their Teslas, the Wall Connector and installation will be free to the property managers.This isn’t too different from Tesla’s Destination Charging program, which offers chargers at restaurants, resorts, and hotels with free installation, as long as property owners cover the cost of electricity. However, those chargers are displayed on Tesla’s navigation systems and available to use publicly.The Workplace Charging program is meant to serve only employees who work in the buildings whose property managers or businesses apply for the program. So don’t expect to roll up to some random HQ and juice up the Model 3.As Tesla’s fleet continues to grow, it is more important than ever for our customers to be able to easily charge their cars where they park. The most convenient way to charge is to plug in overnight at home, and for most people, this is all that is needed. For others, such as those who live in an apartment, Tesla is introducing its new Workplace Charging program. Charging at work is simple and convenient, just plug in and your car is charged by the time you’re done for the day.For qualified employers or commercial property managers who choose to provide an EV charging option, Tesla will review, donate their Tesla Wall Connectors and provide installation assistance. Energy costs will be the responsibility of the property.",Tesla wants to install chargers at the office
9719,3733158,2018-02-23 20:35:28,"TNW SitesAn Apple employee rings 911 by accident… 1,600 timesTwo 911 call centers in California have confessed to getting hundreds of calls from a single location. The caller? Apple — specifically a repair center in Elk Grove.No one seems to know exactly how or why these calls were made — Apple told Buzzfeedit was aware of the calls and was looking into it. The popular theory seems to be that, because iPhones and Apple watches ring 911 when you hold one button down, employees are calling by accident. Jaded by mystery novels as I am, I find it hard to believe Apple employees can make that many butt-dials that consistently from a single location. Personally, I suspect they’re communicating in code.Employees don’t seem to be aware when they make the calls — dispatchers told CBS Local they hear people talking about work, when they hear anything at all. If it is butt-dials, then someone at that Apple facility is keeping far too many devices in a back pocket. Either that, or one of Apple’s employees is running the world’s longest, most annoying prank.911 centers say they’ve received as many as 20 calls a day from the Apple location, beginning in October. In total, Apple has called them over 1,600 times and waylaid who knows how many minutes of lifesaving attention. A spokesperson from the local police department assured everyone they were working with Apple to make sure the mystery was solved. “911 is a lifeline for everyone in our community, so having these lines open and available is paramount and so getting this problem resolved.”",Apple and the Case of the Mysterious 911 Butt-Dials
9720,3742299,2018-02-24 16:00:00,"We're live from MWC 2018 in Barcelona!Spring season is right around the corner, and that means it's time for Mobile World Congress in Barcelona, the world's biggest phone show. This year, you can expect to be introduced to Samsung's next flagship smartphone, the Galaxy S9, as well as a slew of other handsets from big-name companies like BlackBerry and Nokia. Yes, you know you love BlackBerry and Nokia. Of course, we'll likely also come across a bunch of other tech products, such as wearables and others things that could be revealed at the show -- Facebook has a press conference, for instance. We're on the ground for the next week, which means you need to keep your eyes peeled to the site so you won't miss a thing from MWC 2018.",We're live from MWC 2018 in Barcelona!
9721,3742301,2018-02-24 14:00:00,"'Black Panther' is amazing. Why are its CG models so terrible?Black Panther is a refreshing answer to the increasingly stale world of superhero cinema. But there's one glaring flaw throughout the film: its use of CG models to replace humans during action sequences. They're weightless, ugly and, worst of all, incredibly distracting. You'd think that in the year 2018, following the recent glut of comic book films, visual effects (VFX) studios would have perfected the art of creating realistic CG humans. Instead, we appear to have peaked at Avatar in 2010. What gives?It's not just a matter of visual effects companies getting lazy. As movies have started to rely even more on complex VFX, the firms creating them are overworked, underpaid and, at times, literally fighting for survival, according to one person who has worked on several recent blockbusters (and who asked to remain anonymous due to the sensitivity of their work). That's led to a decline in overall quality, even while some studios continue to push new boundaries, like WETA, with its work in the recent Planet of the Apes trilogy.Big-budget films used to require between 500 and 1,000 VFX shots, but that number is now regularly between 1,000 and 2,000, according to VFX Movies' comprehensive chart. For example, The Fellowship of the Ring had just 480 visual effects shots in 2001, while the recent Hobbit films each featured around 2,000. (That's also a clear example of how more effects don't necessarily lead to better-looking movies.) Some of the biggest blockbusters today, like Captain America: Civil War and Avengers: Age of Ultron, required an astounding 3,000 VFX shots. To get all of this work done, Hollywood studios regularly enlist a large number of firms for a single film. Around a dozen worked on Black Panther, while Thor: Ragnarok had more than 20 companies churning out visual effects.""It takes enormous teams to put this [VFX work] together. It takes individuals with specific skills sets to do it,"" the insider said. ""Movie studios need so much work, and they're only willing to pay so much. The VFX are accounting for a pretty serious chunk of these $100 to $200 million budgets, but even that isn't enough to cover the sheer amount of shots.""Many VFX firms today are in a race to the bottom. They're trying to undercut rivals that might steal potential jobs and taking on an excessive amount of work, often without making a profit. Most of these firms also have to pitch their talents to Hollywood, which occasionally involves doing $20,000 to $80,000 worth of work up front for free. Sometimes they end up working on huge films at cost, in hopes that it'll lead to more lucrative (and less demanding) commercial work.As you can imagine, this has made life rough for VFX workers. ""In an attempt to slash costs the vfx facilities have eliminated benefits such as sick days, health insurance, and retirement accounts,"" writes Daniel Lay, the formerly anonymous activist blogger VFX Soldier, who's worked at studios like Digital Domain and DreamWorks Animation. ""Many are forced to work under illegal conditions with unpaid overtime and 1099 tax statuses where we are responsible for paying the employer's portion of social security. The projects have become more volatile as the vfx facilities try to please the demands of the director put in place by the studio.""The sorry state of this industry is best encapsulated in the story of Rhythm and Hues, the company behind the astounding effects in Ang Lee's The Life of Pi. Rhythm and Hues ended up declaring bankruptcy just two weeks before winning an Academy Award for their work in 2013. Their acceptance speech is now infamous. As the firm's visual effects supervisor, Bill Westenhofer, tried to address the issues facing his industry from the podium, he was drowned out by the theme from Jaws and his microphone was cut off. It seemed that Hollywood's interest in VFX studios lasted less than 60 seconds. (The short documentary ""Life After Pi"" offers an inside look into what went wrong for Rhythm and Hues.)""Nowadays, almost every shot in a blockbuster film has some CG element,"" one Industrial Light and Magic (ILM) worker told us. (They also asked to remain anonymous.) ""Direction can change almost on a daily basis, where artists are redoing their work multiple times just to hit the ever-changing vision."" Hollywood studios are also seeking cheaper labor, which forces VFX companies to open satellite offices in places like Canada, where the US dollar is worth more, and locations that offer significant tax breaks.Given the hyper-competitive, high-pressure nature of the VFX world, it's no surprise that we're not seeing the best work from these studios. In Black Panther, there are two shots that are particularly disappointing: one in which the superhero flips over a car as it crashes beneath him, and another sequence where two CG characters punch each other as they fall. (The latter sequence feels like a nod to the excellent midair fight in Spider-Man 2, except it looks significantly worse.) The insider's team (who didn't work on those shots) was worried about these scenes when they caught glimpses of them in the film's trailer. They had all the trademarks of bad CG modeling. Their team held out hope that the trailers were using early, unfinished renders, but unfortunately those problems remained in the final cut of the film. We've reached out to Disney for comment, and will report back if they respond.In most cases, the industry insider tells us, the issues come down to blending the digital effects with physical environments. Even the most incredibly detailed character models will feel a bit off if any of those elements are weak. Studios typically don't have enough time to smooth out the rough edges, so they just have to submit what they've got. In many cases, this leads to incomplete work actually making it to theaters.One ""major"" release last year had more than 100 VFX shots that needed fixing, the insider says. Those issues were eventually resolved for the home video release, something that's becoming increasingly common. (And, no, the VFX companies aren't paid extra for those fixes.) While you'd think that faster computers and rendering capabilities would help solve some of these problems, instead studios are using that extra horsepower to take on even more work.It doesn't help that movie studios are leaning even more on visual effects to fix issues in postproduction. ""The idea that you can continue to refine and update whatever is done in a computer means there's less of a need to commit to certain ideas and concepts,"" the source says. This is both a blessing and a curse for everyone involved. Being able to fix minor issues is genuinely helpful, but it also leads to designers changing the look of suits and other elements late in production. That forces VFX firms to rush out updated work, with very little fine-tuning.It's not entirely doom and gloom, according to Lay (AKA VFXSoldier). ""While I've warned about the bad conditions leading to a bad product in aggregate that hasn't really shaken out,"" he said in an email. ""While conditions continue to be very difficult there continues to be very good VFX work being done in films and that's probably a good indication why things haven't changed... Even with these terrible conditions the professionals and facilities continue to soldier through the challenges and make a great product.""So what can be done about the systemic issues facing the visual effects world? It's a long shot, but our source suggests that plenty of issues could be solved if VFX studios unionized and worked together. That's not something Hollywood wants, since it would probably end up making visual effects work cost more and take longer.Visual effects firms should also explore newer technology, our ILM source suggested. ""A lot of companies don't have time or budget to develop their tools,"" they said. ""It seems like we just throw people at the problems instead of figuring out better ways to work. I've been dabbling in real time [rendering] with Unity and Unreal lately and it's amazing. I think we're not far off from doing VFX film work in a real-time environment, and when that happens the whole industry is in for a little shake-up.""While Black Panther will certainly be remembered for breaking new ground, it's also a fitting example of the issues facing the VFX industry today. A movie that gets so many things right, with the backing of the biggest movie studios in the world, can still fall victim to the crushing workflow of the visual effects industry. That's not just embarrassing for the people involved -- it's a shame shared by all of Hollywood.Devindra has been obsessed with technology for as long as he can remember -- starting with the first time he ever glimpsed an NES. He spent several years fixing other people's computers before he started down the treacherous path of writing about technology. Mission accomplished?",'Black Panther' is amazing. Why are its CG models so terrible?
9722,3742395,2018-02-24 16:37:19,"Open Source Color Management is brokenSince I am now in the business of photography and image processing (see my travel photography blog here), I thought it was time to finally get proper monitors and calibrate them. I wanted to do this with Open Source tools and use the calibration data for my Linux desktop, so I ordered a ColorHug2 colorimeter, which is Open Hardware compliant and all the tools are FOSS licensed. And from then on everything just went downhill.The ColorHug2 comes with a small USB storage key which is supposed to contain a Fedora Live Session environment with all the tools to calibrate the display. The created ICC color profiles can then also be used on other devices (even on Windows and MacOS). Except that both my laptop and PC refused to even offer the key as a bootable device (which seems to be a known issue). Well, it uses the DisplayCAL GUI frontend for ArgyllCMS anyways, and both these packages are in the repositories of any modern Linux distribution. I had already used them two years ago to calibrate some devices at work, so I put the USB key aside, installed DisplayCAL on my devices, set up the ColorHug2, and started calibration runs. Only to see ArgyllCMS fail every time time because the USB connection to the ColorHug2 kept failing after a while, which also seems to be a known issue. Grunt. This is absolutely not what I expect when I invest more than 100 € in an Open Hardware device.The known workaround for the problem seems to be an additional USB reset after the device is released (pointing to a firmware bug in the ColorHug2), which is contained in the very latest ArgyllCMS 2.0.1 beta development soure code. So I downloaded, built and installed it, and the calibration finally ran through. But before the calibration starts, DisplayCAL shows an interactive display adjustment screen to have you prepare the display as good as possible before the actual calibration starts. A work colleague lent me his Pantone huey colorimeter, which also works with ArgyllCMS. My photo editing monitor at home is a BenQ BL2711U, which is sold as a professional 4K CAD Monitor, has 100% sRGB color space coverage and is known to have very good color representation even on factory defaults. So I expected both the huey and the ColorHug2 to see just minor deviations, and the measurements to be consistent.The DisplayCAL interactive display adjustment screen.Which was absolutely not the case. The dialogue shows the distribution between the three color channels, and if the display has the necessary controls (laptops don’t), you correct until all three channels meet in the middle. With the huey, AryllCMS detected a very slight green tint with the factory settings, which I corrected by lowering the Green channel from 100 to 99 in the monitor settings. The ColorHug2 measured a non-existing very heavy green tint, and I had to lower the Green channel from 100 to 80 to make the bars meet. At that point the whole display had become a quite heavy purple (the Blue and Red channels being way overblown in comparison to Green), which definitely wasn’t right. So I gave up on the ColorHug2 for the moment and continued with the huey, but there are some indications that this might be another ColorHug2 issue. (I later found out that there is a firmware update to version 2.0.7 for the ColorHug2, which only seems to be available via the fwupd tool, but not via the colorhug-tools or from the repository where all the other releases can be found. I haven’t tried this update yet.)DisplayCAL/ArgyllCMS gave me a good profile using the huey and confirmed that the BL2711U actually exceeds the advertised 100% sRGB coverage. The difference between using the calibration data and not using it was barely noticeable. Now I wanted to have the system and every application use my generated profile. Turns out the XFCE desktop I use simply doesn’t have any color management, like most other “smaller” FOSS desktop environments. You can use a tool like dispwin to manually write the correction values into the video look-up table (VideoLUT) of the graphics card, but the applications also have to know about the profile so they can account for it, e.g. when converting between color spaces (every browser has to do this). And this is where it gets really complicated: This is Linux, so there are no rules or standards.An example of the KDE color management settings screen provided by colord-kde.There are multiple ways to tell an application about the current profile. There is colord, a daemon which can create, store and manage calibration profiles and sensors and offers its services via D-Bus. And there is the _ICC_PROFILE X11 atom, which allows applications to attach profiles to and query profiles from X.Org display outputs. Ideally colord would do everything, but it runs in the background and doesn’t have access to the running X session, so it needs helpers. GNOME and KDE Plasma have GUI frontends for this, and both kded and gnome-settings-daemon have plugins for setting the _ICC_PROFILE X11 atom. Except that they only set it for the primary output. Turns out if you have multiple displays, the profile for the first one is put into the _ICC_PROFILE X11 atom, but the profile for the second one in the _ICC_PROFILE_1 X11 atom, for the third display in the _ICC_PROFILE_2 X11 atom, and so on. It’s just that nobody seems to do this.That means your profiles might actually be written into the VideoLUTs of the respective outputs, but every application which relies on the _ICC_PROFILE X11 atoms will only be able to get the profile for the primary display. darktable pretty much seems to be the only clever one out there, because it supports both _ICC_PROFILE and colord and also supplies the excellent darktable-cmstest tool which can be used to check which setting is currently set to which value.Example output of the darktable-cmstest tool while running under GNOME Shell. Notice that _ICC_PROFILE_1 and _ICC_PROFILE_2 are not set, while colord knows the correct profiles for all displays.But it gets even worse. GNOME (and Ubuntu Unity, which borrows most of its tools) at least has color management built into the desktop environment by default. KDE Plasma offers kde-colord, but it’s an optional package, and for example in Ubuntu 18.04 it is four years old, two releases behind and doesn’t work because it seems to still assume we live in a KDE 4 world. If you use KDE Plasma on Ubuntu today, you can’t even have color management. All in all this is probably why many developers, if they even care about color management, just give up and let the user set the profile manually. This at least seems to be true for GIMP, PhotoFlow and RawTherapee. Firefox by default only does color management for tagged images. Chrome/Chromium, Gwenview and digiKam seem to use _ICC_PROFILE, but probably only for the first display as well.So these are my current suggestions for color management on Linux:Make sure you really need and want it. As I’ve written the factory settings of a good monitor can be quite good already.Make sure the applications you need color management for know what color management is. Most don’t.Only use one display, or make sure you can live with having color management on just the primary one.Use GNOME Shell or Ubuntu Unity with X.Org. I don’t think Wayland even has a specification on how to do color management.If you really want KDE Plasma, make sure colord-kde works. It is broken on Ubuntu 18.04, but works fine on Arch.Find someone who lends you a working, professional-grade calibration device supported by ArgyllCMS (it supports many) before you spend more than 100 € on something just because it has an Open Hardware sticker.",Open Source Color Management is broken
9723,3742400,2018-02-23 19:06:37,"As part of a state initiative, Tesla deployed over 300 Powerwalls in schools to cool down hot classrooms in Hawaii.Home Solar PowerHawaii has a problem with hot temperatures in public classrooms that is affecting students negatively. The problem was so significant that the Hawaii State Department of Education had to intervene.They put together a $100 million fund, which has already helped cool down 1,190 classrooms to date, with contracts set for more than 1,300 classrooms, according to The Garden Island.In order to roll out the program without significantly increasing energy costs for public schools, they partnered with Tesla to pair Powerwalls with solar power to reduce the impact of running the air conditioners in classrooms across the state.It also resulted in an interesting learning opportunity about renewable energy and energy storage for students.Tesla produced the following video about it:Electrek’s TakeIt’s definitely a good use of energy storage, though I find the actual setup shown in the video and pictured above particularly strange.Hawaii has some of the highest electricity rates in the country, which is why they have heavily invested in solar energy. The state also has several initiatives to combine their renewable energy production with energy storage.",Tesla deployed over 300 Powerwalls in Hawaiian schools to cool down hot classrooms
9724,3742402,2017-12-28 05:00:00,"Why we made this changeVisitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access.How to Get Wyoming Wind to California, and Cut 80% of U.S. Carbon EmissionsSeveral miles south of Rawlins, Wyoming, on a cattle ranch east of the Continental Divide, construction crews have begun laying down roads and pads that could eventually underpin up to 1,000 wind turbines. Once complete, the Chokecherry and Sierra Madre project could generate around 12 million megawatt-hours of electricity annually, making it the nation’s largest wind farm.But how do you get that much wind power to where it’s actually needed?The Denver-based company behind the project hopes to erect a series of steel transmission towers that would stretch a high-voltage direct-current transmission line 730 miles across the American West. It could carry as much as 3,000 megawatts of Wyoming wind power to the electricity markets of California, Nevada, and Arizona. With the right deals in place, the transmission line could deliver solar-generated electricity back as well, balancing Wyoming’s powerful late-afternoon winds with California’s bright daytime sun.How Wyoming wind (in green) could balance out California solar (in red) throughout the day in June. Source: Wind Diversity Enhancement of Wyoming and California Wind Energy Projects, 2015.Jonathan Naughton, Wind Energy Research Center.The $3 billion TransWest Express Transmission Project is among a handful of proposed direct-current transmission lines in the United States, and one of the furthest along in the planning process. It underscores the huge promise of these high-capacity lines to unlock the full potential of renewable energy.But a growing body of studies conclude that building out a nationwide network of DC transmission lines could help enable renewable sources to supplant the majority of U.S. energy generation, offering perhaps the fastest, cheapest, and most efficient way of slashing greenhouse-gas emissions.Developing these transmission lines, however, is incredibly time-consuming and expensive. The TransWest project was first proposed in 2005, but the developers will be lucky to secure their final permits and begin moving dirt at the end of next year.There’s no single agency in charge of overseeing or ushering along such projects, leaving companies to navigate a thicket of overlapping federal, state, county, and city jurisdictions—every one of which must sign off for a project to begin. As a result, few such transmission lines ever get built.A macro gridDirect current, in which electric charges constantly flow in a single direction, is an old technology. It and alternating current were the subject of one of the world’s first technology standards battles, pitting Thomas Edison against his former protégé Nikola Tesla in the “War of the Currents” starting in the 1880s (see “Edison’s Revenge: The Rise of DC Power”).AC won this early war, mainly because, thanks to the development of transformers, its voltage could be cranked up for long-distance transmission and stepped down for homes and businesses.But a series of technological improvements have substantially increased the functionality of DC, opening up new ways of designing and interconnecting the electricity grid.Starting in the 1950s, some companies and countries began to deploy next-generation high-voltage DC transmission lines. These systems could carry more power much farther than AC lines, which suffer far more losses over greater distances. Crucially, direct-current lines can also be used to transmit power between “asynchronous” alternating-current systems like the nation’s three major regional grids, which otherwise can’t share power.For the past two years, James McCalley, an engineering professor at Iowa State University, has been studying the best way to tie together those massive grid systems as part of the Department of Energy’s $220 million Grid Modernization Initiative.One way to solve the problem is to expand existing “back-to-back” conversion stations to provide more east-to-west transmission capacity. These systems allow transmission between two grids, by converting the power to DC and then back to AC again at the point where they “cross the seam.”Another approach adds three point-to-point transmission lines, running east to west, connecting the heart of each grid to that of the other. Yet another solution is a so-called “macro grid” of long DC transmission lines covering much of the country. It runs up the Florida panhandle, across the South, north to Seattle, east to Minneapolis, and back down to Louisiana, with several additional lines crisscrossing the West.McCalley and his team developed models to simulate each of these scenarios over a 15-year period. They found that all three demonstrated a strong economic payoff, providing a benefit of at least $2.50 in savings for every $1 invested in the transmission system.With direct-current lines, grid operators have more options for energy sources throughout the day, allowing them to tap into, say, cheap wind two states away during times of peak demand instead of turning to nearby but more expensive natural-gas plants for a few hours. The fact that regions can depend on energy from distant states for their peak demand also means they don’t have to build as much high-cost generation locally.The point-to-point DC transmission scenario demonstrated the highest immediate economic return in the study, which will be published in the months ahead. But the macro-grid approach offers far greater redundancy and resilience, ensuring that the grid keeps operating if any one line goes down. It also makes it possible to build out far more renewable energy generation.“The macro grid gives you a highway to all those loads and ties all those markets together,” says Dale Osborn, transmission planning technical director at the Midcontinent Independent System Operator (MISO), which first designed the system. “You get the most efficient, lowest-cost energy possible.”Giant batteriesA national direct-current grid could also help lower emissions to as much as 80 percent below 1990 levels within 15 years, all with commercially available technology and without increasing the costs of electricity, according to an earlier study in Nature Climate Change.The researchers produced an idealized transmission network that connected 32 nodes across the nation, linking hydroelectric power in the Pacific Northwest, solar in California, wind energy in the Southwest, and nuclear energy on the East Coast, among other sources.A national high-voltage direct-current transmission network designed by Vibrant Clean Energy.Simply put, the system balances out the intermittency of renewable energy sources over long distances, meaning there’s always reliable generation somewhere. Being able to tap into it from any corner of the nation lowers the cost of supplying energy at peak demand, reduces the amount of generation required in any single area, minimizes excess generation, and eliminates the need to develop expensive grid-scale storage systems (see “Serial Battery Entrepreneur’s New Venture Tackles Clean Energy’s Biggest Problem”).“We’re basically getting that big battery we want for free,” says Christopher Clack, one of the lead authors of the study and chief executive of Vibrant Clean Energy.Super wiresSome emerging technologies could push the benefits of high-voltage direct-current transmission even further.Voltage source converters, developed in the 1990s, make it possible to tap into power at any point along HVDC lines, rather than just at its two end points. They’re not widely deployed now, but they could enable an even more interconnected grid than the one MISO designed, writes Varun Sivaram, the science and technology fellow at the Council on Foreign Relations, in his forthcoming book Taming the Sun.In addition, researchers are exploring different types of semiconductors to improve the cost and performance of circuit breakers and voltage converters, Sivaram says, and working on superconducting cables that could boost power capacity and cut energy losses to nearly zero.China pushes aheadMeanwhile, China has already begun wide deployment of so-called ultra-high-voltage DC transmission lines, capable of transmitting around a million volts.Much of the nation’s transmission building boom has been driven by an effort to connect energy generation concentrated in remote northern provinces with distant population centers, a geographic mismatch that’s resulted in huge amounts of excess solar and wind energy. But, of course, it also reflects the fact that China’s energy development doesn’t slow down much for public comments and environmental impact reports.There are already a handful of DC transmission lines in the United States and a growing number of proposals, including the New England Clean Power Link, which would transport 1,000 megawatts of renewable power from Canada into New England. Houston’s Clean Line Energy has at least a half-dozen proposals in various stages, including the Plains and Eastern Clean Line connecting western Oklahoma to markets in the Southeast, and the Grain Belt Express Clean Line stretching from Kansas to Indiana.The Eastern Alberta Transmission Line, a high-voltage direct-current line in Alberta, Canada.But all of these are moving through the approvals process at a dawdling pace. The TransWest developers have secured permission along the two-thirds of the line’s path that lies on federal land since taking over the project in 2008. But they’re still working to finalize approvals from states and private landowners.Clean Line’s proposed Grain Belt line has been approved by three of the four states it crosses, but Missouri regulators have rejected it twice.Most developers and energy policy experts say what’s needed to accelerate these projects is a federal authority with greater power to push them through. A report released by Stanford in October highlighted a number of possibilities, including granting the Federal Energy Regulatory Commission the same “siting authority” for transmission lines that it already has over natural-gas pipelines.“Without clear, predictable siting authority, it’s going to be very difficult to build out an intelligent, comprehensive HVDC network,” says Dan Reicher, executive director of the Steyer-Taylor Center for Energy Policy and Finance at Stanford, a coauthor of the report.Another critical challenge for long-range transmission is that utilities have little incentive to pay for these lines, MISO’s Osborn adds. The rules would need to change to allow utilities to recoup their investments and encourage them to cooperate across regions, he says.Without additional regulatory shifts and concentrated federal authority, we’re likely to see more of what we’ve been seeing: one-off projects where the economics may work to connect one particular energy source to one particular market. But we’re not likely to see an integrated network like the ones envisioned by MISO and others.And without those or some grid storage breakthrough, renewables aren’t likely to ever reach their full potential. Plants will sometimes generate more wind or solar power than a single market can use, at which point the price drops—and the incentive to add more renewable generation shrinks. And at other times they won’t generate enough for that market, forcing the region to keep leaning on fossil-fuel plants.“So you end up diverging away from what would be optimal, and can end up hurting renewables in the long run,” Clack says.In a world of more electronics and solar energy, there’s less and less need for AC power.One thing direct-current lines do have going for them, at least in some cases, is strong bipartisan support. In fact, Trump’s transition team placed several transmission proposals on its “priority list” of “Emergency and National Security Projects,” including TransWest.That could be in part because the lines themselves lower costs and link markets even if they’re carrying electricity from coal or natural gas rather than renewable sources. But another factor is almost certainly that the companies proposing such lines are sometimes Republican Party backers. Notably, billionaire and major GOP donor Philip Anschutz owns the company behind the Wyoming wind farm and TransWest line.That may mean HVDC transmission could represent one of the few areas—alongside advanced nuclear power—where clean-energy proponents could potentially work with the Trump administration on a common goal.There are already legal battles playing out, however, that will test how much power federal bodies have over these matters. Several groups representing Arkansas landowners have filed a complaint in U.S. District Court challenging the Department of Energy’s use of a 2005 law to advance Clean Line’s Plains and Eastern project.Similar cases are likely to emerge as other new approaches are tried. But Clean Line’s president, Michael Skelly, says the company is investing heavily to win these fights and set the legal precedents necessary to advance other projects.ShareTaggedI am the senior editor for energy at MIT Technology Review. I’m focused on renewable energy and the use of technology to combat climate change. Previously, I was a senior director at the Verge, deputy managing editor at Recode,… More and columnist at the San Francisco Chronicle. When I’m not writing about energy and climate change, I’m often hiking with my dog or shooting video of California landscapes.You've read of three free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Subscribe now for unlimited online access. This is your last free article this month. Subscribe now for unlimited online access. You've read all your free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Log in for more, or subscribe now for unlimited online access. Log in for two more free articles, or subscribe now for unlimited online access.","How to Get Wyoming Wind to California, and Cut 80% of U.S. Carbon Emissions"
9725,3742558,2018-02-24 15:04:06,"Programming as craft0Can programming be a craft? I was thinking about this as I was reading Matthew Crawford’s excellent book The World Beyond Your Head. Much like Crawford’s earlier work Shop Class as Soulcraft, he argues that craftsmanship is an activity that doesn’t just provide us with satisfaction, but also makes us fundamentally human by enchanting us with the world right around us.He provides the extended example of musical organ makers, who work on instruments that are designed to last centuries. They innovate, but within the tight context of a lineage of apprenticeships that goes back many generations. While repairing an organ, an organ maker can identify the marks and craftsmanship of his or her historical peers, recognizing great or slipshod quality that occurred several hundred years earlier.There is a reverence in witnessing the craftsmanship of skilled artisans, but Crawford is careful to argue that reverence does not mean sycophantic obsession with the past. Instead, he makes a persistent plea to avoid seeing tradition as constraining, but rather considering it as a base upon which to innovate on. There are rules, yes, but those rules come from the deep experience of those before us. Better to consider their purpose deeply rather than replace them in a callow attempt at newness.What gives craftsmanship its fundamental character is the progress latent in the work. Works by young novices can be easily distinguished from the works of experienced masters, and following a single craftsman’s career can be enlightening as they learn their trade.Returning to my opening question then, should we see programming as a craft? The glib answer is yes, of course it is. There is a skill that is developed over time, and masters hopefully understand the field better than novices. Quality work can be identified by other masters, and at least some programmers have reverence for the computer that Crawford’s organ masters would find recognizable.Yet, I can’t help but feel that programming is far from being a craft. Reinvention is fundamentally anathema to craftsmanship, for quality has to build up in layers of skill over time. In programming though, it’s not just the code we write that is constantly being throw away, it’s the entire toolkit. Programming languages ebb and flow in popularity, models and frameworks appear and disappear. Three whole new programming paradigms will be in vogue in the time it takes an organ maker to construct one instrument. Nothing in this field is timeless.Taking a chisel to stone can create an artifact that can last millennia, and inspires such awe we build magnificent palaces to put them on display. Yet, when code stays in production for too long, we aren’t enchanted, but actively disgusted. That COBOL mainframe at the heart of the bank from the 1960s-70s isn’t something to revere, but rather something that needs to be replaced, something that needs to be “modernized.”Latest Crunch ReportThere is a concept in sociology and evolutionary sciences for that obsession with constantly improving known as the Red Queen principle, which is the amount of energy it takes to maintain our social status (from Alice in Wonderland’s Red Queen, who says that “it takes all the running you can do, to keep in the same place”). In programming, this is represented by the constant background learning required just to maintain an up-to-date knowledge of the field.That’s not just theoretical, it’s been my experience as well. I started programming in elementary school with HyperStudio, and moved on to a TI-86 graphing calculator programming TI-BASIC. TI-BASIC is exactly what it sounds like, and it was a joy seeing a small program take inputs, transform them to outputs, and getting immediate feedback on whether the “app” did what I wanted it to do. I made choose-your-own-adventure games and a Tic-Tac-Toe game (complete with what a startup might call “AI” but is probably better referred to as a deterministic finite state machine).Much like the organ makers in Crawford’s book, I had mentors who volunteered at school to teach me more about this craft. I learned more control structures to manage my code, and best practices on how to encapsulate functionality to make code reusable. I “discovered” priority queues and other data structures in high school, only to find out later that such things had been invented before I was born. That was exciting though — I was part of this programming lineage, and it was a blast.As I left my job last year, I started re-learning programming, trying to “modernize” my skillset in the Red Queen fashion. As I wrote on Medium at the time, when I last worked with JavaScript, jQuery had just been released. So there was a lot to learn as any modern web developer appreciates. I spent a few months learning what had changed, and then tried to implement some applications I had been wanting to build for the past few years.And I stopped, burdened by the sheer amount of effort it takes just to get the most basic of apps running. Maybe I have changed as well, but programming just didn’t inspire the same level of awe as it did before. It didn’t feel so much as a craft as a never-ending trial of stamina. It just wasn’t fun, in the sense that I walked away after an afternoon with a sense of joy at the creation that I had unearthed. I walked away instead with JSON type errors and some weird Chrome bug.Professions and crafts are heavily correlated. A veteran surgeon has seen thousands of patients, and can apply the pattern recognition and judgment — the quality — that has been developed over a lifetime to ever more complex cases. A writer can watch as their craft improves article by article, edit by edit, tackling harder and more nuanced subjects with finesse.In programming though, we throw away our most experienced. Age discrimination is rampant in Silicon Valley, because the old ways of doing things are just that: old. The only knowledge that matters is the knowledge that came in the last few years. No field has greater amnesia nor a greater desire to reinvent itself as rapidly, and that to me is a problem worthy of solving.There is a need for something of a “slow code” movement, one that doesn’t reinvent the wheel every few years. A movement toward more stable and complete libraries, of languages that don’t break backwards compatibility. Of seeing the field as a craft and part of a long lineage of intellectual forebears instead of as a terrain where the masters need to be overthrown for the latest junk to be installed.I do want to address some obvious criticisms. One is what might be called the computer science / programming distinction. University computer science programs train students on concepts like algorithm design, automata theory, and probability theory since these concepts are considered timeless. That may well be true, but what this model gets wrong is that algorithms aren’t crafts. You cannot make something with knowledge of the O(n) performance of merge sort or what Rice’s Theorem is. It’s the difference between building organs and studying music theory.Another critique might be to avoid certain programming environments due to their complexity. Web development and app programming are just flagrantly bad platforms for craftsmanship, and one should start with a more hobby-friendly platform like maybe a Raspberry Pi. There is some truth here, and that’s a fair option to take. But if programming is a craft, why do we need hobby-friendly tools at all? Why aren’t our existing tools sufficient for the craft?A third criticism is one that I admit is a bit harder to address: isn’t the rapid evolution of software engineering fundamentally a good thing? Programming is a very new field, and we are still learning the intricacies and best practices in a way that the physics of bridge building was discovered millennia ago. Shouldn’t we throw out the old, since it is actually old as in archaic?I admit I don’t have a great answer here, as there is indeed something to be said for reinvention. However, I would ask a simple question: has all the newness of our code ultimately improved the end-user experience? Does our modern model of cloud delivery of software services synchronized across data centers actually improve upon installed software from decades ago? I can imagine some reflexive “YES!” answers, but I would take a step back and ponder that question deeply. More features than before? Absolutely. Better productivity? Better experience and design? To me, there’s not an obvious answer, and that is telling.Ultimately, this isn’t a plea to make programming better for hobbyists, it is a plea to make the field hospitable to all of us. It is a push to think about permanence, on how to write code that can stand up for years to come. Ultimately, it is a request to think in terms of creating a field that makes us more human in Crawford’s conception, as craftsman who can continue a lineage of excellence. Programming can be a craft, but it truly isn’t one today.",Programming as craft
9726,3742559,2018-02-23 20:43:49,"We knew that it had already filed confidentially, but the company has now unveiled its filing, meaning the actual IPO is likely very soon, probably late March.The company says it will be targeting a $500 million fundraise, but this number is usually just a placeholder.The filing shows that Dropbox had $1.1 billion in revenue last year. This compares to $845 million in revenue the year before and $604 million for 2015.The company is not yet profitable, having lost nearly $112 million last year. This shows significantly improved margins when compared to losses of $210 million for 2016 and $326 million for 2015.Dropbox has been cash flow positive since 2016.Dropbox, which has a freemium model, says it has 11 million paying users, just a small fraction of the more than 500 million registered users who use its cloud services for free.Its average revenue per paying user is $111.91.The big question is whether the company will achieve the $10 billion valuation it raised in the private markets. Part of its success will be measured relative to Box, which went public in 2015 and will be considered a comparable.The prospectus warns of the competitive landscape.“The market for content collaboration platforms is competitive and rapidly changing. Certain features of our platform compete in the cloud storage market with products offered by Amazon, Apple, Google, and Microsoft, and in the content collaboration market with products offered by Atlassian, Google, and Microsoft. We compete with Box on a more limited basis in the cloud storage market for deployments by large enterprises.”",The Dropbox IPO filing is here
9727,3742560,2018-02-23 19:50:44,"Jeremy Fiance, the 26-year-old founder of House Fund, ups his VC ambitions0A little less than two years ago, we reported on Jeremy Fiance, a then 24-year-old recent UC Berkeley graduate who’d just taken the wraps off his new firm, The House Fund. It had secured $6 million in capital commitments from an array of individual investors, many of them venture capitalists, to fund startups coming out of UC Berkeley.At the time, Fiance argued persuasively that the school — known for its academic rigor — has largely and unwisely been overlooked by angel investors and VCs alike, sometimes owing simply to proximity. (Many investors live in communities that are closer to Stanford, roughly 40 miles away.)He further sold himself on his own unique abilities to spot talent at UC Berkeley, including because as a freshman, in 2010, he’d brought to campus Kairos Society, a now 11-year-old organization that encourages budding entrepreneurs to tackle global-scale social challenges.His salesmanship may be paying off. According to a three-month-old SEC filing spied earlier today by Axios, Fiance’s ambitions have already grown eightfold. At least, according to the filing, the House Fund is now raising upwards of $50 million for its second fund. (We’ve reached out Fiance for more information and will update this post if we hear more.)Whether he can gather as much in capital commitments remains to be seen, but the diverse portfolio he has assembled suggests that Fiance — who counts VC and Cal alum Jeff Brody of Redpoint Ventures among his advisors — has made connections across the startup industry. Among House Fund’s bets is Gradescope, a four-year-old, Berkeley, Ca.-based cloud-based educational grading platform that raised $2.6 million in seed funding in 2016, including from Freestyle Capital, Bloomberg Beta, Reach Capital, and K9 Ventures.House Fund also invested more recently in Superhuman, an ostensibly much faster email service created three years ago by the founder of Rapportive, a Gmail add-on that LinkedIn acquired in 2012. (Superhuman has disclosed its backers but not how much it has raised.)Yet House Fund’s most promising bet to date may be Flexport, a five-year-old global freight forwarder and logistics platform that has raised at least $204 million so far, according to Crunchbase. (Founder Ryan Petersen graduated from UC Berkeley in 2002.)That amount of funding suggests that House Fund’s stake in Flexport is likely quite small at this point. Early investors are typically diluted in subsequent funding rounds unless they can pay to maintain their percentage of ownership, and it’s doubtful that House Fund could do this given the size of its debut fund. Still, the deal could conceivably persuade investors that Fiance has an eye for talent.That’s not to say it’s been smooth sailing all the way. As with any venture firm, the outfit is already seeing its ups and downs.House Fund lists eight startups at its website, and says it has funded “dozens of other unannounced companies.” One of those featured companies is Essential, which was founded by star entrepreneur Andy Rubin but is reportedly struggling to sell its smartphones.When we’d talked with Fiance in April 2016, he was also understandably proud of an early investment in Lily Robotics, a camera drone startup that had started out in a UC Berkeley robotics lab and gone on to raise $15 million in funding, including from Spark Capital. Six months later, unable to raise a subsequent round, the company shut down.","Jeremy Fiance, the 26-year-old founder of House Fund, ups his VC ambitions"
9728,3742638,2018-02-24 15:00:00,"About TNWTNW SitesBring home design plans to life with this 3D app that’s just $24.99If you’ve ever wanted to redecorate your home (or maybe even change the layout by knocking out a wall or two), you may have run into difficulty when you tried to relay that vision to others. Let’s face it, some of us aren’t artists or particularly good at expressing the intricacies of such a grand design.Live Home 3D is like taking an interior designer or architect’s drafting table and putting it on your computer. Thankfully, it makes visualizing a home redesign or renovation of virtually any scope as easy as point-and-click.This app comes chock full of hundreds of 3D objects from furniture to walls, which can be fully customized to fit with any floor plan you create. Just set the dimensions of your room, outline where doors and windows should be, then start laying in your furniture and aesthetic choices like paint colors and surface finishes. It really is that simple.You can watch your design ideas expand from 2D renderings to full 3D glory. You can even punch in a custom light source so you can really get a feel for what the actual room will look like with your real-world lighting conditions.Once you’re done, you can also export your project as a high-quality, ultra HD video file, so you can show everyone exactly what you’re envisioning.A $69.99 value, you can pick up Live Home 3D Pro for Mac with this limited time offer for almost a third of its regular price, only $24.99.",Bring home design plans to life with this 3D app that’s just $24.99
9729,3742639,2018-02-22 13:07:09,"I was fortunate enough to personally meet Guy Kawasaki, best-selling author, empowering speaker, and all-around entrepreneurship evangelist, at SXSW 2017. If anybody has the inside scoop on how to get a startup off the ground, it’s him.Here are three important things I learned from speaking with Guy, which I bet can help you on your startup journey:#1 Focus on your prototype, not your pitchYou’ve met them at startup cons and bootcamps. Maybe you are one yourself. You know who I’m talking about — those guys who obsess over their pitch, who are so concerned with getting it pitch perfect (pardon the pun) that they lose sight of what’s really important.Unless you’re in the business of creating pitches, a PowerPoint show or an Excel sheet won’t help you as an entrepreneur. The only thing that needs your full attention — that should be perfect — is your prototype. Get your product right, get it out there, and make sure people are using it.#2 Forget about your business plan, just create a product you’d want to useSome of the biggest companies out there, like Facebook or Airbnb, didn’t feel the need to spend countless hours (not to mention dollars) doing market research or business plans before they started. And you shouldn’t need to either. Chances are it will be outdated before you finish it, anyway.Your guiding principle should be to build a product that you would want to use. It’s as simple as that. That’s not a guarantee of success, but neither is a ton of market research. Just get out there and do it. If it succeeds, great! If not, take your real-world experience and try again.#3 Don’t make your startup all about fundraising and befriending VCsIt’s so easy to get stuck on the fundraising treadmill and wear yourself out trying to woo reluctant investors, or meeting with VCs who don’t get excited about your product. What they care about is making money, and so should you. Make sure you’re working on sales, because even if you’ve got a great product, if you haven’t figured out how you’re going to sell it, you’re not going to get very far.And last but not least, according to Kawasaki, timing is everything. And part of that is just luck. But remember: preparation + opportunity = luck. In other words, we make our own luck!This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",3 simple rules venture capitalist Guy Kawasaki gave me for running a startup
9730,3742640,2018-02-23 16:03:43,"About TNWTNW SitesVoice assistants are making the smartphone redundantWe have been using smartphones for over 20 years now. They have evolved from clunky brick-like devices, only to return back to clunky brick-like devices. Companies have toyed with various form factors, from flip phones to bendy phones and several other impractical designs. But it seems like the ‘slab’ is here to stay.Limitations of the smartphone, include its small screen sizes, inefficiency multitasking, being a general nuisance in public, and not to mention the small fact that they can be used to spy on us.So perhaps it is time to stop trying to evolve the smartphone and ditch the technology once and for all? Like all othertechnologies that have become part of our daily life, smartphones will not just simply disappear. It will be decades before they are rendered obsolete, however, the process has already begun.Say ‘Hey Siri’Siri made its debut in 2010 on the AppleiPhone; while at that time it was largely considered a bit gimmicky, the voice assistant has managed to carve a special space for itself in our lives. Siri was basically able to do things like schedule appointments, find emails and make small notes. Its emergence signified an improvement in our ability to engage with our smartphones, and this has developed even further over time.Today it can help you navigate while driving, choose what to watch on your Apple TV, and can even turn the lights on and off in your bedroom. While Siri may be the most recognizable digital voice assistant, it is not the only one out there by any measure.There is Google Assistant, Samsung’s Bixby, Microsoft Cortana and Amazon’s Alexa. In fact, some of these assistants are far smarter than Siri. Despite the fact that it was first on the scene, Siri does not have access to the same array of information that is at the disposal of search giant’s like Google.Stone Temple collected a set of 5000 different questions about ‘everyday factual knowledge’ to try to gauge which personal assistant is the smartest.Voice assistants are becoming ubiquitousThe technology that was once unique to our smartphones has now moved beyond the glass slabs in our pockets. Smart speakers are here, and this means that you now have instant access to a digital voice assistant simply by placing a speaker in a room.According to Quartz, 17% of US consumers already own a smart speaker. Granted, the initial appeal of these speakers may not be particularly high and their current use is likely trivial. However, as noted byQuartz, who quoted data from the Activate Tech and Media Outlook 2018 report, most users are not using the speakers to their full potential. This included even to the so-called ‘super users’ of these speakers.For all of that, the real excitement surrounding digital voice assistants lies beyond these speakers. Sooner rather than later, these assistants will permeate into devices that we are all too familiar with – headphones, car stereos and more. We will be able to speak into the air in front of us and get whatever information we need whenever we want.Smart assistants are moving beyond operating systemsAt the moment you might think that Siri is limited to Apple devices, Alexa can only be found on Amazon platform, and that Cortana and Google Assistant are tied to Windows Mobile and Android platforms. You would be largely correct in making that assumption, however, things are changing so fast that these assistants are jumping platforms in the blink of an eye.A notable example of this is the ambitious plans that Amazon has for Alexa. As disclosed by The Verge, Amazon has plans to integrate Alexa into cars; its focus is squarely on devices that would make the voice assistant available everywhere. The Verge alsoreportedthat a partnership between Microsoft and Amazon was on the cards. According to the article, “Amazon CEO Jeff Bezos reached out to Microsoft CEO Satya Nadella to form a partnership last month. Amazon and Microsoft are partnering to better integrate their Alexa and Cortana digital assistants. Alexa users will be able to access Cortana, and vice versa on a range of devices. While the integration will be a little awkward at first, it paves the way for digital assistants to talk to each other across platforms.”Companies are essentially pushing their voice assistants applications across platforms, in order to extend their reach and gain users. Google Assistant is already available on iPhones, and Cortana accessible to both Android and Apple users.Talking toastersFrom Jarvis of Iron Man fame to the computer system in Star Trek, humanity has always fantasized about computer technology that we can communicate with. Over the last decade, we’ve seen a vast improvement in mobile user interface, but this still hasn’t satisfied our desire to be able to talk to our machines like we talk to each other.Wired quotedGartner, who predicts that 30 percent of our interactions with technology will happen through ‘conversations’ with smart machines. Now, these smart machines will not only be speakers or headphones that you will be able to bark commands into but will also include household devices.Imagine a toaster that tells you when you’re out of bread or a fridge that automatically texts you a list of items when it needs to be refilled. Fridges could essentially even order things online while you’re at work. More significantly, imagine a world where Siri knows that your blood sugar level is about to spike because the fridge told her that you ate half of the cake that was in it.There is nothing stopping the integration of digital voice assistants with everyday ‘things’. In fact, if anything, we are currently witnessing the race between companies to be the first to do it.Digital assistants are not toysDigital assistants may, at present, seem pretty redundant and nothing more than glorified toys – but this isn’t exactly true. According toTechCrunch, Apple is planning to add translation services to Siri. Google Assistant already offers translations, albeit as Google search results. Moreover, these digital assistants could play an important role in the workplace. They can be used to translate text, alert workers when certain work-related objectives have or haven’t been met, or even take over the role of personal assistants.Wall Street Journalrecently reported on how digital voice assistants are already moving into the office place. They quoted Vinay Patankar, CEO of the workflow-management startup Process Street, who noted that “Workers at Goodwinds Inc. in New York City, for example, have used an Amazon Echo attached to the office ceiling for such tasks as adding events to their calendars and setting reminders for meetings.” However the problem, according to WSJ, is that the products aren’t quite yet ready for the workplace and are still more geared towards home use. Nevertheless, they recognize that this will change in the future.The walls have earsVoice assistants may have been designed to make our lives easier but are they also being used to spy on us?The privacy nightmare begins when you start asking your digital assistant questions. Your queries leave a trail, and these questions from the data that IT giants use to develop the voice assistants.For example, according to the Atlantic, Apple stores Siri requests with device ID for six months, they then delete the device ID but keep the audio for another 18 months. Then there’s the issue of vulnerabilities in devices that have these smart digital assistants. Google’s Home Mini Speaker wascaughtrecording everything its users were saying 24*7.Amazon’s smart speakers also have issues. Not only are they triggered by the word ‘Alexa’ and certain other words, they can be triggered by literally anyone. Supposing you know someone called Alexa and you mention them in a casual conversation around a speaker, you would inadvertently be telling the speaker to record everything you are saying. The obvious risk here is that this vulnerability can be exploited to listen in on people’s conversations.Last year Amazon was involved in a murder investigation in which prosecutors requested information from a suspect’s Amazon Echo smart speaker. According toCNN, Amazon had been resistant to hand over the information recorded through the Echo to Benton County (AR) Prosecution Attorney Nathan Smith. In this case, the smart speaker was actually a “witness” to the murder. If this case is anything to go by, a future where your freedom may hinge on a bot is entirely feasible.Convenience comes at a costThe limitations of one technology can be overcome by the advent of another. The telegraph missed the personal element of communication, so the telephone gave us a voice. The landline phone did not offer mobility, and cellular phones fixed that.These days smartphones are not as smart as we would like, but we now have voice assistants. Eventually, this technology will reach its limitation and a new technology will be developed to solve that. This is how progress works.After all, necessity is the mother of all invention. The integration of digital voice assistants with everyday devices will free us from having to carry a phone all the time. On the other hand, this may mean giving up the ability to speak freely without the worry of someone eavesdropping. The question is – is this a price that we are willing to pay?This post was written by Shivdeep Dhaliwal for Binary District, an international сollaborative technology community which creates unique competency-based workshops and events on new technologies. Follow them down here:",Voice assistants are making the smartphone redundant
9731,3742641,2018-02-22 11:05:14,"About TNWTNW SitesWe all hate advertising, but there’s an easy fixLike most people, I hate advertising. I hate it because of its manipulative nature, its constant nagging, the off-target communication, and for trying to literally follow you everywhere and trampling your privacy along the way.Luckily some companies are trying to do things differently. They try to earn a spot in your life. By sharing knowledge, giving guidance, and actually becoming relevant in your life.But first I’ll explain why I hate advertising, and why it deserves it. All ads contain a form of deceitfulness and have an imbalance between what the product actually is and what absurd or exaggerated story they choose to match it with.Advertising doesn’t bother too much with pesky reality and has helped accelerate many dubious payoffs and products. The industry is known for promoting things like smoking and fast food with a straight face. They are guilty of publishing overly photoshopped fashion and makeup campaigns that create unachievable beauty standards and have ruined many young people’s self-image. Not all advertising is lying however, some of it is ironically, actually true.“Once you pop you can’t stop!” For Pringles, it’s actually true. Research has shown that the salt on the chips drives the neurons in your brain to hunger for more. Your brain doesn’t really register that it has had enough salt. Which can in the long term result in cardiovascular disease. Many people in advertising have taken little responsibility for these types of insights and are often the last to find their moral compass.To make an already bad situation worse, the development in tech has made ads become even more intrusive by giving them tracking abilities. Behind the ads now lies a dark world of data collection and distribution. Not to better service you but to better target and track you. Which they call tracking but might as well have been called surveillance.The ad industry in a turmoil and the silver lining for the ad-hatersIf you don’t like advertising, like me, you must have had a great time seeing the ad industry struggling in recent months. Two of the biggest spenders on advertising, Unilever and P&G, are putting less money in advertising each passing year.The reason why the biggest players are decreasing their ad spending is because they find the system for online advertising murky at best, and fraudulent at worst. Which sounds quite ironic to me, considering that we’re talking about ads here, which one could argue are there to con people into buying a product — so not exactly the antithesis of murky/fraudulent.We ad-haters can also celebrate the increase of adblocking, with over 26 percent of the desktop users blocking ads and around 16 percent on mobile. This is quite a large feat considering how difficult it is for less tech-literate users to set up adblockers. Adblocking is actually the largest boycott of anything in the world.Scandals like VW had with its “clean diesel cars,” which turned out to be not so clean at all. VW’s complementary deceitful campaigns were a treat to watch as well. No wonder overall trust in companies is at an all time low at 35 percent according to the Edelman Trust Barometer. Companies are producing terrible ads and continuously losing their customers’ trust — it’s time for some change.However, it would be naive to think we could get rid of all advertising for good. With over $542 billion flowing through the system in 2016 alone — it’s still a force to be reckoned with. I do however see a shift towards companies acting more as guides rather than sales people, flipping the traditional relationship so that they first prove themselves useful, trying to earn a spot in your life with relevant content, then sell their product.Maybe it’s wishful thinking on my part that this shift is at all possible, but some disruptive brands might have already set this paradigm shift into motion and are reaping the benefits while others are still stuck in old methods.How to properly spend your ad dollarsCasper, the online mattress company, has become popular due to people sharing their “unboxing” videos, with customers filming their newly delivered mattress unfurling from its tiny box.But Casper also ‘advertises’ itself through content marketing by producing articles on separate sites like Van Winkle’s and now Woolly — which are both run by journalists independent of Casper.The articles focus on health and wellness — topics that are beneficial to everyone — but without any mentions of mattresses at all! By sharing information that’s valuable to their potential customers — and also aligned with their brand’s vision — Casper is charting a sincere and successful route to financial success.I noticed that it changed the perception I had of the brand. I almost felt like I owed the company something because they gave me such great insights — and that’s the perfect way to advertise! I get entertainment and/or information, and instead I might decide to pay some attention to your brand. The choice is mine.Another brand that used their ad dollars and spent them on something relevant which doesn’t irritate me to death is Under Armour, which battles giants like Nike and Adidas in the world of sports apparel. Under Armour’s secret weapon is the extremely helpful MyFitnessPal smartphone app.The app has all the typical features for a health app: weight tracking, step counter, and contextual advice, but where it really shines is in tracking your diet. The nutritional value of nearly every food in the world has been entered into their system — by consumers, by the way — so you only need to scan a barcode and the nutritional info appears as if by magic. Plus, they use big data to find out what the most successful users of the app are eating.By becoming a daily utility, Under Armour has turned me into a nutritional god and slowly but steadily I started to notice the company’s clothing more in the streets. It was almost like everybody I saw was wearing it and a thought crept into my mind that I should maybe acquire some Under Armour apparel…Under Armour managed to sneak into my consciousness, but I feel they did it fair and square. I got a valuable app and they got brand awareness. Many brands have also been successful by simply sponsoring a valuable service, rather than providing it themselves. Citibank is the sponsor of New York City’sCiti Bike, a bicycle sharing system.Citibank reported that following the launch of Citi Bike in 2013, the public’s perception of the company improved by 17 points. Citibank was seen as a more innovative company and as one that valued social responsibility. By aligning themselves with a useful service, Citibank has seen tremendous brand recognition throughout New York and turned people’s opinions of the company around.This truly baffled me because Citibank was at the center of the economic meltdown, but it seems that was quickly forgotten in New York, simply due to the sponsoring of some bicycles. To be honest, I’m not sure how I feel about that, but at least the image restoration of the bank was achieved by giving people something positive, instead of just bombarding them with ads.As you might have guessed, these types of initiatives don’t necessarily represent all aspects of a company. After all, Casper has sued mattress review sites and tried to wrangle better reviews for their products and Citibank had a company wide focus on subprime mortgages, which made them tumble during the financial crisis. It’s still advertising. But I believe these service-oriented campaigns can change a company’s DNA for the better.These are the early steps of companies breaking through the advertising clutter to connect with audiences. They find new ways to become relevant and available in their customers’ lives.Can you imagine what good it would do even if you only manage to channel a fraction of the $542 billion total ad spend to something more relevant and useful? The impact would be enormous! There would also be less frustrated people on earth, because there would be less useless ads. So saving advertising has an easy fix, put your marketing budget into initiatives that give people something.This is the right time to set yourself apart before adblocking rises even further and trust declines even more. Now is the time to reboot your ad strategy and build on a more service oriented relationship with your customer. You can start small, that’s ok. Move your ad spend from sales to being of service and come to the light side, it’s the only way.","We all hate advertising, but there's an easy fix"
9732,3742642,2018-02-24 00:25:54,"Clients may choose to conduct business with other market participants who engage in business or offer products in areas we deem speculative or risky, such as cryptocurrencies. Increased competition may negatively affect our earnings by creating pressure to lower prices or credit standards on our products and services requiring additional investment to improve the quality and delivery of our technology and/or reducing our market share, or affecting the willingness of clients to do business with us.Still on the fringes of the mainstream, cryptocurrencies have gained notoriety in recent months — particularly after a strong December that saw Bitcoin reach new heights of almost $20,000 per coin, and the addition of futures contracts on two major exchanges.But a January “dip” may have led newer investors, or speculators, to ice their wounds in safer markets after prices tumbled over 50 percent.As Bitcoin and the altcoin market continue to recover, expect the hype cycle to start again and for cryptocurrency to wedge its way, firmly this time, into the lexicon.",Bank of America admits cryptocurrency is threatening its business model
9733,3742643,2018-02-24 00:05:15,"TNW SitesTop Trump aide indicted thanks to paper trail from his inability to convert a PDF to DOCFormer Trump campaign manager Paul Manafort may have gift-wrapped a smoking gun in his upcoming trial by not bothering to learn a few basic computer skills.Special prosecutor Robert Mueller filed a second indictment yesterday against Manafort and his associate Richard Gates, accusing them of multiple money-laundering charges. Allegedly, the pair falsified documents in order to help hide their overseas income.One of the main pieces of evidence in their case? An email exchange between Gates and Manafort that only exists because Manafort couldn’t be bothered to figure out how to turn a PDF into a Word doc.Y'all I'm dying, Manafort created an incriminating paper trail because he needed someone to help him convert a Word doc to PDF. pic.twitter.com/CTE4oV7zj0In order to allegedly falsify a profit-and-loss document for his company, he had to send it to Gates first. Here’s the relevant passage in the indictment. Insert derisive snickering at your own discretion:To create the false 2016 P&L, on or about October 21, 2016, Manafort emailed Gates a .pdf version of the real 2016 DMI P&L, which showed a loss of more than $600,000. Gates converted that .pdf into a “Word” document so that it could be edited, which Gates sent back to Manafort. Manafort altered that “Word” document by adding more than $3.5 million in income. He then sent this falsified P&L to Gates and asked that the “Word” document be converted back to a .pdf, which Gates did and returned to Manafort. Manafort then sent the falsified 2016 DMI P&L .pdf to Lender D.So they emailed the document between themselves, leaving a paper trail, because Manafort couldn’t do the five minutes of Googling it takes to find a conversion tutorial.TNW Conference 2018 features a track called Power Shift, which is dedicated to showing other ways tech shifts the dynamics of power. For more information, check out our event page.",Here's why you want to know how to convert word files
9734,3742644,2018-02-23 23:45:15,"About TNWTNW SitesYouTube spanks Alex Jones, offers dire warning for future videosA media organization known for dealing in half-truths and complete falsehoods found itself on shaky ground this week after YouTube pulled one of its videos. Alex Jones, a purveyor of far-reaching conspiracy theories aimed at the right — and pills that apparently turn you red — received a warning from YouTube after portraying survivors of the Parkland school shootings as paid crisis actors.Seriously, what does this pill do?The video focused on David Hogg, a survivor of the Parkland massacre on Valentine’s Day that saw 17 of his classmates gunned down. Hogg, a 17-year-old senior fell into the spotlight over the past week after a shooter opened fire at his Parkland, Florida high school. He’s since found solace in using his newfound platform to advocate for gun control, a position he and his peers have proven to be remarkably adept at.Maybe too adept, according to Jones and his fringe news site, InfoWars.Jones alleged that Hogg, a strong voice among the survivors, was perhaps too skilled at public speaking and alleged both he and his peers were paid crisis actors — a term that refers to those paid to play the victims in emergency drills.The InfoWars channel is notorious for creating these sorts of conspiracies out of thin air, often offering little proof to accompany provocative headlines and pervasive rhetoric on its main YouTube channel, The Alex Jones Show. Previous stories offered up head-scratching spin on the school shooting at Sandy Hook (which Jones states was a false flag operation carried out by government operatives) and a train wreck carrying dozens of Republican congressmen (which Jones attempts to spin into a devious plot by democrats to kill congressional rivals).About a day’s worth of content on InfoWars’ main account: The Alex Jones Channel.Whether Jones believes the narratives he spins is unclear. His lawyer last year claimed it was all an act. “He’s playing a character,” attorney Randall Wilhite said during a pretrial hearing at a custody proceeding after Jones’ divorce last year. “He is a performance artist.”The comments accompanied allegations by his ex-wife, Kelly, that “he’s not a stable person,” citing his broadcasts as proof that he’s an unfit parent.YouTube though, has had enough. Sources familiar with the matter told CNN that The Alex Jones Channel had received a strike for his attack on Hogg, part of YouTube’s three strikes approach to dealing with misinformation, harassment, and hate speech — Jones has, at some point or another, been responsible for all three.According to a YouTube spokesperson:Last summer we updated the application of our harassment policy to include hoax videos that target the victims of these tragedies. Any video flagged to us that violates this policy is reviewed and then removed.The video in question, titled “David Hogg Can’t Remember His Lines in TV Interview,” has already been removed, but more immediate danger looms for Jones and his 2.2 million subscribers. YouTube’s guidelines state that if the account receives a second strike in the same three month period, it won’t be able to post new content for two weeks.Being unable to publish would be a blow to Jones and InfoWars, who produce a staggering number of videos daily — often a dozen or more.If the channel receives a third strike, it’s removed from the platform entirely.Jones, we expect, will play nice. Syndication is key for his particular brand of news, and limiting InfoWars’ ability to distribute content isn’t going to be kind on his wallet — money that, unlike his integrity, we’re certain he’d miss.Want to hear more about tech and politics from the world’s leading experts? Join in the discussion with our Debate track at TNW Conference 2018. Check out info and get your tickets here.","YouTube spanks Alex Jones, offers dire warning for future videos"
9735,3742645,2018-02-23 23:24:26,"TNW SitesFCC chairman awarded gun from NRA for repealing net neutralityThe National Rifle Association (NRA) today awarded its most prestigious honor to a man who, like the organization itself, truly understands what it means to be despised: former Verizon lawyer, and tech’s biggest douche bag in 2017, Ajit Pai.The “Charlton Heston Courage Under Fire” award, which isn’t issued every year, is given to those who the NRA sees fit to join the ranks of its “distinguished pantheon.”Previous winners include Rush Limbaugh, a recovering drug addict who dislikes public schools, David Clarke, a racist real-world embodiment of Yosemite Sam, Phyllis Schlafly, who once charmingly said “Sex education classes are like in-home sales parties for abortions,” and Roy Innis, who believed racial segregation was a good thing.There were some empty chairs at the Legion of Doom while this awards presentation was going down.The presentation was held at the Conservative Political Action Conference (CPAC), where members of the GOP and their fans get together to celebrate their accomplishments. And, presumably, drink the blood of baby unicorns.The best thing about the “Charlton Heston Courage Under Fire” award is that it comes with a gift. They couldn’t present that gift to Pai during the presentation, however, because it’s a rifle. And guns are too dangerous to be allowed on stage at CPAC.Want to hear more about tech and politics from the world’s leading experts? Join in the discussion with our Debate track at TNW Conference 2018. Check out info and get your tickets here.",FCC Chairman awarded a gun from the NRA for repealing net neutrality
9736,3742646,2018-02-23 21:27:45,"TNW SitesCheck out these 5 wild and wacky quantum computer facts for cool peopleQuantum computers are getting popular, and all the cool publications are writing about them. But, for the most part, it’s all very serious and cautious reporting. That’s a shame, because quantum mechanics are wild and wacky. So, let’s have some fun.The world of quantum mechanics, and by extension quantum computing, is full of difficult to grasp concepts like time travel, teleportation, and parallel universes. It captivated both Einstein and Schrodinger and many of its secrets still allude scientists today. That’s because none of it makes any damn sense.Even though we’re already making them, nobody quite knows how they work.Quantum computer breakthroughs were a dime-a-dozen in 2017. Companies like Google and IBM played tit-for-tat upping qubit counts and Intel’s already managed to put one on a silicon chip.But, scientists have yet to figure out how “quantum entanglement” works. At the heart of quantum computing lies the ability for a quantum bit, or qubit, to be two things at the same time. This is done through quantum entanglement, and it’s just as screwy as it sounds – in nature things aren’t always what they seem.That’s right, quantum computers are natural – unlike binary systems.When scientists from IBM last year used a seven-qubit processor to conduct a successful simulation of a beryllium-hydride molecule it was important for several reasons. First, because it was the first time it’d been done, but more importantly it showed off one of quantum computing’s biggest promises: the ability to accurately simulate the natural world.Physics simulators, to date, rely on binary math. In other words, things either ‘are’ or they ‘are not’ when it comes to their ability to create deterministic models of the natural world. Quantum computers don’t suffer from this limitation, which means they’ll be able to more accurately imitate reality.Time-travel is real and quantum particles can do it.It’s a well-known fact that it takes 1.21 gigawatts of power for a Delorean to travel through time, but not every lab has enough space to fit a car inside. Also, Deloreans can’t time travel, quantum particles actually can.Work published last year by an international team of scientists indicate they exploited quantum mechanics to send some tiny quantum particles back in time. It’s a far cry from the Tardis, but we gotta start somewhere.Quantum entanglement is actually teleportation.Last year a group of Chinese scientists used quantum teleportation to send a message from a satellite in space to two separate ground stations on Earth.The team created pairs of photons that were separated by distance, but not time. These photons mirror one another, so a message inserted into one of them is instantly reflected in the other. This allowed the scientists to communicate using light without being limited by the speed of light.Quantum computing might be proof of an alternate universe.Maybe you don’t like the fact that Pluto isn’t a planet, or perhaps the politics of Alpha Centauri piss you off. Either way, there’s good news: there’s a decent likelihood we’re only seeing half the picture because there are alternate universes.Scientists like Oxford’s Dr. David Deutsch believe that quantum bits can exist in two states at once because they’re in two universes at the same time.The quantum theory of parallel universes is not the problem, it is the solution. It is not some troublesome, optional interpretation emerging from arcane theoretical considerations. It is the explanation — the only one that is tenable — of a remarkable and counter-intuitive reality.And with an official title like “Visiting Professor in the Department of Atomic and Laser Physics” he sounds like someone who knows what’s going on.Quantum computers are insanely complex machines that operate on incomprehensible principles. It’s quite possible that, in the future, they’ll unlock the mysteries of the universe.But right now they’re just plain spooky.Want to hear more about the future from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.","Time travel, teleportation, and other cool things quantum computers can do"
9737,3745661,2018-02-22 14:25:39,"About TNWTNW Sites5 surprisingly wholesome things I’ve found on the dark webThe last thing you might expect to come across on the shady digital underground is a book on origami. But there it was, on the virtual shelves of the dark web, wedged between the edibles, the LSD, the stolen credit cards, and the counterfeit passports: Mind Blowing Modular Origami, just $2.Not familiar with the dark web? Never fear, I’m here to help. The dark web is a mix of eBay for criminals, the weirdest flea market you’ve ever been to, and a bad guy meet-up. It’s a strange place, and it’s never quite clear what you’re getting yourself into when you show up.As someone who spends a lot of time on the illicit side of the dark web, I have to tell you: it can take a lot to make you look twice. When you spend years scrolling through listings for stolen tax documents, porn accounts, fake bank statements, and hacking guides, “unusual” takes on a new meaning. Over the years, I’ve begun building a collection of the weirdest things I come across. These are a few of my favorites — five of the strangest, silliest things you’ll find on the dark web.Getting carded (on purpose)Fake IDs for sale doesn’t normally raise any eyebrows. The dark web trades in all kinds of identity documentation, and drivers’ licenses are pretty standard offerings. One vendor decided to do things a little differently and started selling underage fake IDs.According to the vendor, the listing started off as a joke and grew into a legitimate offering. Perfect for cheap hockey tickets, discounted gym memberships, and confusing your local bouncer.We’re going to need a bigger boatPeople in the dark web criminal communities like to help each other out. They write guides – instructions, really – on how to hack certain sites, how to use stolen credit cards, how to use malware, etc.One big pack of fraud guides (yes, they come in packs) had a guide on deep sea fishing. No, not phishing like sending sketchy emails. Deep sea fishing, for kingfish no less.Polly wanna… what?Right before the holiday season a few years ago, one vendor decided to up the gift-giving ante with a pair of parrots. Yes, parrots. A listing went up in early December for a pair of African grey parrots. The dark web does facilitate the illicit animal trade, but these guys seemed like pets in need of a new home.The ad even said as much, promising the parrots will make “the best pets and companion for you and your kids.” The vendor offered free Skype calls for proof of life, and was happy to include “all accessories and cage if necessary with all toys for the well beign [sic] of these babies.” The listing disappeared right before Valentine’s Day. Coincidence? I think not.Needs more jazz fluteAs a friendly reminder, not everything on the dark web is illegal: more than one million people use Tor to access Facebook each month. The dark web allows for private, secure browsing — and apparently, some pretty sweet beats.Deep Web Radio was home to the smoothest jazz this side of the internet (they had baroque and heavy metal channels too, if that’s more your speed). I am sorry to report the site operator did not go by Duke Silver.I can haz dark webz?I promised you there would be cats, and here we are. No, not cryptokittes — no blockchain scritchy-scratches here. Tor Kittenz was a pure joy, exactly what any of us need after a long day: a never-ending slideshow of cat pictures.Just cats being cats, looking cute and finding weird places to sit. I still check the site from time to time, hoping it will come back up.The dark web is more than you think it isThe dark web is not all scary. It’s easy to get caught up in the fear and uncertainty of something unfamiliar. Imagine the dark web is a faraway city that you keep hearing about — and the more you hear, the less you want to visit.Sure, there are sections of the dark web that are dangerous, and sometimes criminal, just like there are parts of a city that you wouldn’t want to go at night. We should talk about those parts, especially when sensitive information — information we entrust to other people — is being bought and sold in huge volumes.But that’s not the entire city (if you’ll stick with me on the analogy) and that shouldn’t be all we talk about when we talk about the dark web. There’s plenty of space to sit back and marvel at the strange things people put online.The dark web is just another part of the internet: there’s music, and cats, and weird people doing weird things, off whispering in their own little corners. It’s all much more familiar than you might expect.Bonus pic: A person selling a grilled cheese sandwich on the dark web.",5 surprisingly wholesome things I’ve found on the dark web
9738,3748056,2018-02-24 16:27:00,"Why 3.5 million Americans in their prime years aren’t working — and no, it’s not video gamesMillions of Americans who would have been working 20 years ago no longer do so because of vast changes in the U.S. and global economies.The sizzling U.S. labor market has knocked the unemployment rate down to a 17-year low, but millions of Americans in their prime who would have been working back then do not have jobs now.How come? China, robots, disability benefits, minimum wages and jail-time are the biggest culprits, according to a pair of researchers at the University of Maryland.The percentage of the U.S. population with jobs sank from a record 64.7% in 2000 to a 28-year low of 58.2% by 2011 before beginning a gradual recovery. The brunt of the decline occurred during the 2007-2009 recession, but the problem had been long in the making.“These worrisome developments were exacerbated by the Great Recession, but their roots preceded its onset,” wrote economists Katharine Abraham and Melissa Kearney at the University of Maryland in a new report distributed by the National Bureau of Economic Research. Abraham is a former commissioner of the Bureau of Labor Statistics.The problem is still acute among young people and even Americans in their prime working years of 25 to 54, especially men.Surprisingly it’s not the case for older people nearing retirement age. The share of those ages 55 to 64 actually rose until just very recently.Whatever the case, the impact on the economy is profound.If men and women from the ages 25 to 54 took part in the labor market at the same rates as they did in 1999, another 3.5 million Americans could either be at work today or looking for jobs. That would be more fuel for the U.S. economy and a bigger source of workers for businesses crying out about a shortage of labor.The China tradeWhat caused these people to leave the labor force — or not even enter?Not surprisingly the acceleration in global trade, punctuated by the emergence of China, had the biggest impact, the researchers found.Millions of jobs that used to be performed by working-class Americans shifted overseas or disappeared after the turn of the century. Manufacturing was particularly hard hit — more than 5 million jobs were lost from 1999 to 2016.Trade with China flooded the U.S. with cheap imports, forced domestic firms to shift operations overseas and put downward pressure on wages of less-skilled Americans.The rise of robots appears to be another critical factor even though the effects of automation are harder to determine, the report said. Think of how many bank jobs have been lost due to ATMs and online banking, for example.A smaller but not insignificant factor is an increase in federal disability benefits. The percentage of men ages 25-54 on disability insurance rose by a third since the mid-1980s. Many used to work in manufacturing.The disability program plays “an increasingly important role in providing income for less educated workers negatively impacted by” trade, automation and the like, the authors said. A “sizable” number of those approved for disability benefits would have worked if there applications were turned down.Higher minimum wages are probably another cause.After declining in inflation-adjusted terms from 1998 to 2007, minimum wages rose almost 11% in real terms between 2007 and 2016, Abraham and Kearney estimated. Firms may have outsourced jobs or spent more on automation to offset the higher costs of low-skilled labor.A greater percentage of men serving time in jail, especially African Americans, had a similar impact as higher minimum wages. Even though crime rates fell, tighter sentencing laws resulted in more people going to prison.The ill effects of a prison sentence also last well beyond time served. Companies are reluctant or unwilling to hire those with criminal records unless they have no choice.Video games not to blameThe authors found little evidence to support other popular theories.The share of unemployed men with working wives has actually fallen, for instance.The authors also could not find evidence that declining labor-force participation is tied to opioid addiction or more young men playing video games in their parents’ basements. Nor is lack of child care or more people getting food stamps to blame.It’s possible these explanations played a role, but more research is needed, Abraham and Kearney said.Intraday Data provided by SIX Financial Information and subject to terms of use. Historical and current end-of-day data provided by SIX Financial Information. All quotes are in local exchange time. Real-time last sale data for U.S. stock quotes reflect trades reported through Nasdaq only. Intraday data delayed at least 15 minutes or per exchange requirements.","Why 3.5 million Americans in their prime years aren’t working — and no, it’s not video games"
9739,3748058,2018-03-01 00:00:00,"In the ideal world, software developers would analyze each problem in the language of its domain and then articulate solutions in matching terms. They could thus easily communicate with domain experts and separate problem-specific ideas from the details of general-purpose languages and specific program design decisions.Key InsightsIn the real world, however, programmers use a mainstream programming language someone else picked for them. To address this conflict, they resort to—and on occasion build their own—domain-specific languages embedded in the chosen language (embedded domain-specific languages, or eDSLs). For example, JavaScript programmers employ jQuery for interacting with the Document Object Model and React for dealing with events and concurrency. As developers solve their problems in appropriate eDSLs, they compose these solutions into one system; that is, they effectively write multilingual software in a common host language.aSadly, multilingual eDSL programming is done today on an ad hoc basis and is rather cumbersome. To create and deploy a language, programmers usually must step outside the chosen language to set up configuration files and run compilation tools and link-in the resulting object-code files. Worse, the host languages fail to support the proper and sound integration of components in different eDSLs. Moreover, most available integrated development environments (IDEs) do not even understand eDSLs or perceive the presence of code written in eDSLs.The goal of the Racket project is to explore this emerging idea of language-oriented programming, or LOP, at two different levels. At the practical level, the goal is to build a programming language that enables language-oriented software design. This language must facilitate easy creation of eDSLs, immediate development of components in these newly created languages, and integration of components in distinct eDSLs; Racket is available at http://racket-lang.org/At the conceptual level, the case for LOP is analogous to the ones for object-oriented programming and for concurrency-oriented programming.3 The former arose from making the creation and manipulation of objects syntactically simple and dynamically cheap, the latter from Erlang's inexpensive process creation and message passing. Both innovations enabled new ways to develop software and triggered research projects. The question is how our discipline will realize LOP and how it will affect the world of software.Our decision to develop a new language—Racket—is partly an historical artifact and partly due to our desire to free ourselves from any unnecessary constraints of industrial mainstream languages as we investigate LOP. The next section spells out how Racket got started, how we honed in on LOP, and what the idea of LOP implies.Principles of RacketThe Racket project dates to January 1995 when we started it as a language for experimenting with pedagogic programming languages.15 Working on them quickly taught us that a language itself is a problem-solving tool. We soon found ourselves developing different languages for different parts of the project: a (meta-) language for expressing many pedagogic languages, another for specializing the DrRacket IDE,15 and a third for managing configurations. In the end, the software was a multilingual system, as outlined earlier.Racket's guiding principle reflects the insight we gained: empower programmers to create new programming languages easily and add them with a friction-free process to a codebase. By ""language,"" we mean a new syntax, a static semantics, and a dynamic semantics that usually maps the new syntax to elements of the host language and possibly external languages via a foreign-function interface (FFI). For a concrete example, see Figure 1 for a diagram of the architecture of a recently developed pair of scripting languages for video editing2,b designed to assist people who turn recordings of conference presentations into YouTube videos and channels. Most of that work is repetitive—adding preludes and post-ludes, concatenating playlists, and superimposing audio—with few steps demanding manual intervention. This task calls for a domain-specific scripting language; video is a declarative eDSL that meets this need.Figure 1. Small language-oriented programming example.The typed/video language adds a type system to video. Clearly, the domain of type systems comes with its own language of expertise, and typed/video's implementation thus uses turnstile,6 an eDSL created for expressing type systems. Likewise, the implementation of video's rendering facility calls for bindings to a multimedia framework. Ours separates the binding definitions from the repetitive details of FFI calls, yielding two parts: an eDSL for multimedia FFIs, dubbed video/ffi, and a single program in the eDSL. Finally, in support of creating all these eDSLs, Racket comes with the syntax parse eDSL,7 which targets eDSL creation.The LOP principle implies two subsidiary guidelines:Enable creators of a language to enforce its invariants. A programming language is an abstraction, and abstractions are about integrity. Java, for example, comes with memory safety and type soundness. When a program consists of pieces in different languages, values flow from one context into another and need protection from operations that might violate their integrity, as we discuss later; andMost notably, Racket eliminates the hard boundary between library and language, overcoming a seemingly intractable conflict.Turn extra-linguistic mechanisms into linguistic constructs. A LOP programmer who resorts to extra-linguistic mechanisms effectively acknowledges that the chosen language lacks expressive power.13,c The numerous external languages required to deal with Java projects—a configuration language, a project description language, and a makefile language—represent symptoms of this problem. We treat such gaps as challenges later in the article.They have been developed in a feedback loop that includes DrRacket15 plus typed,36 lazy,4 and pedagogical languages.15Libraries and Languages ReconciledRacket is an heir of Lisp and Scheme. Unlike these ancestors, however, Racket emphasizes functional over imperative programming without enforcing an ideology. Racket is agnostic when it comes to surface syntax, accommodating even conventional variants (such as Algol 60).d Like many languages, Racket comes with ""batteries included.""Most notably, Racket eliminates the hard boundary between library and language, overcoming a seemingly intractable conflict. In practice, this means new linguistic constructs are as seamlessly imported as functions and classes from libraries and packages. For example, Racket's class system and for loops are imports from plain libraries, yet most programmers use these constructs without ever noticing their nature as user-defined concepts.Racket's key innovation is a modular syntax system,17,26 an improvement over Scheme's macro system,11,24,25 which in turn improved on Lisp's tree-transformation system. A Racket module provides such services as functions, classes, and linguistic constructs. To implement them, a module may require the services of other modules. In this world of modules, creating a new language means simply creating a module that provides the services for a language. Such a module may subtract linguistic constructs from a base language, reinterpret others, and add a few new ones. A language is rarely built from scratch.Like Unix shell scripts, which specify their dialect on the first line, every Racket module specifies its language on the first line, too. This language specification refers to a file that contains a language-defining module. Creating this file is all it takes to install a language built in Racket. Practically speaking, a programmer may develop a language in one tab of the IDE, while another tab may be a module written in the language of the first. Without ever leaving the IDE to run compilers, linkers, or other tools, the developer can modify the language implementation in the first tab and immediately experience the modification in the second; that is, language development is a friction-free process in Racket.In the world of shell scripts, the first-line convention eventually opened the door to a slew of alternatives to shells, including Perl, Python, and Ruby. The Racket world today reflects a similar phenomenon, with language libraries proliferating within its ecosystem: racket/base, the Racket core language; racket, the ""batteries included"" variant; and typed/racket, a typed variant. Some lesser-known examples are datalog and a web-server language.27,30 When precision is needed, we use the lowercase name of the language in typewriter font; otherwise we use just ""Racket.""Figure 2 is an illustrative module. Its first line—pronounced ""hash lang racket base""—says it is written in racket/base. The module provides a single function, walk-simplex. The accompanying line comments—introduced with semicolons—informally state a type definition and a function signature in terms of this type definition; later, we show how developers can use typed/racket to replace such comments with statically checked types, as in Figure 5. To implement this function, the module imports functionality from the constraints module outlined in Figure 3. The last three lines of Figure 2 sketch the definition of the walk-simplex function, which refers to the maximizer function imported from constraints.Figure 2. A plain Racket module.Figure 3. A module for describing a simplex shape.The ""constraints"" module in Figure 3 expresses the implementation of its only service in a domain-specific language because it deals with simplexes, which are naturally expressed through a system of inequalities. The module's simplex language inherits the line-comment syntax from racket/base but uses infix syntax otherwise. As the comments state, the module exports a single function, maximizer, which consumes two optional keyword parameters. When called as (maximizer #:xn), as in Figure 2, it produces the maximal y value of the system of constraints. As in the lower half of Figure 3, these constraints are specified with conventional syntax.In general, cooperating multilingual components must respect the invariants established by each participating language.In support of this kind of programming, Racket's modular syntax system benefits from several key innovations. A particularly illustrative one is the ability to incrementally redefine the meaning of existing language constructs via the module system. It allows eDSL creators to ease their users into a new language by reusing familiar syntax, but reinterpreted.line 09 As with many functional languages, Racket comes with pattern-matching constructs. This one uses syntax-parse from the library mentioned earlier. Its first piece specifies the to-be-matched tree (stx); the remainder specifies a series of pattern-responses clauses.line 10 This pattern matches any syntax tree with first token as new-lambda followed by a parameter specification and a body. The annotation :id demands that the pattern variables x and predicate match only identifiers in the respective positions. Likewise, :expr allows only expressions to match the body pattern variable.line 11 A compile-time function synthesizes new trees with syntax.line 12 The generated syntax tree is a lambda expression. Specifically, the function generates an expression that uses lambda. The underline in the code marks its origin as the ambient language, here racket.other lines Wherever the syntax system encounters the pattern variables x, predicate, and body, it inserts the respective subtrees that match x, predicate, and body.When another module uses ""new-lam"" as its language, the compiler elaborates the surface syntax into the core language like thisThe first elaboration step resolves lambda to its imported meaning,18 or lambda. The second reverses the ""rename on export"" instruction. Finally, the new-lambda compile-time function translates the given syntax tree into a racket function.Inessence, Figure 4 implements a simplistic precondition system for one-argument functions. Next, the language developer might wish to introduce multiargument lambda expressions, add a position for specifying the post-condition, or make the annotations optional. Naturally, the compile-time functions could then be modified to check some or all of these annotations statically, eventually resulting in a language that resembles typed/racket.Sound Cooperation Between LanguagesA LOP-based software system consists of multiple cooperating components, each written in domain-specific languages. Cooperation means the components exchange values, while ""multiple languages"" implies these values are created in distinct languages. In this setting, things can easily go wrong, as demonstrated in Figure 5 with a toy scenario. On the left, a module written in typed/racket exports a numeric differentiation function. On the right, a module written in racket imports this function and applies it in three different ways, all illegal. If such illegal uses of the function were to go undiscovered, developers would not be able to rely on type information for designing functions or for debugging, nor could compilers rely on them for optimizations. In general, cooperating multilingual components must respect the invariants established by each participating language.Figure 5. Protecting invariants.In the real world, programming languages satisfy a spectrum of guarantees about invariants. For example, C++ is unsound. A running C++ program may apply any operation to any bit pattern and, as long as the hardware does not object, program execution continues. The program may even terminate ""normally,"" printing all kinds of output after the misinterpretation of the bits. In contrast, Java does not allow the misrepresentation of bits but is only somewhat more sound than C++.1 ML improves on Java again and is completely sound, with no value ever manipulated by an inappropriate operation.Racket aims to mirror this spectrum of soundness at two levels: language implementation itself and cooperation between two components written in different embedded languages. First consider the soundness of languages. As the literature on domain-specific languages suggests,20 such languages normally evolve in a particular manner, as is true for the Racket world, as in Figure 6. A first implementation is often a thin veneer over an efficient C-level API. Racket developers create such a veneer with a foreign interface that allows parenthesized C-level programming.5 Programmers can refer to a C library, import functions and data structures, and wrap these imports in Racket values. Figure 7 illustrates the idea with a sketch of a module; video's initial implementation consisted of just such a set of bindings to a video-rendering framework. When a racket/base module imports the ffi/unsafe library, the language of the module is unsound.Figure 6. Hardening a module.Figure 7. A Racket module using the foreign-function interface.A language developer who starts with an unsound eDSL is likely to make it sound as the immediate next step. To this end, the language is equipped with runtime checks similar to those found in dynamically typed scripting languages to prevent the flow of bad values to unsound primitives. Unfortunately, such protection is ad hoc, and, unless developers are hypersensitive, the error messages may originate from inside the library, thus blaming some racket/base primitive operation for the error. To address this problem, Racket comes with higher-order contracts16 with which a language developer might uniformly protect the API of a library from bad values. For example, the video/ffi language provides language constructs for making the bindings to the video-rendering framework safe. In addition to plain logical assertions, Racket's developers are also experimenting with contracts for checking protocols, especially temporal ones.9 The built-in blame mechanism of the contract library ensures sound blame assignment.10Finally, a language developer may wish to check some logical invariants before the programs run. Checking simple types is one example, though other forms of static checking are also possible. The typed/video language illustrates this point with a type system that checks the input and output types of functions that may include numeric constraints on the integer arguments; as a result, no script can possibly render a video of negative length. Likewise, typed/racket is a typed variant of (most of) racket.Now consider the soundness of cooperating languages. It is again up to the language developer to anticipate how programs in this language interact with others. For example, the creator of typed/video provides no protection for its programs. In contrast, the creators of typed/racket intended the language to be used in a multilingual context; typed/racket thus compiles the types of exported functions into the higher-order contracts mentioned. When, for example, an exported function must always be applied to integer values, the generated contract inserts a check that ensures the ""integerness"" of the argument at every application site for this function; there is no need to insert such a check for the function's return points because the function is statically type checked. For a function that consumes an integer-valued function, the contract must ensure the function argument always returns an integer. In general, a contract wraps exported values with a proxy31 that controls access to the value. The idea is due to Matthews and Findler,29 while Tobin-Hochstadt's and Felleisen's Blame Theorem35 showed that if something goes wrong with such a mixed system, the runtime exception points to two faulty components and their boundary as the source of the problem.10 In general, Racket supplies a range of protection mechanisms, and a language creator can use them to implement a range of soundness guarantees for cooperating eDSLs.Universality vs. ExpressivenessJust because a general-purpose language can compute all partial-recursive functions, programmers cannot necessarily express all their ideas about programs in this language.13 This point is best illustrated through an example. So, imagine the challenge of building an IDE for a new programming language in the very same language. Like any modern IDE, it is supposed to enable users to compile and run their code. If the code goes into an infinite loop, the user must be able to terminate it with a simple mouse click. To implement this capability in a naturale manner, the language must internalize the idea of a controllable process, a thread. If it does not internalize such a notion, the implementer of the IDE must step outside the language and somehow re-use processes from the underlying operating system.For a programming language researcher, ""stepping outside the language"" signals failure. Or, as Ingalls21 said, ""[an] operating system is a collection of things that don't fit into a language[; t]here shouldn't be one."" We, Racket creators, have sought to identify services Racket borrows from the surrounding operating system and assimilate them into the language itself.19 Here are three sample constructs for which programmers used to step outside of Racket but no longer need to:Sandboxes. That restrict access to resources;Inspectors. That control reflective capabilities; andCustodians. That manage resources (such as threads and sockets).To understand how inclusion of such services helps language designers, consider a 2014 example, the shill language.32 Roughly speaking, shill is a secure scripting language in Racket's ecosystem. With shill, a developer articulates fine-grain security and resource policies—along with, say, what files a function may access or what binaries the script may run—and the language ensures these constraints are satisfied. To make this concrete, consider a homework server to which students can submit their programs. The instructor might wish to run an autograde process for all submissions. Using a shill script, the homework server can execute student programs that cannot successfully attack the server, poke around in the file system for solutions, or access external connections to steal other students' solutions. Naturally, shill's implementation makes extensive use of Racket's means of running code in sandboxes and harvesting resources via custodians.State of AffairsThe preceding sections explained how Racket enables programmers to do the following:Create languages. Create by way of linguistic reuse for specific tasks and aspects of a problem;Equip with soundness. Equip a language with almost any conventional level of soundness, as found in ordinary language implementations; andExploit services. Exploit a variety of internalized operating system services for constructing runtime libraries for these embedded languages.What makes such language-oriented programming work is ""incrementality,"" or the ability to develop languages in small pieces, step by step. If conventional syntax is not a concern, developers can create new languages from old ones, one construct at a time. Likewise, they do not have to deliver a sound and secure product all at once; they can thus create a new language as a wrapper around, say, an existing C-level library, gradually tease out more of the language from the interface, and make the language as sound or secure as time permits or a growing user base demands.Racket borrows from the surrounding operating system and assimilates such extra-linguistic mechanisms into the language itself.Moreover, the entire process takes place within the Racket ecosystem. A developer creates a language as a Racket module and installs it by ""importing"" it into another module. This tight coupling has two implications: the development tools of the ecosystem can be used for creating language modules and their clients; and the language becomes available for creating more languages. Large projects often employ a tower involving a few dozen languages, all helping manage the daunting complexity in modern software systems.Sony's Naughty Dog game studio has created just such a large project, actually a framework for creating projects. Roughly speaking, Sony's Racket-based architecture provides languages for describing scenes, transitions between scenes, scores for scenes, and more. Domain specialists use the languages to describe aspects of the game. The Racket implementation composes these domain-specific programs, then compiles them into dynamically linked libraries for a C-based game engine; Figure 8 sketches the arrangement graphically.Figure 8. A sketch of an industrial example of language-oriented programming.Racket's approach to language-oriented programming is by no means perfect. To start with, recognizing when a library should become a language requires a discriminating judgment call. The next steps require good choices in terms of linguistic constructs, syntax, and runtime primitives.As for concrete syntax, Racket currently has strong support for typical, incremental Lisp-style syntax development, including traditional support for conventional syntax, or generating lexers and parsers. While traditional parsing introduces the natural separation between surface syntax and meaning mentioned earlier, it also means the development process is no longer incremental. The proper solution would be to inject Racket ideas into a context where conventional syntax is the default.fAs for static checking, Racket forces language designers to develop such checkers wholesale, not incrementally. The type checker for typed/racket looks like, for example, the type checker for any conventionally typed language; it is a complete recursive-descent algorithm that traverses the module's representation and algebraically checks types. What Racket developers really want is a way to attach type-checking rules to linguistic constructs, so such algorithms can be synthesized as needed.Chang et al.6 probably took a first step toward a solution for this problem and have thus far demonstrated how their approach can equip a DSL with any structural type system in an incremental and modular manner. A fully general solution must also cope with substructural type systems (such as the Rust programming language) and static program analyses (such as those found in most compilers).As for dynamic checking, Racket suffers from two notable limitations: On one hand, it provides the building blocks for making language cooperation sound, but developers must create the necessary soundness harnesses on an ad hoc basis. To facilitate the composition of components in different languages, Racket developers need both a theoretical framework and abstractions for the partial automation of this task. On the other hand, the available spectrum of soundness mechanisms lacks power at both ends, and how to integrate these powers seamlessly is unclear. To achieve full control over its context, Racket probably needs access to assembly languages on all possible platforms, from hardware to browsers. To realize the full power of types, typed/racket will have to be equipped with dependent types. For example, when a Racket program uses vectors, its corresponding typed variant type-checks what goes into them and what comes out, but like ML or Haskell, indexing is left to a (contractual) check in the runtime system. Tobin-Hochstadt and his Typed Racket group are working on first steps in this direction, focusing on numeric constraints,23 similar to Xi's and Pfenning's research.37To achieve full control over its context, Racket probably needs access to assembly languages on all possible platforms, from hardware to browsers.As for security, the Racket project is still looking for a significant break-through. While the shill team was able to construct the language inside the Racket ecosystem, its work exposed serious gaps between Racket's principle of language-oriented programming and its approach to enforcing security policies. It thus had to alter many of Racket's security mechanisms and invent new ones. Racket must clearly make this step much easier, meaning more research is needed to turn security into an integral part of language creation.Finally, LOP also poses brand-new challenges for tool builders. An IDE typically provides tools for a single programming language or a family of related languages, including debuggers, tracers, and profilers. Good tools communicate with developers in terms of the source language. Due to its very nature, LOP calls for customization of such tools to many languages, along with their abstractions and invariants. We have partially succeeded in building a tool for debugging programs in the syntax language,8 have the foundations of a debugging framework,28 and started to explore how to infer scoping rules and high-level semantics for newly introduced, language-level abstractions.33,34 Customizing these tools automatically to newly created (combinations of) languages remains an open challenge.ConclusionProgramming language research is short of its ultimate goal—provide software developers tools for formulating solutions in the languages of problem domains. Racket is one attempt to continue the search for proper linguistic abstractions. While it has achieved remarkable success in this direction, it also shows that programming-language research has many problems to address before the vision of language-oriented programming becomes reality.A preliminary version of this article appeared in the Proceedings of the First Summit on Advances in Programming Languages conference in 2015.14 In addition to its reviewers, Sam Caldwell, Eduardo Cavazos, John Clements, Byron Davies, Ben Greenman, Greg Hendershott, Manos Renieris, Marc Smith, Vincent St-Amour, and Asumu Takikawa suggested improvements to the presentation of this material. The anonymous Communications reviewers challenged several aspects of our original submission and thus forced us to greatly improve the exposition.Since the mid-1990s, this work has been generously supported by our host institutions—Rice University, University of Utah, Brown University, University of Chicago, Northeastern University, Northwestern University, Brigham Young University, University of Massachusetts Lowell, and Indiana University—as well as a number of funding agencies, foundations, and companies, including the Air Force Office of Scientific Research, Cisco Systems Inc., the Center for Occupational Research and Development, the Defense Advanced Research Projects Agency, the U.S. Department of Education's Fund for the Improvement of Postsecondary Education, the ExxonMobil Foundation, Microsoft, the Mozilla Foundation, the National Science Foundation, and the Texas Advanced Technology Program.19. Flatt, M., Findler, R.B., Krishnamurthi, S., and Felleisen, M. Programming languages as operating systems (or revenge of the son of the Lisp machine). In Proceedings of the International Conference on Functional Programming, 1999, 138–147.36. Tobin-Hochstadt, S. and Felleisen, M. The design and implementation of Typed Scheme. In Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Conference on the Principles of Programming Languages, 2008, 395–406.Footnotesa. The numerous language-like libraries in scripting languages (such as JavaScript, Python, and Ruby), books (such as Fowler and Parson),20 and websites (such as Federico Tomassetti's, https://tomassetti.me/resources-create-programming-languages/) are evidence of the desire by programmers to use and develop eDSLs.e. An alternative is to rewrite the entire program before handing it to the given compiler, exactly what distinguishes ""expressiveness"" from ""universality.""f. Language workbenches (such as Spoofax22) deal with conventional syntax for DSLs but do not support the incremental modification of existing languages. A 2015 report12 suggests, however, these tool chains are also converging toward the idea of language creation as language modification. We conjecture that, given sufficient time, development of Racket and language workbenches will converge on similar designs.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or fee. Request permission to publish from permissions@acm.org or fax (212) 869-0481.",A Programmable Programming Language
9740,3748062,2018-02-23 00:00:00,"Trump Administration Restricts H-1B Worker Visas Coveted By High Tech : The Two-WayIt's another step in fighting potential fraud and abuse in the program that brings 85,000 highly skilled workers into the U.S. every year. More detail is required on why the workers are needed.Trump Administration Restricts H-1B Worker Visas Coveted By High TechCitizenship candidates wait for a naturalization ceremony to begin in downtown Manhattan on July 2, 2013 in New York City. U.S. Mario Tama/Getty Images hide captiontoggle captionMario Tama/Getty ImagesCitizenship candidates wait for a naturalization ceremony to begin in downtown Manhattan on July 2, 2013 in New York City. U.S.Mario Tama/Getty ImagesThe Trump administration is tightening the rules for companies that contract out high-skilled workers who are in this country on H-1B visas.The U.S. Citizenship and Immigration Services agency issued a new policy memo on Thursday that requires ""detailed statements of work or work orders"" about the work that will be performed when an H-1B visa worker is employed at a third-party work site. Employers will have to file more details that support the need for foreign talent.H-1B visas are controversial. American tech companies use them to hire highly skilled foreign workers, such as engineers, IT specialists, architects among others, in situations in which they say there is a shortage of U.S.-born talent. The visas are good for three years and renewable for another three-year term.Critics of the visas — 85,000 of which are issued every year — say American workers are aced out of competition with workers who can be paid less.As CNN reports, ""Indian outsourcing firms will be the hardest hit. Indian workers receive more than 70% of all H-1B visas.""The USCIS memo says that if a visa beneficiary will be placed at one or more third-party worksites, the employer ""has specific and non-speculative qualifying assignments in a specialty occupation for the beneficiary for the entire time requested in the petition; and the employer will maintain an employer-employee relationship with the beneficiary for the duration of the requested validity period.""The memo says USCIS recognizes that visa-holders may wind up earning less money than promised or might perform ""non-specialty"" jobs when they are contracted out to third-party worksites.The policy change comes as the Trump administration has signaled its desire to change the visa program with a ""Buy American, Hire American"" policy outlined in an executive order signed in April 2017. The order promised to root out fraud and abuse in the program.As the Mercury News reports, the H-1B program has come under intense federal scrutiny.""A Bay Area News Group report earlier this week found a sharp rise in the number of reviews immigration officials were conducting on H-1B applications. From January to August 2017, the U.S. Citizenship and Immigration Services sent 85,265 requests for evidence in response to H-1B visa applications, a 45 percent increase compared to the same period a year earlier, agency data show. Such requests are made when an application is missing required documents or when the agency determines it needs more proof to decide if a worker is eligible for the visa. Immigration lawyers say the extra enforcement could discourage companies and individuals from seeking an H-1B visa in the first place.""",Trump Administration Restricts H-1B Worker Visas Coveted By High Tech
9741,3751076,2018-02-24 19:30:00,"Site NavigationSite Mobile NavigationDoctors, Revolt!Dr. Bernard Lown, 96, at home in Newton, Mass. A celebrated pioneer in cardiology, Dr. Lown laments that modern medicine too often disregards the healing aspect.Credit Katherine Taylor for The New York TimesBoston — The 96-year-old patient with pneumonia in Bed 11 was angry. “Do you really need to check my vital signs every four hours?” he asked.Checking things like temperature, blood pressure and respiratory rate every four hours on hospitalized patients has been the standard of care since the 1890s, yet scant data indicates that it helps. In fact, data shows that close to half of patients are unnecessarily awakened for such checks, perhaps to the detriment of their recovery. My patient wanted to know how, with all that poking and prodding, he was supposed to rest and get better.“I understand your frustration,” I replied, “and wish I could help to change the situation.”I may have been a lowly intern, but it was a feeble reply. And he knew it. “Understanding is not enough,” he said. “You should be doing something to help fix this system.”The hospital, he lamented, is more like a factory — “it tests every ache and treats every laboratory abnormality, but it does little to heal its patients.” Treating and healing are both necessary, but modern health care too often disregards the latter.Few understand this better than the patient in Bed 11. He turned out to be Bernard Lown, emeritus professor of cardiology at Harvard, a senior physician at Brigham and Women’s Hospital in Boston, and the founder of the Lown Cardiovascular Group. He is celebrated for pioneering the use of the direct-current defibrillator for cardiac resuscitation and an implant called the cardioverter for correcting errant heart rhythms. He also co-founded the International Physicians for the Prevention of Nuclear War, which was awarded a Nobel Peace Prize and helped to educate millions on the medical consequences of nuclear war.But Dr. Lown identifies first and foremost as a healer. In 1996, he published “The Lost Art of Healing,” an appeal to restore the “3,000-year tradition, which bonded doctor and patient in a special affinity of trust.” The biomedical sciences had begun to dominate our conception of health care, and he warned that “healing is replaced with treating, caring is supplanted by managing, and the art of listening is taken over by technological procedures.”He called for a return to the fundamentals of doctoring — listening to know the patient behind the symptoms; carefully touching the patient during the physical exam to communicate caring; using words that affirm the patient’s vitality; and attending to the stresses and situations of his life circumstances.This time he was the patient in need of healing. And I was the doctor, the product of a system that has, if anything, become even more impersonal and transactional since he first wrote those words.Despite his reputation, Dr. Lown was treated like just another widget on the hospital’s conveyor belt. “Each day, one person on the medical team would say one thing in the morning, and by the afternoon the plan had changed,” he later told me. “I always was the last to know what exactly was going on, and my opinion hardly mattered.”An error has occurred. Please try again later.You are already subscribed to this email.What he needed was “the feeling of being a major partner in this decision,” he said. “Even though I am a doctor, I am still a human with anxieties.”The medical team was concerned that because Dr. Lown was having trouble swallowing, he was at risk for recurrent pneumonias. So we restricted his diet to purées. Soon the speech therapist recommended that we forbid him to ingest anything by mouth. Then the conversation spiraled into ideas for alternative feeding methods — a temporary tube through the nose followed, perhaps, by a feeding tube in the stomach.“Doctors no longer minister to a distinctive person but concern themselves with fragmented, malfunctioning” body parts, Dr. Lown wrote in “The Lost Art of Healing.” Now, two decades later, he’d become a victim of exactly what he had warned against.As the intern and the perpetrator of the orders, I felt impossibly torn and terribly guilty. So after Dr. Lown was discharged the next week, I kept in touch, hoping to continue this important conversation.We have since spent time together at his home, where he is back to living peacefully and swallowing carefully (no alternative feeding methods necessary).I had known Dr. Lown as a doctor and a patient; now I got to know him as an activist. We agreed that the health care system needed to change. To do that, Dr. Lown said, “doctors of conscience” have to “resist the industrialization of their profession.”This begins with our own training. Certainly doctors must understand disease, but medical education is overly skewed toward the biomedical sciences and minutiae about esoteric and rare disease processes. Doctors also need time to engage with the humanities, because they are the gateway to the human experience.To restore balance between the art and the science of medicine, we should curtail initial coursework in topics like genetics, developmental biology and biochemistry, making room for training in communication, interpersonal dynamics and leadership.Such skills would not only help doctors care for our fellow human beings but would also strengthen our ability to advocate for health care as a human right and begin to rectify the broken economics and perverse incentives of the system.Finally, hospitals should be a last resort, not the hallmark of the health care system. The bulk of health care resources should go instead into homes and communities. After all, a large majority of health problems are shaped by nonmedical factors like pollution and limited access to healthy food. Doctors must partner with public health and community development efforts to create a culture of health and well-being in patients’ daily lives.As I navigate my professional journey, Dr. Lown’s example inspires me to go to work every day with the perspective of a patient, the spirit of an activist and the heart of a healer.","Opinion | Doctors, Revolt!"
9742,3751079,2018-02-09 18:37:50,"AMP: the missing controversyHow Google cheats with performanceIntroductionAMP, Accelerated Mobile Pages, is a technology first launched in 2015 by Google. The main goal of the initiative is to drastically speed up the loading of web pages on mobile. Sorely needed, as the typical web page on slow 3G (a very typical network condition the world over) takes a long time to load.So the goal in itself seems worthy. Yet the initiative has been met with lots of controversy over the years. I’m going to go briefly over these main points of controversy.The main goal of this article though is to add a new point of controversy, one hardly discussed. The reason why AMP has instant performance.First, let’s look at the other points of controversy.Controversy #1: a new web “standard”AMP has been created completely outside of W3C and WHATWG, the main standard bodies for the web. Standard bodies in which Google has a large, if not dominant presence.AMP makes up its own standards that break with what is considered valid HTML. Case in point, have a look at how the AMP project’s homepage, which itself is an AMP page, produces over a 100 validation errors:In AMP’s defense, despite it breaking the standard, browsers will render that page just fine because browser know how to deal with invalid HTML quite well. Because, well, pretty much every web page ever created is invalid HTML.So this is merely a theoretical controversy. We have a shared web, governed by standards bodies. AMP ignores this reality.Google’s main defense is that AMP is open source. Which isn’t just a weak defense, it’s no defense at all. I can open source a plan for genocide. The term “open source” is meaningless if the thing that is open source is harmful.Controversy #2: loss of sovereignty of your websiteOne of the main implications of publishing an AMP page is that the page will be served from the Google domain. Or whoever is serving the AMP cache, yet mostly that will be Google.This means less direct traffic on your origin, and more time spent at Google. Less traffic on your origin could mean less monetization opportunities. In general, it means less control of anything. You’re subject to whatever the AMP standard allows or disallows.You’re giving up the sovereignty of your web site. Similar to when you publish all your content directly inside Facebook, you’re effectively losing all control of it now and in the future.A simple defense here could be to just not use AMP. Ironically, AMP can actually increase traffic and drive better conversions. For the simple reason that it performs so much better than a normal web page. Hence, in the desperate race for traffic, organizations are lured towards AMP. The only price they have to pay is an almost complete loss of control over their web experience.Build faster web pages then, and don’t use AMP? Even with this strategy, you cannot win from AMP’s performance. More on this in a minute.Controversy #3: loss of diversityOn a diverse web, you would visit different origins, each having a different experience. Now compare this to Facebook, where every piece of content looks the same, coming from all over the web, yet flattened into a dull timeline. You don’t get to experience the home of that content, just the content itself.This same effect of everything looking the same applies to AMP, in a lesser extend. In AMP’s defense, it’s possible to style an AMP page to fit your brand, yet there’s severe limitations in what you can and cannot do. Not just in styling, even more so in interactivity.Controversy #4: performanceThe items discussed above are not new, they’ve been discussed for years. So let us move on to the point of this article: the performance controversy of AMP. Which hardly is a topic of discussion it seems.What we know about AMP is that the technical standard in itself enforces good performance. For example, CSS is limited to 50KB and only inline, custom JS is not allowed (other than via an iframe method), and all images are guaranteed to be lazy loading.Hence, this is why AMP pages load instantly compared to a “normal” web page. It’s enforced by the standard. Right?It’s an ordinary AMP page. I didn’t pick an intentionally bad example. We’re going to check the performance of this AMP page by directly measuring it from the origin itself (so not from AMP cache):On webpagetest.org/easy (which preselects average 3G), the first meaningful paint is at 3.5s. A number that is not terrible in itself, pretty good actually, yet strangely far away from AMP’s typical instant performance.Testing directly in Chrome, using “low-end mobile” and “mid-tier mobile” as presets (these slow down both the network and the CPU), we get 8.4s and 2.3s as first meaningful paint. Both aren’t close to instant performance, and 8.4s is just plain awful. This is an AMP page!?Using a Lighthouse audit, the page fails to score on the good side of performance. Despite being fully valid AMP, a standard that enforces performance.So whilst obviously the technical restrictions of the AMP standard help with performance, they are in no way responsible for having instant performance. Technically correct AMP pages will perform very similar to any non-horrible web page.The difference between AMP performing instantly and getting numbers ranging from 2–8s as seen above have to be explained.Part of that answer you can probably guess: the cache is simply very fast. It’s hard to compete with a Google-class CDN. I imagine thousands of servers strategically placed worldwide on the best connections available.Yet this fast cache doesn’t explain a 2–8s difference. It hardly affects it. This is what does:Here we are on Google Search on mobile. We searched for a term (“Elon Musk”). We scroll down in the results, in the bottom you can start to see the “Scientias” article that we profiled starting to appear.At this moment, the network panel fills up with resources from that AMP page. Pretty much anything that page needs to render is preloaded, whether you actually open it not. If you do, it’s going to render instantly.Not in 2–8s. Instantly. Technically, a clever trick. It’s hard to argue with that. Yet I consider it cheating and anti competitive behavior.The AMP page, which we all believe to be super fast and optimized for slow mobiles because it is AMP, isn’t that fast. Its true speed comes from preloading.What it meansThe above findings have a few very serious implications:Although AMP as a standard helps with mobile performance, the standard itself is not responsible for instant performance. AMP fails at those first crucial seconds just like any other web page. AMP pages on their own are in no way faster than a well designed mobile web page (in which you would have freedom to do anything).You can never compete with AMP’s instant performance, even if you’d build the fastest website in the world.Here’s why I consider the scenario described above cheating and anti-competitive:You likely are not the owner of a search monopoly, hence you cannot control loading behavior. Only Google can. Taking that page from 2–8s to instant performance is something only Google is capable of, because it is the only entity in the world controlling the most important information portal: search.Preloading is exclusive to AMP. Google does not preload non-AMP pages. If Google would have a genuine interest in speeding up the whole web on mobile, it could simply preload resources of non-AMP pages as well. Not doing this is a strong hint that another agenda is at work, to say the least.What it means for youThe dilemma that comes with AMP, is that it works. Very well. End users will experience much faster pages and it’s very likely that your conversions will improve dramatically. A tempting, almost irresistible proposition for many organizations and businesses.The point of my article is not to point you in any direction, even though it seems that way. I merely point out the main controversial points of AMP, and how and why it works the way it does. In particular, why it’s so fast. Which is not because of AMP. Not at all because of AMP.",AMP: the missing controversy
9743,3751239,2018-02-24 18:00:12,"Some founders toil for years to secure a meager seed round. Others seem to go from launch to a massive fundraise in no time. Why is that, and how does one get into that second group?There’s no single formula, of course. But data indicates it helps to be famous, involved in a hot technology sector or working to cure cancer.Those are the findings from a Crunchbase News analysis of the fastest growing North American startups by capital raised. Our dataset included companies founded in 2015 or later that have raised $100 million or more in venture funding to date. We looked for patterns that could shed some light on why some startups are able to take off so quickly.These fast growers constitute a fairly small club. Our list includes just 39 companies, after culling some corporate spin-outs.The top companies span a broad variety of sectors, from autonomous driving to insurance tech to cancer immunotherapy. And although it’s a varied group, we did see some commonalities.So, if you’re hoping to raise $100 million in less than three years, here are some top traits shared by companies that have recently reached that milestone.Trait No. 1: Focus on cancer immunotherapyCancer immunotherapy has been a hot startup investment space for a number of years now. Over that time, companies in the field — which develop therapies to corral the body’s own immune system to destroy cancer cells — have generated both enormous returns and remarkable clinical trial results.Beyond immunotherapy, we found that the fight against cancer accounts for about a quarter of fast growers. Funding for the space comes primarily from traditional venture firms, but we also see corporate and philanthropic investors in the mix.There also are big exits to be had. Last month, for instance, immunotherapy pioneer Juno Therapeutics sold to pharma giant Celgene in a deal valued at $9 billion. Five years earlier, Seattle-based Juno launched as a venture-backed startup; it went public less than two years later with a multi-billion-dollar valuation.Trait No. 2: Have a well-known founderIf you want to raise a lot of money, it helps to look like you don’t need it.Often, the companies that raise huge sums quickly have well-known, previously successful entrepreneur founders. Two examples from this past month are Katerra and Celularity.Katerra, which is aiming to disrupt the industry, raised a staggering $865 million in a SoftBank-led round last month. It helps that the company’s co-founder, Michael Marks, was formerly longstanding CEO of Flex (previously Flextronics), one of the largest global electronics manufacturers. Another co-founder is Jim Davison, who earlier launched Silver Lake, the largest technology buyouts firm.Essential, the mobile phone and device startup led by Andy Rubin, creator of the Android operating system, is another case in point. Rubin’s track record with Android certainly contributed to the company’s ability to raise $330 million in less than two years of operation.In the chart below, we look at five fast climbers with well-known founders:Trait No. 3: Have expertise in self-driving carsThere’s a talent shortage in the autonomous driving sector, just as automakers are competing fiercely to get the technology road-ready. For those with in-demand skills, that has translated into enormous investments for comparatively immature companies.In our fast-climber list, we counted at least three companies: Argo AI, Pony.ai and Nauto. Of those, Pittsburgh-based Argo scored the largest sum, a $1 billion financing from Ford that has the startup developing technology for its vehicles.The others didn’t do too badly on the fundraising trail either. Pony.ai, which has teams in both Silicon Valley and China, raised $112 million in Series A funding last month to build out a platform connecting a self-driving car’s sensors, software, cameras and other technologies. Nauto, meanwhile, has closed on nearly $175 million to date for its AI-powered connected camera technology.Trait #4: Structure as a biotech platform companyBiotech is heavily represented in the fast-climber list, and the type of startup that seems particularly prevalent is what’s commonly called a platform company. For our purposes here, we’re using the term “biotech platform company” less as a rigid category and more as a description of a startup that deploys its expertise toward therapies for a wide number of possible ailments.Celularity, which is investing placental stem-derived treatments for everything from immuno-oncology to nerve and tendon repair, would fit this description. So could GRAIL, which has raised $1.3 billion for cancer diagnostics; Evelo Biosciences, a developer of therapies based on the human microbiome, and others.The platform approach has become increasingly popular with biotech investors of late. While there are challenges in managing a broad array of clinical trials and R&D efforts, the reward is greater potential for successful outcomes in one or more areas.Trait #5: Get to know ARCH Venture Partners, SoftBank and CelgeneA few investors showed up as particularly active in backing members of the fast-climbers list.SoftBank was the most predictable member, as the firm has spent the past year shaking up the venture industry as it deploys its $100 billion Vision Fund in an unprecedented spree of huge financing rounds. The firm backed five members of our fast climber list. (See the five here.)ARCH Venture Partners, a big name in biotech, among other sectors, was another repeat backer of fast climbers, investing in four members of the list. (See the four here.)Celgene was a surprise addition to our most active funders list. In addition to being one of the biggest acquirers in biotech of late, the company has also been an important strategic investor. It backed funding rounds for four of our fast climbers, including Celularity, which is expanding on much of Celgene’s work in the placental stem cell sector. (See the four here.)What’s next?We’ll plan to revisit the fast-climber list in a year or so to see what’s changed. For now, however, we’ll venture to make one prediction about who will be scaling up next.Looking at the current list, we see at least two insurance-focused companies, Lemonade and Bright Health. Insurance has been a particularly popular space for early-stage deals over the past couple of years, so it’s likely other companies with high levels of initial traction will score big rounds in the coming months.Even so, it’s probable biotech, with its historically high scaling costs, will remain the top sector for fast climbers.",What does it take to be a startup that raises huge sums quickly?
9744,3756907,2018-02-24 23:51:25,"This is the Samsung Galaxy S9 launch video0And this year is all about business. While the Samsung Galaxy S9 is going to be announced tomorrow at Mobile World Congress in Barcelona, Slashleaks shared the official promo video for the Samsung’s upcoming flagship device.It’s a polished 3-minute video packed with information about the new phone. First, just like rumors said, it looks just like the Galaxy S8. There are some minor changes, such as the location of the fingerprint reader. But it’s going to feel just like holding a Samsung Galaxy S8.This isn’t necessarily a bad thing as the Galaxy S8 was a great phone with a tall 18:9 display. If you want a well-designed Android phone, Samsung has some nice options.When it comes to software features, the video shows an improved DeX dock. It lets you plug your phone to a display or a TV and turn it into an Android-powered desktop experience. It’s a good way to show presentations for instance. And because the dock is flat now, you can use the phone as a trackpad.The video also says that the Galaxy S9 is water resistant and comes with a new translation feature. You can point the camera at text to translate it into a familiar language. This feature is powered by Google Translate as you can see in the corner of the screen in the video.Samsung really insists on the business features of the device. The company wants you to think about the Galaxy S9 when it’s time to buy a bunch of new phones for your employees. There will be an enterprise edition with maintenance, extended guaranty and security updates for up to four years. You can also update the firmware of your employees over the air.The 3-minute video probably only tells part of the story. That’s why we have a team on the ground in Barcelona to give you more details about the Galaxy S9 tomorrow.",This is the Samsung Galaxy S9 launch video
9745,3759774,2018-02-24 05:15:51,"Apple moves to store iCloud keys in China, raising human rights fearsSAN FRANCISCO/BEIJING (Reuters) - When Apple Inc begins hosting Chinese users’ iCloud accounts in a new Chinese data center at the end of this month to comply with new laws there, Chinese authorities will have far easier access to text messages, email and other data stored in the cloud.That’s because of a change to how the company handles the cryptographic keys needed to unlock an iCloud account. Until now, such keys have always been stored in the United States, meaning that any government or law enforcement authority seeking access to a Chinese iCloud account needed to go through the U.S. legal system.Now, according to Apple, for the first time the company will store the keys for Chinese iCloud accounts in China itself. That means Chinese authorities will no longer have to use the U.S. courts to seek information on iCloud users and can instead use their own legal system to ask Apple to hand over iCloud data for Chinese users, legal experts said.Human rights activists say they fear the authorities could use that power to track down dissidents, citing cases from more than a decade ago in which Yahoo Inc handed over user data that led to arrests and prison sentences for two democracy advocates. Jing Zhao, a human rights activist and Apple shareholder, said he could envisage worse human rights issues arising from Apple handing over iCloud data than occurred in the Yahoo case.In a statement, Apple said it had to comply with recently introduced Chinese laws that require cloud services offered to Chinese citizens be operated by Chinese companies and that the data be stored in China. It said that while the company’s values don’t change in different parts of the world, it is subject to each country’s laws.“While we advocated against iCloud being subject to these laws, we were ultimately unsuccessful,” it said. Apple said it decided it was better to offer iCloud under the new system because discontinuing it would lead to a bad user experience and actually lead to less data privacy and security for its Chinese customers.As a result, Apple has established a data center for Chinese users in a joint venture with state-owned firm Guizhou - Cloud Big Data Industry Co Ltd. The firm was set up and funded by the provincial government in the relatively poor southwestern Chinese province of Guizhou in 2014. The Guizhou company has close ties to the Chinese government and the Chinese Communist Party.The Apple decision highlights a difficult reality for many U.S. technology companies operating in China. If they don’t accept demands to partner with Chinese companies and store data in China then they risk losing access to the lucrative Chinese market, despite fears about trade secret theft and the rights of Chinese customers.BROAD POWERSApple says the joint venture does not mean that China has any kind of “backdoor” into user data and that Apple alone – not its Chinese partner – will control the encryption keys. But Chinese customers will notice some differences from the start: their iCloud accounts will now be co-branded with the name of the local partner, a first for Apple.And even though Chinese iPhones will retain the security features that can make it all but impossible for anyone, even Apple, to get access to the phone itself, that will not apply to the iCloud accounts. Any information in the iCloud account could be accessible to Chinese authorities who can present Apple with a legal order.Apple said it will only respond to valid legal requests in China, but China’s domestic legal process is very different than that in the U.S., lacking anything quite like an American “warrant” reviewed by an independent court, Chinese legal experts said. Court approval isn’t required under Chinese law and police can issue and execute warrants.“Even very early in a criminal investigation, police have broad powers to collect evidence,” said Jeremy Daum, an attorney and research fellow at Yale Law School’s Paul Tsai China Center in Beijing. “(They are) authorized by internal police procedures rather than independent court review, and the public has an obligation to cooperate.”Guizhou - Cloud Big Data and China’s cyber and industry regulators did not immediately respond to requests for comment. The Guizhou provincial government said it had no specific comment.There are few penalties for breaking what rules do exist around obtaining warrants in China. And while China does have data privacy laws, there are broad exceptions when authorities investigate criminal acts, which can include undermining communist values, “picking quarrels” online, or even using a virtual private network to browse the Internet privately.Apple says the cryptographic keys stored in China will be specific to the data of Chinese customers, meaning Chinese authorities can’t ask Apple to use them to decrypt data in other countries like the United States.“The U.S. standard, when it’s a warrant and when it’s properly executed, is the most privacy-protecting standard,” said Camille Fischer of the Electronic Frontier Foundation.WARNED CUSTOMERSApple has given its Chinese users notifications about the Feb. 28 switchover data to the Chinese data center in the form of emailed warnings and so-called push alerts, reminding users that they can chose to opt out of iCloud and store information solely on their device. The change only affects users who set China as their country on Apple devices and doesn’t affect users who select Hong Kong, Macau or Taiwan.The default settings on the iPhone will automatically create an iCloud back-up when a phone is activated. Apple declined to comment on whether it would change its default settings to make iCloud an opt-in service, rather than opt-out, for Chinese users.Apple said it will not switch customers’ accounts to the Chinese data center until they agree to new terms of service and that more than 99.9 percent of current users have already done so.Until now, Apple appears to have handed over very little data about Chinese users. From mid-2013 to mid-2017, Apple said it did not give customer account content to Chinese authorities, despite having received 176 requests, according to transparency reports published by the company. By contrast, Apple has given the United States customer account content in response to 2,366 out of 8,475 government requests.Those figures are from before the Chinese cyber security laws took effect and also don’t include special national security requests in which U.S. officials might have requested data about Chinese nationals. Apple, along with other companies, is prevented by law from disclosing the targets of those requests.Apple said requests for data from the new Chinese datacentre will be reflected in its transparency reports and that it won’t respond to “bulk” data requests.Human rights activists say they are also concerned about such a close relationship with a state-controlled entity like Guizhou-Cloud Big Data.Sharon Hom, executive director of Human Rights in China, said the Chinese Communist Party could also pressure Apple through a committee of members it will have within the company. These committees have been pushing for more influence over decision making within foreign-invested companies in the past couple of years.Reporting by Stephen NellisEditing by Jonathan Weber and Martin Howell","Apple moves to store iCloud keys in China, raising human rights fears"
9746,3759775,2018-02-23 01:06:58,"Trump administration cracks down H-1B visa abuseThe Trump administration is cracking down on companies that get visas for foreign workers and farm them out to employers.Some staffing agencies seek hard-to-get H-1B visas for high-skilled workers, only to contract them out to other companies. There's nothing inherently illegal about contracting out visa recipients, but the workers are supposed to maintain a relationship with their employers, among other requirements.In some cases, outsourcing firms flood the system with applicants. The U.S. Citizenship and Immigration Services agency said in a new policy memo released Thursday it will require more information about H-1B workers' employment to ensure the workers are doing what they were hired for.Companies will have to provide specific work assignments, including dates and locations, to verify the ""employer-employee"" relationship between the company applying for an H-1B and its visa recipient.H-1B visas are valid for three years and can be renewed for another three years. It is a visa that is near and dear to the tech community, with many engineers vying for one of the program's 85,000 visas each year. (20,000 of that quota are reserved for advanced degree holders.) Demand for the visa often exceeds the supply -- in that case, a lottery system is activated.""Since there is a limited number of H-1B visas it is important that those visa workers go where they are legitimately needed,"" attorney Sara Blackwell told CNN. Blackwell advocates for American workers replaced by foreigner visa holders.The USCIS says it may limit the length of the visa to shorter than three years based the information an employer provides. For example, if an employer can't prove the H-1B holder is ""more likely than not"" needed for the full three years, the government might issue the visa for fewer than three years.The memo also says the administration wants to prevent employee ""benching."" That's when firms bring on H-1B visa holders but don't give them work and don't pay them the required wages while they wait for jobs. Most projects don't need foreign workers for the full term, according to Monty Hamilton, CEO of IT contractor Rural Sourcing.Although most agree that some employers abuse H-1B visas, how pervasive the abuse -- and how to prevent it -- remains a sensitive and divisive issue.Robert Cormier, a retired criminal investigator for federal law enforcement, says that asking for more information from third party companies could help the government crack down on bad actors. The threat of prosecution if companies are caught lying could be enough to deter fraud.""That could change the game completely,"" he told CNN.Others say it could have an unintended consequence: Hurting those who are using the H-1B properly, according to Betsy Lawrence, the American Immigration Lawyers Association's director of government relations.Immigration attorney Tahmina Watson said the government has already been much stricter about H-1B enforcement since Trump took office.""Much of what is said in the memo has been carried out in the last 12 months leading to record number of denials,"" Watson said.Indian outsourcing firms will be the hardest hit. Indian workers receive more than 70% of all H-1B visas.Companies and immigration lawyers are preparing for the new H-1B lottery season. Applications must be filed on April 1st.",Trump administration cracks down H-1B visa abuse
9747,3763679,2018-02-22 14:01:25,"In an Era of ‘Smart’ Things, Sometimes Dumb Stuff Is BetterThere are times an old-fashioned alarm clock may be a better choice than an Amazon Echo Spot. When you don’t want someone taking a picture of what you do in bed, for example.Credit From left: Nick Bilton/The New York Times; Ruth Fremson, via The New York TimesIt still feels magical to light up your living room by saying “Alexa, turn on the lights.” But with all the hype surrounding so-called smart things — everyday devices that are connected to the internet — it’s easy to forget that sometimes the dumb stuff is just better.Tech companies are adding internet connections to just about everything you can imagine so that they can be controlled with smart speakers or phones. Thermostats, surveillance cameras, mosquito zappers, coffee makers — you name it.And smart devices are becoming more popular. In 2017, 15 percent of American households owned a home automation device, up from 10 percent in April 2016, according to NPD Group, a research firm.But before we get carried away setting up the Wi-Fi connections on all our appliances, lights and fashion accessories, let me play Luddite for a second. Some of the most mundane devices are designed to accomplish a simple task extremely well — and in some cases they still execute those duties better than their high-tech brethren.So let’s take a moment to appreciate some of the best dumb things. Here are my top picks.A wristwatch vs. Apple WatchThe Apple Watch, by all measures, is a hit. The latest iteration of the Apple Watch, called Series 3, is fast, water-resistant and versatile with long battery life, making it a superb smart watch for tracking your fitness activity.PhotoFor telling time, a regular wristwatch is often superior to a smart watch.Credit From left: Maaserhit Honda for The New York Times; Jim Wilson, via The New York TimesYet a normal wristwatch is still superior at one crucial task: Telling the time.The Apple Watch’s screen wakes up when you tilt your wrist at an angle, which indicates you are trying to check the time. That helps conserve battery life. But any Apple Watch wearer is familiar with situations where this feature gets frustrating.While riding a bicycle, for example, you often have to let go of the handle bar and lift the watch toward your face to check the time. When you’re standing on a bus or subway train and holding onto a pole, it is difficult to tilt your wrist at the correct angle to look at the time. Or when you’re in a meeting and want to see if you’re staying on schedule, flicking your wrist isn’t very subtle.Until the Apple Watch manages to constantly display the time without sapping the battery, a normal wristwatch is better for telling the time in all those scenarios. That’s why you’ll see me wearing a normal watch at work but an Apple Watch at the gym.A car mount vs. a smart car consoleMany cars are now equipped with a touch-screen on the console that essentially mirrors your smartphone screen. Android phone users get to use Android Auto, and iPhone users hook into CarPlay.These smart car systems are designed to seamlessly work with your smartphone. Plugging in an iPhone, for example, loads a screen of apps like Apple Maps, Apple Music and Apple’s podcast app, which you can then control on the console or with Siri instead of fiddling with your smartphone screen.The problem with this concept is there are a limited number of apps that work with these smart infotainment systems. For example, if on CarPlay you prefer to use Google Maps or Waze, you’re out of luck and are stuck with Apple Maps.In addition, if your smart car system needs a major software update, some car brands are lagging in allowing you to download and install the updates yourself. Instead, they require you to bring the car to the dealer and pay for the updates to be installed there. General Motors, for example, has for years declined to offer so-called over-the-air updates and will only say it plans to support them before 2020.Using a phone mount is a cheap and simple solution that is far less frustrating. You just attach the mount to the dash, a CD player slot or an air conditioning vent, mount your phone and plug it into a power charger via the accessories port.An error has occurred. Please try again later.You are already subscribed to this email.Voilà, your phone has become your infotainment system, capable of running your favorite navigation and music apps and using voice controls to place calls over speakerphone. The screen is large enough to clearly read maps, and you can update the operating system on your own. What more do you need?An alarm clock vs. Amazon Echo SpotAmazon recently introduced the Echo Spot, a smart alarm clock with a touch-screen and the Alexa virtual assistant. A less desirable feature is a built-in camera for placing video calls.A camera on your nightstand that is constantly pointed at your bed? It’s like asking for your privacy to be violated. You might as well shop for your groceries in your underwear or post all your smartphone photos publicly on the web.Amazon promises the camera software on the Echo Spot can be turned off whenever you aren’t using it. But it’s an obvious feature for hackers to target with malware.So if your primary goal is to have a device that wakes you up on time to go to work, just get an old-school alarm clock.A kitchen timer vs. Amazon EchoOne of the most common uses of Amazon’s Echo is to set a kitchen timer. Just say “Alexa, set a timer for 80 minutes” while you’re busy chopping vegetables.PhotoIt’s time to appreciate a kitchen timer over an Amazon Echo.Credit From left: Tony Cenicola/The New York Times; Mark Lennihan, via Associated PressBut there are reasons a cheap kitchen timer can be superior.Cooking timing can vary depending on your heating element, among other factors. So if you have to check your food for doneness and change the kitchen timer, an old-school timer — either the analog variety or the type with a digital time display and two or three physical buttons — can be easier. It simply dings or beeps when the time is up and it’s quicker to add or subtract a few minutes by turning a dial or pressing a button or two.You can also constantly see how much time is left on the timer, whereas with the Echo, you have to open a smartphone app to see the remaining time or ask Alexa to tell you how much time is left. Over the long term, using a smart speaker as a timer gets tedious.A piece of paper vs. a tabletWhen people buy new iPads or Amazon Fire tablets, they often give their older tablet a second life by designating it for the kitchen. There, the ancient tablet gets mounted to the refrigerator with a magnet and becomes a glorified recipe reader.Having tried this experiment, it’s a hassle. You often have to clean the tablet after smearing food on the screen. The battery eventually needs to be recharged. And if you want to double or halve a recipe, you have to do some mental math, which makes multitasking more challenging when you are busy in the kitchen.Printing out or jotting down a recipe on a piece of paper is just simpler. You can easily scribble additional notes, like changes and improvements to the recipe. Assuming you have decent handwriting, it’s easy to read the steps and ingredients.A version of this article appears in print on February 22, 2018, on Page B7 of the New York edition with the headline: In ‘Smart’ Things Era, Sometimes Dumb Is Better. Order Reprints|Today's Paper|Subscribe","In an Era of ‘Smart’ Things, Sometimes Dumb Stuff Is Better"
9748,3764571,2018-02-23 21:55:32,"After making the decision to roll its own infrastructure and reduce its dependence on Amazon Web Services, Dropbox reduced its operating costs by $74.6 million over the next two years, the company said in its S-1 statement Friday.Starting in 2015, Dropbox began to move users of its file-storage service away from AWS’s S3 storage service and onto its own custom-designed infrastructure and software, and the cost benefits were immediate. From 2015 to 2016, Dropbox saved $39.5 million in the cost of revenue bucket thanks to the project, which reduced spending on “our third-party datacenter service provider” by $92.5 million offset by increased expenses of $53 million for its own data centers. The following year in 2017, it saved an additional $35.1 million in operating costs beyond the 2016 numbers.Dropbox was once the quintessential cloud success story, a startup that built a massive user base and brand presence thanks to its use of Amazon Web Services. Obviously, lots of companies are still happy to pay AWS to manage their infrastructure, as evidenced by the steady gains the cloud market leader made in 2017.But once certain startups turn into big companies with hundreds of millions of users, with computing needs that they’ve come to intimately understand, it can be far more efficient to set up computing infrastructure designed exactly with those needs in mind. After a multiyear process, Dropbox completed what it calls its “Infrastructure Optimization” project in the fourth quarter of 2016.“Our Infrastructure Optimization reduced unit costs and helped limit capital expenditures and associated depreciation. Combined with the concurrent increase in our base of paying users, we experienced a reduction in our cost of revenue, an increase in our gross margins, and an improvement in our free cash flow in the periods presented,” the company said in its S-1 statement, referring to 2015 through 2017.Dropbox still uses AWS for less than 10 percent of its storage needs, it said in the statement. The company operates three data centers in the U.S. but none in Europe, and uses AWS resources in Europe to serve some customers on that continent.Sponsor PostWe pioneered the global shift to cloud, social, mobile, and data science technologies, and we’ve been disrupting enterprise software ever since. Are you ready to push your limits at the world’s fastest-growing enterprise software company? We’re looking for the industry’s best technical talent—we’re looking for you! Join us in our Bellevue office.Tom Krazit, GeekWire's Cloud & Enterprise Editor, covered technology for news organizations including IDG, CNET, and paidContent before serving as executive editor of Gigaom and the Structure conference series. Reach him at tom@geekwire.com and follow him @tomkrazit.",Dropbox saved almost $75 million over two years by building its own tech infrastructure
9749,3767593,2018-02-21 12:00:00,"Why we made this changeVisitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access.Serious quantum computers are finally here. What are we going to do with them?Hello, quantum world.Inside a small laboratory in lush countryside about 50 miles north of New York City, an elaborate tangle of tubes and electronics dangles from the ceiling. This mess of equipment is a computer. Not just any computer, but one on the verge of passing what may, perhaps, go down as one of the most important milestones in the history of the field.Quantum computers promise to run calculations far beyond the reach of any conventional supercomputer. They might revolutionize the discovery of new materials by making it possible to simulate the behavior of matter down to the atomic level. Or they could upend cryptography and security by cracking otherwise invincible codes. There is even hope they will supercharge artificial intelligence by crunching through data more efficiently.Yet only now, after decades of gradual progress, are researchers finally close to building quantum computers powerful enough to do things that conventional computers cannot. It’s a landmark somewhat theatrically dubbed “quantum supremacy.” Google has been leading the charge toward this milestone, while Intel and Microsoft also have significant quantum efforts. And then there are well-funded startups including Rigetti Computing, IonQ, and Quantum Circuits.“Nature is quantum, goddamn it! So if we want to simulate it, we need a quantum computer.”No other contender can match IBM’s pedigree in this area, though. Starting 50 years ago, the company produced advances in materials science that laid the foundations for the computer revolution. Which is why, last October, I found myself at IBM’s Thomas J. Watson Research Center to try to answer these questions: What, if anything, will a quantum computer be good for? And can a practical, reliable one even be built?Why we think we need a quantum computerThe research center, located in Yorktown Heights, looks a bit like a flying saucer as imagined in 1961. It was designed by the neo-futurist architect Eero Saarinen and built during IBM’s heyday as a maker of large mainframe business machines. IBM was the world’s largest computer company, and within a decade of the research center’s construction it had become the world’s fifth-largest company of any kind, just behind Ford and General Electric.While the hallways of the building look out onto the countryside, the design is such that none of the offices inside have any windows. It was in one of these cloistered rooms that I met Charles Bennett. Now in his 70s, he has large white sideburns, wears black socks with sandals, and even sports a pocket protector with pens in it. Surrounded by old computer monitors, chemistry models, and, curiously, a small disco ball, he recalled the birth of quantum computing as if it were yesterday.Charles Bennett of IBM Research is one of the founding fathers of quantum information theory. His work at IBM helped create a theoretical foundation for quantum computing.bartek sadowskiWhen Bennett joined IBM in 1972, quantum physics was already half a century old, but computing still relied on classical physics and the mathematical theory of information that Claude Shannon had developed at MIT in the 1950s. It was Shannon who defined the quantity of information in terms of the number of “bits” (a term he popularized but did not coin) required to store it. Those bits, the 0s and 1s of binary code, are the basis of all conventional computing.A year after arriving at Yorktown Heights, Bennett helped lay the foundation for a quantum information theory that would challenge all that. It relies on exploiting the peculiar behavior of objects at the atomic scale. At that size, a particle can exist “superposed” in many states (e.g., many different positions) at once. Two particles can also exhibit “entanglement,” so that changing the state of one may instantaneously affect the other.Bennett and others realized that some kinds of computations that are exponentially time consuming, or even impossible, could be efficiently performed with the help of quantum phenomena. A quantum computer would store information in quantum bits, or qubits. Qubits can exist in superpositions of 1 and 0, and entanglement and a trick called interference can be used to find the solution to a computation over an exponentially large number of states. It’s annoyingly hard to compare quantum and classical computers, but roughly speaking, a quantum computer with just a few hundred qubits would be able to perform more calculations simultaneously than there are atoms in the known universe.In the summer of 1981, IBM and MIT organized a landmark event called the First Conference on the Physics of Computation. It took place at Endicott House, a French-style mansion not far from the MIT campus.In a photo that Bennett took during the conference, several of the most influential figures from the history of computing and quantum physics can be seen on the lawn, including Konrad Zuse, who developed the first programmable computer, and Richard Feynman, an important contributor to quantum theory. Feynman gave the conference’s keynote speech, in which he raised the idea of computing using quantum effects. “The biggest boost quantum information theory got was from Feynman,” Bennett told me. “He said, ‘Nature is quantum, goddamn it! So if we want to simulate it, we need a quantum computer.’”IBM’s quantum computer—one of the most promising in existence—is located just down the hall from Bennett’s office. The machine is designed to create and manipulate the essential element in a quantum computer: the qubits that store information.This lab at IBM houses quantum machines connected to the cloud.jeremy liebmanThe gap between the dream and the realityThe IBM machine exploits quantum phenomena that occur in superconducting materials. For instance, sometimes current will flow clockwise and counterclockwise at the same time. IBM’s computer usessuperconducting circuits in which two distinct electromagnetic energy states make up a qubit.The superconducting approach has key advantages. The hardware can be made using well--established manufacturing methods, and a conventional computer can be used to control the system. The qubits in a superconducting circuit are also easier to manipulate and less delicate than individual photons or ions.Inside IBM’s quantum lab, engineers are working on a version of the computer with 50 qubits. You can run a simulation of a simple quantum computer on a normal computer, but at around 50 qubits it becomes nearly impossible. That means IBM is theoretically approaching the point where a quantum computer can solve problems a classical computer cannot: in other words, quantum supremacy.But as IBM’s researchers will tell you, quantum supremacy is an elusive concept. You would need all 50 qubits to work perfectly, when in reality quantum computers are beset by errors that need to be corrected for. It is also devilishly difficult to maintain qubits for any length of time; they tend to “decohere,” or lose their delicate quantum nature, much as a smoke ring breaks up at the slightest air current. And the more qubits, the harder both challenges become.“If you had 50 or 100 qubits and they really worked well enough, and were fully error-corrected—you could do unfathomable calculations that can’t be replicated on any classical machine, now or ever,” says Robert Schoelkopf, a Yale professor and founder of a company called Quantum Circuits. “The flip side to quantum computing is that there are exponential ways for it to go wrong.”Another reason for caution is that it isn’t obvious how useful even a perfectly functioning quantum computer would be. It doesn’t simply speed up any task you throw at it; in fact, for many calculations, it would actually be slower than classical machines. Only a handful of algorithms have so far been devised where a quantum computer would clearly have an edge. And even for those, that edge might be short-lived. The most famous quantum algorithm, developed by Peter Shor at MIT, is for finding the prime factors of an integer. Many common cryptographic schemes rely on the fact that this is hard for a conventional computer to do. But cryptography could adapt, creating new kinds of codes that don’t rely on factorization.“The thing driving the hype is the realization that quantum computing is actually real. It is no longer a physicist’s dream—it is an engineer’s nightmare.”This is why, even as they near the 50-qubit milestone, IBM’s own researchers are keen to dispel the hype around it. At a table in the hallway that looks out onto the lush lawn outside, I encountered Jay Gambetta, a tall, easygoing Australian who researches quantum algorithms and potential applications for IBM’s hardware. “We’re at this unique stage,” he said, choosing his words with care. “We have this device that is more complicated than you can simulate on a classical computer, but it’s not yet controllable to the precision that you could do the algorithms you know how to do.”What gives the IBMers hope is that even an imperfect quantum computer might still be a useful one.Dueling neural networks. Artificial embryos. AI in the cloud. Welcome to our annual list of the 10 technology advances we think will shape the way we work and live now and for years to come.Gambetta and other researchers have zeroed in on an application that Feynman envisioned back in 1981. Chemical reactions and the properties of materials are determined by the interactions between atoms and molecules. Those interactions are governed by quantum phenomena. A quantum computer can—at least in theory—model those in a way a conventional one cannot.Last year, Gambetta and colleagues at IBM used a seven-qubit machine to simulate the precise structure of beryllium hydride. At just three atoms, it is the most complex molecule ever modeled with a quantum system. Ultimately, researchers might use quantum computers to design more efficient solar cells, more effective drugs, or catalysts that turn sunlight into clean fuels.Those goals are a long way off. But, Gambetta says, it may be possible to get valuable results from an error-prone quantum machine paired with a classical computer.From a physicist’s dream to an engineer’s nightmare“The thing driving the hype is the realization that quantum computing is actually real,” says Isaac Chuang, a lean, soft-spoken MIT professor. “It is no longer a physicist’s dream—it is an engineer’s nightmare.”Chuang led the development of some of the earliest quantum computers, working at IBM in Almaden, California, during the late 1990s and early 2000s. Though he is no longer working on them, he thinks we are at the beginning of something very big—that quantum computing will eventually even play a role in artificial intelligence.But he also suspects that the revolution will not really begin until a new generation of students and hackers get to play with practical machines. Quantum computers require not just different programming languages but a fundamentally different way of thinking about what programming is. As Gambetta puts it: “We don’t really know what the equivalent of ‘Hello, world’ is on a quantum computer.”We are beginning to find out. In 2016 IBM connected a small quantum computer to the cloud. Using a programming tool kit called QISKit, you can run simple programs on it; thousands of people, from academic researchers to schoolkids, have built QISKit programs that run basic quantum algorithms. Now Google and other companies are also putting their nascent quantum computers online. You can’t do much with them, but at least they give people outside the leading labs a taste of what may be coming.The startup community is also getting excited. A short while after seeing IBM’s quantum computer, I went to the University of Toronto’s business school to sit in on a pitch competition for quantum startups. Teams of entrepreneurs nervously got up and presented their ideas to a group of professors and investors. One company hoped to use quantum computers to model the financial markets. Another planned to have them design new proteins. Yet another wanted to build more advanced AI systems. What went unacknowledged in the room was that each team was proposing a business built on a technology so revolutionary that it barely exists. Few seemed daunted by that fact.This enthusiasm could sour if the first quantum computers are slow to find a practical use. The best guess from those who truly know the difficulties—people like Bennett and Chuang—is that the first useful machines are still several years away. And that’s assuming the problem of managing and manipulating a large collection of qubits won’t ultimately prove intractable.Still, the experts hold out hope. When I asked him what the world might be like when my two-year-old son grows up, Chuang, who learned to use computers by playing with microchips, responded with a grin. “Maybe your kid will have a kit for building a quantum computer,” he said.Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on quantum computing.ShareTaggedI am the senior editor for AI at MIT Technology Review. I mainly cover machine intelligence, robots, and automation, but I’m interested in most aspects of computing. I grew up in south London, and I wrote my first line of code (a spell-binding… More infinite loop) on a mighty Sinclair ZX Spectrum. Before joining this publication, I worked as the online editor at New Scientist magazine. If you’d like to get in touch, please send an e-mail to will.knight@technologyreview.com.You've read of three free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Subscribe now for unlimited online access. This is your last free article this month. Subscribe now for unlimited online access. You've read all your free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Log in for more, or subscribe now for unlimited online access. Log in for two more free articles, or subscribe now for unlimited online access.",Serious quantum computers are finally here. What are we going to do with them?
9750,3767837,2018-02-23 10:35:16,"TNW SitesAs US dismantles net neutrality, will EU tighten its grip on mobile operators?To the great dismay of internet advertisers and publishers, the US Federal Communications Commission voted to do away with regulations ensuring net neutrality in December 2017 — protections largely seen as vital to keeping the internet open and free. On the contrary, the government argued, freedom from the burden of net neutrality will help the economy flourish and innovation soar.But we all know there’s a catch: Without mandated net neutrality, internet service providers gain complete control over the flow of digital content, leaving everyone else at their mercy.Net neutrality is still alive and well, it’s just across the Atlantic ocean, in the European Union. In fact, the EU now stands as one of the largest markets where net neutrality still reigns. The 2015open internet law established commercial regulations that permanently prevent European internet service providers from favoring or discriminating against particular internet traffic, and demands transparency from all ISPs operating in Europe.What’s the big deal?Essentially, net neutrality holds that everyone in the digital universe should enjoy free and open access to all information on the web, visiting any site they choose, and get the same level of service every time — always equal to the level of service all other users receive.US net neutrality regulations, enacted in 2015, were designed toprotect users from the possibility that powerful internet service providers like Verizon, Comcast, and AT&T might speed up or slow down service to specific sites, or even block particular content or sites.The idea was to give every internet user a fair chance to access content from any source. Now that net neutrality regulations have been struck down, ISPs in the US are free to favor some sites over others, providing speedier service to sites that are potentially profitable to them, say, or otherwise of interest to them. If your site isn’t among those favorites, you’re stuck on an uneven playing field, and you’ve got plenty of company.No perfect worldsDespite it’s seemingly bullet-proof laws, the European Union can actually be quite lax in making carriers play by the rules. Operators may still favor certain apps, streaming services, and TV networks. What’s more, European operators are allowed to sell “packages” of content, known as “zero-rating,” where certain services are offered without data limits, and others can be slowed down.Sweden’s Telia Company offers unlimited access to Facebook, Spotify, and Instagram, for example, while access to other media companies is still constricted by data consumption limits. In Germany, Deutsche Telekom offers StreamOn, which allows users to access unlimited videos and music from partners like Netflix. Both of those relationships drew pointed criticism from activists, demonstrating that when it comes to net neutrality, there are no perfect worlds.Even so, European users have many more mobile carriers to choose from — there are four major telcos and nine more low-cost offshoots in France alone, for instance — resulting in a more competitive environment for operators and more options for users.Plus, Europe doesn’t have the sort of mega-mergers between carriers and content providers we’ve seen in the US (Comcast and NBC Universal, Verizon, and AOL). EU carriers are spared the fear that carriers like these will favor their own newly-acquired content, to the loss of other content providers.Driving forcesIt appears that US telcos will benefit most from the end of net neutrality — they’re in the driver’s seat today, with the power to decide which services they’ll favor, based on their own business interests. Further down the road, though, users and their preferences will determine revenue and affect the sites that get highest priority.Consumers will demand more personalized experiences from the telcos, and ultimately their demand for unlimited data plans will balance out the operators’ favoritism toward certain content providers. In contrast, a subscription service scheme continues to predominate in the EU.We’ll have to wait and see how the end of net neutrality will change the US telco industry, and how the regulatory environment will compare to the shifting European Union environment. Most likely the differences between the two regions will continue to grow.Will anyone benefit from the repeal? Will market forces drive revenue, or will telcos take the wheel? And how will it all change the face of the internet as we know it today? With these questions and more hanging in the air, it’s a safe bet that net neutrality is going to be a hot topic around the globe this year — so keep an eye out.","As US dismantles net neutrality, will EU tighten its grip on mobile operators?"
9751,3767838,2018-02-24 22:18:12,"TNW Sites6 ways to effectively ‘get the word out’ about your new e-commerce shopSo, what’s next?You’ve come up with a great product idea. You’ve found a trustworthy manufacturer. Your website is on point. Having done so much, where do you go from here?No worries — I’ve been there before. More than once, actually …I grew an e-commerce brand to over six figures in sales when I was only 19 years old, and a lot of that revolved around putting my product in front of the right eyes. So how do you get the word out? And how do you make sure that it’s effective — that you’re reaching the right people?1) Keep Google happySearch engine optimization is an absolute necessity for any e-commerce shop that’s trying to get off the ground. SEO is trending towards voice search and mobile, and having a way to measure, test and track your keywords is going to be essential moving forward.Moz is one of the tried-and-true solutions for SEO, and it’s affordable for both small and large shops. There are plenty of other solutions out there, too, including Microsoft’s BrightEdge — one of the more common alternatives.I think in some ways it’s going to be even more important in the future than it is now, especially as voice search and long-tail keywords become more common methods of discovery. Don’t ignore it.2) Become an honored guestOne avenue that I (and many others) have pursued is guest posting.Even outlets that are normally strict on outbound links or advertising (always play by the rules) will let you put a link to your website in your bio, which helps both search engines and actual people find your shop.Target blogs that are commonly read by your audience and have high levels of engagement. You can also target larger, well-trafficked sites like Forbes, Entrepreneur and The Next Web (goes without saying), which have high cachet with Google and instant name recognition.It’s not enough to just put together an interesting, audience-facing post, though …Remember there’s a purpose to the blog, and your author bio is often an effective place to drive traffic to your site. Use it as a call to action, a way to generate leads from an email link, a direct link to your site, and a source of credible backlinks for Google. It’s a jack-of-all-trades solution.3) Use technology to your advantageNo, you don’t need to take out a roadside billboard to get into “advertising.”Google’s Adwords platform and Facebook ads are easily the most cost-effective, proven and targeted methods for reaching the right customers for your business. There are a number of affordable tools that can help you optimize them, however.StoreYa’s Traffic Booster, amongst other shop-driven resources, is one that I’ve personally had the best experiences with in the past. Traffic Booster is different in that it uses learning algorithms to optimize (and automate) ad buys and make sure PPC dollars are being used effectively.You can drive traffic to your shop, while simultaneously stretching ad dollars — when you’re getting a new shop off the ground, that’s important. Take it form me — I’ve been burnt by more than enough poor-performing PPC agencies and Upwork “experts” to know that this kind of system is better than a long-term retainer.4) Send a letterOne of the key avenues a lot of people don’t think about as much as they should is email. Huckberry’s one of the biggest success stories for use of email, and they did it by speaking the language of their customers.As an outdoor lifestyle company, they found exactly what their customers were looking for and served it to them, even if it didn’t immediately scream “BUY ME”. And they grew from $10,000 to $1,000,000 revenue in one year.Email didn’t do all of that, but it was integral to how they grew.Tools like MailChimp make it simple to create email marketing campaigns, and since customers that give you an email address tend to be further down the sales funnel, you know it’s worth investing the time to communicate with these high-quality leads.5) Get sociableOrganic social is harder than it used to be, but it’s still an essential part of your strategy to get the word out. It’s important to find the right platforms to focus on, though. If you’re creating an accessory or apparel brand, Instagram or Pinterest might be your best bet.Maybe you want a broader cross-section and longer text posts, so you’re looking at Facebook. Whatever platform you decide to focus on, make sure it’s one that tends to have high levels of engagement with users in your market segment.6) Be authentic and consistentI can’t emphasize this enough …No matter what method you’re using to reach out, the voice of your brand has to be both authentic and consistent across every platform. Marketing guru Seth Godin defines authenticity as “consistent emotional labor”—having a coherent voice across every platform you use to communicate, and making sure your actions are in line with what you promise.Speak it into existenceThere’s an audience out there for what you’re creating.Any time I’ve undertaken a new e-commerce project, I’ve always been able to find ‘em — and chances are, if you’ve gotten to this point, you have some idea of who your audience is and where they are, as well. Focus on the above areas and you’ll find that “getting the word out” might be easier than you thought.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",6 ways to effectively ‘get the word out’ about your new e-commerce shop
9752,3770243,2018-02-24 07:31:51,"Standing desks increase bodily pain and slow down people’s cognitive functions at work, new research suggests.Experts have warned that despite the “feverish” trend towards adopting the adjustable desks, there is little solid evidence to support their use, as well as concerns they may do more harm than good.The devices are becoming increasingly commonplace as awareness improves regarding the dangers of sedentary living - most office workers spend more than 80 per cent of the time sitting - and they are also popular with people suffering from back pain.But the new study, published in the journal Ergonomics, has linked prolonged use of standing desks with lower limb discomfort and deteriorating mental reactiveness.Researchers at Curtin University in Australia observed 20 participants working at standing desks for two hours.They found discomfort “significantly” increased for the lower back and lower limb regions, which correlates with previous research suggesting standing desk is responsible for swelling of the veins, which can endanger the heart.Mental reactiveness also slowed down after roughly an hour and a quarter, however “creative” decision making was shown to marginally improve.Professor Alan Taylor, a physiotherapy expert at Nottingham University, said: “The bottom line is that this expansion has been driven more by commercial reasons than scientific evidence.“But the evidence is catching up and it’s showing there are some drawbacks.“They are not a panacea for back pain, yet companies are worried that if they provide them they’ll be sued.”Last year a report by Edinburgh University revealed some office workers, in particular middle-aged men, spend more time sitting down than pensioners, with large parts of the population described as “dangerously sedentary”.It builds on research showing that a third of Britons are putting themselves at risk of an early grave because of their lack of exercise.Professor Taylor said that, while future evidence may emerge suggesting some benefits of standing desks, office workers should not ignore current advice to go for regular walks at work just because they are standing rather than sitting.“Get up, go and make a cup of tea or coffee - don’t just stand there,” he said.Standing desks, or systems to elevate computers, range range from around £200 to three or four times that sum.Some scientists believe their use can help office workers lose weight, particularly if combined with a treadmill to form a so-called “walking desk”.","Standing desks 'increase pain' and slow down mental ability, study suggests"
9753,3770401,2018-02-25 11:13:53,"GoBee Bike throws in the towel in France0Bike-sharing startup GoBee Bike is giving up and shutting down in all French cities where it operates. GoBee Bike operates just like Chinese giants Ofo and Mobike. You open the app, you find a bike on the map and you unlock it by scanning a QR code. Once you’re done, you lock it again and leave it there — there’s no dock.And yet, the startup is blaming vandalism and says that the service would stop immediately. It’s worth noting that users will get a refund on their remaining balances and €15 deposit. This is a nice gesture.According to the announcement, GoBee Bike managed to attract 150,000 users in Europe who used the service hundreds of thousands of times. But the company’s bikes slowly became unusable. 3,200 bikes became dysfunctional, 1,000 bikes were illegally parked in someone’s home. Overall, GoBee Bike had to send someone in 6,500 cases.The startup couldn’t keep up and it became clear that the business model wasn’t scalable if you needed to fix the bikes all the time. As a user, it also felt like you couldn’t unlock most of the bikes because the lock battery was dead most of the time.GoBee Bike first announced that it would stop operating in Brussels, Lille and Reims. The startup also exited the Italian market. And now, users in Paris and Lyon can’t access the service either. The company is still operating in its home city Hong Kong.In Paris in particular, there were four different free-floating bike companies — Ofo, Mobike, Obike and GoBee Bike. GoBee Bike is clearly underfunded compared to those giants. According to CrunchBase, GoBee Bike has raised $9 million. Ofo and Mobike have raised over $2 billion combined.And you can feel it as a user. While Ofo has been operating in Paris since mid-December, all rides have been free for the past two and half months. Mobike has been around for a month and rides are free as well. Even Obike gave you 50 free rides when you signed up.",GoBee Bike throws in the towel in France
9754,3770402,2018-02-25 11:00:30,"Samsung MWC 2018 Liveblog0Mobile World Congress isn’t always a huge show for Samsung (last year we got a couple of tablets and some vague 5G news), but this year the company’s not messing around. Between the many teasers and seemingly endless deluge of leaks, the Galaxy S9 is all but announced at this point.Even so, once of the nice things about the company is that it always seems to have a little something extra up its sleeve, and we’re sure to see a surprise or two at today’s press conference. Besides, it’s still Sunday morning in the U.S., so it’s not like you’ve got anywhere more exciting to be than in front of your computer, watching it all unfold live from Barcelona.",Samsung MWC 2018 Liveblog
9755,3770403,2018-02-25 10:08:46,"Watch Samsung’s MWC conference live right here0Samsung’s press conference at MWC in Barcelona is happening today. The conference starts at 6 PM CET (5 PM GMT, 12 PM EST, 9 AM PST). You can also check out our liveblog to get our commentary on Samsung’s news.And of course, everybody is waiting for the Samsung Galaxy S9 and S9+. Details about the company’s new flagship devices already leaked, but we don’t know much about new software features. You don’t want to miss this one — Samsung’s conferences are always full of surprises.You can check it out live via Samsung’s official stream above, and stay tuned on TechCrunch.com for ongoing coverage of all the news coming out of MWC.",Watch Samsung’s MWC conference live right here
9756,3770404,2018-02-25 09:57:20,"HMD licenses the Nokia brand so that the company can release smartphones under the Nokia brand. It isn’t the company we used to know and love, but it still says Nokia. So get ready for some new Nokia phones running Android.",Watch HMD unveil Nokia phones live right here
9757,3771413,2018-02-24 23:45:00,"LG’s V30S ThinQ is the AI-fueled phone the V30 should’ve beenThis isn’t the opportunistic update we expected.In the midst of a tricky fiscal year, LG decided to run with a new strategy: cook up interim smartphones to sell between its major flagship releases. The plan was based in part on the surprising success of phones like the X cam, and since the company needed to try something different, it pressed on with its experiment. And lo, the LG V30S ThinQ was born.If you were looking for dramatic changes to the original V30 hardware, you might be disappointed by this ""interim"" release. In fact, let's run through all of the way the V30S ThinQ we received for testing differs from the original:It has 6GB of RAM, up from 4GB.Internal storage ranges from 128GB to 256GB, compared to 64GB-128GB in the first release.This P-OLED display doesn't have the original's scratchy, uneven appearance in low light.It's blue.That's it. Broadly speaking, LG is selling last year's phone with a fancy firmware update. This news will disappoint anyone who wanted a proper G-series flagship, like the ones we got two years running here at MWC, but all of the company's messaging in the run up to the show indicated that we wouldn't get a striking, brand-new machine.Gallery: LG V30S ThinQ hands-on | 11 PhotosTo make this whole situation even weirder, Ian Hwang (LG's director of product portfolio) confirmed that these AI features should eventually wind up on the original V30 as well. Subsequent conversations with an LG spokesperson instead suggested that some AI features would not be ported to earlier devices and that these decisions would be made on a market-by-market basis -- suffice to say, we're still trying to figure out exactly what's what.By now, you might wondering why LG had to make a brand new version of an existing phone for these AI features to live on the first place. We're not really sure ourselves. LG has struggled to find its footing in the mobile world, so it's little surprise that the LG V30S ThinQ kinda feels like a cash-grab. It's not unlike Essential pushing out limited edition colorways to drum up some more momentum ahead of its brand new smartphone. Asking people to pay for a new version of a phone with features that are largely compatible with the original can be upsetting, but here's the thing: these AI features actually work pretty well.Chris Velazco/EngadgetLG worked with a company called EyeEm to train its image recognition algorithm on over 100 million photos, and in most of our tests, that work seems to have paid off. You'll tap into all that machine learning through the camera app, where you'll see two new options beneath the viewfinder: QLens and AI Cam. The former honestly feels like a barebones version of Bixby, with shortcuts to feed an image of what's in front of the camera to Amazon (for shopping links) and Pinterest (to add things to your visual collection).Shopping with a visual AI is always a little tricky because you can't always count on the AI to correctly identify what's in front of it. We pointed the QLens at a box of clearly marked cereal, and a handful of correct Amazon shopping links appeared at the bottom of the screen. Nice. When we pointed it at a red sweatshirt and a MacBook Pro and even another V30, we got a handful of suggestions we could tap to see related Amazon listings. In those cases, the camera was able to roughly identify the object -- suggestions included ""Apple laptop"" and ""LG cell phone"" -- but wasn't smart enough to pin down the model and offer the correct links.Chris Velazco/EngadgetWe fared a little better with QLens's Pinterest integration, though: taking a picture of a shirt prompted Pinterest suggestions that, in some cases at least, matched the shirt surprisingly well. Some suggestions were red and some had a similar texture to the real thing, so LG's algorithm was getting pretty close.AI Cam was more immediately useful. There are eight scene mode presets -- portrait, animal, city/building, flower, sunrise, sunset, food and landscape -- and as the camera is meant to fire up whichever is appropriate for what it's looking at. While that identification process is happening, little ethereal keywords bubble up onto the screen to illustrate how the phone is ""thinking"" about the object. It's completely unnecessary, but just about everyone I've shown it to has enjoyed it -- it's a neat way to illustrate the algorithm in action and seeing those keywords slowly become more relevant is actually kind of fascinating.More importantly, when objects in front of the camera -- like some flowers or donuts on a table -- matched one of the presets, the correct shooting mode kicked in just about every time. Mismatches and false positives are certainly possible, but in the few hours we've been testing the V30S ThinQ, we didn't encounter any. If anything, it just took a while for phone to decipher certain images. You don't even need to be using the dual camera for this trick to work; identifications made through the front-facing camera worked just as quickly. If LG could tune the algorithm's performance to the point where this feature could run by default, the company might really be onto something.And then there's Google Assistant. For the most part, it works exactly the way you'd expect it to: throw an ""OK, Google"" or a ""Hey, Google"" at it, and the Assistant cheerily responds and takes your requests. The difference is, the V30S ThinQ (I will never not hate that name) is the first Android phone we've tested with device-specific Google Assistant commands. At present, there are 23 such commands, ranging from ""OK, Google, take a wide-angle selfie"" to ""Hey, Google, take a documentary video."" (For the uninitiated, the latter opens the camera with a specific Cine video mode to emulate classic docs.)You can find the full list of commands here, and almost all of them worked exactly as designed. The experience isn't perfect, though: Google Assistant doesn't seem to understand the word ""Cine"" as LG uses it, so that list of commands is flawed from the get-go. And for some reason, the Assistant was never able to understand me when I said the word ""noir."" Go figure.The issues I encountered were surprisingly minor, and that's made more impressive by the fact that the software on this tester phone isn't final yet. That LG (with quite a bit of help) managed to build an AI experience that functions as well as it does is no small feat, and I really wish LG had just waited and released this thing as the definitive V30 from the get-go.Chris Velazco/EngadgetLG's chosen path, however, leaves me on the fence. I honestly can't tell how crucial the extra 2GB of RAM here is to the usefulness of this AI, but either way, it feels like original V30 owners are getting a raw deal. And while I can't blame LG for trying to reverse its fortunes, interim releases like this one can be hard to swallow, especially since the first V30 only appeared on the market about six months ago. This whole interim release thing raises some questions, too: does it mean, for instance, that LG is abandoning its annual upgrade cycle? Only time will tell.Of course, there's one other big question: should anyone actually buy this thing? That's hard to answer without an actual price tag, but one thing seems clear: there's more to LG's seemingly opportunistic update than meets the eye.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",LG’s V30S ThinQ is the AI-fueled phone the V30 should’ve been
9758,3771507,2018-02-25 10:42:30,"China sets stage for Xi to stay in office indefinitelyBEIJING (Reuters) - China’s ruling Communist Party on Sunday set the stage for President Xi Jinping to stay in office indefinitely, with a proposal to remove a constitutional clause limiting presidential service to just two terms in office.Since taking office more than five years ago, Xi has overseen a radical shake-up of the party, including taking down top leaders once thought untouchable as part of his popular war on deep-rooted corruption.Sunday’s announcement, carried by state news agency Xinhua, gave few details. It said the proposal had been made by the party’s Central Committee, the largest of its elite ruling bodies. The proposal also covers the vice president position.Xi, 64, is currently required by China’s constitution to step down as president after two five-year terms. Nearing the end of his first term, he will be formally elected to a second at the annual meeting of China’s largely rubber-stamp parliament opening on March 5.There is no limit on his tenure as the party and military chief, though a maximum 10-year term is the norm. He began his second term as head of the party and military in October at the end of a party congress held once every five years.Zhang Lifan, a historian and political commentator, said the news was not unexpected, and it was hard to predict exactly how long Xi could stay on in power.“In theory he could serve longer than Mugabe but in reality, no one is sure exactly what will happen,” Zhang said, referring to Zimbabwe’s former president whose four decades in office ended in November, after the army and his former political allies moved to force him out.Though positive remarks filled the comments section under the pages of main state media outlets like the People’s Daily, the move was not welcomed by everyone on China’s Twitter-like Weibo service,“If two terms are not enough, then they can write in a third term, but there needs to be a limit. Getting rid of it is not good!,” wrote one Weibo user.EMPEROR XIConstitutional reform needs to be approved by parliament, which is stacked with members chosen for their loyalty to the party - meaning the reform will not be blocked.There has been persistent speculation that Xi wants to stay on in office past the customary two five-year terms.The October party congress ended without appointing a clear eventual successor for Xi.However, the role of party chief is more senior than that of president. At some point, Xi could be given a party position that also enables him to stay on as long as he likes.Chinese flag is seen in front of the financial district of Pudong amid heavy smog in Shanghai, China, December 23, 2015. REUTERS/Aly SongXi is currently the party’s general secretary, but not chairman. China’s first three leaders after the founding of the People’s Republic in 1949 all carried the title party chairman -Mao Zedong, Hua Guofeng and then Hu Yaobang. It has not been used since.“Whether Xi ends up being Party Chairman or just remains Party Secretary doesn’t really matter. What matters is whether he holds onto power,” said Zhang Ming, a professor of political science at Renmin University of China in Beijing.“Titles don’t matter as much in China as they do in the West. Here what matters is whether you are the emperor,” he added. “In China, ordinary people already consider Xi Jinping to be the emperor.”EYES ON VICE PRESIDENTMao, the founder of Communist China and still held in god-like awe by many Chinese, died while still Communist Party chairman in 1976, having never retired.State media has also increasingly been using the term “lingxiu” to refer to Xi, which means “leader”. Distinct from the standard usage of “lingdao” for leader, “lingxiu” evokes grander, almost spiritual, connotations.Mao, for example, was referred to as “lingxiu”, but Xi’s two immediate predecessors, Hu Jintao and Jiang Zemin, were not.In a Sunday commentary on its WeChat account, state television said: “The people love the people’s ‘lingxiu’!”, above a picture of Xi being greeted by an adoring crowd in Sichuan province earlier this month.The move to lift the presidential term limits is not unexpected. The party has been laying the groundwork for Xi not to have to go.One of Xi’s closest political allies, former top graft buster Wang Qishan, stepped down from the party’s Standing Committee - the seven-man body that runs China - in October.Aged 69, Wang had reached the age at which top officials tend to retire. But he has been chosen as a parliament delegate this year and is likely to become vice president, sources with ties to the leadership and diplomats say.The move is significant because if Wang does not retire, that would set a precedent for Xi to stay on in power too after he reaches what is normally considered retirement age.The Central Committee also proposed other changes to the country’s constitution, including inserting “Xi Jinping Thought on Socialism with Chinese Characteristics for a New Era” into the constitution, referring to Xi’s rather wordy guiding political thought that is already in the party constitution.The constitution will further ensure that party control over the country is not in any doubt, too, strengthening existing clauses about the leading role of the Communist Party in China.“The leadership of the Communist Party of China is the defining feature of socialism with Chinese characteristics,” Xinhua cited one of the proposed new clauses as saying.Reporting by Ben Blanchard and Sue-Lin Wong; Editing by Robert Birsel and Bill Tarrant",China sets stage for Xi to stay in office indefinitely
9759,3772780,2018-02-25 13:54:30,"Founder and Engineer at FitFriend. Runner, Orienteer. Life is about evolution and I want to contribute to thatFeb 25Let’s have no managers, instead of managers with no engineering experienceOver the course of my career as a developer I’ve been involved in a bad management situation more than once.I don’t mean where a manager is bad in the way where they’re simply just not nice. But where they single handedly derail a project due to micro-managing and over simplifying developers’ implementation concerns.Example? I’ve experienced too many instances of just-justification, where a manager has said something like, “it’s just JSON”, or “it’s just UI”, or “it’s just talking to the backend”. This not only unravels their credibility in knowing anything about software, but it also devalues the work that a software developer needs to do to build a working product.You could prepend the words “it’s just” to anything, it’s not going to change the fact that you sound completely ignorant and will ignore any challenges that your team communicates to you. “It’s just a performance tweak”, for algorithm refinement. “It just needs to be on another thread”, for asynchronous thread management. “It just needs to be stored locally” for Core Data or SQL data storage and management. See how this works?The A players at least listen if you take the bait and try to engage them on the complexity involved, and they may even change their simple minded opinion. The really bad managers never see your perspective no matter how many times you explain it.In their eyes they think that making the backend talk to the frontend is just like plugging a phone charger into a wall. This is highly dangerous, because they don’t respect the work that you’re doing, and are therefore never going to allow you time. They will pressure you and ask you when is it going to be done every hour, when you’ve already communicated to them it will probably take 1–3 days to build, possibly longer to make tweaks around testing.When you think about how other industries work, it’s kind of insane that tech isn’t doing the same. I thought we were supposed to be innovative!Do other industries place business types into management?Just pulling from my own experience, before I established a longer term career in programming, I’ve dabbled in being a track runner in school, a chef, a musician, a sound engineer, and finally a salesperson in a bike store while I was studying CS. I also applied to be trained as a networking engineer in the army prior to studying CS, but was rejected due to a previous heart condition (SVT), so let’s throw that industry into the analysis as well.What do we have? In each case I refer to “business school manager” as a manager who comes solely from a business background, with no experience of doing the grunt work:While running track in school was far from doing it professionally, it exposed me to how that industry worked, and elite runners have been a part of my life in some capacity ever since. You would rarely have a coach who is not a former and/or current runner themselves. Conclusion: it would be considered bad practise for a business school manager to coach (manage) a team of athletes.As a trainee chef I was managed by a sous chef. A sous chef is obviously a qualified chef, so you can never get to the level of sous, and hence managing a team, unless you’re also trained as chef. Arguably you could never be a head chef unless being a chef first as well. Conclusion: it would actually be considered insane for a business school manager to manage a team of chefs.A musician is never technically managed on how to create their music. While you do have managers in music, a manager of an artist or a band is more of a “finder of work” rather than leading a team on how to create their product (the music). On another level, a manager would be involved with a band from a record label if they’re signed to that label. In this situation they could be someone with no music experience, but this has always been controversial. Conclusion: business school managers generally don’t play a role in producing the music, and if they do, it’s universally considered bad practise.In music, a sound engineer works closely with musicians, a producer, and a masterer to produce the music. In film, a sound engineer works closely with actors, the director, and the producer to either record or edit the sound. In either case the producer would be considered the closest to a manager, and in the film context possibly the director too. Neither the director or producer would be expected to have sound experience, but it’s always nice when one of them has had some. Conclusion:it is considered acceptable for a business school manager to manage a sound engineer.As a salesperson working in a bike store, I was working close to min wage and therefore could’ve come from any background. It would’ve been technically possible and acceptable for me or anyone from a non-cycling background to work their way up as manager. My manager was pretty much the store owner. He was a Dutch cyclist who had been cycling on semi-pro teams for years before retiring and opening up a store. So while he had a lot of experience in the industry, it is not critically accepted criteria to have experience in the retail space to be a retail manager. Conclusion: it is conventionally accepted that business school managers could manage a team of retail staff, although people who go to business school generally don’t seek out a career in retail.As touched on above, I didn’t get into the army, so I can’t draw a conclusion from previous experience, only from assumptions and from friends who have been in the armed forces. You generally don’t have people managing you on the ground with no military experience. Although business school managers could find their way into the Pentagon and other areas of high command, it’s generally frowned upon if you don’t have field experience. Conclusion: it would be considered bad practise for a business school manager to manage a team in the armed forces.Just from my own direct life experience, that is 2 cases of it being accepted, 3 cases of it being bad practise, and 1 case where it would be insane.But even in sales, once you get out of the world of retail, in an office I doubt that you’d have a VP of sales, or even a manager of a sales team, without any previous sales experience.When I think about other industries that I haven’t been exposed to, like law or medicine, my assumption based on family or friends in the industry is that it would be considered insane for a business school manager to manage a team of surgeons or be chief of medicine, or to be a partner at a law firm.No management is the new managementWhy are we so different to the management practises of other industries? We really shouldn’t be. Why do they excel at having qualified managers with experience in their discipline and we don’t?Business types with none to little engineering experience should never, under any circumstances, manage engineers — I’m sorry to speak in absolute terms, but I think we’ve been experimenting with this failure for far too long! It’s time for this madness to stop. It needs to die.It’s better to have no management than unqualified management.This suggestion may seem extreme at first, but clearly I’m not the only one thinking it. This HN discussion is just 3 days old…I didn’t even contribute to it, and I didn’t have to scour the internet to find it. The thread’s concluding consensus of using Aha!’s roadmapping with Pivotal Tracker’s distribution of tasks linking to the roadmap, seems like an approach that a few companies have already used with success.Now, #notallmanagers you might say, and objectively you have a point. Only about 5% of managers I’ve worked with in the tech industry have not been completely, atrociously, bad. 2 counter points:If completely atrociously bad is the bar that we’re setting, that’s way too low; andThe rare number of good managers I’ve had, have 100% come from previous engineering experience.Ok then, so perhaps the answer lies in promoting quality engineers to management instead of business types?We’ve been trying for decades, apparently. There’s a shortage of engineers who want to become managers…and that’s how destructive people are even allocated to these positions in the first place. This is the experiment that I referred to above. It’s been going on for far too long, and it’s time that we abandoned it.Management has it’s place though. In coordinating different departments together, allocating resources, and resolving conflicts. That’s on a department level. I am not undervaluing the advantage management brings to a company. But on a project level, or even a team level, I can see perfectly functional teams and projects happening without them.We already have product managers who convey product specifications and help scope out the stories with engineers, and we already have scrum masters who can easily be a QA member or developer simply just facilitating the communication of the daily standup. The scrum master should be nothing more than someone who conveys information and provides an outlet for communication. That’s it.So why do we need project managers? We don’t. We don’t need bad ones, but we’ve been failing at that. Project management is a left over relic from a male dominated industry, where hostile attitudes were normalized, and seen as the sole way to motivate staff to build products.Time to try something innovative and progressive — we don’t need project managers at all.","Let’s have no managers, instead of managers with no engineering experience"
9760,3772786,2018-02-21 21:30:00,"One Artist’s Mission to Illustrate All the World’s Mythical BeastsSquonks, Lindworms, Dijiangs, and more.A Kotobuki, a creature consisting of all 12 signs in the Zodiac, from Japanese mythology. All Images: Iman Joy El Shami-MaderEmail This ArticleFromToPlease separate multiple addresses with commas. We won't share addresses with third parties.MessageSubscribe me to theAtlas Obscura NewsletterEvery culture has its own distinctive mythological beasts. In Brazil, there’s the Headless Mule, a cursed creature whose decapitated head hovers above a fire-spewing neck as it gallops across the country. From Japan, the Kotobuki is a Zodiac Frankenstein’s monster: it consists of all 12 signs, from the nose of the rat to the tail of the snake. Peru has the Huayramama, which looks like a vast snake plus the billowing hair and face of an old woman.With such rich and broad source material to draw from, the artist Iman Joy El Shami-Mader has lately been pursuing one very particular goal: she wants to illustrate as many mythical beasts as she can find. Since October 2017, El Shami-Mader has been illustrating one such creature a day, which she then features on her Instagram account. To keep up a steady supply of beasts to draw, El Shami-Mader initially worked from books. “It all started with the book Phantasmagoria—which is great—but there are many creatures that are only mentioned in passing or without any description at all,” she says. So she ordered more books, researched online, and tried her local library. “I’m from a tiny town in the Alps, so other than local creatures, there was little to be found.”From Greek mythology, the Khrysaor was a boar with wings, whose mother was the Medusa.Lately she’s decided to try to crowdsource ideas to keep her project going. Through Instagram, she’s asked her followers to send stories and descriptions of mythical beasts she’s still missing. Her illustrated bestiary now spans mythologies from around the world and across a variety of time periods, and even includes the odd fictional character (she has a porg from Star Wars: The Last Jedi and an Owlbear from Dungeons & Dragons).Atlas Obscura spoke with El Shami-Mader about her project, the challenges of depicting mythical creatures, and the appeal of the lovable Squonk. If you’d like to suggest a creature, email her at mythical.creaturologist@gmail.com.Where did the idea for this project come from?It actually started as a stress-relief strategy and ‘self-challenge’ last fall. I was working five jobs and felt extremely drained and worn-out all the time. I really needed something to balance out the lack of creative expression I was feeling and to get my mind off things, at least for an hour a day.A few years back I did a series of fairytale illustrations and came across many amazing creatures, like the Bøyg in Per Gynt. Since I always wanted to deepen my knowledge about these creatures, I ordered the book Phantasmagoria by Terry Beverton and it arrived on my doorstep on September 30, just in time for me to begin a daily monster-drawing challenge I’d set myself for the month of October. I started to use my lunch breaks to have a quick snack and do a drawing of a creature each day. I was fairly sure I would give up after a week, but it really helped with the stress; for an hour or two each day, all that was on my mind was bringing a creature to paper, nothing else. It was also great to learn about a new monster each day, so when October was over, I didn’t really want to stop.Why mythical creatures?I am generally a history buff and I love fairytales, sagas, myths and legends. In this already pretty epic realm, these beasts feel even more magical. I find them extremely interesting for so many reasons. They can give you an incredible insight to different cultures—what people were afraid of, and what simply was inexplicable at the time and needed to be put into a physical form. I feel like they also show humanity’s need to have a reason for both good and bad things happening. Sometimes they are a ray of hope, the only thing able to cure an incurable illness; other times they bring plagues and death. They are wise helpful spirits, and they are malicious tricksters. It can also be really funny—you can tell that some only exist because of the bad descriptions the scholars wrote down.A Keukegen, which featured in the Japanese 18th century beastiary Konjaku Hyakki Shūi, is a small creature covered in long hair that causes sickness.Tell us a little bit about how you research and plan how these illustrations will look.When someone tells me about a new beast, I still try to do as much research as possible and find the best description available, either on the internet or by asking more people from that region about their version of it. Sometimes the descriptions are very detailed, which makes it easy to come up with a general idea of how proportions and form should be; other times it just says “aquatic creature” or that it has “serpentine appearance,” which makes it harder on one hand, because you cannot depict them “accurately” (as far as drawing a mythical creature can be, anyway), but on the other hand really lets your imagination run wild. I usually have an image in my head of how I’d like it to look. I start by slowly sketching out the first lines in pencil, then elaborate them a bit, and when I’m happy enough with the results I start tracing my pencil drawing with ink pens.A Lindworm, a dragon-like snake with arms, which features in both Germanic, Scandinavian, and British mythology.A Lyngbakur, a vast creature from Icelandic mythology, is half-whale, half-island.What’s the goal of this project?Well, I’ve ‘tasted blood’ now, and am on a mission: I would love to create a complete illustrated bestiary. There are many great books on creatures out there, but so far I haven’t found a complete one. I know this is a Sisyphean task, but I’m motivated. I’d love to turn my findings into a book, or—even better—a series of books that can be continually expanded. For now there is only an idea, but a friend of mine is a composer and we were thinking of collaborating on a trilingual ‘monsters set to music’ book. My current priority, however, is finding as many mythical creatures as possible.A Dijiang, from Chinese mythology, is a bird with multiple wings and feet, but no head.Tell us about your favorite mythical creatures in this project.That is really hard to answer—they are all so unique. I love the Dijiang, because I feel it’s my spirit animal (living in a perpetual state of confusion, but fond of singing and dancing). I love the idea of a Valravne eating a king’s heart and thus gaining human knowledge and becoming evil (eating another human’s flesh was really thought to give you his strength at some point in history!). I think it’s amazing that the Chouyu falls asleep when it sees people, and that the Ovinnik holds a grudge against barns, but is appeased by pancakes.But if I had to choose a favorite one, it would have to be the Squonk, a creature from the forests of Pennsylvania, who was always sad over its hideous appearance. All the love for the Squonk!The sad Squonk, who cries almost constantly over the state of its appearance, is allegedly native to northern Pennsylvania.In Slavic mythology, an Ovinnik is a type of house spirit who looks like a cat, barks like a dog and likes to burn down barns.A Brethmechin is a semi-aquatic creature found near Java, Indonesia.This multi-headed dragon—Zmey Gorynych—features in Slavic mythology.A Valravn depicted carrying a human heart. In Danish folklore, ravens who ate the dead on battlefields took on supernatural, evil powers and became valravns, “the ravens of the slain.”Stay in Touch!No purchase necessary. Winner will be selected at random on 03/01/2018. Offer available only in the U.S. (including Puerto Rico). Offer subject to change without notice. See contest rules for full details.Add Some Wonder to Your InboxEvery weekday we compile our most wondrous stories and deliver them straight to you.",One Artist's Mission to Illustrate All the World's Mythical Beasts
9761,3772946,2018-02-25 11:30:53,"Samsung Galaxy S9 live stream: how to watch today's big launch eventThe launch is today!SharesSamsung's next flagship phone is launching today, with the brand's MWC 2018 event expected to include the unveiling of both the Galaxy S9 and Galaxy S9 Plus smartphones to the world.We've put together this handy guide so you can be among the first to know all about the phones when they're revealed on Sunday 25 February. You'll want to use this guide just before the event kicks off, which is scheduled for 6PM CET - that's 5PM GMT / 12PM EST / 9AM PST / 4AM AEDT.The event takes place in Barcelona, Spain but you'll be able to watch it wherever you are in the world using the methods below.Official websiteHead to the official Samsung website close to the time of the launch and you'll be able to find the livestream ready and raring to go there.If this one goes down in the mania of the launch as thousands of people try to tune in, it's worth noting there's also a separate livestream set to go on the Samsung Mobile Press site too.Samsung's YouTube channelIf you'd rather tune into the launch stream on YouTube, Samsung plans to have that up and running on the official Samsung Mobile channel.Don't expect this to appear until a few hours before the launch itself, but we'll be sure to include the official link in this article as soon as we spot it go live.Watching on YouTube offers a few benefits over the official website too, as you can tune in on your smart TV, Chromecast or even select games consoles.Samsung Unpacked 2018 appSamsung has made its own app for the Unpacked 2018 event, which if you download to your phone or tablet will allow you to watch it directly on your phone.You can download it now on either Android or iOS. Exactly what else the app will be able to do around the launch isn't very clear right now, but it's worth knowing about as an extra option to livestream the event if you're super keen to learn about the Galaxy S9 first.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",Samsung Galaxy S9 live stream: how to watch today's big launch event
9762,3772947,2018-02-25 13:52:10,"Jump to sectionMWC (or Mobile World Congress to give it its full name) is one of the biggest events on the mobile calendar, with 2017's Barcelona show bringing us the LG G6, Sony Xperia XZ Premium and Huawei P10 among dozens of other devices.Tired of your old Android tablet? Huawei has something newHuawei may have delayed its P20 launch until March but it's not letting MWC go to waste. Instead it has announced a new range of premium tablets called MediaPad M5 alongside a new MateBook X Pro. The M5 range is three tablets in total, all with varying specs but all running the latest Android Oreo. These look to be the latest in premium tablet offerings.Blackberry is here for you, whether you like it or notBlackberry knows it's not the phone brand for all, but that doesn't mean it's going anywhere. The company's Global General Manager, Francois Mahieu told TechRadar he feels that Blackberry has established itself and the KeyOne over the past year and will continue to grow the BlackBerry handset brand in the year to come.Prepare yourself for three minutes of Samsung Galaxy S9We're but hours from launch but HMS Samsung has sprung another leak. This time it was the launch video for the highly anticipated Galaxy S9 and S9 Plus smartphones. The video, taken down and then re-posted shows off the handsets in all their glory.The Galaxy S9 and its larger sibling are rumored to have a similar design and screen to the Galaxy S8 range, meaning a metal frame, a glass back and an almost bezel-free curved display, but with the fingerprint scanner likely moved from beside the camera to below it.Speaking of the camera, that's rumored to be heavily upgraded, with two lenses in the case of the S9 Plus and a variable aperture in the case of the S9.The camera, whatever form it takes, is likely to be one of the highlights of the pair, as upgrades to it have been teased by Samsung.The S9 and S9 Plus are also rumored to have new chipsets and an improved iris scanner, which might be better able to recognize your eyes and may scan your face at the same time.Other aspects of the pair may not be so upgraded though - their batteries are rumored to be the same size as that of their predecessors (3,000mAh and 3,500mAh for the S9 and S9 Plus respectively), and the standard S9 may have just 4GB of RAM - though there are rumors of 6GB in the S9 Plus.Huawei at MWC 2018Huawei MateBook X ProThe MateBook X Pro may only be Huawei's second ever laptop but we're impressed by what we've seen so far. This is a machine that's geared to compete with the best offerings from Apple and Microsoft and though we think that's pretty ambitious, we're not inclined to dismiss it from what we've seen so far. An excellent screen, powerful innards and sleek design have left us keen to see more.Huawei MediaPad 5It sounds like one thing, but Huawei's MediaPad 5 is actually a range of three tablets. Two by the name of MediaPad 5, and one with a Pro fixed to the end. These are premium entries into the tablet market with design and specs that reflect this. We liked what we experienced in our hands-on time, but a lot is going to depend on pricing when it's revealed.LG at MWC 2018LG V30S ThinQWe've already had our first taste of LG at this year's show with the V30S ThinQ. This handset takes a lot of what we loved about the original V30 when it was unleashed on the world just six months ago, but brings more RAM, new internal storage configurations and a new blue color to the table. We like what we've seen so far but its specs are behind the competition now, so a lot depends on what price LG decides to set.Sony at MWC 2018Sony Xperia XZ Premium 2With the Sony Xperia XZ Premium announced at MWC 2017 there's a good chance we'll see the Sony Xperia XZ Premium 2 at MWC 2018.This could be a real powerhouse, with rumors of a flagship Snapdragon 845 chipset, 6GB of RAM, a dual-lens rear camera and front-facing stereo speakers.We've also heard that the Xperia XZ Premium 2 might have a 5.7-inch 4K display, a 3,420mAh battery and - perhaps most excitingly of all - a whole new design, with smaller bezels and there's some talk that the fingerprint scanner might even be built into the screen.It might also have a new name, with some rumors pointing to the Xperia XZ2 Pro.This might be an early look at the Sony Xperia XZ Premium 2 and XZ2. Credit: Vortex.geSony Xperia XZ2 and XZ2 CompactAs the Sony Xperia XZ1 only arrived in September you might not be expecting the Sony Xperia XZ2 for a while, but more than one recent rumor suggests it's actually coming soon, so an MWC launch is very possible.As with the Sony Xperia XZ Premium 2 we're expecting a whole new design from the Xperia XZ2, which likely means less bezel than past Xperia handsets.The specs will probably be slightly lower end than on the Premium 2 though, with current rumors pointing to a Snapdragon 845 chipset, 4GB of RAM, a 19MP camera, a 3,180mAh battery and a 5.7-inch full HD 18:9 screen. We've also heard that it has stereo speakers and a system that syncs up vibrations to the sound.This could be the Sony Xperia XZ2. Credit: @evleaksWe know less about the Sony Xperia XZ2 Compact, and are less sure it will land at MWC 2018, but a leaked photo supposedly showing the phone - complete with a curvy new design - suggests it might.The source of the image adds that it has a fingerprint scanner on the back but doesn't have a 3.5mm headphone port.Elsewhere we've heard that it has a Snapdragon 845 chipset, a 5.0-inch full HD screen, a 2,870mAh battery, water resistance and stereo speakers, but a polycarbonate back.Alcatel at MWC 2018Alcatel wants to be your low-cost tablet optionAlcatel is keeping the tablet form going with two brand new offerings that will be available to buy before June this year. The Alcatel 10 inch 1T 10 and 7 inch 1T 7 won't exactly leave iPads struggling in a spec war, but with price points starting from €70 price tag (around £60 / $85 / AU$60) it's hard to complain. These are the kind of tablets we think will be perfect for throwing around the house and handing to the kids without fear.Nokia at MWC 2018Nokia 9MWC 2018 might be where we see the first realNokia flagship since HMD Global took over the brand. Because as good as the Nokia 8 was, it struggled to stand out from the competition.For the Nokia 9 there are rumors of a 5.5-inch QHD OLED screen, a water-resistant metal body, tiny bezels and dual-lens cameras on both the front and back of the phone.The Nokia 9 could be light on bezel. Credit: 91mobiles/WeiboOf course, it will be powerful too, with a Snapdragon 845 chipset and anywhere from 4GB to 8GB of RAM rumored, and it could have a big 3,800mAh battery and an iris scanner.We don't know for sure that the Nokia 9 is headed to MWC 2018, but HMD Global has claimed to be ""super busy"" planning for the event, so something's sure to be announced.Google at MWC 2018Android Go is making us hyped for a hyper-cheap handsetAlcatel has unveiled a handful of impressive and affordable smartphones at this year's MWC. With decent specifications and hard-to-beat price points, the Alcatel 5, 3 series and 1X are worth your time. Though it's the cheapest of the lot, the Alcatel 1X is probably the most innovative because it comes with the new Android Oreo Go Edition. This means smaller apps which require less power to operate allowing you to squeeze more out of your budget handset.Other possible announcementsWe've covered the major likely announcements above, but there are sure to be loads of other phones unveiled at MWC 2018, as well probably as wearables and tablets.Many may be a complete surprise, but some likely announcements include the LG V30S, which is said to be a revised version of the LG V30, complete with 256GB of storage and new AI and augmented reality features.There's also the Nokia 7 Plus, which we've recently started hearing rumblings about. Rumored specs include a 6.0-inch 18:9 screen, a mid-range Snapdragon 660 chipset, 4GB of RAM, a dual-lens 12MP and 13MP rear camera, a 16MP front-facing camera and an aluminum build.Could we also see the Samsung Galaxy Tab S4? A slate believed to be the Tab S4 has recently been benchmarked with a 10.5-inch 1600 x 2560 screen, a 2.3GHz octa-core chipset and 4GB of RAM, which would make it a high but not quite top-end slate.We haven't heard that it's landing at MWC, but the Samsung Galaxy Tab S3 landed around a year ago, so it's possible that another flagship slate is on the way soon.And we already know the low-end LG K8 (2018) and LG K10 (2018) are on the way.What we probably won't seeThere are a number of flagships approaching a year old that probably won't get refreshed at MWC 2018.For example there's the LG G7, which probably won't show as the company has confirmed to Tech Advisor that it's not holding a press conference at MWC this year.Separately we've heard that the company might have recently ditched its work on the LG G7 and started from scratch, which could be the reason for its likely absence at MWC, and means we probably won't see it for another couple of months at least.There's also the HTC U12, as an ""inside source"" has apparently said to expect the phone at a separate event in March or April. This wouldn't be a surprise, since HTC didn't unveil a flagship at MWC 2017 and since the HTC U11 Plus only landed fairly recently.And we probably won't see the Huawei P20 at MWC 2018 either, as we've received an invitation from Huawei for an event on March 27, and we've also heard a rumor that the P20 won't land until after the show.Though with Huawei's VP of Handsets Product Line, Bruce Lee, previously telling Android Central that the company will ""probably launch devices at Mobile World Congress"" in future we can't completely rule out an MWC launch.What we want to seeWith MWC 2018 not yet underway we’re free to dream of all the things we might see, or at least hope to see. The following are among our most wanted.1. Samsung Galaxy S9Samsung didn’t entirely skip MWC 2017, in fact it even held a press conference, but instead of the Galaxy S8 and S8 Plus (which arrived later) we got the Samsung Galaxy Tab S3.Samsung's Galaxy S8 skipped MWC 2017.It’s understandable that Samsung would want a separate event away from the madness of MWC to launch its biggest phone, but then the S8 is big enough that it would easily be the most popular launch of the show, so a separate announcement isn’t really needed.All of that changes now that it's 2018. Samsung is ready to take the big stage at MWC 2018 and unveil the Samsung Galaxy S9 and Galaxy S9 Plus, according to the leaks and Samsung's own Unpacked 2018 invite.2. A folding phoneIt’s felt like folding phones have been just over the horizon for years now, but there’s growing evidence that 2018 could be when we’ll finally see one, and MWC 2018 would be the ideal place for it to launch.Hopefully when it does launch it will be more than just a gimmick, and won’t cost so much we need to sell our kidneys to buy it.3. A smartwatch renaissanceWe were hoping that the first major Android Wear update would be accompanied by a shower of new smartwatches, but it wasn’t to be.More began to land over the course of 2017, but only from a few manufacturers, so hopefully MWC 2018 will be used as an opportunity for companies to get back into the Android Wear game in big numbers.But we don’t just want new watches, they also need to do genuinely new things and improve on the existing models to make them more desirable.Better battery life tops our list of wants from them, since a watch we need to charge every day is not the future we envisioned.4. More from NokiaThe big success story from MWC 2017 wasn't a flagship smartphone that costs hundreds - it was the reboot of the iconic Nokia 3310.Launched to whip up interest in the Nokia brand again, the new 3310 was used to draw attention to the return of the famous name to the mobile market. It arrived alongside a trio of Android phones, but we're hungry for more.We'd love to see HMD Global (the firm that's licencing the Nokia name) produce a Samsung and Apple-rivaling handset at MWC 2018. We've since had the Nokia 8 and that didn't quite cut it, but perhaps the Nokia 9 will land at MWC?5. A real iPad rivalThere hasn’t been much of interest in the world of Android tablets lately, other than the Samsung Galaxy Tab S3, which itself was announced at MWC 2017.At 2018’s show we’d like to see a larger number of slates, and for at least some of them to genuinely match up to the iPad Pro 2 duo.6. InnovationMWC 2017 saw the launch of numerous bigger and better phones across the low, mid and high-end, but the improvements were usually the expected ones – sharper screens, faster chipsets and so on.Innovation wasn’t absent, but it was in short supply, so while we certainly wouldn’t say no to all the usual improvements next year, we hope they’re accompanied by more truly new, different and interesting things.7. Big phones from small companiesWhile smaller companies weren’t absent from MWC 2017, they generally failed to grab much attention, and that’s not just because they’re not household names, but also because – in many cases at least - their announcements were genuinely less interesting.We’d like to see more lesser known brands grab headlines with exciting announcements at MWC 2018.That might be tough for small companies, but there’s potential for huge – but little known in the west – companies like Xiaomi to do it.8. The next generation of fitness trackersAs well as an absence of smartwatches there wasn’t much for fitness fans to see at MWC 2017 either, so we’d like to see an influx of sporty wearables at MWC 2018.And as with smartwatches these need to be significant improvements on what’s gone before – perhaps somehow automatically tracking a wider range of workouts than most of the current selection, like weights and yoga.",MWC 2018: All the latest news from the show
9763,3772949,2018-02-25 13:30:04,"Hands on: Huawei MateBook X Pro reviewHuawei squares up against Apple and MicrosoftOur Early VerdictConsider us impressed by Huawei’s second-ever traditional laptop. With the MateBook X Pro, Huawei has a machine tailor-made to compete not only with the luxe laptops of the world, but those that are often put in front of creative professionals.ForExcellent screen with tiny bezelsGorgeous designPowerful innardsAgainstClever webcam placement still poorMystic Silver is Core i7 onlyAt the announcement of its second laptop, the MateBook X Pro, Huawei was in a bullish mood. Buoyed by the success of its debut laptop, theMateBook X, it is pitching its second laptop to directly compete with both Apple’s MacBook and MacBook Pro range of laptops.You’ve got to admire Huawei’s ambition, and some may dismiss Huawei’s goals of going against just an established and successful rival, the fact that were were pretty impressed with its debut laptop means we shouldn’t be so hasty to dismiss this new attempt.While we haven’t had a chance to fully test out the MateBook X Pro, we have got some hands-on time to get a feel for the device and, so far, we like what we’ve seen.Price and availabilityHuawei has revealed the MateBook X Pro will start at €1499 (about $1850, £1300, AU$2350) for the lowest spec version while the price will rise to €1899 (about $2350 £1670, AU$3000) for the highest spec.The exact release date for the MateBook X Pro is also unclear as the company has only shared the date of Q2 so far. In fact, the UK and Australia weren't mentioned in the first wave of markets during the Huawei press conference.The original MateBook X started at €1,399 (about $1,570, £1,210, AU$2,090), so we were right to expect the price to be higher due to the improved components.Those components, specifically in the US, start with an 8th-generation Intel Core i5 processor paired with 8GB of RAM and a 256GB solid-state drive (SSD). From there, you will be able to upgrade to a Core i5 with Nvidia’s MX150 graphics chip, and finally up to a Core i7 system with those dedicated graphics.Elsewhere in the world, RAM configurations will start at 4GB, while the screen size and resolution will remain the same throughout. Also, all MateBook X Pro configurations come with the cleanest Windows 10 install possible: Signature Edition.However, again, we were told at the launch of the MateBook X Pro was that Huawei was looking at being ‘disruptive’ when it comes to price. Is it considering an eye-catching price much lower than what we’re expecting? It would certainly help differentiate the MateBook X Pro against Apple’s MacBook and Microsoft 13.5-inch Surface Book 2.DesignThe design of the MateBook X Pro continues the premium feel of its predecessor, and features a metal unibody design with diamond-cut edges and a sandblast finish. This results in a laptop that certainly looks luxe – and feels that way as well.When closed, the MateBook X Pro is just 4.9mm (0.19 inches) at its thinnest end, and 14.6mm (0.57 inches) at its thickest, and weighs just 1.33kg (2.93 pounds), which – as Huawei is keen to point out – is lighter than theMacBook Air (which is 1.35kg).These dimensions lead to a laptop that is slim and light enough to comfortably carry around. It comes in two colors, Space Gray and Mystic Silver, and both look very nice up close. (Note that, in the US, the Mystic Silver option only comes in the highest configuration.)There’s a full size keyboard that’s backlit and spill-proof, and the 13.9-inch IPS (in-plane switching) screen now has impressively thin bezels surrounding it, keeping the overall size of the laptop down to just around 12 inches wide. Huawei claims that the MateBook X Pro is the world’s first ‘FullView’ notebook, with a 91% screen to body ratio.Image 1 of 15Image 2 of 15Image 3 of 15Image 4 of 15Image 5 of 15Image 6 of 15Image 7 of 15Image 8 of 15Image 9 of 15Image 10 of 15Image 11 of 15Image 12 of 15Image 13 of 15Image 14 of 15Image 15 of 15In comparison, the original MateBook had a ratio of 84%. We’re big fans of screens with small bezels, and by giving over so much space to its display, the MateBook X Pro is certainly eye catching.In fact, the bezels are so small that Huawei came up with a unique placement for the MateBook X Pro’s webcam: it’s actually in the keyboard beneath a key with a camera icon on it. Pressing the key depresses a latch that lets the 720p camera appear. This doesn’t really solve the whole ‘ChinCam’ problem we’ve seen on laptops like Dell XPS 13, but at least it’s a more elegant and clever solution than those.Now, speaking of that display, it’s now a touchscreen, with Gorilla Glass protection to keep it from getting scratched. It also has a 3K (3,000 x 2,000) resolution, which leads to a 260 PPI (pixels per inch) pixel density. So, the display is nice and sharp, though you may notice that the screen is more ‘square’ than other laptops.This is because it features a 3:2 aspect ratio, like the MateBook X, rather than the more common ratio of 16:9. So, you’ll get more vertical space than other laptops, which can help with productivity, though watching media on it will result in black bars at the top and bottom for widescreen videos.No matter what you use it for, the screen certainly looks sharp and vibrant, with a 450 nit brightness and 1,500:1 contrast ratio.On the bottom half of the laptop, beneath the deep-and-punchy keyboard, is the large trackpad – which Huawei says is the largest one found on a 14-inch laptop. We want to spend more time with the MateBook X Pro to find out if the size of the trackpad is a help or a hindrance. Being a Microsoft Precision trackpad, we’re confident that it will mitigate mistaken presses well.On either side of the keyboard are two top-firing, stereo speakers (for a total of four) that support Dolby Atmos, and it shows. A demonstration of the audio performance at 80% volume was loud and impressively nuanced. We could actually hear the sound travel across the four distinct audio channels.Above the keyboard is the power button, which – like the MateBook X – has a built in fingerprint scanner. This is a great idea, as it means you can turn on the laptop, and sign in to it, with just a single touch, and Huawei told us that from cold startup to logging in to Windows 10, it takes just 7.8 seconds – and just 6.6 seconds from hibernation.This is thanks to the fingerprint sensor being a hardware-level part, meaning it’s controlled directly by the processor, not by Windows 10 – of course, it still works with Windows Hello. It’s little touches like this that will help Huawei’s laptops standout from the competition.PerformanceWe didn’t get long enough of a time with the MateBook X Pro to fully put it through its paces, but it did feel nice and quick when using Windows 10.Considering the specifications, this shouldn’t be too surprising, with an Intel 8th-gen Core i7 8550U processor, which is 40% faster than the 7th-gen CPU in the MateBook X. Huawei is keen to point out its use of a U series CPU, rather than an M series, which is more often found in laptops of this size.This is thanks to the power management features Huawei has included, with learnings from its smartphone division helping to keep battery life up, while remaining thin and light.As for the battery, the MateBook X Pro offers all-day battery life with 57.4Wh worth of juice, and it uses intelligent algorithms for power efficiency – improved by 15%, Huawei promises.It is also one of the thinnest laptops to feature a discrete GPU – the aforementioned Nvidia GeForce MX150 with 2GB of GDDR5 memory. Also, via Thunderbolt 3, you can add an external graphics card, up to a GTX 1080, making it a capable gaming machine as well.Plus, in the US, the laptop comes with Huawei’s MateDock 2.0, which feeds HDMI, VGA, USB 3.0 and a USB-C passthrough via its Thunderbolt 3. Rounding out the port selection is another USB-C 3.1 port and the return of a USB 3.0 port, for the traditionalists out there.All in all, we anticipate the MateBook X Pro to be a solid performer with even some slight gaming chops, but certainly enough graphical oomph to edit images and video at high resolutions.Early verdictConsider us impressed by Huawei’s second-ever, honest-to-goodness laptop. With the MateBook X Pro, Huawei appears to have crafted a machine tailor-made to compete not only with the luxe laptops, but those that are often marketed toward creative professionals.Of course, we’ll have to wait and see just how performant this laptop can be in a full review, and its price will determine quite a bit regarding its place in the laptop pantheon. That said, it’s clear just how deeply serious Huawei is about making a name for itself in the mobile computing game, and we’re excited by the prospect.MWC (Mobile World Congress) is the world’s largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicated MWC 2018 hub to see all the new releases, along with TechRadar’s world-class analysis and buying advice about your next phone.What is a hands on review?'Hands on reviews' are a journalist's first impressions of a piece of kit based on spending some time with it. It may be just a few moments, or a few hours. The important thing is we have been able to play with it ourselves and can give you some sense of what it's like to use, even if it's only an embryonic view. For more information, see TechRadar's Reviews Guarantee.",Huawei MateBook X Pro review
9764,3773022,2018-02-25 12:32:34,"TNW SitesMateBook X Pro hands-on: Huawei is getting really good at making laptops“Guess where there webcam is,” asked a Huawei executive as the new MateBook Pro X was passed around room full of tech reporters.After a couple of minutes of confused searching – is it hidden in the tiny bezel? behind the LCD? – someone figured it out. “It’s in the keyboard.”Specifically, the webcam is tucked under the keyboard camera button, physically hidden to protecting any snooping. Press the button and the camera pops up with a super-satisfying click.It’s only 1MP and probably sucks, but still.That’s frikkin’ clever. So clever, I might not even mind the up-the-nostrils perspective.Impressive design touches throughout the MateBook X Pro make a strong case that Huawei’s PCs are worth considering next to the Dells and HPs of the world – even though it’s only the second traditional laptop the company’s ever made.It’s a followup to the company’s excellent MateBook X, aiming at a higher-end market while maintaining its predecessor’s portability. A look at the spec sheet reveals a few interesting details:13.9-inch touchscreen display91-percent screen-to-body ratio3000 x 2000 resolution450 nits brightness8/16 GB RAM256/512 GB PCIe SSD57.4Wh battery8th Gen quad-core Intel i7 and i5 (8550U/8250U)Nvidia MX150 GPU14.6mm thick1.33 kg (2.93 lb)2x USB-C (1x Thunderbolt 3), 1x USB-AMatedock 2 included (USB-A, HDMI, VGA)Fingerprint reader integrated in power buttonDolby Atmos capable quad speakers100 percent sRGB coverage14.6mm thickBattery rated at 12 hours local video, 14 hours mixed use.Microsoft Signature Edition (no bloatware)Huawei says the MateBook X is the thinnest laptop with a dedicated graphics card. While the MX150 isn’t going to play games at native resolution and max settings, it’s still a huge jump over integrated graphics. You’ll be able to play modern titles at lower resolutions and medium settings, but the dedicated graphics is more likely aimed at people doing video and photo work on the go.The battery is also pretty hefty for its size, larger than competitors like the Surface Laptop and XPS 13 – I’m curious to see how the 14-hour battery life claim holds up. And the fact that Huawei managed to fit a USB-A port in such a thin frame is welcome.But the screen-to-body ratio is what’s most remarkable. It’s an impressively small laptop considering the a 14-inch screen and 3:2 aspect ratio. Other laptops with tiny bezels like the XPS 13 kind of cheat. They have small bezels on the sides and top, but will have a huge chin at the bottom. The XPS 13 has an 80.7 percent screen-to-body ratio. The MateBook X is nearly all screen, at 91 percent.Like its predecessor, build quality seemed solid and the design attractive, if a little too MacBook-like for my liking. The keyboard feels pretty great for a device its size, and the touchpad is huge.The display looked gorgeous and bright – and I’m glad it’s touchscreen this time around. The speakers were impressively loud; they filled up a conference room at 70 percent volume, and I could even hear some stereo separation from the other end of the room. The original Matebook X had some of the best speakers I’ve heard on an ultrabook, so it’s nice to see Huawei improving on them here.Unlike Huawei’s phones, the MateBook X Pro also will arrive free of bloatware – at least in the US. Because it’s a Microsoft Signature Edition build, you won’t see anything on the desktop other than the recycle bin.Quite frankly, this is one of the rare cases I can’t think of something to complain about from my brief hands-on. That said, Huawei hasn’t announced a price as of the time of publishing though, so lets hope it’s reasonable. Let’s also hope the MateBook X Pro holds up to scrutiny when we do our full review, because right now, there’s a whole lot to like.",MateBook X Pro hands-on: Huawei is getting really good at making laptops
9765,3773023,2018-02-23 13:58:43,"TNW SitesWhy UX designers need to understand the psychology of usersI am of the opinion that a good UX leader needs to understand how people think, feel, perceive, and react, and then use the knowledge to deliver suitable products. To do this, we need some understanding of the human psyche, which remains at the heart of almost every decision people make. Recognizing this psychology and applying it to design is crucial if UX leaders hope to get desired results.If I had a penny for every time someone called end-users “dumb,” well… never mind. I have, on multiple occasions, heard the need to simplify application design or interfaces so even children may work with them.Better still, test them with our mothers, some say. I feel the problem with this approach is we do away with the psychology that is required in achieving a great user experience.The psychology of attentionSimply put, attention is our ability to notice something or someone based on different stimuli. Whether we want to deal with a situation or ignore it depends on our attention. While we focus our attention in some instances, in some others it works subconsciously.By understanding the psychology of attention, I find myself in a better position to deal with mechanisms of perception. Then, I determine how to create consistent behavior. I address both, spatial and feature-based attention.It’s safe to say that I have reduced my cognitive overload by understanding how selective, divided, and sustained attention works. For example, I, like most people I know, find an auditory task less interfering than a secondary visual task. What do you think is simpler — talking while driving or using your car’s navigation system?Subconscious processingMuch of what the human brain processes happens subconsciously. Factors that people are not even aware of can affect their behavior. While we might expect our brains to always arrive at conscious and rational decisions, I have noticed that psychology does not work that way. The subconscious mind, after all, does not account for rationality at all time.The framing effect, as described in psychology, can bring about different reactions depending on how particular choices are presented. In an experiment I was part of, simply using words like “tired” and “retired” got even young people to slower their pace while walking.If UX managers can get users to commit to a small action like signing up for a newsletter, it increases the likelihood of their committing to larger actions like subscribing to a service later.EmpathyI feel better equipped at putting myself in someone else’s shoes if I understand his or her psyche, as it puts me in a better position to provide a solution. Empathy plays a crucial role in UX design because if you are not in sync with your end users, there is little chance that you will understand their needs or meet their expectations. The user experience, in any such scenario, will be bad to terrible.By understanding the psychology of users, I can take measures to address their pain points. Getting an insight into human behavior and motivating factors gives me the ability to add more value to my offerings. When we fail to understand and apply user psychology, UX becomes no more than a commodity, where good designs remain few and far in between.",Why UX designers need to understand the psychology of users
9766,3773953,2018-02-25 15:35:00,"The Nokia 8110 Reloaded is HMD's latest retro feature phoneMany companies descend on the annual Mobile World Congress event to plug their first smartphone launches of the year. It's an established routine, but HMD Global undeniably stole the show in 2017 with, of all things, a new feature phone. Flexing its newly-acquired license to the Nokia brand, HMD put on a marketing masterclass by announcing a re-release of the iconic Nokia 3310. This year, it's attempting a similar trick, preying on '90s nostalgia with the new Nokia 8110 Reloaded.Gallery: Nokia 8110 Reloaded | 16 PhotosThe 8110 was de rigueur when it launched two decades ago, but if the late '90s are a little hazy, you might best remember the phone for its recurring cameo in 1999 sci-fi classic, The Matrix.Quick tangent: HMD's use of 'Reloaded' is kinda clumsy here, since it was Samsung that created a weird, futuristic phone specifically as a promotional tie-in to the Matrix sequel of the same name. Also, sorry to burst your bubble this early, but the new 8110 doesn't have a spring-loaded slider like the modified versions of its predecessor used in the movie. Looks like you'll have to wait for a Nokia 7110 re-release for that.Much like HMD's take on the 3310, the new 8110 is by no means a carbon copy of the 20-year-old handset that inspired it. They share the same banana-shaped design, which can't possibly feel comfortable in your pocket for any length of time, but the stubby antenna of old is now gone.Naturally, it's also a lot smaller than the original 8110, but nowhere near as small as HMD could've made it. Compared to the cute and neat 3310 re-issue, the new 8110 is still rather long and chunky, particularly for a feature phone. Given that components and materials are much lighter these days, though, it doesn't have that brick-like quality old Nokia phones are known for. The plastic slider feels conspicuously flimsy, for instance.The recognizable curved frame carries with it a number of features you won't find on turn-of-the-century phones, such as a 2.4-inch color screen (320 x 240), an awful reimagining of Snake, a 2-megapixel camera, much more capable feature-phone software, WiFi and even a 4G modem. This is primarily so you turn the thing into a retro-styled WiFi hotspot -- the 4G version of HMD's 3310 also has this functionality, but that handset's only available in China. A speedy 4G connection isn't totally useless if you don't have another device that'll benefit from a hotspot, though. Compared to the 3310, the browser on the 8110 is much better at rendering mobile websites in a readable way. Surfing on such a small screen is still a test of patience, mind you.HMD adds that several popular apps will be able to take advantage of the data connection on the 8110. (For reference, you can get basic Twitter, Facebook and Facebook Messenger clients on the 3310.) At our briefing, HMD wouldn't be any more specific, but we're thinking maybe Instagram, WhatsApp or Snapchat -- perhaps a decent navigation app. Those kind of names would make the 8110 more appealing to hipsters wanting a talking-point phone with up to 25-day battery life, as opposed to a digital detox device.As HMD now has two re-issues in the 3310 and 8110 Reloaded, it's going to start calling these Nokia Originals. It's the company's way of saying this series of nostalgic handsets won't end with the new 8110. HMD still refuses to explicitly comment on whether the 3310 was, as most journalists suspect, a calculated marketing ploy. Representatives won't talk sales figures, only saying it was a ""success."" Beyond the countries where feature phones still enjoy significant market share, apparently plenty of people bought it as a back-up to store in their glove box or to take to festivals, as something new for their less tech-savvy parents or just as a fun collector's item.HMD is hoping the 8110 Reloaded will have the same broad appeal when it starts shipping in May in classic black and bright yellow (an obvious nod to its banana shape). However, at €79 (roughly $97, or £70), that puts it in the same price bracket as HMD's new entry-level Android smartphone, the Nokia 1. 'Success' comes in many forms, though, including simply getting people talking about Nokia again.",The Nokia 8110 Reloaded is HMD's latest retro feature phone
9767,3773955,2018-02-25 01:17:00,"Nearly half of 2017's cryptocurrencies have already failedThe surging price of bitcoin (among others) in 2017 led more than a few companies to hop on the cryptocurrency bandwagon with hopes of striking it rich almost overnight. Many of their initial coin offerings seemed dodgy from the outset... and it turns out they were. Bitcoin.com has conducted a study of ICOs tracked by Tokendata, and a whopping 46 percent of the 902 crowdsale-based virtual currencies have already failed. Of these, 142 never got enough funding; another 276 have either slowly faded away or were out and out scams.The number of casualties might be higher in practice. Another 113 ICOs have either stopped talking on social networks (a good sign interest has died) or have so few adopters that success is very unlikely. And the survivors aren't necessarily doing much better. Only a ""handful"" raised over $10 million, which left an uphill battle for the rest.It doesn't take much divination to understand why many of these virtual coins fell flat. Excluding the scams, a large chunk of them were targeted at niches like dentistry, freight trucking or real estate -- they were never going to attract broad audiences. Others, meanwhile, were me-too efforts that had no real advantage over pouring money into an established format, where prices were more likely to climb.ICOs are still popular options in 2018, but it doesn't look like the new wave will fare much better. We've already seen Kodak and other companies start cryptocurrencies for little more than a momentary stock boost. Pair that with falling prices and many ICOs face daunting prospects.",Nearly half of 2017's cryptocurrencies have already failed
9768,3773956,2018-02-25 13:30:00,"Huawei's MateBook X Pro crams a pop-up webcam into the keyboardThe company's classy take on the MacBook has a weird home for your webcam.When Huawei unveiled its first true laptop barely a year ago, it was a slightly more useful MacBook lookalike. While the MateBook X looked a lot like Apple's device, it offered some minor changes that alleviated common grievances, including the MacBook's solo USB-C port and its barely-there key travel. At MWC this year, Huawei is unveiling the next generation of the series, called the MateBook X Pro. It's a larger, more powerful notebook with a better display and impressive audio tech, but one quirky new feature could prevent it from being a good option for professionals.Gallery: Huawei MateBook X Pro hands-on | 9 PhotosHuawei moved the webcam from above the screen and hid it in the keyboard, primarily to tackle privacy concerns. Instead of integrating shutters like Lenovo and HP have done, Huawei's solution to the sticky-note-over-the-camera situation is to embed it inside a key.Push down the spring-loaded middle button of the top function row, and up pops a 1-megapixel webcam with a light next to it indicating it's on. Press it down when you're done, and it not only deactivates and gets out of the way but even if it is hacked to spy on you, it will see only the darkness inside your laptop.Kudos for creativity, Huawei -- this is unique. But as we've already seen with Dell's XPS line of laptops, webcams placed below the display make for seriously unflattering, unclear angles in photos and during conference calls. Unlike the XPS laptops, you can't move the screen to try and find a less-awkward angle, either. When I tried out the MateBook X Pro's camera, I was annoyed by how difficult it was to avoid having an on-screen double-chin -- I wouldn't be happy using this device to make any video calls.The one other benefit of moving the webcam down to the keyboard is the MateBook X Pro's noticeably skinnier bezels compared with the original. It doesn't seem like a good enough tradeoff, though.Still, the Pro is a gorgeous piece of hardware. Sure, it looks very much like a MacBook, but I'm not complaining -- nice is nice. The new MateBook feels just as sturdy and premium as its predecessor, once again combining a lightweight build with a sleek profile.If you can look past the awkward camera placement, or if you don't intend to use your laptop's webcam much, there are a few more things to like about the MateBook X Pro. In addition to getting bumped up to the latest eighth-generation Intel Core i5 (or i7) CPUs, the notebook can also be configured with a discrete NVIDIA MX150 graphics chip. This makes it the thinnest laptop of its size to sport discrete graphics, according to Huawei.The MateBook X Pro has a larger screen than the original -- 14 inches -- which packs a 13-inch display. It also goes up to 450 nits in brightness over the smaller laptop's 350 nits. Since I only saw the Pro in a meeting room, I couldn't tell if the increased luminance would make it easier to see in direct sunlight, but it was certainly clear during our demo. The new model's screen is now touch-friendly, and got a slight resolution bump to 3,000 x 2,000, maintaining a similar pixel density to the smaller version.To complement the display during your movies or games, the MateBook X Pro comes with Dolby Atmos 2.0 for crisper and more immersive surround sound. During our demo, Dolby's sample of nature sounds rang out loud and clear from the laptop's quad speakers, even when I was facing the back of the device. Huawei also moved the speaker grills from above the keyboard to either side of the keys. The Pro also packs a quad-mic array, two more than before, which should allow for clearer voice quality. A much larger touchpad now sits below the keyboard, making mouse navigation more convenient, too.The company has apparently managed to squeeze more battery life out of the Pro, and it can clock about 12 hours on a charge -- two hours longer than the regular model (looping 1080p video). While the smaller MateBook X had two USB-C ports (one more than the MacBook), the new laptop adds a USB Type A socket to the mix, at USB 3.0 speeds. The two USB-C slots will not only support power and data transfer, but one of them is also compatible with Thunderbolt 3.Questionable webcam placement aside, the MateBook X Pro appears to be an attractive, powerful 14-inch laptop. If the camera is not a dealbreaker, you might want to consider this PC as your next workhorse. Unfortunately, we still don't know how much it will cost and when it will be available, but we do know that it will come in two colors (silver and dark gray). Because the original MateBook X started at $1,099, you can expect the Pro model to cost slightly more than that at launch. That's not a small investment, so you might want to wait until we get a review unit in for more testing before splurging on it.Update: At its press conference in Barcelona, Huawei shared some European pricing information, which should be a good reference for when the device arrives stateside. The base configuration, which packs the MX150 discrete graphics, a Core i5 CPU and 256GB of storage, will cost €1,499.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.",Huawei's MateBook X Pro crams a pop-up webcam into the keyboard
9769,3774050,2018-02-25 16:07:13,"This SVG always shows today's dateFor my contact page, I wanted a generic calendar icon to let people view my diary. Calendar icons are almost always a skeuomorph of a paper calendar, but I wondered if I could make it slightly more useful by creating a dynamic icon.Text positioning is relatively simplistic. An X & Y position which is anchored to the bottom of the text - remember that letters with descenders like g will extend beyond the bottom of the Y co-ordinate. This is also where we set the colour of the text, its size, and a font.",This SVG always shows today's date
9770,3774053,2018-02-25 16:10:15,"China Moves to Let Xi Stay in Power by Abolishing Term LimitPresident Xi Jinping of China at the Communist Party congress in Beijing last October. Current law restricts the president to two terms.Credit How Hwee Young/European Pressphoto AgencyBEIJING — China’s Communist Party has cleared the way for President Xi Jinping to stay in power, perhaps indefinitely, by announcing on Sunday that it wants to abolish the two-term limit on the presidency — a dramatic move that would mark the country’s biggest political change in decades.The party leadership “proposed to remove the expression that the president and vice president of the People’s Republic of China ‘shall serve no more than two consecutive terms’ from the country’s Constitution,” Xinhua, the official news agency, reported.With each term set at five years, the Constitution currently limits Mr. Xi, who became president in 2013, to 10 years in office. But the announcement appears to be the strongest signal yet that Mr. Xi, 64, intends to hold onto power longer than any Chinese leader in at least a generation.“I think this is without a doubt the clearest confirmation we’ve had yet that Xi Jinping plans to stay in power much longer than we thought,” said Jude Blanchette, an expert on Chinese politics in Beijing who works for the Conference Board, which provides research for companies. “We should expect Xi Jinping to be the dominant political force in China for the next decade.”The announcement also confirmed that Mr. Xi has amassed enough power to rewrite the rules that had constrained his predecessors, Hu Jintao and Jiang Zemin, both of whom stepped down after two terms. Those rules were aimed at preventing the reappearance of the cult of personality that had surrounded the People’s Republic’s founding father, Mao Zedong.“I don’t see any reasonable challenges for him,” Wu Qiang, a political analyst in Beijing who formerly taught at Tsinghua University, said of Mr. Xi. “He has removed any potential political competitors.”The proposed constitutional changes were released in the name of the Central Committee, a council of hundreds of senior party officials, who will meet from Monday for three days.According to state media reports on Sunday, the Central Committee approved the amendments to the Constitution at a meeting last month. But the vague official announcement released at that time did not hint at the momentous expansion of Mr. Xi’s presidential power, which was kept secret until Sunday.In another victory for Mr. Xi, the draft amendments to the Constitution would also add his trademark expression for his main ideas — “Xi Jinping Thought on Socialism with Chinese Characteristics for a New Era” — into the preamble of the Constitution, as well as adding a nod to the ideological contributions of his predecessor, Mr. Hu.The amendments are almost certain to be passed into law by the party-controlled legislature, the National People’s Congress, which holds its annual full session from March 5. The congress has never voted down a proposal from party leaders.Sunday’s move will make Mr. Xi much more powerful than he already was, and will dampen any remnants of resistance to his rule, said Zhang Baohui, professor of international affairs at Lingnan University in Hong Kong.“Once people know he will serve for who knows how long, it will strengthen his power and motivate everybody to bandwagon with him,” said Mr. Zhang. “Any rival will think he will be almighty.”An error has occurred. Please try again later.You are already subscribed to this email.If Mr. Xi had retained the two-term limit, his power would have begun to wane in a year or two as he effectively became a lame duck, analysts said. Traditionally, Chinese leaders’ powers weaken once they approach the second half of their second term, they said.In what would be another break with tradition, Wang Qishan, a close ally of Mr. Xi who enforced his harsh campaign against corruption and disloyalty in the party, appears set to return to power as vice president. Mr. Wang, 69, stepped down from a party position last year because of his age.Mr. Xi is overturning a two-term limit on national leaders that evolved from the 1990s, when the country sought a more predictable system for handing power to new generations. Greater stability was sought after the upheavals of Mao’s era and then Deng Xiaoping’s failed efforts to engineer a smooth succession.Mr. Jiang, the party leader installed during the Tiananmen protests of 1989, served two terms as president from 1993 to 2003, but he lingered in power until 2004 by retaining control of the committee that runs China’s military. Mr. Jiang’s successor, Mr. Hu, stepped down from all his positions, however, and did not try to hang onto power — an example that some experts at first expected Mr. Xi to follow.The abolition of the term limit may also explain another recent move by Mr. Xi to send one of his closest advisers, Liu He, to Washington on Tuesday. While that trip had initially looked like an attempt to discuss the Trump administration’s tougher rhetoric on trade, it now seems likely to also be a mission to explain Mr. Xi’s plans to American leaders.Mr. Liu is a longtime economist and associate of Mr. Xi who jumped several rungs in the Communist Party hierarchy when Mr. Xi promoted him into the Politburo during a Communist Party national congress in October.At the same party congress, Mr. Xi conspicuously broke with precedent by choosing not to name a pair of much younger officials to the Politburo’s ruling inner circle, the seven-member standing committee, to serve as his heirs-in-waiting. Instead, Mr. Xi chose men — no women — who were closer to his own age or older.Mr. Xi’s strongman style has been compared to that of the Russian president, Vladimir V. Putin. But even Mr. Putin, who has amassed considerable personal power, did not try to erase his country’s constitutional limit on serving more than two consecutive terms as president as he approached that limit in 2008.Instead, he arranged for a close adviser with limited personal influence, Dmitri A. Medvedev, to serve as president for a single term while Mr. Putin held the post of prime minister. Mr. Putin then returned to the presidency in 2012, and is running this year for re-election to another term.Mr. Xi may now have even greater power, and the question will be how he chooses to use it.“Xi Jinping is susceptible to making big mistakes because there are now almost no checks or balances,” said Willy Lam, an adjunct professor at the Chinese University of Hong Kong who is the author of a biography of Mr. Xi in 2015. “Essentially, he has become emperor for life.”",China Moves to Let Xi Stay in Power by Abolishing Term Limit
9771,3774211,2018-02-25 16:17:30,"Samsung MWC 2018 Liveblog0Mobile World Congress isn’t always a huge show for Samsung (last year we got a couple of tablets and some vague 5G news), but this year the company’s not messing around. Between the many teasers and seemingly endless deluge of leaks, the Galaxy S9 is all but announced at this point.Even so, once of the nice things about the company is that it always seems to have a little something extra up its sleeve, and we’re sure to see a surprise or two at today’s press conference. Besides, it’s still Sunday morning in the U.S., so it’s not like you’ve got anywhere more exciting to be than in front of your computer, watching it all unfold live from Barcelona.Jonathan Wong is back on stage to create his own AR Emoji. It looks like you’ll have to select your hair style, glasses and clothes as the S9 mostly scans your facial traits. I have to say it looks quite convincing.Denison is now asking everyone to take their badge and scan the badge with the Galaxy Unpacked app. It’s a fairly unimpressive augmented reality representation of the Galaxy S9. You can tilt it to see it from all angles.A few key aesthetic changes here: the bezels on the top and bottom of the Infinity Display are a bit smaller than last time around. Also, the fingerprint sensor has been moved to a better spot beneath the camera.",Samsung MWC 2018 Liveblog
9772,3774291,2018-02-25 15:00:00,"About TNWTNW SitesBuild a great website with no coding skills and host it for life for under $50If you start a hot new project or get struck with divine inspiration, you may need to get a website up and running fast. Unfortunately, if website building isn’t necessarily your thing, that task can feel like a monstrous headache.First, Dragify Website Builder will help even the most uninitiated person piece together their own website in minutes. As their name would suggest, Dragify is all about the simplicity, including drag-and-drop site construction. No coding knowledge is necessary — just upload your images, choose from 44 pre-designed HTML blocks, then just drop those components on to your page where you want ‘em.Of course, building a site doesn’t do you much good if no one can see it…which is where Dragify’s lifetime of web hosting comes in handy. Not only do you get all your hosting issues settled forever (no annual or even monthly bills to pay), Dragify Web Hosting also supplies a host of other services, including CageFS Hacker Protection to keep it secure, CloudLinux to keep it running fast, and Enterprise RAID storage to house your pages and make sure you’ve always got the horsepower you need to keep your site up and operational.Those pieces together usually aren’t cheap — in fact, Website Builder and Web Hosting would normally run you almost $600. But with this limited time deal, you’ll get the whole one-stop web-building experience for just $49.99.",Build a great website with no coding skills and host it for life for under $50
9773,3776596,2018-02-25 17:00:00,"Samsung Galaxy S9 and S9+ hands-on: The devil is in the detailsThe new flagship is a big bundle of small changes.Getting to know the Galaxy S9 is like unwrapping a present you think you've already opened. On the surface, it looks familiar, but once you peel back the layers, you'll notice the contents are new.That's not to say that the changes between the Galaxy S9 and the Galaxy S8 are insignificant. There's a ton of them, and you'll have to look carefully to spot them all, but when considered as a package, this update feels more incremental than monumental. At least, based on our brief time with it so far.Gallery: Samsung Galaxy S9/S9 Plus hands-on | 23 PhotosCameraLike I said, there are many small changes in the Galaxy S9, but the bulk of them have to do with the camera. This should come as no surprise -- Samsung has been teasing some of the new features for a while now. Right off the bat, this is the first Galaxy S flagship with dual cameras (and dual optical image stabilization). The setup here is similar to the Note 8: two 12-megapixel cameras with one wide-angle and one telephoto lens, and it offers similar extra tools like Dual Capture and Live Focus (a la iPhone's Portrait Mode).Chris Velazco/EngadgetThe paired sensors are only available on the larger S9+, though, so fans of daintier handsets might have to make do with slightly reduced image quality (more on the specific differences on that front later).A dual lens setup is only one of a slew of new camera features for the S9 and S9+. They both come with something the company is calling ""Dual Aperture"" which sounds like it allows for two openings on a single lens. But really, it's one aperture that jumps between two f-stops -- f/2.4 and f/1.5. That last one is the widest aperture on a smartphone yet -- the record was last held by the LG V30, with its glass lens reaching f/1.6.While the mechanical shutter is on both phones, the S9+ only has it on the primary (wide-angle) lens. The shutter switches between the f-stops automatically by default, jumping to f/1.5 when the phone detects low-light conditions and f/2.4 in bright scenes. You can also manually control it in the camera's Pro mode, but you can't select any f-stops between the two -- your only options are f/1.5 and f/2.4.Still, the wide max aperture helps the S9 and S9+ let in more light, which makes for brighter and clearer images in the dark. At our demo, Samsung showed off how well this worked on a model scene encased in a black-out cylinder. I peered through the dime-sized hole at the top and barely made out a circle in the middle of an otherwise pitch-black setting. After placing the S9's lens on the cylinder, I clearly saw that it was actually a traffic circle, surrounded by houses and trees.Gallery: Samsung Galaxy S9 and S9+ camera samples | 12 PhotosThe picture that the S9 snapped in these conditions was surprisingly noise-free, thanks to what the company named ""Super Speed Dual Pixel."" It combines the dual pixel technology that was introduced with the S7s and adds dedicated processing and memory to enable multi-frame noise reduction using 12 separate images per photo you shoot. Thanks to the RAM built into the sensor, the S9 is able to stitch those together quickly, resulting in hardly any delay between pressing the shutter button and the picture showing up.The S9 and S9+ can also shoot some serious slow-mo. Samsung took a page right out of Sony's playbook, adding 960fps video capture. That'll create clips that are four times slower than the S8 could take (at 240fps). Or, in real-world terms, 0.2 seconds of actual time will be slowed down to about 6 seconds of playback.As we saw with the Sony Xperia XZ Premium, taking video at that speed can be tricky. To avoid amassing gigantic files at such a high framerate, smartphones typically limit ultra slow-mo recording to brief windows. On the Xperia XZ Premium, this meant you had to go to a specific mode and hit a separate button to start the slow-motion capture. That's flawed, as it expected that you'd be able to predict when something interesting would happen.Samsung's approach is to add an automatic mode. When you start recording slow-mo video on the S9, a square frame appears in the middle of the screen. The moment something moves into that frame, the phone automatically switches to 960 fps for 0.2 seconds. You're allowed up to six slow-mo bursts in each recording. During our preview, this feature behaved erratically. It worked well when the subject was perfectly still before something moved into the frame, but most other times, it didn't use slow-mo when you'd expect. To be fair, we were checking out an early version of the software, and this could become more reliable by the time it ships.Mat Smith/EngadgetThose who want (slightly) more control over when to use 960fps can use Pro mode, which is more like Sony's method. The nice thing here is that you have the option to do it yourself or let the phone think for you, even if it is finicky right now.I really liked the ultra-slow-mo videos we got from our tests, although most footage at that exaggerated speed appeals to me. Engadget's senior mobile editor Chris Velazco played with a fidget spinner and a deck of cards while making a truly appalling face, and the S9 recorded it in excruciatingly smooth glory. Since we were using a set with studio lights, the clips turned out bright and clear, but we don't know how the camera will fare in less-than-optimal conditions yet.All the new camera tools I've described so far can be found on both versions of the S9, and like I said before the dual camera setup is only on the S9+. From what we can tell so far, the main difference resulting from that is the S9+ captures better Live Focus portraits than the smaller phone since the latter is relying on software to create the blurred background.Samsung didn't say if the new flagships can record 4K HDR video, even though they both pack Qualcomm's latest Snapdragon 845 chipset, which supports that feature. The company's reps haven't answered our queries on that yet, so we'll have to wait for confirmation.AR EmojiChris Velazco/EngadgetAnother camera-related feature is AR Emoji -- Samsung's answer to Apple's Animoji. So far, I find the S9's version more fun, mostly because it creates an emoji of my face and not an impersonal, generic one (although I do love me some talking poop). You can use the S9 to create a cute cartoon version of yourself by taking a photo of you staring straight forward. The phone was better at reproducing my likeness than Bitmoji or any other app I've used.But the replication didn't work as well for everyone. The avatars for some other journalists or Samsung's own reps, looked nothing like them. For these people, AR Emoji might be less fun, but I was definitely sucked into styling up my avatar with great hair, a cool outfit and cute glasses. After you're done customizing your emoji, the S9 auto-generates a set of animated stickers that you can insert from within the Messaging app's keyboard (other apps can find these from the Gallery).There are a plethora of options right from the get-go, and their resemblance to actual gestures I would do in real life is uncanny. You can also make facial expressions, and the S9's front cameras will pick them up to be conveyed by your avatar -- just like you can do with Animoji.BixbyAnother area where Samsung made major changes is Bixby -- specifically in the augmented reality Vision section. Besides interface tweaks that make Bixby Vision look more in tune with the camera app, Samsung also added three new modes -- Live Translate, Makeup and Food. The previously available shopping and landmark recognition features are now individual modes, too.The Live Translate tool uses Google's Translate service, and that has worked well for me in the past. So I'm not surprised that Bixby was able to quickly and accurately translate text in images in real time. What impressed me was Bixby's ability to read handwritten words -- especially when it correctly interpreted my ugly Chinese characters.Although I had way too much fun playing with the new Makeup feature, I wasn't impressed with its performance. Like a slew of other selfie-makeup apps, it lets you apply digital makeup in real time and buy products you like without leaving the app. Bixby has a limited offering of color and pattern options, though, which limits its appeal. Still, this is something that's easily addressed by software updates over time.Another new Bixby feature is its ability to estimate the number of calories in food you point the S9 at. Samsung teased this at CES, but we only saw a preview version on a Note 8 then. Now, the software is more mature and was able to accurately identify an onion bagel, showing that it contained 270 calories. Based on a quick Google search, that count is pretty accurate.During our hands-on, Bixby struggled to identify dishes that contained more (and mixed) ingredients. It mistook a cup of granola parfait, which included berries and yogurt, as just a cup of granola alone. Because of that, it also provided an incorrect calorie count. Again, this is an issue that can be fixed over time as Samsung's software team figures out how to differentiate components of a meal, and when it works, it provides a nifty service.As before, Bixby Vision recognizes objects like your fresh new sneakers or favorite watch and displays a card in the viewfinder to show you its make, model and price. This feature is still hit-or-miss, though. It correctly identified Velazco's Adidas kicks, but not my DKNY Minute watch, mistaking it for a Rolex (I wish). It works well with things like wine bottles or artwork, though. Vision can also display information about places of interest when you point it at landmarks.One final update on the camera -- the front 8-megapixel camera and the iris scanner now work together for more reliable unlocking via what Samsung calls ""Intelligent Scan."" According to the company's reps, the iris scanner isn't as effective in bright light as it is in the dark. With the new system, the S9 will try to sign you in with your eyes by default, but when that fails, it will use facial recognition. Unfortunately, we didn't get to test out this feature during our demo, so we'll have to wait till we get our hands on a review unit to try it.Chris Velazco/EngadgetAnd everything elseSamsung built stereo speakers into the S9's front and bottom, which make it 1.4 times as loud as the S8, according to the company. The latest flagship certainly sounded noticeably louder than its predecessor during our demo, although we had too short a time with it to evaluate audio quality.Samsung also tidied up the S9's front by trimming the bezel and masking the array of cameras and sensors above the screen. You won't really notice these differences until you look closely, but the effect is a slightly more minimalist design than before.There are also some updates that you probably already expected. In the US, both the S9 and S9+ will have Snapdragon 845 CPUs, with the smaller phone packing 4GB of RAM while the larger handset carries 6GB. They'll ship with Android 8.0 Oreo, too.Otherwise, the rest of the S9 remains largely the same as the S8. Their respective screens are the same size (5.8-inch and 6.2-inch), their batteries have the same capacity (3,000 mAh for the S9, 3,500 mAh for the S9+), they're just as durable and water-resistant as their predecessors, and will still support fast wireless charging.Many of the new software features like Bixby and camera updates will ultimately become available for older handsets like the S8 and Note 8, as long as the hardware supports it. That means that if you're an S8 owner and don't feel the need for a camera upgrade, you could easily skip the S9 and not miss much.It'll take more time with the phone before I can properly evaluate whether the S9's bundle of changes amount to more than the sum of their parts, but for now, Samsung's latest feels like an interim update.AT&T has already announced it will sell the S9 on its Next installment payment program from $26.34 over 30 months. The S9+ will cost $30.50 each month. That works out to about $790 for the S9 and $915 for the S9+ -- expect the other carriers to come in at about the same price, give or take $50. In the UK, the Galaxy S9 will cost £739 while the S9 Plus comes in at £869.You'll have a bit more time before you have to decide if you want to get the new flagships, though -- pre-orders start March 2nd, while the devices arrive March 16th.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.",Samsung Galaxy S9 and S9+ hands-on: The devil is in the details
9774,3776598,2018-02-25 16:00:00,"Nokia's new affordable smartphones prioritize designGenerating hype around the Nokia brand with retro-inspired feature phones is all well and good, but that doesn't necessarily result in people opening their wallets. And to best cash in on any interest, you need a little something for everyone. To that end, HMD Global has announced a number of new Android smartphones at MWC beyond its pricey flagship. There's the Nokia 1, the company's cheapest entry-level device to date; the second-gen Nokia 6, which is going global after debuting in China last month and the Nokia 7 Plus, a bigger version of the China-exclusive 7, which was released last fall.Nokia 1Nokia 6Nokia 7 PlusProcessorQuad-core 1.1GHz MediaTek MT6737MOcta-core 2.2GHz Snapdragon 630Octa-core 2.2GHz Snapdragon 660RAM / storage1GB / 8GB3GB / 32GB (or 4GB / 64GB)4GB / 64GBMicroSD card supportUp to 128GBUp to 128GBUp to 256GBDisplay size4.5-inch5.5-inch6-inchDisplay resolution854 x 480 (16:9)1,920 x 1,080 (16:9)2,160 x 1,080 (18:9)Rear camera5MP16MP12MP / 13MP (2x optical zoom)Front-facing camera2MP8MP16MPOSAndroid Oreo (Go edition)Android OreoAndroid OreoBattery2,150mAh (removable)3,000mAh3,800mAhChargingMicro-USBUSB Type-CUSB Type-CDimensions133.6 x 67.8 x 9.5mm148.8 x 75.8 x 8.2mm158.4 x 75.6 x 8.0mmFingerprint sensorNoYesYesNFCNoYesYesHeadphone jackYesYesYesI'm not going to dedicate much time to poring over the specs. As you can see from the table, it's all more-or-less par for the course. By the numbers, the Nokia 1 packs the usual specifications you'd expect for an entry-level device at an affordable price. Similarly, the Nokia 6 (a known quantity anyway) hits that mid-range sweet spot, and the Nokia 7 Plus is appropriately beefed up to reflect the added pressure it puts on the purse strings. However, beyond the specs, each of the handsets also has its own distinct design and personality.The Nokia 1 is by far the cutest of the three, conjuring up memories of some of the old Nokia Lumia Windows Phones (particularly the Lumia 620). It still feels compact despite a wealth of bezel framing its 4.5-inch display, and because the curvature at the top and bottom of the handset flow differently, it's a more interesting shape than a simple square with rounded corners. The Nokia 1 is also unashamedly plastic, with HMD spinning that as a way to further personalize your phone with different Xpress-on covers (a term we haven't heard in quite a while).Gallery: Nokia 1 | 12 PhotosThe affordable device will be one of the first running Android Oreo (Go Edition) when it ships in April. This offshoot of Google's mobile OS includes leaner Go versions of apps like Gmail, Assistant and Maps, which are designed to perform better on entry-level hardware. On a related note, all other Nokia smartphones from MWC onwards will be part of the Android One program, meaning they run stock clean software and are guaranteed to get OS updates for two years and security updates for three.HMD only launched the Nokia 2 last fall -- an entry-level device itself -- but the Nokia 1 is even cheaper at $85 (circa €69/£61). Like Motorola's bargain bucket Moto C, it won't be for everyone but is aimed squarely at those wanting a cheap smartphone that's more than capable of doing all the basics.You may have already read about the second-gen Nokia 6 when it was announced for the Chinese market in early January, but at the beginning of April, it makes its global debut. The €279 (around $343 or £246) mid-ranger hasn't picked up any upgrades for 2018, but boasts a more refined design. Much like Nokia phones from many moons ago, this thing is an absolute brick (in a good way), machined out of a single block of aluminum. Of all HMD's smartphones, it's by far the most geometric and industrial.Gallery: Nokia 6 | 10 PhotosThough it's mostly flat and rectangular, it's softened by thin chamfered edges and lovely, luxurious two-tone color schemes. But if boxy isn't your style, then maybe the mellow curves of the Nokia 7 Plus will be more appealing. This slightly higher-end handset has the same robust build quality as the Nokia 6 but feels altogether different due to its ceramic-esque back -- an effect created by bonding a number of layers of paint to the aluminum frame. You will have two colors to choose from, but I feel the bronze-tinted metal dotted around the device blends better on the black model than the white.Gallery: Nokia 7 Plus | 10 PhotosIt might be called the Nokia 7 Plus, but it's quite different to the China-exclusive Nokia 7 released in October last year. As you've probably guessed, the Plus denotes a bigger screen: a 6-inch LCD display with full HD+ resolution (2,160 x 1,080), an 18:9 aspect ratio and super-slim bezels, to be exact. The camera situation has changed dramatically, too. Instead of a 16-megapixel shooter, the 7 Plus has a 12MP camera with Zeiss optics as well as a 13MP number with 2x optical zoom. The front-facing camera has also been bumped up from five megapixels to 16. The Nokia 7 Plus launches in April like the other two devices and will cost €399 (around $490, or £351), putting it somewhere between a mid-range device and a flagship.Like many phone manufacturers, HMD is trying to make sure there's a Nokia phone for everyone and every budget. It's not solely concerned with making the spec sheet as attractive as possible, though, ensuring each handset has its own aesthetic appeal.",Nokia's new affordable smartphones prioritize design
9775,3776692,2018-02-25 00:00:00,"Web application from scratch, Part IThis is the first in a series of posts in which I’m going to go through the process of building a web application (and its web server) from scratch in Python. For the purposes of this series, I’m going to solely rely on the Python standard library and I’m going to ignore the WSGI standard.Without any further ado, let’s get to it!The web serverTo being with, we’re going to write the HTTP server that will power our web app. But first, we need to spend a little time looking into how the HTTP protocol works.How HTTP worksSimply put, HTTP clients connect to HTTP servers over the network and send them a string of data representing the request. The server then interprets that request and sends the client back a response. The entire protocol and the formats of those requests and responses are described in RFC2616, but I’m going to informally describe them below so you don’t have to read the whole thing.Request formatRequests are represented by a series of \r\n-separated lines, the first of which is called the “request line”. The request line is made up of an HTTP method, followed by a space, followed by the path of the file being requested, followed by another space, followed by the HTTP protocol version the client speaks and, finally, followed by a carriage return (\r) and a line feed (\n) character:GET /some-path HTTP/1.1\r\nAfter the request line come zero or more header lines. Each header line is made up of the header name, followed by a colon, followed by an optional value, followed by \r\n:Host: example.com\r\n Accept: text/html\r\nThe end of the headers section is signaled by an empty line:\r\nFinally, the request may contain a “body” – an arbitrary payload that is sent to the server with the request.Response formatResponses, like requests, are made up of a series of \r\n-separated lines. The first line in the response is called the “status line” and it is made up of the HTTP protocol version, followed by a space, followed by the response status code, followed by another space, then the status code reason, followed by \r\n:HTTP/1.1 200 OK\r\nAfter the status line come the response headers, then an empty line and then an optional response body:A simple serverBased on what we know so far about the protocol, let’s write a server that sends the same response regardless of the incoming request.To start out, we need to create a socket, bind it to an address and then start listening for connections.import socket HOST = ""127.0.0.1"" PORT = 9000 # By default, socket.socket creates TCP sockets. with socket.socket() as server_sock: # This tells the kernel to reuse sockets that are in `TIME_WAIT` state. server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # This tells the socket what address to bind to. server_sock.bind((HOST, PORT)) # 0 is the number of pending connections the socket may have before # new connections are refused. Since this server is going to process # one connection at a time, we want to refuse any additional connections. server_sock.listen(0) print(f""Listening on {HOST}:{PORT}..."")If you try to run this code now, it’ll print to standard out that it’s listening on 127.0.0.1:9000 and then exit. In order to actually process incoming connections we need to call the accept method on our socket. Doing so will block the process until a client connects to our server.If you run the code now and then visit http://127.0.0.1:9000 in your favourite browser, it should render the string “Hello!”. Unfortunately, the server will exit after it sends the response so refreshing the page will fail. Let’s fix that:At this point we have a web server that can serve a simple HTML web page on every request, all in about 25 lines of code. That’s not too bad!A file serverLet’s extend the HTTP server so that it can serve files off of disk.Request abstractionBefore we can do that, we have to be able to read and parse incoming request data from the client. Since we know that request data is represented by a series of lines, each separated by \r\n characters, let’s write a generator function that reads data from a socket and yields each individual line:This may look a bit daunting, but essentially what it does is it reads as much data as it can from the socket (in bufsize chunks), joins that data together in a buffer (buff) and continually splits the buffer into individual lines, yielding one at a time. Once it finds an empty line, it returns the extra data that it read.Using iter_lines, we can begin printing the requests we get from our clients:It uses the iter_lines function we defined earlier to read the request line. That’s where it gets the method and the path, then it reads each individual header line and parses those. Finally, it builds the Request object and returns it. If we plug that into our server loop, it should look something like this:Because from_socket can raise an exception under certain circumstances, the server might crash if given an invalid request right now. To simulate this, you can use telnet to connect to the server and send it some bogus data:serve_file takes the client socket and a path to a file. It then tries to resolve that path to a real file inside of the SERVER_ROOT, returning a “not found” response if the file resolves outside of the server root. Then it tries to open the file and figure out its mime type and size (using os.fstat), then it constructs the response headers and uses the sendfile system call to write the file to the socket. If it can’t find the file on disk, then it sends a “not found” response.If we add serve_file into the mix, our server loop should now look like this:If you add a file called www/index.html next to your server.py file and visit http://localhost:9000 you should see the contents of that file. Cool, eh?Winding downThat’s it for part 1. In part 2 we’re going to cover extracting Server and Response abstractions as well as making the server handle multiple concurrent connections. If you’d like to check out the full source code and follow along, you can find it here.See ya next time!NewsletterIf you’d like to find out about new parts as they come out, you can subscribe to the newsletter for this series by filling the form below.","Web application from scratch, Part I"
9776,3776698,2018-02-26 00:00:00,"Introducing theGalaxy S9 | S9+The revolutionary camera that adapts like the human eye.Video spotlighting the Galaxy S9+ camera* Image of Galaxy S9+.Dual ApertureCapture stunning pictures in bright daylight and super low light.Our category-defining Dual Aperture lens adapts like the human eye. It's able to automatically switch between various lighting conditions with ease—making your photos look great whether it's bright or dark, day or night.But we didn’t stop there.Super Slow-moThe camera that slows down time, making everyday moments epic.* Super Slow-mo only supports HD resolution. Limited to 20 shots per video with approximately 0.2 seconds of recording and 6 seconds of playback for each shot.Add music. Make GIFs. Get likes.Super Slow-mo lets you see the things you could have missed in the blink of an eye. Add your favourite music to the video or turn the video into a looping GIF, and share it with a tap. Then sit back and watch the reactions roll in.What we've changed in here will change how you experience everything out there.Now it’s time to take it for a spin.* All specifications and descriptions provided herein may be different from the actual specifications and descriptions for the product. Samsung reserves the right to make changes to this document and the product described herein, at anytime, without obligation on Samsung to provide notification of such change. All functionality, features, specifications, GUI and other product information provided in this document including, but not limited to the benefits, design, pricing, components, performance, availability, and capabilities of the product are subject to change without notice or obligation. The contents within the screen are simulated images and are for demonstration purposes only. Images shown here are for representational purpose only, actual may vary. All features, specifications and prices are subject to change without prior notice. Model availability may vary from location to location. * Screen measured diagonally as a full rectangle without accounting for the rounded corners.","Samsung Galaxy S9 and S9+ - Price, Specs and Features"
9777,3776856,2018-02-25 17:00:35,"Samsung’s Galaxy S9 wants to turn the camera into a new home screen0Samsung’s Galaxy S9 is an update that mostly sticks to the look and feel of the previous generation device, which is probably a big reason why the company is focusing specifically on the flagship smartphone’s camera for this reveal.The Galaxy S8 and S8+ already had one of the better smartphone cameras in the industry, and the Galaxy S9 and S9+ both seem to be top contenders to secure Samsung a place among the best options out there in 2018, too. But the most interesting thing about the camera might just be how central it is to the S9 and its launch.A home away from home(screen)Samsung is fully aware that people spend a lot of time in the camera app, and it wants to provide users with even fewer reasons to ever leave. A Samsung rep actually described it as a kind of destination unto itself during an advance briefing about the S9, and you can tell based on the features packed into the app, as well as the attention paid to redesign its interface to make it more user-friendly.On the UX side, you no longer have to dig through a submenu to find all the various shooting modes available – they’re listed at the top in a tappable header, and you can also just swipe left and right to quickly cycle through the various options. It’s a smart and easy fix to what was one of the most lamentable aspects of the S8’s camera experience.Everything can also be easily shared to various social apps and with friends via messaging, etc. from within the camera app, meaning it’s kind of a hub for your personal broadcasting experience. And it’s a home for shopping and information gathering, too, thanks to Bixby intelligence features that use image recognition to serve up direct shopping links, or to translate signage on the fly.Samsung’s smartphone is still equipped with a traditional home screen, of course (it’s not like it boots right into the camera) but you can almost see that they may have toyed with that very idea, before landing on this hub model. It still feels like Samsung’s just a breath away from borrowing Snap’s famous phrase to describe itself: ‘We’re a camera company’ – of course, Samsung actually used to make dedicated cameras, so it’s not too much of a stretch.Refined opticsSamsung’s behaving like a camera company when it comes to the actual photographic optics of the S9, too. The basic camera in both the S9 and S9+ gets a variable mechanical aperture, for instance, going from F1.5 in low light to F2.4 in brighter conditions. This is huge not only for helping to reduce noise in dimmer spots, but also when it comes to the camera’s ‘Pro’ shooting mode, since you can manually set the aperture to whichever of the two you desire. That’s great for when you’re specifically trying to add depth to your image via real bokeh, for instance, or when you want a flatter depth of field for portraits.20180221_171601Manual settings of scene with F1.5.20180221_171616Automatic settings for the same scene (f2.4).20180221_171724Portrait taking in Pro mode with the F1.5 aperture selected to increase bokeh.It’s also going to lead to better results in almost all situations for automatic shooters, and I found that was definitely the case during my limited hands-on time. Plus, watching an aperture actually close down and expand on a smartphone camera when you adjust the settings is just pretty darn cool.Finally, Samsung has also decided to go the way of Apple and other rivals and put more camera into the larger S9+. That means you get a dedicated physical 2x optical zoom thanks to the second lens, which has a longer focal length. That additional lens doesn’t have the variable aperture feature, however, so it’s not going to be able to let in as much light in darker shooting situations. Still, it’s great to have and a big bonus for anyone hoping to leave their compact zoom camera at home while traveling.Latest Crunch ReportIn terms of straight specs, the Galaxy S9 and S9+ both have 12 megapixel rear cameras, and an 8 megapixel front-facing shooter, and the S9+ also has that 2x tele lens which is also powered by a 12 megapixel sensor. It’s funny, I barely even thought to ask about this – megapixels basically don’t mean anything anymore given the state of modern devices.Better lowlightSamsung has also tweaked its dual sensor capture to help make the most of low light capture. The new system catches sixteen different exposures of a single scene to compare for noise and cancel out more of those tell-tale specks and digital fuzz you get when using a small sensor in poor lighting. The result is a smoother, brighter image with less of what we’d call ‘destructive’ digital enhancement – or obscuring/destroying actual detail in the scene because the noise reduction algorithm is mistaking it for more sensor static.Samsung demonstrated a lot of their own samples that showed this working to great effect, but it was difficult to test in the conditions of the pre-brief area. This is something that will require extensive night-time and low light environment testing to test in full, but early results (visible below) show plenty of detail retained and bright scenes.20180221_17133020180221_17131520180221_171402Suffice to say, Samsung knows that lowlight shooting, without using flash, is something people really want to do with their phones, and they’ve spent a lot of effort on making sure that generates the best results possible. Based on what we’ve seen, it looks like they’re definitely moving the needle in a positive direction for this clutch mobile photography metric.Bitmoji meets Animoji meets true horrorSo Samsung is doing some very smart, very good things with its camera. It’s also doing… some other stuff. Like its animated emoji take, which is clearly a response to Apple’s Animoji. The issue is that even in controlled demos where you’d imagine Samsung would be doing everything possible to make these look good, they look… not good.Samsung-Galaxy-S9-2It me.Samsung-Galaxy-S9-3You can see my horrific visage in the examples provided, but even when Samsung reps were demoing this the effect was very much ‘uncanny valley’ rather than fun, friendly feature. Whatever else you may think about Bitmoji or Animoji, they’re not terrifying to look at, whereas the Samsung variety never escaped being unnerving first and foremost, and perhaps slightly whimsical afterwards. You mileage may very, and they do look like me, but they strike me as unnerving in a way similar offerings from other companies do not.MyEmoji_180221_172720_1MyEmoji_180221_172720_2MyEmoji_180221_172720_3MyEmoji_180221_172720_4MyEmoji_180221_172720_5MyEmoji_180221_172720_6MyEmoji_180221_172720_7MyEmoji_180221_172720_8MyEmoji_180221_172720_9MyEmoji_180221_172720_10MyEmoji_180221_172720_11MyEmoji_180221_172720_12MyEmoji_180221_172720_13MyEmoji_180221_172720_14MyEmoji_180221_172720_15MyEmoji_180221_172720_16MyEmoji_180221_172720_17MyEmoji_180221_172720_18You generate your own animated avatar by taking a still photo (during which you’re advised not to smile) and Samsung allows you to use these to express emotions with a preset animated emoji set, and also to map them to your real face for catching photos and stuff like karaoke clips. But if anything, these are even more disturbing in motion.It’s understandable to try to capture some of the fun of the current self-styled emoji craze, and Samsung is good at the camera part of this, but the end result feels half-baked and best avoided. Luckily, it’s easy enough to do just that.Bixby gets smarter – but not yet smartSamsung’s Bixby assistant is back again on the S9 and S9+, and it’s found a home within the camera app, too. Bixby has the ability to identify products and offer up shopping links, for instance, like working with a new makeup virtual try-on mode to provide Sephora shopping. This is impressive and the makeup mapping works well, based on the limited demo. and Samsung also showed off its ability to do things like identify bikes and provide the correct shopping link.This is pretty cool, though I think the appeal of having commerce directly in you camera app is limited (and potentially overly obtrusive). Samsung’s also using Bixby to do live translation of signage, however, and the company showed this off using real signage they brought into their demo space to prove it.The weird thing is, this just revealed how far they have yet to go. The signs (again, Samsung selected) featured phrases including the Spanish equivalent of “Vehicle speed monitored electronically” but managed to butcher it into a literal, line-by-line translation with jumbled meaning, saying something along the lines of “Speed electronic maintained.” Likewise for sample restaurant menus and other examples they showed.It became clear that Bixby’s translation prowess was limited to just literal, direct word translation – there seemed to be no attempt at conveying meaning, as you might find with Google Translate for instance. It was a weird choice to highlight Bixby’s supposed improvements, and left me with no really high hopes that Samsung’s virtual assistant has become any more of a value-add feature: It’s probably best avoided with the S9 and S9+ just like it was with the S8 generation.The Slowest of MosBack to things Samsung’s Galaxy S9 does well – there’s a new slow-motion mode that can capture up to 960 frames per second for 0.2 seconds. That sounds like a short amount of time, but it translates into 6 seconds of footage played back at normal speed, and it’s fantastically fun.The super slow mo mode (which, to be fair, Sony had on its XZ Premium last year) is made especially great because it can work automatically with a defined motion detection box. You can set the size and position of the box yourself, so that you’re framed up for exactly where the action is going to be, and then it’ll grab whatever happens in that area and slow it down to a mind-blowing degree.It’s way easier to show off what this feature can do than to explain it, so check out the included examples:This mode is also available manually if you think you’re better than software at finding the moment, or if you’re just slowing down some continual action, like rapids or rainfall. Captured clips can be easily shared as GIFs anywhere you’d want to, and you can even immediately save it as a motion wallpaper, which is really quite great.Bottom lineSamsung’s Galaxy S9 and S9+ are billed as “The camera reimagined” and though that includes your standard amount of marketing hyperbole, they really have pushed the camera forward. That’s great because the S8 camera remains one of the better options available on the market currently, so this improvement is building on top of an already stellar reputation, with tweaks that should make a big difference in terms of the types of photos and clips you can capture.Check out our general photo samples below, taken with the Galaxy S9+ (unedited out of the camera), along with a video showing how stabilization works when capturing movies and walking at a normal pace indoors.20180221_17201920180221_17200520180221_17180120180221_17172420180221_17161620180221_17160120180221_17153720180221_17150820180221_17143520180221_17140220180221_17133020180221_17131520180221_17123920180221_17110120180221_17104220180221_17043320180221_170426Samsung’s Galaxy S9 and S9+ will be available for pre-order in the U.S. and Canada from February 25, and available in stores on March 16. For more info, check out our post detailing all the new goodness for Samsung’s 2018 flagship here.",Samsung’s Galaxy S9 wants to turn the camera into a new home screen
9778,3776858,2018-02-25 13:30:23,"Huawei launches its new MediaPad M5 tablets0Tablets may not be the most exciting product category these days, but they also aren’t quite as dead as some pundits would like you to believe — and for Huawei, they are actually a growing business. Indeed, the company argues that it’s now the third largest tablet maker in the world. It’s no surprise then that the company today unveiled two (or three, depending on how you want to count) new tablets under its MediaPad brand at its annual Mobile World Congress press conference in Barcelona.Unlike Huawei’s new MateBook laptops, which feature a rather odd camera that’s built into the keyboard, we’re talking about some pretty standard tablets here.The new MediaPad M5 tablets will come in two sizes: 8.4 and 10.8 inches, starting at 349 and 399 Euro respectively. For the most part, the smaller and larger tablets have exactly the same specs, with 2560×1600 displays (for a 359 and 280 PPI rating, respectively), Kirin 960 quad-core processors, 4GB of RAM and either 32, 64 or 128 GB of storage space. They also come with pretty standard 8MP front cameras and 13MP rear cameras, all the usual sensors (including a fingerprint sensor) and Android 8.0. All of the SKUs also offer an SD card expansion slot that can bring the total on-board storage up to 256 GB.DSCF4347DSCF4372DSCF4371DSCF4360DSCF4369DSCF4359DSCF4358There is a third version, too, the 10.8-inch MediaPad M5 Pro, which features the exact same specs as the regular 10.8-inch M5, but with added support for Huawei’s M-Pen. As the company stressed during a briefing ahead of the event, the pen features 4096 levels of pressure sensitivity and support for tilting and shading. The Pro will also support a detachable keyboard and offer a desktop view with expanded file management features.All versions of the tablet are available in, as Huawei notes, “two iconic colors: Space Gray and Mystic Silver.” LTE support is an option on all versions, too.The M5s will start shipping in March and will be available in most European countries, as well as the U.S.We had a bit of time with all of the tablets ahead of today’s announcement. It’s hard to get excited about tablets these days, but the M5’s look like perfectly solid choices if you’re in the market for an Android tablet, with both the build quality and screens being the stand-out attributes here.",Huawei launches its new MediaPad M5 tablets
9779,3776860,2018-02-25 17:00:53,"Hands on: Samsung Galaxy S9 reviewIt's all about the cameraOur Early VerdictThe Samsung Galaxy S9's party piece is a seriously impressive camera, but there's much more than a passing resemblance to the Galaxy S8 which is now considerably cheaper. It may make the S9 slightly harder to justify, but it's still set to be an excellent smartphone.ForImpressive cameraBetter-placed fingerprint sensorAgainstNot all that different to S8AR Emoji a little limitedThe Samsung Galaxy S9 is all about the camera. Samsung's tagline for its ninth generation flagship phone is ‘The camera. Reimagined.’, and it's keen to let you know it has done a lot of work on the snapper.Why is it so keen to focus on this feature? It may be because, on the surface, there's very little to visually differentiate the new Galaxy S9 from the phone it's replacing – the Samsung Galaxy S8.A quick scan of the S9's spec sheet shows the same Infinity Display screen size (5.8 inches), resolution (QHD+) and aspect ratio (18.5:9) as its predecessor, and the similarities don't end there.The Samsung Galaxy S9 also packs the same size battery (3,000mAh), and the same amount of RAM and internal storage (4GB and 64GB), while also retaining the headphone jack and IP68 dust-and-water-resistant rating.What, you may ask, is actually new about the Samsung Galaxy S9 then? Well, there's Samsung's new Exynos 9810 chipset sitting at the heart of the phone (unless you're in the US, where you get Qualcomm's Snapdragon 845), Google's freshest software in the shape of Android 8 Oreo and the phone's real party piece… the 12MP rear camera with the firm's dual-aperture technology.Is this enough to get excited about though? After 2017's Galaxy S8 and iPhone X, the arrival of the Galaxy S9 feels like a far more muted affair, despite this camera still packing the core components of an excellent smartphone.Check out our Samsung Galaxy S9 hands on video belowWant to go bigger? Read our hands-on: Samsung Galaxy S9 Plus reviewSamsung Galaxy S9 release date and priceSamsung Galaxy S9 release date: March 16Samsung Galaxy S9 pre-orders: February 25 (EU), March 2 (US)Samsung Galaxy S9 price: £739, $719.99The Samsung Galaxy S9 release date is set for March 16 globally, but you can pre-order the handset right now if you're in Europe.Galaxy S9 pre-orders opened at 7pm CET (6pm GMT, 1pm ET, 10am PT), on Sunday 25 February, but currently there's no word on price or availability from retailers and carriers.If you're in the US then you'll have to wait a little longer to declare your S9 intentions, with pre-orders opening on March 2.Meanwhile there's less good news for those in the UK when it comes to the Samsung Galaxy S9 price, as at £739 ($719.99, around AU$1,300) it's more expensive than its predecessor.The Galaxy S8 launched at £689, so you're looking at a £50 price hike in the UK for a phone which is an incremental upgrade.EE has announced that the S9 is now available for pre-order. Its Essential plan starts at £150 down and £53 a month for 4GB of data. For users who upgrade to the Max plan get 60GB of data plus two years of access to the BT Sports app. Additionally, buyers can get £250 off by trading in their old Samsung Galaxy S7 or Samsung Galaxy S7 Edge.T-Mobile in the US has announced that users can receive half off the S9 by pre-ordering. Otherwise, it can be picked up for $0 down and $30 per month.DesignPremium glass and metal, very similar to the S8Better fingerprint scanner placement, stereo speakers tuned by AKGAs we said at the outset, the Samsung Galaxy S9 doesn't look all that different to the phone it's replacing.On closer inspection you may notice that the bezels above and below the display have been trimmed down a little, helping to reduce the overall height of the phone.It's meant Samsung has added a few fractions of a millimeter to the width and thickness of the Galaxy S9 to ensure it can still fit all the internal components in, with it measuring 147.7 x 68.7 x 8.5mm, but it's still more than manageable in the hand.It's heavier than the S8 as well at 168g (vs 155g), but it remains lighter than the iPhone X (174g).Samsung has made the screen darker when it's off, and hidden the Iris scanner hole in the bezel above the display to allow it to obscure the boundaries between screen and bezel, and to minimize visual elements for a cleaner, smarter and more minimalist appearance.On the right side of the phone the only interruption to the metal frame is the power/lock key, while on the opposite side you'll find the volume rocker above the Bixby button – yep, Samsung is sticking with its assistant shortcut key.Samsung Galaxy S9 hands on galleryImage 1 of 14Image 2 of 14Image 3 of 14Image 4 of 14Image 5 of 14Image 6 of 14Image 7 of 14Image 8 of 14Image 9 of 14Image 10 of 14Image 11 of 14Image 12 of 14Image 13 of 14Image 14 of 14The SIM/microSD card tray is on the top edge, while on the base audiophiles will be pleased to find that Samsung has continued to include a headphone jack on its flagship device.That port sits alongside a centralized USB-C port and a speaker – one of two, with the other built into the earpiece on the front of the phone.The two speakers provide stereo sound, and they've been tuned by Austrian audio wizards AKG Acoustics (now owned by Samsung) to further improve audio quality on the Galaxy S9. Dolby Atmos support is also included, giving you 360-degree sound for a more immersive experience.We listened to a few demo tracks during our hands-on time with the phone and it sounds great, with a relatively good spacial audio effect.We'll have to put the speakers to the test during our full review to see just how good they are; the Razer Phone currently offers a superb mobile audio experience, but the Galaxy S9 has a strong foundation.It certainly sounds louder – and it should do as Samsung says the speakers on the Galaxy S9 are 1.5 times noisier than those on the S8.Another positive design point is the location of the fingerprint scanner. It's no secret that we, and many of you, were less than happy with Samsung's placement of the digit reader on the Galaxy S8, S8 Plus and Note 8 last year, and the good news is the South Korean firm has learned its lesson.The fingerprint scanner on the Galaxy S9 is better placed than on its predecessorIt's shifted the location from beside the camera sensor to below it. This is a far more natural position for the scanner, and it easily falls beneath your forefinger for speedy unlocking action.What's more, the process of registering a fingerprint has been streamlined, with the Galaxy S9 requiring you to swipe your digit over the reader around three times, versus the 16 attempts needed on the S8.If fingerprint scanning isn’t your style, the Galaxy S9 also offers facial and iris recognition. Rather than two seperate biometric options, the face and eye scanners work in tandem to provide a faster unlock time, and as it uses an IR (infrared) camera it’ll even work in the dark (like Face ID on the iPhone X).The Samsung Galaxy S9 will be available in three colors: midnight black, coral blue and the new shade of lilac pink.We're told some countries will also have a titanium grey option, but this won't be available in the US, UK or Europe.Lilac pink is a new color for the Samsung Galaxy S9Display5.8-inch QHD+ Super AMOLED 18.5:9 Infinity DisplaySame as the Galaxy S8There are no surprises when it comes to the display on the Samsung Galaxy S9, as it's exactly the same as the screen on the S8.That means you get a 5.8-inch QHD+ offering with Samsung's eye-popping Super AMOLED panel and the taller 18.5:9 aspect ratio, which is increasingly becoming the norm for flagships after appearing on a handful of phones last year.It still features the Infinity Display too, ensuring that 90% of the front of the phone is screen, leaving even less bezel than on its predecessor for an excellent viewing experience.The curved edges of the screen provide a futuristic finish, and it's a great display for watching movies and playing games.Samsung has made it easier to use the Galaxy S9 in landscape mode as well, with the whole UI (user interface) now rotating to fit the widescreen layout, reducing the frequency you need to rotate the phone between portrait and landscape.The QHD+ Infinity Display is looks fabulous on the Galaxy S9Rear cameraWorld's first f/1.5 aperture on a phoneDual-aperture technology works like the human eye960fps slow-motion video captureThe big talking point for the Samsung Galaxy S9 is the camera, and more specifically the rear-facing 12MP option.It features Samsung's dual-aperture technology, allowing the camera aperture to expand and contract depending on light levels, just as the human eye does.In scenarios where the light is over 100 lux – roughly from sunrise onwards – the Galaxy S9 camera defaults to f/2.4, reducing the amount of light hitting the sensor to prevent images from being overexposed.When the light drops below 100 lux though, the aperture widens to f/1.5, allowing the Galaxy S9 camera to suck in more light – 28% more light than the S8, to be precise.The 12MP rear camera on the S9 boasts dual-aperture technologyThe difference is dramatic, and we witnessed just how good the S9's camera is with a demo which put it up against the excellent camera of the Google Pixel 2 XL.The handset used was actually the Galaxy S9 Plus, but it uses the same technology, which means you can expect the same results from the S9.In short, the Galaxy S9 Plus beat the Pixel 2 XL comfortably. It wasn't even close: the Samsung captured the far superior image with more detail, more clarity, and less noise (according to Samsung, 30% less noise than its previous generation of cameras).Of course, this was in a controlled environment, and we'll be putting the Galaxy S9 camera to the test during our in-depth review, but it's safe to say we're excited by its potential.That's not all the Samsung Galaxy S9 camera can do, however – oh no.The rear camera can also record slow motion footage at 960fpsSamsung has also improved its slow-motion video capture, upping the frame rate to 960fps to match phones from rival Sony such as the Xperia XZ1 and XZ1 Compact.Slow-motion video at 960fps is recorded at 720p (HD), while 240fps slo-mo can be grabbed at Full HD (1080p) resolution.A smart feature that's been included here is Motion Detection. This has the phone do all the work – all you have to do is hold it up to the space you want to film. You don't even need to hit the shutter key.That's because the Galaxy S9 will automatically start recording when it senses movement, ensuring you don't miss that fleeting, perfect moment.Something we’ve struggled with on Sony phones is getting the timing right for slo-mo capture, so this is a welcome addition on the Galaxy S9.Bixby Vision cameraBixby Vision can provide live translations and show calories in foodInternet connection required for features to workYet another feature in the Samsung Galaxy S9 camera is Bixby Vision. The smart assistant was already baked into the camera app on the Galaxy S8, but this time around it's been given some extra juice.One of Bixby Vision's features is a live translation mode: point the camera at text in a foreign language and the Galaxy S9 will translate the words into your native tongue automatically.In the demos we were shown the feature was able to cope with different font types and sizes, and while it did take a short while to get the translation it appeared to work well.Bixby Vision can translate a menu liveIt's worth noting that you need an internet connection to use live translation, so if you're roaming abroad you'll want to keep an eye on your data usage.Something else the Bixby camera can do is tell you how many calories are in your food, although we were unable to test this feature out during our hands-on time with the phone so we can't vouch for its accuracy. Keep an eye out for our full Samsung Galaxy S9 review where we’ll put this feature to the test.Bixby Vision is also location-aware, and can suggest places to go and provide information on landmarks you point the camera at, while the make-up mode allows you to harness the 8MP front-facing camera to give yourself a mini makeover.AR EmojiSamsung's answer to Apple’s Animoji is a little half-bakedEasy to save and share on any third-party applicationAnother big feature for the Galaxy S9, and another that's linked to the camera, is AR Emoji.It's a reaction to Animoji on the iPhone X, but the Galaxy S9 does things slightly differently by snapping a selfie of you and then creating a 3D animated character in your likeness.The quality of the likeness varies quite a lot, and you may question if the character on screen is really based on you.You can select a hairstyle, glasses and clothing for your AR Emoji, although the options here are limited to 12 outfits and seven types of spectacles. We were disappointed with the overall look of our AR Emoji as its hairstyle and clothing didn’t fully reflect us. It's a bit of fun, but there could be more on offer here.AR Emoji hands on galleryImage 1 of 5Image 2 of 5Image 3 of 5Image 4 of 5Image 5 of 5Once you've created your style, you can then start gurning at the screen and your AR Emoji will begin to mimic your expressions as the Galaxy S9 tracks and maps over 100 facial movements.We found the expressions our avatar pulled to be a little wooden though, as Samsung has limited the feature to just 18 actual expressions. Pursing your lips or acting surprised comes across well, but more subtle expressions sometimes get a little lost.From there you can snap a picture of the avatar, or record a video (with sound) of you speaking/singing/shouting as your AR Emoji mimics your actions. These are then saved as GIFs for easy sharing on any third-party social or messaging application.The Samsung Galaxy S9 also creates 18 preset emoji from your avatar and adds them to the Samsung keyboard. You can then call on them whenever you're typing, allowing you to quickly and easily share your feelings with an image of yourself.Battery and performance3,000mAh battery offers same performance as predecessorPlenty of power under the hood keeps Android 8 smoothFor those of you hoping for an improvement in battery life on the Samsung Galaxy S9, we've got some bad news: there isn't one.Samsung has equipped the Galaxy S9 with the same 3,000mAh battery as the S8, and we're told this will offer similar performance to the S8.That's not a terrible thing though, as we found the Galaxy S8 could easily last a day on a single charge, even with moderately high usage.And there has been an upgrade in the power department, with the Samsung Galaxy S9 packing the firm's own Exynos 9810 chipset everywhere apart from the US.Due to the need for specific carrier network support, the US variant of the Galaxy S9 will use Qualcomm's flagship Snapdragon 845 system-on-a-chip to ensure it works with all the major carriers.Both chipsets are paired with 4GB of RAM, which is 2GB less than what's inside the Galaxy S9 Plus, but it still provides enough grunt to keep the Android 8 Oreo operating system running smoothly.Samsung's Touchwiz UI has been plastered over the top of Oreo, although it's a much lighter presence these days compared to the versions running on the likes of the Galaxy S2 and S3. It means Android is still familiar, and operates in a natural and fluid manner.There's 64GB of storage packed into the body of the Galaxy S9, while a microSD card port supports cards up to 200GB, allowing you to expand massively on the internal offering.The USB-C port at the bottom of the S9 is flanked by a speaker and headphone jackEarly verdictThe Samsung Galaxy S9 is a top-notch flagship, and has the potential to be the best phone in the world.It's no secret that this is a decidedly incremental upgrade from Samsung, which makes the price hike harder to accept, and it's unlikely to set pulses racing in the way its predecessor did which could make it a trickier sell.It does have an ace in the hole though: the camera.While further testing is required to find out just how good it is, the early signs are extremely positive. If smartphone photography is important to you, watch this space.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicated MWC 2018 hub to see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.What is a hands on review?'Hands on reviews' are a journalist's first impressions of a piece of kit based on spending some time with it. It may be just a few moments, or a few hours. The important thing is we have been able to play with it ourselves and can give you some sense of what it's like to use, even if it's only an embryonic view. For more information, see TechRadar's Reviews Guarantee.",Samsung Galaxy S9 review
9780,3776861,2018-02-25 17:00:22,"Samsung Galaxy S9 Plus release date, price, news and featuresHere's what the 6.2-inch Galaxy S9 Plus is all aboutSharesThe Samsung Galaxy S9 Plus is the newest really big Android smartphone, and also the worst-kept secret by Samsung. We all knew it was coming today.Samsung's Unpacked 2018 event ahead of MWC 2018 has given us our first official look at the handset and its massive 6.2-inch display – and it honestly doesn't look all that different from last year's S8 Plus.But looks are deceiving. You're in for a decidedly better camera thanks to the new adaptive-aperture technology and the world's first f/1.5 aperture on a smartphone. The low-light photos are going to be top-of-the-line for a phone camera.Big changes worth noting are fixes to the fingerprint sensor location and the long-overdue debut of stereo speakers on a Samsung flagship. The S9 Plus has everything we want, except a convincing price.We're finally able to lay out all of the facts about the Samsung Galaxy S9 Plus. Here's what this camera-focused smartphone is all about.Samsung Galaxy S9 Plus design and displayFingerprint sensor is center-aligned on back, stereo speakers debutSamsung's classy glass-and-metal smartphone designMinor changes to the dimensions compared to the S8 PlusThe Samsung Galaxy S9 Plus is a screen-first smartphone with minimal bezel on the front. It's easy to appreciate its giant 6.2-inch display, Quad HD resolution and Super AMOLED technology, even if practically nothing has changed from the S8 Plus.You will find some newness to the Galaxy S9 Plus look. The fingerprint sensor is on the rear of the phone again, but shifted to the center. That contrasts with the much-maligned offset scanner on the S8 Plus. Samsung listened to last year's overwhelmingly negative feedback when it made the Galaxy S9 Plus, it seems.Another important shift is that Samsung has finally decided to use stereo speakers on the Galaxy S9 Plus. It's 40% louder than the S8 Plus, thanks to the addition of a second speaker in the earpiece that's combined with the familiar bottom-firing speaker. These aren't dual front-facing speakers, but this is a big step in the right direction for a Samsung flagship.The Samsung Galaxy S9 Plus colors include three options, and maybe four depending on where you are. There's Midnight Black, Coral Blue and Lilac Purple – Samsung's new standout color – which you'll see everywhere when Samsung markets the new phone. There's also a Titanium Gray color, although this won't be coming to every region.Dual-aperture, dual-lens Galaxy S9 Plus cameraThe Galaxy S9 Plus camera is a big upgrade for smartphone photography, thanks to Samsung putting all of its energy into the world's first phone with an f/1.5 aperture.It should be fantastic at capturing low-light photos as the camera is now able to pull in up to 28% more light, and capture images with 30% less noise. This is due to the fact that it can switch from the f/1.5 aperture when it's dark to a more typical f/2.4 when there's enough light.Samsung has also added a second telephoto lens to the S9 Plus, just like it did for the Note 8. And beneath the dual-lens is DRAM as part of the image sensor stack. This gives you a photo that's really a composite of 12 frames. That's an upgrade from the three frames used in last year's phone.Super-slow-motion videoThe Samsung Galaxy S9 Plus video capabilities have been upgraded, too. It can now take-super-slow motion video at 960 fames per second at 720p. The previous 240fps has been enhanced to 1080p.Our favorite part of Samsung's take on super slo-mo video is that it automatically starts rolling when the camera senses motion. There's no need to try to haphazardly press the shutter button just to capture split-second action.Samsung Galaxy S9 Plus AR emojiThe font-facing camera is taking on Apple's Animoji viral sensation with more masks and props that appear over your face. Samsung is adding Snapchat-like special effect to the Galaxy S9 selfie camera to the default camera called AR Emoji.You can create an avatar in your likeness or assume a completely different persona. It's also possible to add AR oversized hats, sunglasses and other fun accessories. There's more variety to AR Emoji with customizations that remind us of bitmoji, but with a moving avatar that matches your facial expressions. It's not as expressive as Apple's Animoji, however.Sending AR Emoji to friends is easier thanks to the fact that Samsung decided to export the animations as GIFs. That'll make it easier to send AR Emoji to your friends, even if they're on a non-Samsung device.Samsung Galaxy Plus specs and softwareThe Samsung Galaxy S9 Plus is poised to be the fastest Android phone when it launches. It comes with either Samsung's Exynos 9810 chipset or the Snapdragon 845 chipset (in the US and China). We're expecting them to be 30% faster, according to our recent tests of the Snapdragon 845 chip on a Qualcomm reference phone.There's also 6GB of RAM, a reason to get it over the smaller Galaxy S9 that has just 4GB of RAM. 64GB of internal storage is supplemented by a microSD card slot that can store up to 200GB of extra data.If you're looking for a Samsung phone with Android 8.0 Oreo, this is it. It runs the latest Google operating system with the 'Samsung Experience' skin that was once called TouchWiz. It's streamlined compared to years past, and we no longer have a problem with it. In fact, many of Samsung's default apps, including the camera app are better than Google software.Samsung Galaxy S9 Plus battery lifeYou're likely going to get all-day battery life out of the S9 Plus due to the fact that it contains the same 3,500mAh battery capacity as last year's phone. It's another reason to go with the bigger phone over the normal S9, which has a 3,000mAh battery.We're hoping that its new chips are efficient enough to make the battery last a bit longer – we'll know for sure when we conduct our full Galaxy S9 Plus review. The good news is that the phone supports both fast charging and fast wireless charging, so you should be able to juice up again in no time.","Samsung Galaxy S9 Plus release date, price, news and features"
9781,3776936,2018-02-25 17:54:26,"About TNWTNW SitesAdvertising in the digital age: why online-first is the futureWhile internet video continues to grow its coverage, television advertising is losing ground. Young people no longer watch TV as they turn to YouTube, Netflix, and other sources. As such, advertisers are actively transferring budgets from TV to digital sectors.We are going to look into the questions: “Why is native video advertising likely to replace TV?” and “who is driving the change?”. More importantly, how can companies adapt to the changing face of advertisement in this increasingly digital age?The stats are obvious: TV advertising is failingWhat the “old world” had always feared is happening right before our eyes as digital penetration is outstripping TV in all respects. In 2016, US revenues from digital advertising exceeded revenues from TV for the first time – $72.5 billion (+22%) compared to $71.3 billion from TV. The same happened in Russia in Q3 of 2017. For the first time ever, the internet advertising market outperformed the revenues of the TV segment with 41 billion rubles as opposed to 36.5 billion. The global trend is evident as the share of advertising on digital channels continues to grow while declining on TV. Adidas, for instance, has completely withdrawn from TV advertising, deeming it ineffective for their Central Asian market.The young generations care nothing for traditional advertisingTelevision loses badly in the battle for advertising budgets through its critical shortage of a young audience (13 to 24 year olds). Young people still do watch TV but they seem to be doing fine without it. Only 36% of consumers noted that they cannot do without a TV screen. Meanwhile, 67% cannot imagine their lives without YouTube and 51% seem to lose meaning in life without Netflix.The same audience is watching 2.5 times more internet videos than traditional TV. On average, they spend 12.1 hours each week on free internet content and 8.8 hours on paid internet videos as opposed to 8.2 hours of television.Even if the target audience does watch TV, the advertiser stills pays through the nose to reach it. For the consumer, television is an advertising billboard. It conveys some general information, but it is generalized in its address, not directed at a specific person or even an entire audience. The internet prevails here once again.If advertising in social media or on YouTube has come to the attention of a viewer, then we can confidently say that the viewer has seen it for a reason, as that viewer became part of an audience the advertiser is interested in reaching. Furthermore, preference is given to video advertising. The younger generation is quickly getting used to it due to early contact with pads and smartphones along with fast access to the internet.Traditional and video bloggers use their influence wiselyIn the fight against television, video-bloggers are the internet’s heavy artillery. They are undisputed opinion leaders for the younger generation – average people with common problems who speak about familiar topics.Interestingly enough, vloggers are actively involved in the advertising campaigns of well-known brands, help raise brand awareness, and sell goods and services. Nowadays, becoming successful in vlogging is much easier and with a lot more room for financial growth. For example, Ryan, a six-year-old boy from the US, posts toy reviews and earns $11 million a year.Video bloggers are a guaranteed way for advertisers of reaching target audiences and getting predictable results. Influencers share their experience of using the product and, unlike traditional TV, such advertising is unobtrusive and less annoying. In addition, even working with top 10 video channels is much cheaper than advertising on TV.Promoting native content is much more lucrative than traditional advertisingThe day television is finally laid to rest, the final nail in its coffin will be hammered by influencer marketing. Even now, return on investment from online videos is 77% more than from TV promos. This means that the barrier for entry into the market is considerably lower. The sector is now open to everyone, not just large companies, as those who previously could not afford market entry are actively in the game.The main trend nowadays is native advertising through opinion leaders. The viewers receive the promos discreetly with benefits and such offerings allow companies to present products as efficiently as possible and get transparent results on publication.Subscribers see native video ads in a positive light, so they react actively to the promo, especially when it looks as natural as possible. With the current level of authors’ technical training and creativity, paid videos are more interesting than their usual content.The main question for advertisers is looking out for platforms to place a native promo. There are several ways – either wasting time and negotiating with each resource individually (negotiations can drag on for any reason), or finding those who will do it for them by providing a ready-made solution.As one might expect, such tools are not just a trend’ they are a glimpse into the future. Traditional advertising is slowly dying and a personalised Internet, along with personalised advertising, is the true future.It’s a winning situation for everyoneFor advertisers, reaching a targeted audience for cheaper and thus seeing a bigger return on investment is essentially a dream come true.For regular consumers, removing the need to watch obtrusive ads and instead view products and services that they might actually want to use via people that they trust is the best of both worlds.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Advertising in the digital age: why online-first is the future
9782,3776937,2018-02-23 15:25:37,"About TNWTNW SitesOf course robots will do our cryptocurrency investingThere’s nothing that we as a society get more excited about and panicked by than technological progress.On one hand, we understand that innovations like artificial intelligence and Bitcoin are autonomous, powerful entities that many of us will never truly know — and what we don’t know, we don’t always trust. On the other hand, we recognize that these technological advancements were created to make our lives easier.You might ask yourself how it’s possible to be grateful for something when you are also so skeptical of it. But think about this on a smaller scale: how many times have you used your smartphone to shop online when you were away from home? Or scheduled an Uber to drive you from the airport to a hotel?To be wary of these innovations is to dismiss their profound impact on our personal lives and beyond. I am the most interested in how technology is working to improve our financial prosperity. The coupling of artificial intelligence and digital currency in the world of investing has already become such a profound new trend in fintech. As AI and cryptocurrency become more intertwined, they are actively working to improve the way we make money.Removing all emotions makes investing more efficientFirst, rid yourself of the imagery of human-detesting robots that futuristic movies have built up in your mind. In reality, artificial intelligence is just self-writing software that “learns” from data patterns and teaches itself human-like tasks, including decision-making and speech. This is beneficial to you because financial institutions are using these capabilities of AI to yield higher profits.AI uses deep learning to track patterns of the existing financial markets and uses that data in order to predict their outcomes. Over time, as the software continues to collect more data on the financial markets, it only grows more intelligent.Fintech skeptics may believe that humans have already mastered this, but even the most experienced traders often fail to achieve the same level of success as their robotic counterparts. So, what is the barrier that hinders this success? The very thing that makes them human: theiremotions. Whether it’s greed or doubt that influences their decisions, there is no way for a person to completely rid themselves of the connection between their emotions and corresponding behavior.People have benefited from investing before AI, but there has never been a more efficient way to invest. Certain hedge funds are beginning to take notice of these discrepancies between humans and technology. Statistics show that hedge funds are more profitable when they replace human traders with AI.Take, for example, one hedge fund based out of New York that is already completely reliant on artificial intelligence. One of their funds has been profiting by 20-98 percent consistently between 2002 and 2016.If we want to prosper, our hopes should rest on efficiency. Since that is what more people desire, NS Parthasarathy of Mindtree says that “we have to move towards systems and technologies that make work happen in the most efficient way in terms of service and price.” That is exactly what artificial intelligence works to do.Digital currency is making cash obsoleteIf you’ve been paying attention to investment trends, you know that Bitcoin has been gaining traction over the past few years, which comes as no surprise. As cash grows obsolete, cryptocurrency becomes a more and more attractive form of currency.This is why Bitcoin is replacing our society’s traditional views of currency as a great alternative to paper money. It’s a decentralized form of currency, which means that no government or bank has control of it; it’s exclusively an anonymous, peer-to-peer transactional platform.Anonymity, however, does not mean that these transactions aren’t being recorded — quite the opposite. Every transaction is tracked in a huge network referred to as the blockchain. Blockchain is a secure network that manages Bitcoin, ensuring that no data can be tampered with and solving the issue of double spending, something no network could do previously without the involvement of a third-party (although double-spending is still a possibility).Because of the availability of Bitcoin and the fact that the government cannot intercede in it (although it’s definitely trying), it tends to react quickly to market bias. While this can seem risky to some investors, others use it to their advantage and profit from these influxes.The merging of artificial intelligence and digital currencyThese digital currency markets are still relatively young, but that could work in the favor of artificial intelligence. Traders are using the deep learning capabilities of AI to measure data now that will help to predict the fluctuations of this new market in the near future.Before technology, it would have taken a human multiple years to collect and organize the data they gleaned from watching patterns in the financial market in order to better influence an investor’s decision. Artificial intelligence significantly cuts down on this time. There are already some cryptocurrency trading bots out there, but they still have some considerable faults.This financial market may be more volatile than more traditional markets, but the data it provides AI software can work to change the face of investing for both expert and amateur investors sooner than ever before — and trading bots will keep getting better. This gives individuals better control over their investing success and makes for a more rewarding long-term commitment.As the cryptocurrency market matures, it will become easier to predict positive fluctuation and foresee where higher profit margins exist. With a bot serving as your eyes and ears behind the veil of the stock market, you can expect these innovations to bring you a better understanding of where and how to invest your money.",Of course robots will do our cryptocurrency investing
9783,3776938,2018-02-25 13:44:24,"About TNWTNW SitesHands-on: Samsung’s Galaxy S9 aims for Google’s camera throneSamsung had the best smartphone cameras on the market a few years ago, but I’ve felt it’s lagged behind Google, Apple, and Huawei the last generation or two. DxOMark and other reviewers seemed to agree.With the Galaxy S9, Samsung wants the crown back.I had the chance to play around with the S9 and S9+ before their official announcement at MWC and there’s a lot to like. The smaller device will cost $720 ($30/month), while the Plus will go for $840 ($35/month). Pre-orders begin on March 2, and the phone officially goes on sale March 16.After months of leaks, there isn’t much surprise to the specs and features, but there are some highlights. Starting with the smaller model:Qualcomm Snapdragon 845 (US) / Exynos 9810 (everywhere else)5.8-inch QHD Super AMOLED display4 GB RAM64 GB storage + microSD3,000 mAh battery12MP rear cameraVariable aperture: F1.5/F2.4960fps super slowmo8MP F1.7 front cameraIP68 water resistantStereo speakers (finally!) tuned by AKG, 40 percent louderDolby Atmos supportBluetooth 5.0AR Emoji – Samsung’s take on Apple’s AnimojiColors: Lilac Purple, Midnight Black, Coral Blue (US). Titanium Grey available as well in other markets.There’s a greater difference between the S9 and S9+ than in past generations. In addition to the specs above, the larger model features:6GB RAMSecondary telephoto camera6.2-inch QHD Super AMOLED3,500 mAh batteryPreviously the screen and battery were the only differences. Considering the larger battery was largely negated by the bigger screen, it meant users could simply choose a model based on their preferred size. This time around, the improved camera setup and RAM makes the Plus the superior model. If you want the most powerful Samsung device you have to get the S9+. That sucks.Design wise, it’s hard to immediately tell them apart from their predecessors. You know the deal: metal frame, curved screen, glass back. This time the back panel has more of a frosted look though, and the fingerprint sensor has been moved below the camera, instead of to the side. It’s way better.The top and bottom bezel are also a bit smaller – impressive considering the more powerful speakers, now working in stereo. Conversely, the side bezels seem a small bit larger, but honestly, that’s probably a good thing for preventing accidental touches.Speaking of speakers (heh), I liked what I heard in my brief hands-on. Compared directly to the S8, they were noticeably louder (Samsung says 40 percent), with an impressive amount of stereo separation considering the size. They were certainly better than the Pixel 2’s speakers, even though one of the S9’s speakers is bottom-firing. I wasn’t able to test any Atmos-encoded content though, which would presumably be even better.But the camera is the heart of the S9, aiming to up the ante in both hardware and software. The headlining feature is the variable aperture, which opens up to F1.5 for low light images – the widest we’ve seen yet – and closes to F2.4 for sharper images in the daytime. You can even see the physical aperture changing if you look at the lens closely.Image processing is a crucial part of image quality too, so Samsung says it now combines 12 shots into one to reduce noise. In all, the S9 supposedly offers a 66 percent improvement over the S8 in low light. It was certainly significant in the demos and examples Samsung showed off, but we’ll have to spend more time with the device to know how it holds up to the competition.On the software end, Samsung now has AR emoji – its take on Apple’s Animoji. Basically, Samsung takes a photo of your head, and creates a Nintendo Mii-like avatar, giant head and all.It works… okay. The S9 kept making me look very white, even though I’m quite tan. Your skin tone is adjustable to a degree, but it still looked weird after multiple tries. Samsung also gave me Pantene sleek hair, when it’s extremely curly. Thankfully there are different hairstyles to choose from.After some tweaking I found somewhat decent, but I still feel like Samsung would’ve been better off using more cartoonish avatars. This is a bit too uncanny-valley for my tastes.Here’s a look at what Samsung gave me by default, versus what I actually look like. The third image is the result after some tweaking.The facial tracking doesn’t stand up to the iPhone X’s quality either. That’s not surprising, given Samsung is only using the plain-old selfie camera – no fancy face mapping. Still, as far as I could tell, it’s basically no better than a Snapchat mask.Maybe it’ll work better in the final software or in different lighting conditions.In any case, I doubt people will care about accuracy as long as the emoji are fun. And with a Disney partnership apparently imminent, they’ll have plenty of characters to choose from.A headphone jack, hoorah!Then there’s Bixby. Samsung’s still trying to convince us its assistant is worth its dedicated button by adding real-time image translation. It’s powered by Google Translate, and works pretty well. Of course, you can use Google’s own app for that, but it’s nice to have the feature integrated right into the camera. Bixby can also estimate the calories in your food now so maybe you’ll think twice about eating that donut.Obviously, 960fps slow-mo is super fun. It automatically creates shareable GIFs, and you get the option to loop or play the video backwards too. Also, Samsung uses AI to pick appropriate music. Which of course means it’s going to screw up occasionally, and create hilarious mishaps.Some other miscallaneous notes from my hands-on:A new Intelligent Scan mode combines Iris and Face scanning and pick the best method depending on lighting conditions.DeX has a new dock which lets you use the headphone jack or use your phone as a mouse and keyboard. It also supports resolutions up to QHD now.It’s 2017, and QuickCharge 4.0+ exists. Why is the S9 still on QC2.0?The camera has a much more intuitive (and iPhone-like) camera UI. Just swipe left and right to switch modes.The portrait mode algorithm on the S9+ seems far improved over the mediocre one on the Note 8.The smaller S9 instead uses the selective focus mode that’s been available for several generations, but it also seems better here too.Performance was super fast, but that’s to be expected on a fresh phone with a new processor. Still, Samsung finally did a decent job of optimization with the Note 8, and that’ll hopefully continue here.Despite the stereo speakers and tiny bezels, it still has a headphone jack, in case you were wondering. Are you listening, Google/Apple/everyone?There’s a dab AR emojiAnd that’s about it. Naturally, we need to spend more time with the devices to really suss out the performance and battery life, but considering the previous generation already did well on those fronts, and the S9 has a more efficient processor, things are looking good.The big question mark really is the camera. Will the new hardware and processing help Samsung dethrone the Pixel 2? I’m looking forward to finding out.",Hands-on: Samsung's Galaxy S9 aims for Google's camera throne
9784,3779725,2018-02-20 05:00:00,"Airlines inching closer to dynamic pricingImagine if airlines could tailor fare offers based on who was making the ticket inquiry, rather than strictly on the search criteria.Well, industry technology and revenue-management experts say those days are fast approaching.In fact, a few airlines have already implemented what is known as dynamic pricing on some ticket searches within their own channels, according to the revenue management software provider PROS, which works with some 80 airlines worldwide, including Southwest, Lufthansa, Emirates and Aeromexico.""2018 will be a very phenomenal year in terms of traction,"" said John McBride, director of product management for PROS. ""Based on our backlog of projects, there will be a handful of large carriers that move toward dynamic pricing science.""Critics of the technology warn of a growing lack of transparency if fares are priced dynamically.Conceptually, dynamic ticket pricing is simple. An airline identifies the person making a flight inquiry, then mines its data for that person's flying history. The person could be identified if he or she is logged into an OTA or into an airline website through a frequent flyer account.Much as Amazon does today to remember shopping histories, those sites could also set cookies at login to identify a person (or at least the device being used) for subsequent searches in which the individual is not logged in, said Phocuswright technology analyst Bob Offutt. And there are also technology companies that provide the capability to identify consumers across multiple devices without the need for login information, McBride said.The revenue-management platform then uses an individual's flight-shopping history to generate a person-specific fare offer that differs from the offer some other shoppers might get for the same fare inquiry at the same time.Experts say such technology is most likely to be used to offer discounts to customers with loyalty status and to generate bundled fare offerings that fit the customer's profile. But in theory the technology could also be used for different purposes, such as to induce a new customer with an especially affordable ticket or to offer a higher ticket price to someone who is likely to be undeterred by an upcharge.Dynamic pricing platforms will also generate specialized offerings based on the profile of a fare search, even if they don't have the specific identity of the shopper, said Peter Belobaba, the airline industry program director at MIT who helped author a recent discussion paper on advances in airline industry revenue management and distribution for the Airline Tariff Publishing Co. (ATPCO), the airline-owned corporation that collects and distributes fare data.For example, if a person were to query a one-night, midweek trip from New York to Chicago, the platform might make the assumption that the inquirer is traveling for business, then prepare fare offerings that fit the profile of a business traveler.By employing dynamic price offerings, airlines would hope to increase conversion rates while driving incremental revenue increases, in part by showing more travelers that there is value in paying a bit extra for a more comfortable flight.""I believe a lot of consumers are so focused on the lowest fare that they end up buying a degraded product, then complaining when they don't get a seat assignment,"" Belobaba said.But according to the recent ATPCO paper, prepared by PODS Research, moving into a world of dynamic price offerings has proven technologically difficult for the airline sector, in large part due to the legacy distribution system that was put in place after deregulation in 1978.That distribution system allows for just 26 fare classes, one for each letter of the alphabet. Airlines assign prices and restrictions to each fare class, then file those classes with ATPCO for dissemination to GDSs. At present, carriers are able to update prices in each fare class four times per day on domestic flights and hourly on international flights, said Tom Gregorson, vice president of products and solutions for ATPCO.But in practice, Belobaba said, most airlines typically keep their set of price points for weeks at a time and mainly manage fare offerings by altering the fare classes that are up for sale at any given time. To move into a world of dynamic price offerings, the airline distribution industry will have to get away from the legacy fare-filing system or institute a hybrid solution, Gregorson said.Under a hybrid system, airlines would still do fare filings, but then revenue-management programs would be able to offer price changes from those base fares depending on who is doing the fare search.The key to enabling such technology is the development of a working interface that would enable the shopping engine of a GDS or OTA to communicate with an airline's own pricing engine, which would be doing the adjustment, Gregorson said. He added that ATPCO's Dynamic Pricing Working Group, which reconvenes for three days this week, has implemented a pilot project to work on protocols for such an interface.In a purer version of dynamic pricing, however, airlines wouldn't file fares at all. Instead, a fare offer would be generated from scratch, in real time, based on who the shopper is, the nature of the inquiry and existing demand and availability for a given flight.McBride of PROS said that 11 of the company's airline clients are already using its software to generate real-time dynamic offers within direct sales channels, including their websites. Several of those airlines are making the price offers by adjusting from their published fares, while others are generating offers from scratch.Though he wouldn't identify the airlines for contractual reasons, McBride said they are mainly major carriers and are based around the globe.Rollout of dynamic pricing by those carriers has been cautious and segmented, with much of it concentrated on group travel and on routes that compete against low-cost carriers, including against Europe's Ryanair and EasyJet, neither of which files fares with ATPCO.""Our customers have definitely seen increased conversion rates of up to 50%, and it has enabled airlines to achieve incremental revenue in the 7% to 10% range,"" McBride said. ""Dynamic pricing clearly speaks to the opportunity for airlines to service a wider range of customers with a broader set of fares.""Gregorson said it's unclear when dynamic pricing will become a possibility through GDSs and other indirect sales channels. But he added that ATPCO's working group meeting this week could provide more clarity on that question.",Airlines inching closer to dynamic pricing
9785,3779884,2018-02-25 17:00:53,"The Samsung Galaxy S9 arrives March 16 for $720, with AR emojis, real-time translation and a better camera0Samsung didn’t leave too much up to the imagination at Mobile World Congress this year. The hardware giant took a little wind out of its own sails back at CES when it announced it would be launching its latest flagship sometime this month.And then, of course, the invites went out, bearing a giant number “9” that was either some off-handed homage to The White Album’s most inscrutable track or a less than subtle indicator that the Galaxy S9 (and, for that, matter, S9+) was on its way.The handset was shown off in all its virtually bezel-free glory at tonight’s big press conference in Barcelona. It’s a rare turn for a company that’s grown accustomed to launching handsets on its own terms, but the stars aligned with Qualcomm’s latest super-powered chip, the Snapdragon 845, and Samsung clearly didn’t want to be left behind.The Galaxy S9’s debut almost makes you feel sorry for everyone else with a dog in the fight this year at Mobile World Congress, because the game was really over before it started. For some it is probably a taste of what it must feel like to have your movie open against a Star Wars film. In the world of Android handsets, Samsung is the biggest and blusteriest — a bankable, big-budget powerhouse.To stretch that analogy a little further, the Galaxy S9 is more Force Awakens than The Last Jedi — which is to say, we’re not talking about a radical departure. In fact, you’d probably be hard-pressed to spot too many differences in terms of industrial design this time out. If the phone is indeed the iPhone X killer many were hoping for over the past couple of weeks, it’s courtesy of some of the software-related tricks the company has squeezed in.Of course, Samsung has no reason to rush into any sort of radical design change. The company notched up a notch-less win last year with the introduction of the Infinity Display, a virtually (but not quite) bezel-free design that hides a virtual home button behind the screen. The displays are essentially the same as their predecessors, at 5.8-inch and 6.2-inch, respectively — both Quad HD+, Super AMOLED.The first thing I saw when I stepped foot on the Barcelona streets this week was yet another teaser for the handset, sporting the words “The Camera Reimagined,” and indeed, imaging continues to be the primary battlefield on which the smartphone wars are being fought. Perhaps the most welcome addition on this front is improved low-light shooting. That’s undoubtedly the most requested feature on the camera front.We got to play around with it a bit and the results are pretty impressive at first glimpse. There’s a dual aperture mounted in the rear camera of the S9 and rear wide-lens camera on the S9+ that switches between F1.5 – F2.4 when you need to let more light in for dark settings, like bars and restaurants, offering a 28 percent increase in light. If you take a close look at the rear of the camera, you can actually see the aperture expand and contract as you switch between settings — it’s particularly notable when you have to zoom in.It actually appears to be the same variable aperture that Samsung debuted on the W2018 flip phone, of all things — leave it to Samsung to introduce its latest cutting-edge camera tech in a flip phone. What this means as far as day to day use is up to 30 percent less noise in photos taken in less than ideal circumstances.Also new — but definitely more on the novelty side for a majority of users — is Super Slow-Mo. The trick was the highlight of last year’s Sony Xperia XV, but now it’s on a phone that, you know, people will actually buy. Like Sony’s version, the S9 slows things down from the standard 240 FPS to 960 (4x slower), and it makes a world of difference — a Samsung rep described it as going from standard slow motion to something akin to The Matrix’s Bullet Time. An overstatement, sure, but the result is definitely impressive.The effect is greatly improved by the presence of Motion Detection, which, as advertised, waits until it detects movement in the frame in order to start shooting — after all, a few extra seconds are a lifetime in Super Slow-Mo Land. Once recorded, users can use one of Samsung’s 35 song clips as background, or let the software use its AI to attempt to figure out what fits best.Speaking of AI, Bixby is finally starting to get some truly useful updates this time out. Samsung’s smart assistant got off to a slow start, particularly in the wake of Siri/Alexa/Google Assistant, but it’s finally starting to come into its own. Far and away the most impressive bit — and the thing I’d really like to try on this trip to Barcelona — is Live Translation.It’s similar to what Google’s been offering with Lens, and it’s about as close as you’ll come to a killer app for the frequent travel. Hold the phone up to a sign and it will translate it in a few seconds. The system works with 54 input languages and 150 for output, using augmented reality to overlay words over the original text.The translations aren’t great — as evidenced by the “You’re welcome feed the animals do not” — and the text looks a bit like it was cut out of a newspaper for a ransom letter, but it’s an impressive and useful tool nonetheless. The system will also work with 63 different currencies, offering up a real-time exchange rate — though that feature wasn’t available during our trial.There’s also a fun little feature that overlays estimated calorie counts after identifying food stuff, Not Hotdog-style. It will then feed that information into Samsung Health. Interesting footnote to all of this: I asked if these augmented reality features were built using ARCore. Turns out Samsung just developed everything in-house here, in spite of a recent partnership between the companies. This Bixby development must predate that deal.AR also plays a bit role in Samsung’s long-rumored Animoji competitor. In fact, Samsung’s calling it AR Emoji — decidedly less catchy than the name Apple landed on. There are cartoon characters here that respond to your movements and speech in real time, like Animoji — in fact, Samsung managed to land a much coveted deal with Disney. No doubt Apple’s kicking itself for not managing to score Mickey and friends for its own offering, but between this and the Star Wars AR it introduced for the Pixel, Disney clearly didn’t want to play favorites.The real stars of the AR Emoji offering, however, are its 3D personalized avatars. The cameras take a detailed map of the user’s face, using it as the basis for a 3D character. From there you pick your clothes, hair, glasses and the like, as you would with a Nintendo Mii. I’ll be the first (and probably not the last) to say it: they’re kind of creepy. Samsung hasn’t quite figured out where it wants to play in the uncanny valley on this one. Something to think about for the Galaxy S10, I guess.There also are a pair of stereo speakers tuned by AKG. Samsung deserves a bit of credit here for giving some love to an often neglected piece of the entertainment equation. The speakers will switch to stereo when the phone is held in landscape. It’s notably louder than the last generation, as the company showed in a demo — it is still not something that’s going to be replacing the use of a Bluetooth speaker, but should be handy for short video and music sessions.And yes, the headphone jack is back — not that it ever went away. Samsung’s holding onto that output with dear life, after mercilessly — and understandably — mocking Apple for dropping it a few generations back. In spite of launching a solid pair of Bluetooth sport headphones in the IconX, it is a small but important differentiator for the company.Apple’s forward thinking plans have always included a manner of planned obsolescence built in, while Samsung’s philosophy revolves around an everything-and-the-kitchen-sink approach to hardware. Likely the company will eventually swallow its pride and drop the jack, but that doesn’t appear to be happening any time soon — and plenty of customers are no doubt overjoyed by the decision — especially since the company purchased Harman and started bundling halfway decent earbuds with the phone.Unlocking via iris, face and fingerprint scanning are all back, naturally. There’s also an unlocking method called Intelligent scan, which combines iris and face unlock in one, defaulting to a different method based on lighting. A new Dedicated Fingerprint feature, meanwhile, ties different fingerprints to unlock the device’s secure folder, so information can still be secure on a shared device. And, one imagines, someone will have to cut off two of your fingers to really get into your phone’s good stuff.Samsung-Galaxy-S9-1Samsung Galaxy S9+ camera app147A9738147A9754Samsung-Galaxy-S9-3There are also more differences between the S9 and S9+ than there were with past generations. Both phones have a default 64GB storage (expandable up to 400GB via the Micro SD slot), but the 9+ has a healthy 6GB or RAM — two more than the 9. As touched on above, the S9+ plus also adds a wide angle lens to the camera offer. Oh, and the batteries are different, too, naturally at 3,000mAh on the S9 and 3,500mAh on the S9+. That’s the same as the last generation. Samsung’s clearly still playing it safe on the battery front after, well, you know.The S9 runs $720, and the S9+ will cost you a full $120 more, at $840, unlocked — not cheap, exactly, but a fair bit less that Apple’s premium offering. If that’s still too rich for you blood, the company’s also offering two-year financing at $30/$35 a month, respectively.The phones go up for pre-orders on March 2 and start shipping the 16th. AT&T, Sprint, T-Mobile, U.S. Cellular, Verizon, Cricket Wireless and MetroPCS will all be carrying the phone — along with the new Xfinity Mobile, if that floats your boat. The phone also will be available through the big-name retailers, including Best Buy, Amazon, Costco, Sam’s Club, Target and Walmart. Color availability depends on where you buy the phone — unless, of course, you buy it unlocked, in which case you can get it in, Midnight Black and Coral Blue and the lovely new Lilac Purple.For businesses, Samsung will also be offering an Enterprise Edition of the phone in Midnight Black.","The Samsung Galaxy S9 arrives March 16 for $720, with AR emojis, real-time translation and a better camera"
9786,3779889,2018-02-25 18:49:36,"It's too easy to say the Samsung Galaxy S9 is boringAlthough it's not completely untrueI've just live-blogged my way through the Samsung Galaxy S9 launch, and the one thing that struck me was... well, how little struck me.Admittedly, that's a lot to do with the fact that the Galaxy S9 was leaked so often, and so comprehensively, that I failed to see anything new on stage today.But even if this had been entirely new to me, I'd still have struggled to talk about the headline features of the phone straight away.""Well, yes, the screen is... okay, the same. But the camera is new... yes, it's still 12MP and still a single sensor. No, the design is the same. Yes, entirely the same. And yeah, sorry, the Bixby button is still there.""We've berated brands like Apple and Sony for not updating their phones in years gone by, and Samsung should get the same treatment: this is the most minor upgrade we've ever seen from the brand, and yet we're seeing the highest costs yet for a Galaxy S-series handset.What lies beneathBut here's the thing: where is there left for phone brands to go? Apple made waves with the iPhone X because it had stuck with the same design for so long that any upgrade was going to be innovative; but Samsung made the jump to all-screen last year, and has been leading technical innovation for years in the flagship smartphone space.What does come across with the launch of the Samsung Galaxy S9 is that it's focused on more things you can do with the phone. Yes, we saw super-slow-motion video on the Sony Xperia XZ Premium last year, but Samsung has improved on this with a camera that can intelligently tell when you'll want to capture footage in slo-mo.Will AR Emoji actually catch on?The main camera may only be a single sensor, and stick at the same resolution, but the new dual-aperture feature is something that really will resonate with consumers, as it looks like the camera will take simply incredible low-light photos, doing something nobody else in the industry is.This is also the most powerful phone Samsung has ever put out, and the enterprise message has been upgraded to enable more businesses to use the S9 with confidence that it's going to be incredibly secure; it's not headline-grabbing stuff, but it's the kind of thing that smoothes the Samsung story out for those thinking of making the switch.Do it with styleWhat leaves a unpleasant taste in the mouth are the constant jabs at Apple. Yes, it's key to point out how your phone is the best on the market, but the amount of times we heard how Samsung doesn't do that thing Apple does (the notch, the remaining fingerprint scanner) wasn't edifying.Although there is one element we're glad Samsung highlighted: the remaining headphone jack. So many brands are losing the port in order to save space inside the chassis, but this is still something that really irks.Yes, the headphone port is old – really old – but so are lots of things I don't want to see replaced. The wheel. Bread. Open fires. There are better options, but losing them doesn't always make life better, and it's good that Samsung is remaining the standard-bearer here.I'm yet to be convinced by AR Emoji, but even if they don't work you can use this feature as the identifier of what Samsung is trying to do with the Galaxy S9: make a phone that's accessible and which packs more features that delight the user.I don't think it's necessarily done that – the price alone is alienating, just as it is with the iPhone X – but to just say that the phone is boring because of the lack of cosmetic change misses the point.That new camera needs to absolutely shine though – if it doesn't, it's going to be a tough sell for anyone to buy the Galaxy S9 at its higher price when so many of its features are also found on the still-powerful and much cheaper Galaxy S8.We'll be bringing you our full review of the phone soon, so let's see what shakes out as we spend more time with the Galaxy S9 – we can all reserve judgement until then.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",It's too easy to say the Samsung Galaxy S9 is boring
9787,3790768,2018-02-21 18:43:29,"The ACLU just proved that our courts are being manipulated by private debt collectorsA damning new ACLU report uncovers how the private debt collection industry uses courts to do their dirty workA new report by the ACLU reveals how the private debt collection industry has been able to manipulate America's court system to target the financially disadvantaged.Even though Congress abolished debtors' prison in 1833, private debt collectors have still managed to manipulate the court system to have debtors thrown in jail for the inability to pay up, according to the ACLU. They do so by filing lawsuits which, more than 95 percent of the time, will be decided in favor of the collector. After that, private debt collectors will ask the court to compel a debtor to appear for ""judgment debtor examinations."" If the debtor does not show up, the private debt collectors will often ask the judge to issue a civil warrant so that the debtor can be arrested.""Our investigation found that many people missed their court dates because of work, childcare responsibilities, lack of transportation, physical disability, illness, or because they didn’t receive notification of the court date,"" Jennifer Turner, Human Rights Researcher for the ACLU's Human Rights Program, wrote. ""We found two cases in which elderly women missed hearings because they were terminally ill. They died shortly after warrants were issued for their arrest. The threat of arrest is an incredibly powerful tactic for collectors.""In a separate piece for the ACLU, Turner wrote that the courts have become ""mills for these companies, approving without evaluating scores of arrest warrants, wage garnishments, property seizures, default judgments, and other legal actions against consumers accused by debt collectors of owing money.""The ACLU included a number of harrowing stories of people whose lives were ruined by the new partnership between private debt collectors and the court system.A mother of three in Indiana was jailed for missing hearings over medical bills for her cancer treatment. She was physically unable to climb the stairs to the women’s section of the jail, so she was held in a men’s mental health unit.A Georgia woman was arrested while caring for her terminally ill mother. A debt collection company had bought a 6-year-old rental debt her landlord claimed she owed after evicting her from her trailer home. She was jailed overnight. Her mother died two days later.In Missouri, a single mother of a toddler took out a high-interest payday loan of $425. She wasn’t able to pay it back, and the creditor sued. She didn’t go to court and was arrested and jailed for three days.A Utah man committed suicide while jailed for failing to appear in court over an unpaid ambulance bill. He killed himself shortly after he was asked whether he had the money to post bail.The ACLU's report, aptly titled ""A Pound of Flesh,"" proposed legislation that would bar courts from issuing arrest warrants in debt collection proceedings and protect people from being jailed over debt that they weren't notified about, can't afford or aren't legally obligated to pay.Matthew Rozsa is a breaking news writer for Salon. He holds an MA in History from Rutgers University-Newark and is ABD in his PhD program in History at Lehigh University. His work has appeared in Mic, Quartz and MSNBC.",The ACLU just proved that our courts are being manipulated by private debt collectors
9788,3793098,2018-02-26 02:47:13,"Are Hospitals Becoming Obsolete?Hospitals are disappearing. While they may never completely go away, they will continue to shrink in number and importance. That is inevitable and good.The reputation of hospitals has had its ups and downs. Benjamin Rush, a surgeon general of the Continental Army, called the hospitals of his day the “sinks of human life.” Through the 19th century, most Americans were treated in their homes. Hospitals were a last resort, places only the very poor or those with no family went. And they went mainly to die.Then several innovations made hospitals more attractive. Anesthesia and sterile techniques made surgery less risky and traumatic, while the discovery of X-rays in 1895 enhanced the diagnostic powers of physicians. And the understanding of germ theory reduced the spread of infectious diseases.Middle- and upper-class Americans increasingly turned to hospitals for treatment. Americans also strongly supported the expansion of hospitals through philanthropy and legislation.Today, hospitals house M.R.I.s, surgical robots and other technological wonders, and at $1.1 trillion they account for about a third of all medical spending. That’s nearly the size of the Spanish economy.And yet this enormous sector of the economy has actually been in decline for some time.Consider this: What year saw the maximum number of hospitalizations in the United States? The answer is 1981.That might surprise you. That year, there were over 39 million hospitalizations — 171 admissions per 1,000 Americans. Thirty-five years later, the population has increased by 40 percent, but hospitalizations have decreased by more than 10 percent. There is now a lower rate of hospitalizations than in 1946. As a result, the number of hospitals has declined to 5,534 this year from 6,933 in 1981.An error has occurred. Please try again later.You are already subscribed to this email.This is because, in a throwback to the 19th century, hospitals now seem less therapeutic and more life-threatening. In 2002, researchers from the Centers for Disease Control and Prevention estimated that there were 1.7 million cases of hospital-acquired infections that caused nearly 100,000 deaths. Other problems — from falls to medical errors — seem too frequent. It is clear that a hospital admission is not a rejuvenating stay at a spa, but a trial to be endured. And those beeping machines and middle-of-the-night interruptions are not conducive to recovery.The number of hospitals is also declining because more complex care can safely and effectively be provided elsewhere, and that’s good news.When I was training to become an oncologist, most chemotherapy was administered in the hospital. Now much better anti-nausea medications and more tolerable oral instead of intravenous treatments have made a hospital admission for chemotherapy unusual. Similarly, hip and knee replacements once required days in the hospital; many can now be done overnight in ambulatory surgical centers. Births outside of hospitals are also increasing, as more women have babies at home or at birthing centers.Studies have shown that patients with heart failure, pneumonia and some serious infections can be given intravenous antibiotics and other hospital-level treatments at home by visiting nurses. These “hospital at home” programs usually lead to more rapid recoveries, at a lower cost.As these trends accelerate, many of today’s hospitals will downsize, merge or close. Others will convert to doctors’ offices or outpatient clinics. Those that remain will be devoted to emergency rooms, high-tech services for premature babies, patients requiring brain surgery and organ transplants, and the like. Meanwhile, the nearly one billion annual visits to physicians’ offices, imaging facilities, surgical centers, urgent-care centers and “doc in the box” clinics will grow.Special interests in the hospital business aren’t going to like this. They will lobby for higher hospital payments from the government and insurers and for other preferential treatment, often arguing that we need to retain the “good” jobs hospitals offer. But this is disingenuous; the shift of medical services out of hospitals will create other good jobs — for home nurses, community health care workers and staff at outpatient centers.Hospitals will also continue consolidating into huge, multihospital systems. They say that this will generate cost savings that can be passed along to patients, but in fact, the opposite happens. The mergers create local monopolies that raise prices to counter the decreased revenue from fewer occupied beds. Federal antitrust regulators must be more vigorous in opposing such mergers.Instead of trying to forestall the inevitable, we should welcome the advances that are making hospitals less important. Any change in the health care system that saves money and makes patients healthier deserves to be celebrated.Ezekiel J. Emanuel is a vice provost at the University of Pennsylvania, the author of “Prescription for the Future” and a partner at Oak HC/FT, a health care investment company.",Opinion | Are Hospitals Becoming Obsolete?
9789,3793343,2018-02-25 23:44:10,"About TNWTNW SitesThe importance of having a socially conscious businessEverybody wants to make a difference, and if no one tried, the world would be a lot worse for the wear. Luckily, most people make an effort to exact change on whatever level they can and work toward making the world a better place.Individuals aren’t the only ones working towards change, of course. Businesses as a whole are beginning to implement programs through which they give back to their community, donate to those in need, work on preserving the environment, and so much more.What’s fueling this movement toward more socially conscious business? There are a lot of reasons it’s important for your business to be socially conscious (moral obligations aside), and these factors are behind the influx of socially responsible business models we are seeing today.One factor is the up-and-coming generations. Millennials are a large part of the workforce and, with social responsibility at the top of their list of important things, they are driving businesses to become more socially responsible, not only as consumers but as employees as well. Gen Z is also comprised of socially conscious consumers, and as these younger generations’ buying power steadily increases, we see a corresponding rise in business’ responsiveness to these demands.Despite the increasing focus on social consciousness and responsibility in business, though, we’re still a long way from where we should be. There are a number of reasons a business might resist implementing a social consciousness policy and plan of action, but in my experience, some of the main reasons are the cost, the time, and the lack of a sincere desire to do so.There are a lot of benefits to being socially responsible as a business. All Season Solar, for example, is one company that is not only providing solar energy options and helping the environment but also looks to using solar energy to help create American jobs. These trade opportunities are high-quality, well-paying jobs that mean something for the good of the world. And such examples of social consciousness don’t just benefit the environment and community; while the purpose of social consciousness is to help make a difference, there are also important benefits businesses themselves receive.One of the biggest benefits to businesses is loyal employees. Like I said, Millennials have social good at the top of their list when it comes to purchasing and employment, and when they find a company to work for that puts social responsibility high on the list of important things too, they feel a sense of satisfaction. Satisfied employees stick around, reducing costly turnover and training.What drives you? What is one of the biggest reasons you started your business? For me, one of the biggest motivations is making a difference. While I understand one person may not be able to save the world, I still work to make a difference in my industry, for my clients, and for my employees’ lives; my guess is that you do the same.Just like you, your employees want to feel they’re making a difference, and they want to contribute to a greater cause. Having a social consciousness program can help them do just that, giving them a greater sense of belonging and a deeper sense of purpose at work.Not only does social consciousness make your employees happy, but it also makes investors happy, too. With an increased focus on social consciousness and responsibility coming from consumers, investors are taking that into consideration when looking into a company in which they’re considering investing. If your business model finds a way to give back, sources materials ethically, or can find ways to “go green” in your operations, investors will take notice.As a consumer, I know I am willing to pay slightly more for products that come from companies that are socially conscious. If it were between regular coffee and fair-trade coffee that tasted just as good but was priced higher, I would still choose the coffee that is fair-trade. Fortunately, I’m not alone in making those kinds of purchasing choices; more and more, consumers are leveraging their collective buying power to demand social good.From an investment standpoint, it would be worth your time and money to invest in a company with a social consciousness and responsibility program. It’s not because it makes you feel all warm and fuzzy inside (that’s a bonus); it’s because consumers prefer to financially support socially conscious companies over competitors with no program in place.Speaking of competition – social responsibility is a great way for a business to set itself apart from its competitors. Since not all companies have adopted a social consciousness policy, especially in certain industries, a social good policy is a great way to stand out and attract a new customer base.Toms is a great example; they donate a pair of shoes to a child in need every time a pair of shoes is purchased. Toms’ policy not only acts on social consciousness, but it also sets them apart from their competition and makes for some great marketing campaigns. There aren’t many other popular shoe brands that have such a socially responsible policy, and the brand has skyrocketed to success because customers feel like they are doing good even as they do what they would have done anyway — buy themselves a new pair of shoes.Social consciousness is important to a business not only because of the benefits for society but also the benefits to the business itself. From creating American jobs to giving shoes to those in need, social consciousness can really make a difference, and from loyal employees to more interested investors, it can greatly benefit the business as well.What socially conscious efforts does your business make? What are the benefits you’ve seen?This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",The importance of having a socially conscious business
9790,3795742,2018-02-26 04:33:27,"About TNWTNW SitesNokia enters the Android One arena with its new 7 PlusAt MWC 2018, Nokia unveiled its first-ever Android One phone, the 7 Plus. And unlike some of its previous offerings from 2017, this actually sounds rather promising.The Android Oreo-based 7 plus features an aluminum unibody design, which houses a 6-inch full HD+ (2,160 x 1,080 pixels) 18:9 display between fairly slim bezels; a 16-megapixel shooter graces the top of the front fascia.On the rear, you’ll find a dual-camera system created in partnership with Zeiss: there’s a 12-megapixel wide camera paired with a 13-megapixel telephoto snapper, allowing for 2x optical zoom as we’ve seen on numerous other models.Under the hood, you’ll find a mid-range Snapdragon 660 processor, along with 4GB RAM, 64GB of onboard storage, a microSD slot for adding up to 256GB more, and a 3,800mAh battery. It charges via a USB-C port, and there’s also a 3.5mm audio jack.With the 7 plus, Nokia has finally brought its design up to date. And since this is an Android One phone, you can expect fairly timely updates to the OS, shortly after they roll out from Google.The handset will go on sale in April with a €399 (roughly $490) price tag, pitting it against the likes of some of Xiaomi, Motorola, and OnePlus’ better devices. That’s tough enough competition to test whether the old Nokia label still means something to phone buyers.",Nokia enters the Android One arena with its new 7 Plus
9791,3798076,2018-02-26 05:34:09,"TNW SitesLG’s V30S ThinQ is basically a clone of its predecessor with a smarter cameraEarlier this month, LG revealed that it planned to follow up its immensely capable V30 handset from 2017 with AI-powered camera tech. Now, it’s unveiled what you might call new wine in an old bottle.Unveiled at MWC 2018, the LG V30S ThinQ is almost indistinguishable from the V30, save for blue and platinum gray finishes, and upgrades to its RAM, storage, display, and camera software. That’s fine for the few people who just didn’t get around to buying a V30 last year, but it’s hard to understand why LG didn’t bother to make a new phone. Oh, and here’s the kicker: the AI camera app is coming to the original V30 too.The V30S ThinQ comes with 6GB RAM (a 2GB bump up) and up to 256GB of storage (double what the V30 offered), and Vision AI, which analyzes objects in the camera’s field of view and determines the ideal settings to get a good shot. For example, if your point the shooter at a plate of pasta, the camera will adjust to ‘food mode’, and increase the color temperature for warmer colors, and boost sharpening.The tech was developed in partnership with visual platform EyeEm, which claims that the AI was trained with more than 100 million images.There are a couple more features that sound interesting, but your mileage might vary depending on how much you care to use them. QLens analyzes real-world objects using your camera to help you find out where you can buy them – or related products – online. As Engadget noted, this is usually hit-or-miss, so I can’t say I’m dying to try it out.There’s also Voice AI, which works alongside Google Assistant to help you launch apps and change settings using voice commands.Ultimately, there’s certainly not enough in here to convince V30 fans to upgrade, but if the price is right, the V30S ThinQ could offer formidable competition to upcoming phones through 2018.",LG's V30S ThinQ is basically a clone of its predecessor with a smarter camera
9792,3800229,2018-02-26 06:11:12,"Flaky failures are the worst. In this particular investigation, which spanned twenty months, we suspected hardware failure, compiler bugs, linker bugs, and other possibilities. Jumping too quickly to blaming hardware or build tools is a classic mistake, but in this case the mistake was that we weren’t thinking big enough. Yes, there was a linker bug, but we were also lucky enough to have hit a Windows kernel bug which is triggered by linkers!In September of 2016 we started noticing random failures when building Chrome – 3 out of 200 builds of Chrome failed when protoc.exe, one of the executables that is part of the build, crashed with an access violation. That is, we would build protoc.exe, and then run it to generate header files for the next build stage, but it would crash instead.The developers who investigated knew immediately that something weird was happening but they couldn’t reproduce the bug locally so they were forced to make guesses. A couple of speculative fixes (reordering the tool’s arguments and adding explicit dependencies) were made, and the second fix seemed to work. The bug went away for a year.And then, a few days shy of its first birthday, the bug started happening again. A steady drumbeat of reports came in – ten separate bugs were merged into the master bug over the next few months, representing just a fraction of the crashes.Local reprosI joined the investigation when I hit the bug on my workstation. I ran the bad binary under a debugger and saw this assembly language in the debugger:Now we have a problem statement that we can reason about: why are large chunks of our code segment filled with zeroes?I deleted the binary and relinked it and found that the zeroes were replaced with a series of five-byte jmp instructions. The long array of zeroes was in an array of thunks, used by VC++’s incremental linker so that it can more easily move functions around. It seemed quite obvious that we were hitting a bug in incremental linking. Incremental linking is an important build-time optimization for huge binaries like chrome.dll, but for tiny binaries like protoc.exe it is irrelevant, so the fix was obvious: disable incremental linking for the tiny binaries used in the build.It turned out that this fix did work around an incremental linking bug, but it was not the bug we were looking for.I then ignored the bug until I hit it on my workstation two weeks later. My fix had not worked. And, this time the array of zeroes was in a function, instead of in the incremental linking jump table.I was still assuming that we were dealing with a linker bug so when another two weeks later I hit the problem again I was confused. I was confused because I was not using Microsoft’s linker anymore. I had switched to using lld-link (use_lld=true in my gn args). In fact, when the bug first hit we had been using the VC++ compiler and linker and I’d just hit it with the clang compiler and linker. If switching out your entire toolchain doesn’t fix a bug then it’s clearly not a toolchain bug – mass hysteria was starting to seem like the best explanation.Science!Up to this point I had been hitting this bug randomly. I was doing a lot of builds because I was doing build-speed investigations and these crashes were interfering with my ability to do measurements. It’s frustrating to leave your computer running tests overnight only to have crashes pollute the results. I decided it was time to try science.Instead of doing a dozen builds in a night to test a new build optimization I changed my script to just build Chrome in a loop until it failed. With jumbodistributed builds and a minimal level of symbols I can, on a good day, build Chrome a dozen times in an hour. Even a rare and flaky bug like this one starts happening every single night when you do that. So do other bugs (zombies!) but that’s a different story.And then, I got lucky. I logged on to my computer in the morning, saw that genmodule.exe had crashed overnight (the crashing binary varied), and decided to run it again, to get a live crash instead of looking at crash dumps. And it didn’t crash.The crash dump (I have Windows Error Reporting configured to save local crash dumps, all Windows developers should do this) showed lots of all-zero instructions in the critical path. It was not possible for this binary to run correctly. I ran genmodule.exe under the debugger and halted on the function that had previously crashed – that had previously been all zeroes – and it was fine.Apologies for the strong language, and women and children might want to skip the rest of this paragraph, but WTF?!?I then loaded the crash dump into windbg and typed “!chkimg”. This command compares the code bytes in the crash dump (some of them are saved in the crash dump, just in case) against those on disk. This is helpful when a crash is caused by bad RAM or bad patching and it will sometimes report that a few dozen bytes have been changed. In this case it said that 9322 bytes in the code in the crash dump were wrong. Huh!Now we have a new problem statement: why are we not running the code that the linker wrote to the file?This was starting to look like a Windows file cache bug. It looked like the Windows loader was pulling in pages full of zeroes instead of the pages that we had just written. Maybe something to do with multi-socket coherency of the disk and cache or ???My coworker Zach made the vital suggestion that I run the sysinternals sync command after linking binaries. I resisted at first because the sync command is quite heavyweight and requires administrative privileges, but eventually I ran a weekend long test where I built Chrome from scratch over 1,000 times, as admin, with various mitigations after running the linker:Normal build: 3.5% failure rate7-second sleep after linking exes: 2% failure ratesync.exe after linking exes: 0% failure rateHuzzah! Running sync.exe was not a feasible fix, but it was a proof of concept. The next step was a custom C++ program that opened the just-linked exe and called FlushFileBuffers on it. This is much lighter weight and doesn’t require administrative privileges and this also stopped the bug from happening. The final step was to convert this into Python, land the change, and then make my favorite under-appreciated tweet:Later that day – before I’d had a chance to file an official bug report – I got an email from Mehmet, an ex-coworker at Microsoft, basically saying “Hey, how’s things? What’s this I hear about a kernel bug?”I shared my results (the crash dumps are quite convincing) and my methodology. They were unable to reproduce the bug – probably due to not being able to build Chrome as many times per hour as I can. But, they helped me enable circular-buffer ETW tracing, rigged to save the trace buffers on a build failure. After some back-and-forth I managed to record a trace which contained enough information for them to understand the bug.The underlying bug is that if a program writes a PE file (EXE or DLL) using memory mapped file I/O and if that program is then immediately executed (or loaded with LoadLibrary or LoadLibraryEx), and if the system is under very heavy disk I/O load, then a necessary file-buffer flush may fail. This is very rare and can realistically only happen on build machines, and even then only on monster 24-core machines like I use. They confirmed that my fix should mitigate the bug (I’d already noted that it had allowed ~600 clean builds in a row), and promised to create a proper fix in Windows.Play along at homeYou probably won’t be able to reproduce this bug but if you want to see an example crash dump you can find one (and the .exe and .pdb files) on github. You can load them into Visual Studio and see all the zero bytes in the disassembly, or load them into windbg to run !chkimg and see the !chkimg errors:Investigation complications1) Building Chrome very quickly causes CcmExec.exe to leak process handles. Each build can leak up to 1,600 process handles and about 100 MB. That becomes a problem when you do 300+ builds in a weekend – bye bye to ~32 GB of RAM, consumed by zombies. I now run a loop that periodically kills CcmExec.exe to mitigate this, and Microsoft is working on a fix.2) Most Windows developers have seen 0xC0000005 enough times to remember that it means Access Violation – it means that your program dereferenced memory that it should not have, or in a way that it should not have. But how many Windows programmers recognize the error codes 3221225477 or -1073741819? It turns out that these are the same value, printed as unsigned or signed decimal. But, not surprisingly, when developers see a number around negative one billion their eyes glaze over and the numbers all start to look the same. So when some of the crashes returned error code -1073740791 the difference was either not noticed, or was ignored.3) That’s a shame because it turns out that there were two bugs. crbug.com/644525 is the Chromium bug for investigating what turned out to be this kernel bug. But, once I landed a workaround for that bug and reenabled incremental linking we started hitting different crashes – crbug.com/812421. Some developers were hitting error code –1073740791 which is 0xC0000409 which is STATUS_STACK_BUFFER_OVERRUN. I never saw this crash myself but I asked for a crash dump (I was terrified that crbug.com/644525 had returned) from a coworker and saw that ntdll.dll!RtlpHandleInvalidUserCallTarget was calling RtlFailFast2. I recognized this signature and knew that it had nothing to do with buffer overruns. It’s a Control Flow Guard violation, meaning that the OS thinks that your program is being exploited by bad people to do an illegal indirect function call.It appears that if you use /incremental with /cfg then the Control Flow Guard information isn’t always updated during incremental linking. The simple fix was to update our build configurations to never use /incremental and /cfg at the same time – they aren’t a useful combination anyway.EpilogueWe still don’t know what caused this bug to start showing up in the first place – maybe our switch to gn changed the ordering of build steps to make us more vulnerable?We also don’t know why the bug disappeared for a year. Was the original bug something unrelated that was fixed by this change? Or did we just get lucky or oblivious?Either way, whether we fixed two or three separate bugs, Chrome’s builds are much more reliable now and I can go back to doing build-performance testing without hitting failures.The Chrome workaround is 100% reliable, and both lld-link.exe and Microsoft’s link.exe will be adding FlushFileBuffers calls as mitigations. If you work on a tool that creates binaries (Rust? I filed an internal bug for Go) using memory mapped files you should consider adding a FlushFileBuffers call just before closing the file. This bug shows up from Server 2008 R2 (Windows 7) up to the latest stable build of Windows 10 and OS fixes will take a while to propagate so you might as well be careful.AdvertisementsShare this:Like this:LikeLoading...RelatedAbout brucedawsonI'm a programmer, working for Google, focusing on optimization and reliability. Nothing's more fun than making code run 10x faster. Unless it's eliminating large numbers of bugs. I also unicycle. And play (ice) hockey. And sled hockey. And juggle. And worry about whether this blog should have been called randomutf-8.",Compiler bug? Linker bug? Windows Kernel bug.
9793,3800470,2018-02-26 08:11:45,"About TNWTNW SitesSony finally goes bezel-less with the Xperia XZ2 seriesAt long last, Sony has ditched the bezels on its Xperia lineup with the launch of its latest handsets at MWC: the Xperia XZ2 and XZ2 Compact.While they’re practically identical on the inside, the XZ2 features a 5.7-inch display, while the XZ2 Compact sports a 5-inch screen (both top out at full HD resolution and boast HDR capability). An X-Reality processor lets you upscale standard content to HDR, which should allow for more vibrant images and improved contrast.Sony Xperia XZ2 CompactQualcomm’s new-for-2018 Snapdragon 845 chip powers both dual-SIM Android 8.0 models – along with 4GB RAM and 64GB of space – and you’ll find the same 19-megapixel camera on their rear panels with improved low-light photography capabilities. Sony claims these are the first mobile devices to shoot 4K HDR video; it’s also upgraded the 960fps super-slo-mo shooting to full HD resolution. Oh, and you can capture 3D models of real-world objects and post them directly to Facebook.There are a few other differences in appearance between the two: the XZ2’s aluminum body features 3D glass on both the front and back; the the XZ2 Compact is coated with a more modest non-scratch polycarbonate finish. You’ll be able to choose from four different colors on both models.Other niceties include a feature that syncs up audio with the vibration motor for a more immersive listening experience, water resistance, and fast wireless charging on the larger XZ2.It’ll be interesting to see if this gives Sony a chance to battle heavy hitters like Samsung and LG this year. Both the XZ2 and XZ2 Compact will ship next month; pricing hasn’t been announced yet.",Sony finally goes bezel-less with the Xperia XZ2 and Xperia XZ2 Compact
9794,3800471,2018-02-26 06:26:25,"About TNWTNW SitesNokia brings back the 8110 slide phone from The MatrixBezel-less screens on today’s mobile devices are great and all, but there’s just something so damned nifty about answering a call using a physical slider, like Neo did in The Matrix way back in 1999. Now, Nokia is hoping to bring the magic back by reviving the iconic 8110 from the film in the form of a 4G feature phone.The 8110 4G features the curved banana body from two decades ago, but it’s dropped the antenna and is also a fair bit smaller. As with the original, you can slide the keypad cover down to receive a call, and slide it back up to end the call.There’s also a 2.4-inch color screen (320 x 240 pixels) with which to run apps on KaiOS (which was forked from Firefox OS), including a take on the classic Snake game. The phone is powered by a Qualcomm 205 chip, with 512MB RAM, and also comes with a 2-megapixel camera.While you might not use this device a whole lot to run mobile apps, the 4G modem might come in handy for turning the phone into a hotspot so you can keep your laptop or tablet connected to the internet.",Nokia brings back the 8110 slide phone from the Matrix
9795,3802804,2018-02-26 09:24:23,"TNW SitesWhy you should not store your cryptos on an exchange (but use a wallet instead)On a Friday morning in late January, the users of Coincheck, a Japan-based cryptocurrency exchange, logged in to find their wallets empty of NEM tokens, a cryptocurrency specialized for payments and other financial services.But this wasn’t a temporary server outage, a common problem most exchanges have been grappling with since cryptocurrencies went on a crazy price surge in 2017. Earlier that day, hackers had allegedly compromised Coincheck’s servers and stolen 500 million NEMs (worth $400 million at the time the incident happened), the cryptocurrency equivalent of robbing a bank.Cryptocurrency exchanges—and by extension their users—have been falling victim to hackers for years. In 2014, following a big heist at Mt. Gox, another Japanese exchange, the price of bitcoin took a deep plunge. Bitstamp, Bitfinex and Etherdelta are the names of just some of the other crypto-exchanges that have been compromised in the years that followed.Yet many users make the mistake of storing all their cryptocurrencies in exchanges while there are alternatives that can store your digital fortune without compromising your security. Here’s what you need to know about different cryptocurrency wallet types and how they can serve your different needs.Cryptocurrency exchanges: convenient but unsafeBlockchain, the distributed ledger technology underlying cryptocurrencies, was designed to fend off cyberattacks. And that stands true only as long as you stay true to its basic principles.Every cryptocurrency address on the blockchain is tied to a pair of private and public encryption keys. The public key allows other users to send money to that address, while the private key enables the owner of the address to send payments to other addresses. This means that, as the holder of bitcoins or other digital currencies, your funds are secure only if you keep your private key, well… private.Cryptocurrency exchanges store private keys on behalf of their users. In exchange they enable their users to easily trade between fiat or digital currencies. But this also burdens them with the responsibility of securing all those cryptocurrency accounts and the private keys that go with them.To be fair, most exchanges have dedicated security teams and do a decent job of protecting their stash of private keys against cybercriminals. However, the mere fact that they store thousands and millions of private keys make them attractive targets for motivated and well-resourced hackers.As the saying goes, cybersecurity experts must win every battle—hackers only need to win one. In the case of cryptocurrency exchanges, a single loss can lead to the disappearance of hundreds of millions of dollars.You should also take note that since the cryptocurrency and token sale industry is still largely unregulated, there is plenty of room for fraudsters and scammers to make a quick buck, especially if they hold control of the private keys of unsuspecting users in a bogus exchange.Desktop wallets: the inconvenient alternativeA secure alternative to cryptocurrency exchanges are wallet applications you can install on your own computer. Desktop wallets store and manage private keys on your computer, which means you don’t need to worry about your secrets sitting in a server located thousands of miles away and being surrounded by hordes of crypto-hungry hackers.However, desktop wallets come with their own trade-offs. Many desktop wallets require you to download the entire blockchain of the cryptocurrency they provide access to. In the case of bitcoin, the size of the blockchain is over 150GB at the time of the writing. This limits their use to desktop and laptop PCs, which makes them impractical if you want to send crypto payments from your smartphone or tablet.Desktop wallets can also become compromised if you don’t have sound IT and security practices. There are plenty of malware out there that can either steal the keys or destroy them forever. And in case your computer runs into technical problems or you accidentally dispose of your hard drive, then you can say goodbye to your fortune.Online wallets: the sweet spotOnline wallets are the sweet spot between exchanges and desktop wallets, giving you the perfect balance between convenience and security.On the one hand, online wallets are accessible through browser applications and mobile apps and don’t require you to download an entire blockchain to send and receive cryptocurrency payments, making them easier to use than desktop wallets and accessible on all your devices. On the other hand, they don’t store private keys on their servers, making them impervious to data breaches and more secure than online exchanges.However, online wallets come with their own considerations. They won’t provide you with the conversion of crypto and fiat currencies that you’ll find on crypto exchanges. Also, some online wallets do store private keys on their servers; make sure you avoid those.Final verdictWhere to best store your cryptocurrencies depends on your trading needs, of course. You’ll probably still need an exchange account if you’re into margin trading or if you want to convert your funds. But take note that you shouldn’t use exchanges for the storage of large amounts of cryptocurrency.If you’re an IT and security wonk, and don’t mind processing all your transactions from a laptop or desktop PC, a desktop or hardware wallet will be a secure and convenient option.For most users, however, online wallets are the most convenient option. You’ll have control of your own privacy and security while also being able to easily receive and send cryptocurrencies for different purposes.BTC.com offers an online wallet for web, iOS and Android. This wallet never stores your private keys, so you are the only one with access to your coins. Get your free wallet to buy, send, receive and store Bitcoin and Bitcoin Cash in a simple and secure way.",Why you should not store your cryptos on an exchange (but use a wallet instead)
9796,3804862,2018-02-26 07:45:00,"The Xperia XZ2 is Sony's fastest, strangest flagship phone yetAnd as usual, it has a tiny, equally powerful sibling.Sony has longed to find lasting success in the smartphone market, but none of the devices it released in the past few years have done the job. It's clearly time for a change, and Sony knows that — that's why the new Xperia XZ2 and XZ2 Compact are notable steps away from the company's smartphone norm.They're fast, fascinating and pretty (to me, anyway), and it seems obvious that Sony wanted to strike a balance in these devices between immersive media consumption and powerful content creation. After a little bit of hands-on time with the XZ2 and its little sibling, I don't think Sony hasn't gotten everything right, but at least these things are pleasantly strange.The basicsThe XZ2 is one of many smartphones we'll see this year that use Qualcomm's Snapdragon 845 chipset, paired here with 4GB of RAM and 64GB of internal storage. (Most versions of this phone will take microSD cards as large as 400GB, but some will use a dual SIM setup instead.) In our limited time together, the XZ2 felt as fast as the Galaxy S9 when launching apps and frantically multitasking, and it's nice to see that Sony's custom interface doesn't bog things down noticeably. Curiously, Sony says its implementation of Qualcomm's X20 modem features more RF antennas to help it reach peak download speeds as high as 1.2Gbps.The XZ2 also runs Android 8.0 Oreo, and you'll take in all of those software flourishes on a 5.7-inch Full HD+ HDR display. Sony hasn't confirmed why it didn't use a higher resolution screen, but considering its 3180mAh battery, concern over power consumption is a safe bet. That screen also has one fascinating new trick we haven't had much chance to test yet. Thanks to Sony's XReality engine, the XZ2 will up-convert standard SDR videos into HDR on the fly, a trick that's been a part of televisions for a few years now already. Given how nice the screen is to look at under normal circumstances, consider us cautiously optimistic about how well this actually works.Look and feelIf you've picked up a Sony phone in the past few years, there's a very good chance it looked and felt like many of the devices that came before it. That's because Sony has been using the same aesthetic — ""OmniBalance,"" they call it — since the launch of the Xperia Z more than five years ago. Well, no more. For its new flagships, Sony has embraced a curvier, arguably more organic approach you can also spot hints of in the company's PlayStation 4 controller and, erm, its robot dog Aibo.Gallery: Sony Xperia XZ2 and XZ2 Compact | 26 PhotosAs far as I'm concerned, it's a big improvement over Sony's older, flatter design. The curved Gorilla Glass 5 back helps it sit nicely in the palm of your hand, and the company's decision to use a taller, narrower 18:9 display makes the XZ2's 5.7-inch screen feel very manageable. Make no mistake, though. Sony might've gone with an 18:9 display, but it hasn't gone as far as companies like Samsung in trimming the bezels around that screen, and I'm sure not everyone will find the look anything to write home about. If nothing else, you'll find some small, helpful changes here. The rear-mounted fingerprint sensor sits low on the phone's back, which made it a little easier for me to access without having to stretch for it. And at long last, Sony aligned the camera near the phone's center rather than off in a corner, so you're less likely to accidentally dangle a finger in front of it when trying to shoot stills.Chris Velazco/EngadgetThe cameraImpressive Sony-made camera sensors are almost impossible to avoid when smartphone shopping, but Sony seems to be playing it safe with the XZ2. Much like its predecessor, the XZ2 packs a 19MP Motion Eye sensor around back, and yes, there's still only one of them. It'll be a little while before you can nab a Sony phone with a dual camera, but the company has suggested we'll hear a little more about those plans shortly. We'll be waiting. In the meantime, though, the results I squeezed out of the XZ2's camera were mostly impressive: lots of bright colors and sharp detail appeared, even with non-final software and drab New York weather throwing a wrench into things.The actual sensor Sony used might not have changed, but it has picked up a few new tricks. Smartphone videographers can now shoot 960FPS, super-slow-motion video in full HD (up from 720p), and since there's a Snapdragon 845 tucked away inside, the XZ2 can capture 4K HDR video as well.Meanwhile, a more modest 5-megapixel camera sits above the screen, but it captures more than standard selfies. The original Xperia XZ came with a 3D creator mode that let you ""scan"" objects with the phone's main camera to build 3D models for sharing on Facebook or printing. Sure, it was gimmicky; it was also a classic case of Sony being weird for reasons that are never really clear. In any case, the feature had one notable shortcoming: it was fine for scanning objects in front of you, but less than ideal for scanning your own head. Sony has finally fixed that tragic oversight — those scans can now be created using the front-facing camera, and while the process can be awkward (I was prompted to remove my glasses and do everything blind), the results can be surprisingly impressive. These 3D scans also get bonus points for being slightly less creepy than the Galaxy S9's AR emoji.Chris Velazco/EngadgetRumbling audioSony paid a lot of attention to the XZ2's audio performance, too. Mostly, the work paid off: the phone supports high-resolution audio, and a set of S Force stereo speakers pump up the jams with more power than I expected. (All told, the XZ2 is 20 percent louder than its immediate forebear.) So far, so good, right? Well, things get strange pretty quickly.As some leaks suggested, the XZ2 has a significantly larger haptic actuator inside of it. On the most basic level, that means you're more likely to notice the vibration of a notification rolling in. Sony took things a bit further by whipping up an algorithm to turn that actuator into a sort of video-gamey rumble feature. The idea is that an algorithm processes the audio data of whatever's playing and vibrates the phone when appropriate. If you're playing a game like Angry Birds, the effect can be mildly cool — I felt some haptic jolts as I pulled a bird back in its slingshot, released it and watched it crash into a tower of pigs. Listening to music and feeling the phone rumble in time with the beat, however, was occasionally neat but mostly just odd. At least in its current form, this feature isn't particularly helpful or fun, but I'll reserve final judgment until I get to test a retail-ready model.Chris Velazco/EngadgetWhat about the Compact?As mentioned, the XZ2 has a diminutive sibling: the XZ2 Compact. It packs just about every feature that makes the XZ2 so capable and squeezes them into a smaller, curvaceous polycarbonate frame that feels fantastic to hold onto. There are a few notable absences, like wireless charging and that weirdo haptic rumble features. Sony obviously had to go with a smaller screen, too — in this case, a 5-inch Full HD+ HDR panel.Overall, my eyes couldn't make out much difference in quality between this screen and the XZ's bigger one, but there's a caveat here: smaller 18:9 panels like this one mean keyboards can feel especially cramped. Most of my test texts came out looking like garbage because my chubby thumbs weren't used to this layout. Beyond that though, it shares every major feature with its big brother, and it's awfully nice to know I could get full flagship power in a package this small. My biggest concern right now is just how well the Compact's 2870mAh battery will stand up to the needs of a Snapdragon 845, but we'll have to wait and see how that turns out. Those kinds of trade-offs are unavoidable for smaller phones, but here's hoping the final experience is worth it.Update: When Sony's new Xperias go on sale next month, the unlocked XZ2 and XZ2 Compact will be available in the UK for £699 and £549, respectively. We're still trying to track down US pricing.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.","The Xperia XZ2 is Sony's fastest, strangest flagship phone yet"
9797,3804863,2018-02-26 08:00:00,"Lenovo's Yoga 730 is a cheaper 2-in-1 with Alexa supportThe company also announced the Flex 14 at Mobile World Congress.Mobile World Congress, for obvious reasons, isn't really known for its laptop announcements. Lenovo is bucking that trend today, however, with three mid-range convertibles sporting the Yoga name. The most exciting is the Yoga 730, a sleek 2-in-1 that sits below the company's flagship Thinkpad X1 line and the popular Yoga 920. It will be available in 13- and 15-inch variations, sporting similar designs but slightly different upgrade options. They'll be joined by the Flex 14 (which will be marketed as the Yoga 530 outside of North America) a cheaper alternative with a near-identical form factor.Gallery: Lenovo Yoga 730 and Flex 14 | 23 PhotosFirst, the Yoga 730. We're yet to be told the base specs, but we know they'll go up to an eighth-generation Intel Core i7 processor and 16GB of RAM. The smaller version will offer up to 512GB of SSD storage while its larger sibling ramps up to a beefy 1TB. On the 13-inch laptop, you'll be stuck with integrated graphics (boo) but the 15-inch model can be specced up to a gaming-friendly Nvidia GTX 1050. Both come with built-in JBL speakers and the option of a 1080p or 4K touchscreen — perfect for mindless finger doodles and artsy brushstrokes with Lenovo's optional Active Pen 2 stylus.It's a Yoga, so of course you can flip the screen over and use it like a tablet. I still think it's weird to hold a laptop with the keyboard pressed into your palm, but hey, the option is there if you really dig the 'clipboard' form factor. Both are relatively light — the 13-inch model weighs two and a half pounds, while the 15-inch version comes in at 1.89kg. The display bezels are also fairly slim and I like that the webcam is placed up top, rather than the lower left-hand corner (where you're stuck with an up-the-nose shot).Like the Thinkpad X1 range, the Yoga 730 will support both Cortana and Amazon's Alexa assistant. AI-hopping isn't ideal, but until the smart speaker wars shake out it makes sense for Lenovo to offer multiple options (the company is also backing the Google Assistant with its Smart Display.) Both the 13-inch and 15-inch models will come with far-field microphones that can pick up your voice from across the room. So in theory, you could ask Cortana for your schedule and then book a time-appropriate Uber through Alexa. Or search for an article and order a pizza while you read.The 13-inch and 15-inch Yoga 730 will start at $880 and $900 respectively. They cost considerably more than the $600 Flex 14, which will ship with an Intel Core i3 processor and a 1080p touchscreen. You can upgrade to an eighth-generation Intel Core i7 processor, a 512GB SSD, 16GB of DDR RAM and an Nvidia GeForce MX130 graphics card, but these will cost you extra. At first blush, Lenovo's new laptops seem like decent if predictable performers for people with a sub-$1,000 budget. They all look the part and the Alexa support is nice for people who own a bunch of Echo speakers and compatible smart home appliances. Part of me misses the watchband-inspired hinge found on the Yoga 920, but hey — you can't have it all.Nick is a technology journalist at Engadget, covering video games, internet culture and anything else that takes his fancy. Before joining Engadget he was a reporter at The Next Web. He has a degree in multimedia journalism from Bournemouth University and an NCTJ certificate. He lives in Greenwich, London.",Lenovo's Yoga 730 is a cheaper 2-in-1 with Alexa support
9798,3804958,2018-02-24 07:31:27,"Fail-slow at scale: evidence of hardware performance faults in large production systemsThe first thing that strikes you about this paper is the long list of authors from multiple different establishments. That’s because it’s actually a study of 101 different fail-slow hardware incidents collected across large-scale cluster deployments in 12 different institutions.Last year we looked at ‘Gray failure: the Achilles’ heel of cloud-scale systems,’ in which partial failures and system components running in degraded mode were revealed to be involved in many production incidents, and difficult to detect and diagnose with today’s systems. In ‘Fail-slow at scale’ we have a catalog of ways in which hardware has been shown to run in degraded mode, causing all sorts of problems. Fail-slow hardware, like file system faults and network partitions, should be added to the list of things that occur much more frequently than we’d like to think, and cause a lot of damage when they do occur.This paper highlights an under-studied “new” failure type: fail-slow hardware, hardware that is still running and functional but in a degraded mode, slower than its expected performance. We found that all major hardware components can exhibit fail-slow faults.These faults may be comparatively rare, but they can trigger chains of cascading events, resulting visible failures some distance from the original cause. Perhaps because of this, and because the partial failures are often deliberately masked in the hardware without enough metrics to surface them, troubleshooting fail-slow hardware based incidents can be very time-consuming:The fail-slow hardware incidents in our report took hours or even months to detect (pinpoint). More specifically, 1% of the cases are detected in minutes, 13% in hours, 13% in days, 11% in weeks, and 17% in months (and unknown time in 45%).One of the conclusions is that many modern deployed systems do not anticipate this failure mode. The authors offer a set of suggestions to hardware vendors, operators, and systems designers to improve the state of affairs.Guidelines for system designersMake implicit error masking explicit (this is the software equivalent of the advice given above to hardware vendors). “Software systems should not just silently work around fail-slow hardware, but need to expose enough information to help troubleshooting.”Consider converting fail-slow faults to fail-stop. One example given is skipping a caching layer altogether when SSDs are misbehaving. Another example is to fail-stop after sufficient recurrence of fail-slow incidents (e.g., tripping a circuit breaker after too many retries). Doing this requires an ability to shut-off devices at a fine-grained level.Use fault-injection to explore fail-slow scenarios:… we strongly believe that injecting root causes reported in this paper will reveal many flaws in existing systems. Furthermore, all forms of fail-slow hardware such as slow NIC’s, switches, disks, SSD, NVDIMM, and CPUs need to be exercised as they lead to differrent symptoms. The challenge is then to build future systems that enable various fail-slow behaviors to be injected easily.Let’s now take a look at some of the characteristics and causes of fail-slow incidents.Root causesThere can be a variety of root causes — these can be internal to the hardware component (for example, device wear or firmware issues), or external (for example, configuration, environment, temperature or power issues).Section 4 in the paper describes the myriad ways that SSDs, Disks, Memory, and Network components can suffer from internal failures. And even when a component would otherwise function just fine, section 5 catalogues a long list of external factors that have been known to cause partial failures:Temperature: clogged air filters, cold-air-under-floor systems, broken fans, buggy fan firmware, poor assembly/design such that heat-sinks are ineffective. For example, “there was a case where a fan in a compute node stopped working, and to compensate this failing fan, fans in other compute nods started to operate at their maximal speed, which then generated heavy noise and vibration that degraded the disk performance.”Fault conversionFail-stops in some components can cause others to exhibit fail-slow behaviour:For example, a dead power supply throttled the CPUs by 50% as the backup supply did not deliver enough power; a single bad disk exhausted the entire RAID card’s performance; and a vendor’s buggy firmware made a batch of SSDs stop for seconds, disabling the flash cache layer and making the entire storage stack slow.Likewise frequent transient failures can result in fail-slow behaviour due to the overheads of error masking (retries, repairs, etc..). When errors ceases to be rare, the masking overhead becomes the normal case performance.Partial internal failures whereby only some part of the device is unusable can also lead to fail-slow behaviour.SymptomsFail-slow conditions can manifest in several different ways.The slowdown may be permanent, or the device may fluctuate between normal and degraded performance. Some parts of the device may continue to offer full performance while others are degraded, and in other situations a device may periodically reboot itself, leading to periods of complete unavailability.Cascading failures…between the actual root cause and the hardware’s fail-slow symptom, there is a chain of cascading root causes… the fail-slow symptom then creates cascading impacts to the high-level software stack, and potentially to the entire cluster.We’ve already seen the example of the failing fan leading to a cascade of events ending in degraded disk performance. Another example is a faulty sensor in a motherboard reporting a false value to the OS, thus making the CPUs run slower in energy saving mode. Fail-slow hardware problems can cascade into the software stack – for example, in an HBase deployment a memory card at 25% of normal speed caused backlogs, out-of-memory errors, and crashes.The last wordThe paper is packed with anecdotes that I didn’t have space to cover here. It’s well worth a read to get a finer appreciation of the kinds of hazards that await when building and operating systems at scale.We believe fail-slow hardware is a fundamentally harder problem to solve [than fail-stop]. It is very hard to distinguish such cases from ones that are caused by software performance issues. It is also evident that many modern, advanced deployed systems do not anticipate this failure mode. We hope that our study can influence vendors, operators, and system-designers to treat fail-slow hardware as a separate class of failures and start addressing them more robustly in future system.",Fail-slow at scale: evidence of hardware performance faults in large production systems
9799,3804962,2018-02-21 00:00:00,"Night VisionThe forgotten theory of dreams that inspired Vladimir NabokovFebruary 21, 2018Illustration by Armando VeveWhen I was 16, I read “First Love,” by Vladimir Nabokov. He describes a trip on the Nord Express train, which went from St. Petersburg to Paris. “I would put myself to sleep by the simple act of identifying myself with the engine driver,” Nabokov wrote. And then comes a dream. “In my sleep, I would see something totally different—a glass marble rolling under a grand piano or a toy engine lying on its side with its wheels still working gamely.” That glass marble rolling under the grand piano made me want to be a writer.Nabokov is one of the great dreamers, the great lost-time recapturers, of the twentieth century, and Speak, Memory, his autobiography, where the marble passage appears in book form, is, I think, his masterpiece. Another section of the book, “Portrait of My Uncle,” has a strikingly beautiful ending: “The mirror brims with brightness; a bumblebee has entered the room and bumps against the ceiling. Everything is as it should be, nothing will ever change, nobody will ever die.” This is Nabokov at his best: full of love and heartbreak—not arrogant and summarily dismissive, not weirdly transfixed as he sometimes was by skin blemishes (wens, warts, and hairy moles)—a man who, though he had lost his country, was recreating it now as if at a picnic in a park on the family estate near St. Petersburg, unfurling a blanket of hyperrealistic description on which he set out the long-lost but freshly burnished soup spoons and ladles of his Russian childhood, a “stereoscopic dreamland” wherein certain specific sunsets and windowscapes and birch-dappled garden pathways still existed and would always exist, rendering the terrible dislocations of totalitarian violence irrelevant.“I have ransacked my oldest dreams for keys and clues,” Nabokov wrote in this autobiography, and to good purpose: In the long paragraphs of Speak, Memory, every brimming mirror-flash glows eternally, and no spiraling subparticle of one’s life is ever truly lost. “I confess I do not believe in time,” Nabokov wrote. “I like to fold my magic carpet, after use, in such a way as to superimpose one part of the pattern upon another.”And here’s a funny thing about that memoir. Speak, Memory was written, so it seems, under the influence of an aeronautical engineer and avid fly fisherman named John W. Dunne. Beginning in the 1920s, Dunne set the literary world on fire with a now more or less forgotten theory of dreams that he termed “serialism.” Actually Dunne’s serialism offered three theories bundled into one—one big, clock-melting, brain-squashing chimichanga of pseudoscientific parapsychology—a theory of time, a theory of immortality, and a theory that dreams could predict the future. Dunne’s book, published in 1927, was called An Experiment With Time, and it went into several editions. “I find it a fantastically interesting book,” wrote H.G. Wells in a huge article in The New York Times. Yeats, Joyce, and Walter de la Mare brooded over its implications, and T.S. Eliot’s publishing firm, Faber, brought the book out in paperback in 1934, right about the time when Eliot was writing “Burnt Norton,” all about how time present is contained in time past and time future, and vice versa.In November 1964, Nabokov noted “curious features” of his dreams.Estate of Dmitri Nabokov/ Princeton University PressDunne’s Experiment seems to have become one of the secret wellsprings, or wormholes, of twentieth-century literature. J.B. Priestley believed that An Experiment With Time was “one of the most curious and perhaps most important books of the age,” and he built several plays around it. C.E.M. Joad, the philosopher and radio personality, said of the book: “It can be recommended to everybody who wishes to learn how to anticipate his own future.” C.S. Lewis wrote a short story, “The Dark Tower,” using Dunne’s ideas. J.R.R. Tolkien found the book helpful as he imagined Middle Earth’s elven dreamtime. Agatha Christie wrote that it gave her a “truer knowledge of serenity than I had ever obtained before.” “Everybody in England is talking about J.W. Dunne, the man who made dreams popular,” reported a newspaper columnist in 1935, though he warned that the innumerable geometrical charts would drive the reader “loco.” Robert Heinlein cited Dunne’s theory in his novella “Elsewhen” in 1941. In 1940, Jorge Luis Borges reviewed the book. “Dunne assures us that in death we will finally learn how to handle eternity,” Borges wrote. “He states that the future, with its details and vicissitudes, already exists.” Dunne brought out several follow-up books to An Experiment With Time. One of them, published in 1940, had a memorable title: Nothing Dies.Nothing dies, nobody dies, nobody will ever die. Dunne, a slight man with a “good big head,” according to Priestley, believed that everything had already happened and that everybody was immortal—that time was a paradise of somehow slightly spongy simultaneity that was accessible, albeit imperfectly, via a close study, every waking morning, of one’s dreams.This Aeronautical paraphilosopher developed his theory slowly, over decades. In 1899, the year that Nabokov was born, Dunne dreamed that he was arguing with a waiter about whether it was four-thirty in the afternoon. When he woke, he discovered that his watch had stopped at the very time that he had dreamed that it stopped, four-thirty. Was it a case of clairvoyance? He wasn’t sure. In another dream, he met three weather-beaten men dressed in ragged khaki; one of them told him he’d nearly died of yellow fever in the Sudan. That same morning, he read in The Daily Telegraph that three men had died on an expedition that had just arrived in Khartoum. Hmm, strange.INSOMNIAC DREAMS: EXPERIMENTS WITH TIME BY VLADIMIR NABOKOV compiled, edited, and with commentaries by Gennady Barabtarlo Princeton University Press, 224 pp., $24.95In 1902, Dunne dreamed of an island that was spouting vapor. “Good Lord,” he said to himself in the dream, “the whole thing is going to blow up!” He opened the Telegraph and saw the news of the explosion of Mount Pelée in Martinique. More quasi-prophetic dreams followed—about 20 of them in all. In 1913, he had a dream in which a train went off the tracks near the Firth of Forth in Scotland. Some months later, the Flying Scotsman leapt the track 15 miles north of the Forth Bridge. “I was suffering,” Dunne concluded, “from some extraordinary fault in my relation to reality, something so uniquely wrong that it compelled me to perceive, at rare intervals, large blocks of otherwise perfectly normal personal experience displaced from their proper positions in Time.” Dunne was, he believed, dreaming the future.In 1924, he produced a book on the making of semitransparent fishing lures, lures that looked to a fish as if the sun above were shining attractively through them. Then in 1927 came his bombshell, An Experiment With Time—which included dozens of detailed dream descriptions, plus an exceedingly complicated and in parts all-but-unreadable explanation for what he claimed was going on in his good big head, accompanied by a number of elaborate line drawings. His dream prophecies were not, he insisted, the result of voices from the dead or instances of occult or astral communication. No, his dreams were examples of genuine serial “precognition,” arising from his existence as an immortal soul situated as a point of consciousness in a multidimensional, post-relativistic, somehow infinitely regressive and/or nestingly Chinese-boxed field of Time. (Time always has a capital “t” in Dunne’s world.) Here is a sample of Dunne’s temporal geometry:We conclude stage 2, then, by fitting an arrowhead to Time 2 in the dimension-indicator of Fig. 8, in order to show that GH is a field of presentation moving up Time 2. The motion of field I along Time I is now recovered. For, as GH moves up the diagram, the point O, where GH intersects with O’O”, moves along GH towards H, thus coming upon the cerebral states one after another in succession from left to right.I bought a used copy of the red Faber edition of An Experiment With Time in 1991, at Abacus Book Shop on Gregory Street in Rochester, New York. Abacus has since moved to Monroe Avenue, but according to Dunne’s theory the bookshop still exists in its old location inside one of the matryoshka dolls of universal Time, with the unbought copy of his book still waiting on the shelf. I didn’t get very far in the book. Dunne writes, of his theory: “Serialism discloses the existence of a reasonable kind of ‘soul’—an individual soul which has a definite beginning in absolute Time—a soul whose immortality, being in other dimensions of Time, does not clash with the obvious ending of the individual in the physiologist’s Time dimension.” His dream prophecies and “Master-minds” and “Superbodies” bewildered me—really they seemed like a fancier way of talking about sibyls and angels and the hierarchy of heaven. And the pseudogeometrical figures, reminiscent of drawings in paperback explications of Einsteinian space-time, seemed—not to be rude—quite nutty.In his 1935 novel Invitation to a Beheading, his narrator explains: “What we call dreams is semi-reality, the promise of reality, a foreglimpse and a whiff of it.”Somewhere along the twisty path of the twentieth century, Vladimir Nabokov, our brilliant dreamer-in-chief, came into contact with Dunne’s theories of oneiric prophecy and was evidently inspired by them. It’s difficult to know when that happened—but clearly everyone was buzzing about Dunne’s Experiment by the 1930s. In King, Queen, Knave (1928), Nabokov writes of false awakenings in nested dreams, “as if you were rising up from stratum to stratum but never reaching the surface, never emerging into reality.” In The Gift (1938), he describes the infinite freedom of dreams, which clot like blood at the moment of waking. In his 1935 novel Invitation to a Beheading, his narrator explains: “What we call dreams is semi-reality, the promise of reality, a foreglimpse and a whiff of it; that is, they contain, in a very vague, diluted state, more genuine reality than our vaunted waking life which, in its turn, is semi-sleep, an evil drowsiness into which penetrate in grotesque disguise the sounds and sights of the real world, flowing beyond the periphery of the mind.” In his first English-language novel, The Real Life of Sebastian Knight (1941), a woman expresses her impatience at hearing Sebastian recount his dreams, “and the dreams in his dreams, and the dreams in the dreams of his dreams.”In 1955, in The Times, Graham Greene shocked London by choosing Nabokov’s Lolita, which had just been published in Paris, as one of three books of the year, thereby insulating it from possible prosecution under obscenity laws. The novel became a huge best-seller, and Vladimir and Vera, flush with movie money from Stanley Kubrick, left the United States for Europe. Nabokov met Graham Greene in London and they corresponded. Then, in October 1964, both novelists—in a coincidence that may or may not be astounding (perhaps it was simply the result of a jointly agreed-upon scientific experiment?)—began recording their nightly dream adventures in a manner that closely followed John Dunne’s method of morning-after notation.Greene’s dream diary, which he kept up for years, came out in 1994. A World of My Own is a fascinating, revealing book, which makes only a passing mention of Dunne’s Experiment. Nabokov’s dream notes, however—“undertaken,” he writes, “to illustrate the principle of ‘reverse memory’ ” and recorded in pencil on his famous three-by-five cards over a period of several months, during which he referred to Dunne’s book several times—were not published in his lifetime. Only now are they out for everyone to read, swaddled in footnoted commentary and contextual analysis, in a book titled Insomniac Dreams: Experiments With Time by Vladimir Nabokov, edited by Gennady Barabtarlo, a critic and translator who teaches at the University of Missouri.Barabtarlo does his best to explicate Dunne’s ideas—although he rebels at the many “applied algebraic formulas”—and he includes, in part four of the book, a gorgeous quilt of many-colored passages culled from Nabokov’s lifetime of dream phenomenology and dream paraphrase. There is, for instance, the dream from The Real Life of Sebastian Knight in which Sebastian pulls his glove off: “As it came off, it spilled its only contents—a number of tiny hands, like the front paws of a mouse, mauve-pink and soft—lots of them—and they dropped to the floor,” whereupon a girl dressed in black begins gathering them and putting them in a dish. “All dreams are anagrams of diurnal reality,” Nabokov says in one of his last novels, Transparent Things. The word “dreamy” appears ten times, by my count, in Lolita.But the heart of Barabtarlo’s book—handsomely designed by Princeton University Press, with a hotel pillow on the cover—is the sequence of three-by-five dream cards from 1964. Nabokov, who was a lifelong and increasingly addicted user of sleeping pills and had a dodgy prostate that forced him to get up a lot (he sometimes noted the times of his nightly bathroom trips, under “WC”), managed even so to dream beautifully of the problems of being an internationally published writer, as in December 1964, while he was checking over the French version of Pale Fire: “On second night here (Hotel Due Torri with large coniferous garden) in doomful half-dream saw the scattered streaks of dim light between the slats of the shutters as a passage which I could not identify translated into French.” The next night brought another doomful image, one which sounds like something in a story by M.R. James: “A tremendous very black larch paradoxically posing as a Christmas tree completely stripped of its toys, tinsel, and lights, appeared in its abstract starkness as the emblem of permanent dissolution.”The night after that, Nabokov dreams of searching for a pencil with which to grade student papers, while at the same time being disturbed by a “sordid and complicated” affair that he’s had with someone’s wife—whose husband, a small, gesticulative man, is making a noisy scene. “In exasperation I take him and send him flying and spinning into a revolving door where he continues to twist at some distance from the ground.” Is the dream husband dead? “No, he picks himself up and staggers away. We return to the exam papers.” On December 30, Nabokov dreams that he’s dressed in dark pants and a pale green pajama top. A girl dressed in blue rides up on a bicycle. Aha! But Nabokov merely hunts for his socks, while “Soviet delegates pass along the road.”Vera Nabokov tells her husband her dreams as well, and he notes them down as part of his inventory. In one, she dreams that “big caterpillars, white with black faces, naked, crawl over the furniture.” Early on, Nabokov erroneously believes that he’s found incontestable proof that Dunne’s “reverse memory” thesis is correct. He dreams of a museum, and three days later he watches a film about a museum that seems familiar. He concludes that his dream must have been inspired by a memory of the film before he watched it. Barabtarlo rightly points out that in the dream Nabokov was in fact half-remembering his own short story of years before, “The Visit to the Museum.”This is a looping, chronologically complicated book full of the kind of sleep-deprived, iridescently edged complexity that likes to gather around Vladimir Nabokov’s work. In the end, all theories of nighttime previsioning are red herrings. Nabokov’s stories and novels and autobiographical essays are the true reverse dreams, where futures pass and interpenetrate, and where the past, like John Dunne’s transparent fishing lures, shines.Nicholson Baker’s most recent book is Substitute: Going to School with a Thousand Kids.",Reading Nabokov’s Dream Diary
9800,3805201,2018-02-26 11:15:50,"TNW SitesCan independent review platforms make Google love your site?Online review platforms are ubiquitous. Sites like Yelp, TripAdvisor, and others offer consumers a platform to publicly express their honest opinions on businesses. Everyone and their cousin knows just how influential online reviews are in relation to purchasing decisions.Consumers aren’t the only ones who can reap the benefits of online reviews. The businesses being reviewed are rewarded with factors like rating promotion, website integration, and avenues to interact with customers.One of the most overlooked benefits to these types of third-party review platforms is search engine visibility. Google rewards businesses that have positive reviews posted on trusted sites. Numerous positive reviews indicate that a listing is highly appropriate for its users.Google digs ratings and reviewsNot all review sites are created equally, and not all have relationships with Google (see here for a full list of Google Review Partners). As the search engine’s primary objective is to provide users with the most valuable and authentic information, ratings on trusted, third-party review sites play a significant role in determining a business’ rankings on the result pages.While this directly correlates with the search visibility of local businesses, where “local SEO” works to benefit their rankings in the local pack, based among other things, on the strength of their ratings, it also benefits brands and retailers that aren’t strictly “local.” Google derives brand signals from reviews and ratings, and this contributes towards your site’s domain authority.One of the most attractive and eye-catching elements on any “SERP listing” – SERP being Search Engine Results Page – are the yellow stars, technically known as rich snippets for ratings.When implemented properly, reviews of your company on Google-recognized third-party review sites will (more or less) increase the likelihood that your pages will be displayed with rich snippet stars, increasing chances of click-through from search. Empirical evidence shows that 150+ reviews over a 12-month period with an overall rating of 3.5 almost definitely gets your star rating to show up.Again, this leads to a virtuous cycle. SEO experts have argued based on experimental correlation that when Google sees a click-through rate for a given position that is higher than expected, and the bounce rate (site exits) for those visits to the web page is not high, there is evidence that the web page ranks even higher for the relevant keywords.Ignite Visibility recently published a micro-study that found strong correlations between click-through rates and Google positioning:Review platforms add authenticity signalsTo benefit from the perceived authenticity that rich snippets with rating stars give your brand and product, utilizing credible third-party review sites is a highly-effective way to improve your search engine visibility.Review platforms offer simple mechanisms for inviting customers to leave reviews. You (or, in some cases, anyone wanting to review your products or services) create a company page on the site to display their sentiment. As this page gathers ratings, it will be more likely to appear on Google for searches on your brand name, primarily based on the authenticity, reliability and “trust factor” of the review site in question. Eventually this page will push traffic to your site via a link.Third-party review sites stand as the guarantor that the reviews collected are from real customers. From an SEO perspective, Google accepts that the third-party reviews are reliable and honest appraisals of businesses and their offerings. Assuming you get mostly positive reviews, Google’s algorithm will be inclined to place you higher in the results, as it will be more confident that your company is a good solution for the query entered. The confidence that Google attributes to the various third-party review platforms must be factored in.How review sites improve your digital presenceMajor review platforms can charge a monthly subscription fee so you can edit and maintain your page on their site in a way that is consistent with your Google My Business page as well as various “citations” found across the web, including on other review sites. This is very important for local SEO.Paid-for pages on review sites let you collect reviews about your company, products and services, and present and promote them in an organized, professional way. As with most SaaS models, these review sites have different payment tiers, providing various options on integrations and level of support.Importantly, they all offer code snippets that you can install on your site, from where you directly display your ratings to site visitors. These snippets also feed that information to Google and allow it to display the stars and ratings we see on organic search results as well as AdWords text ads.Obviously, Google works to display the most trusted sites in its results, in order of their reliability. The question is, which review platform does Google see as the most credible? And which sites give more “SEO power,” in general, for businesses across the board?I used SEO tool SEMrush to pull some data about the kind of traffic these review sites get, the keywords and terms they rank for in Google, and the actual value of these keywords (presented in comparison with each other of course). I also checked how they ranked in terms of overall web traffic, and found that all of these sites are in the top 100,000.Here are the top ranked US review sites in terms of SEO power on Google.com:While reviews on any of the five platforms listed above (and a bunch of others) will get you rich snippet stars that provide opportunities to make your appearance more eye catching on the SERPS, Trustpilot is the one that packs the most punch, based on traffic numbers, keywords, and overall popularity.Trustpilot is not just limited to the US; it is a community-driven review platform spanning 65 countries. With 35 million reviews of 190,000 businesses, 45,000 new reviewers each day and 1 million new reviews each month, it is well ahead of the pack from an SEO perspective, with 3 times more SEO visibility than other platforms. Businesses also get a “TrustScore” – an algorithmic measure of customer satisfaction, based on customers’ reviews on the site. While TrustScore may not be the most compelling piece of data from a search perspective, it certainly impacts the way consumers behave and interact with brand websites.That said, every review platform that has a respectable amount of traffic is certainly worth exploring for more than just the SEO value. Review sites are a key factor in developing a positive brand image and word-of-mouth. Positive results like rich snippet stars and SEO visibility can make a momentous difference in conversion rates. In a crowded space, the importance of each online review cannot be overlooked.In conclusionGetting registered on review sites is incredibly easy and reaching out to customers to leave reviews is even easier. For as powerful as online reviews are in modern marketing, expanding your business footprint across as many online channels as possible will serve you well in the long run.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Can independent review platforms make google love your site?
9801,3807290,2018-02-26 10:19:29,"I've Just Added 2,844 New Data Breaches With 80M Records To Have I Been Pwned26 February 2018tl;dr - a collection of nearly 3k alleged data breaches has appeared with a bunch of data already proven legitimate from previous incidents, but also tens of millions of addresses that haven't been seen in HIBP before. Those 80M records are now searchable, read on for the full story:There's an unknown numbers of data breaches floating around the web. There are data breaches we knew of but they just took years to appear publicly (Dropbox, LinkedIn), data breaches we didn't know of that also took years to discover at all (Disqus, imgur) and indeed, data breaches that were deliberately covered up (Lifeboat, Uber). But I suspect the another big slice of data breaches are the ones that both the website operators themselves and the general public know nothing about, the ""unknown unknowns"", as it were. By it's very nature, we don't know how big this list is, but ""very big"" would be a pretty safe bet.In running Have I Been Pwned (HIBP) these last 4 and bit years, one of the things the constantly amazes me is the breadth of data breaches individuals often collect. People hoard it, swap it, crack it, sell it and occasionally, just redistribute it all publicly. I regularly see these massive lists of breaches belonging to a personal stash, often numbering in the hundreds of incidents and frequently containing data I've seen circulating before. Today, however, I came across something a bit different by way of a story from last week titled 3,000 Databases with 200 Million Unique accounts found on Dark Web. Now, as I said only a couple of weeks ago, I'm immediately suspicious when people start saying ""dark web"". That 1.4B list I reference in that post, for example, was almost entirely data I'd seen before and it was being distributed via Reddit, ""the front page of the internet"". But these things are always worth a look anyway so I set about locating the data.After some number of single-digit number of minutes looking for it, someone pointed me to a well-known hacking forum with a post from 4 days before the story mentioned above. Consistent with my aforementioned ""debunking the dark web"" blog post, the forum in question is located in the ""very clear web"" and is easily discovered (although I'm not going to make it any easier here). It then links directly through to 8.8GB worth of easily downloadable data breaches, all obtainable in a single ZIP file. In total, there were 2,889 text files in the archive but it's what's inside them which I found particularly interesting.Almost all the files are just email addresses and plain text passwords (the occasional file has a username that's not an email address and a password). This is interesting in that it's reminiscant of the Explouit.In and Anti Public credential stuffing lists I loaded back in May. However, in those cases they were single lists amalgamated from multiple sources whilst in this case, we're looking at individual website names that appear to have had merely the credentials extracted from the source data breaches. It's also interesting because among nearly 3k other breaches, the data contains Dropbox. I wrote about the Dropbox breach back in August last year and I pointed out the structure of the breached files at the time:There were 2 files with bcrypt hashes and 2 with SHA-1 hashes. But here's what was particularly interesting:the bcrypt accounts include the salt whilst the SHA1 accounts don'tIn other words, you've got one highly resilient hashing algorithm in bcrypt (work factor of 8) and one fairly weak one in SHA-1, albeit without the salt which would usually be needed to crack it. But there's 18.6M rows of email addresses and plain text passwords in this new file, so where are the passwords from? I grabbed a few email addresses then went back to the original data breach and pulled the corresponding records for them. I then tested them against an online bcrypt hash generator to see if the passwords in the new set of data were the ones used in the original Dropbox breach. Here's a couple that matched:But they didn't all match, in fact most I tested didn't. They didn't all come from the bcrypt files in the Dropbox data either, a bunch were from the SHA-1 files which had no salt. So what can we conclude from this? Well firstly, Dropbox allowed some pretty atrocious passwords at one time there! And secondly, these passwords have almost certainly not been cracked out of the Dropbox data otherwise I would have found a lot more matches (I tested some pretty terrible passwords too). But it does contain email addresses from the Dropbox breach (none of the ones I tested weren't in the original breach) and we know people reuse passwords so the logical conclusion is that someone has joined email addresses from one source with passwords from another.Moving on, regardless of how the data inside the files was put together, I wanted to get a sense of how many of them were new versus incidents I'd seen before. Any existing breaches in HIBP that I could identify in this new set were omitted. Some of them were obvious, for example Dropbox and MySpace so I pulled these (among a handful of others) out. I then grabbed a unique set of addresses from the remaining data and tested a random 10k of them against HIBP. Only 70% of them were already in the system which indicates a lot of new data; 30% of the addresses I'd never seen before. Of course, of the ones I had seen before there'd still be many addresses in data breaches that weren't in HIBP and the addresses had simply been pwned more than once, but the checks against the system also gave me an opportunity to do a bit more source cleanup.In analysing the results of the HIBP checks, further duplication came to light. For example, the largest remaining file after my initial cleanup was ""SGB.net.txt"" but the domain sgb.net is presently parked and archive.org doesn't show anything of substance on it in the past either. But when checking the data against HIBP, I kept getting hits against the Lifeboat data breach . That site runs on lbsg.net which is not too dissimilar to the filename in the set I was dealing with here. The file named ""Alpari.com.txt"" was full of Chinese addresses and constantly showed hits against the NetEase and Aipai.com breaches. Given that alpari.com is a financial services site located in the Caribbean, something doesn't add up here so I removed that one as well.In total, I distilled the data down to 2,844 files which contained a total of 80,115,532 unique email addresses. Another sample set of these was showing much closer to only 66% of them having been in HIBP already which is much closer to the normal ""hit rate"" I see with a brand new, genuine data breach (have a read through the HIBP Twitter feed to see actual numbers). This was now data I was comfortable loading because we're talking tens of millions of people in (alleged) breaches I've never seen before. But I'm also conscious that I can't clearly say ""this is the breach you were in"" as there's no direct association between the accounts in HIBP and the source file. However, I can list those source files in the hope that it'll help people who might recognise a service they've used in the past. Here's the complete list:It should be abundantly clear from this post, but let me explicitly state it anyway: I have no idea how many of these are legitimate, how many are partially correct and how many are outright fabricated. I've consequently flagged this ""breach"" in HIBP as unverified. However, I can confidently say that amongst this set was a large number of records in breaches that I've previously verified and that per the Dropbox example, there are passwords that have been used by the email addresses they're associated with. I'm conscious that people can be left feeling like they don't know what action to now take, but when I've asked in the past people are overwhelmingly in favour of know where their data has been exposed. As with almost every other data breach, treat this as a reminder of how important a dedicated password manager is for ensuring all your passwords are unique and genuinely strong. Read the only secure password is the one you can’t remember for more on that.Finally, if you find your data in this set and recognise the source of it from the list above, do leave a comment below as it may help others identify where their information has been exposed.","I've Just Added 2,844 New Data Breaches With 80M Records To Have I Been Pwned"
9802,3807293,2018-01-17 14:20:00,"Why we made this changeVisitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access.The technology is limited for now, but it could be the start of something big. Building and optimizing a deep neural network algorithm normally requires a detailed understanding of the underlying math and code, as well as extensive practice tweaking the parameters of algorithms to get things just right. The difficulty of developing AI systems has created a race to recruit talent, and it means that only big companies with deep pockets can usually afford to build their own bespoke AI algorithms.“We need to scale AI out to more people,” Fei-Fei Li, chief scientist at Google Cloud, said ahead of the launch today. Li estimates there are at most a few thousand people worldwide with the expertise needed to build the very best deep-learning models. “But there are an estimated 21 million developers worldwide today,” she says. “We want to reach out to them all, and make AI accessible to these developers.”Cloud computing is one of the keys to making AI more accessible. Google, Amazon, Microsoft, and other companies are rushing to add machine-learning capabilities to their cloud platforms. Google Cloud already offers many such tools, but they use pretrained models. That limits what they can do—for example, programmers will only be able to use the tools to recognize a limited range of objects or scenes that they have already been trained to recognize. A new generation of cloud-based machine-learning tools that can train themselves would make the technology far more versatile and easier to use.Several companies have been testing Google Cloud AutoML for the past few months. Disney used the service to develop a way to search its merchandise for particular cartoon characters, even if those products are not tagged with that character’s name.Joaquin Vanschoren, a professor at the Eindhoven Institute of Technology in the Netherlands who specializes in automated machine learning, says it’s still a relatively new research topic, though interest in the area has been heating up lately. “It is impressive that they can release this as a production service so quickly,” he says.Vanschoren says automation can add a lot of computational cost, so Google must be throwing plenty of resources at the service. That’s only likely to get worse as programmers attempt to design AI systems that move beyond simple image classification and attempt to tackle ever broader tasks.Efforts in this area might ultimately feed into the grand effort to build more general and adaptable forms of artificial intelligence. But before the machines take over completely, you can at least try your hand developing your very own AI.ShareI am the senior editor for AI at MIT Technology Review. I mainly cover machine intelligence, robots, and automation, but I’m interested in most aspects of computing. I grew up in south London, and I wrote my first line of code (a spell-binding… More infinite loop) on a mighty Sinclair ZX Spectrum. Before joining this publication, I worked as the online editor at New Scientist magazine. If you’d like to get in touch, please send an e-mail to will.knight@technologyreview.com.You've read of three free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Subscribe now for unlimited online access. This is your last free article this month. Subscribe now for unlimited online access. You've read all your free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Log in for more, or subscribe now for unlimited online access. Log in for two more free articles, or subscribe now for unlimited online access.",Google’s self-training AI turns coders into machine-learning masters
9803,3807534,2018-02-26 12:47:23,"TNW SitesIndex and AvP launch Startup Banking service to bridge early stage funding gap in EuropeThe lifecycle of early stage startups is a feedback loop between two states: developing a product and raising funding. The better the product, the better the chances at funding. The more flush with cash the startup, the more room for product improvement there is.Startups don’t have it easy, though. Even if the product is solid, netting investment is an uphill battle. Fundraising is a process of convincing people with money to make an emotional decision based on objective metrics. An opaque undertaking to begin with, and often a guessing game as to which investors will be receptive to your pitch. And it costs time – a lot of time.Contrary to large corporates – which can afford to hire an investment banker to take care of financial matters and attract funding – many early stage startups struggle to position themselves in a favourable light to the right selection of investors.Stacking the deck against startups even further, investors in Europe’s most advanced tech ecosystems are notoriously risk-averse. Data (collected between 2016 and 2017) from our intelligence platform Index shows that in the European ecosystem, the survival rate for seed-stage funded startups to Series A is below 20% – and that’s the generous interpretation.Index and Amsterdam Venture Partners (AvP) are collaborating to fill this gap by launching Startup Banking, a metrics-driven, hands-on approach to fundraising for startups.The new partnership offers early stage European startups the expertise of investment banking at favourable terms and affordable rates. It can help startups raise their Seed or Series A round by combining the experience of AvP’s former investment bankers with world-class machine learning and matchmaking algorithms developed by Index.A certified name in the European startup world, the tried and tested intelligence platform Index was used at our 2017 conference to successfully set up over 3,800 meetings between startups and investors over the course of two days.Measure, match, meetStartup Banking is a metrics-driven approach to fundraising. This means Startup Bankers help you make your company’s metrics insightful to investors. Based on their knowledge of what data-oriented investors are looking for when making funding decisions, Startup Bankers will coach you through developing a strategy by initially identifying, calculating and monitoring relevant KPIs.Working closely with you and your team, they will craft a narrative and a tailor made pitch for investors and distribute it to the most suitable ones in their international network.Finally, they will keep the investors updated on your progress and make sure to get you a meeting with them.How to sign upWhether you already have a proposal deck and want to get into the market or you’re just starting out, startup bankers can coach you through the process, freeing up time so you can focus on what’s important: the product.Startup Banking is a bespoke service for startups looking to get funded, and is focused on select companies with an international focus and ambition for exponential growth. Keen to see what this is all about and whether your startup fits the bill? Fill in the application form to request an intro call and take the first step towards getting funded.",Index and AvP launch Startup Banking service to bridge early stage funding gap in Europe
9804,3810214,2018-02-26 15:00:00,"What does Fitbit need to succeed?Fitbit is the current king of wearables, but for how much longer? The company has recorded four successive quarters of losses since the tail end of 2016, and that's a problem. Fitbit will publish its latest earnings results later today, and those numbers won't just tell us how the company is faring, but also how the wearables industry is doing as a whole.Since 2007, Fitbit has sold more than 70 million devices, from its early, belt-worn pedometers to the wrist-worn activity trackers that made its name. It's a brand that is synonymous with the wearables market, despite also selling headphones and smart scales. But the numbers suggest that the huge sales successes of 2015 and 2016 are not going to be repeated.""The Apple Watch has undoubtedly put pressure on Fitbit,"" explained George Jijiashvili, CCS Insight's wearables industry analyst. Unfortunately, Fitbit famously struggled to build a full-featured smartwatch, with the Ionic making its debut in late 2017. Engadget's Cherlynn Low said that the device's ""smartwatch-like features are either limited in functionality or require tedious, time-consuming setup,"" though she did praise the watch's ""comprehensive tools"" and ""long-lasting battery life.""The Ionic's long gestation enabled others to steal Fitbit's lead at the high end, with Apple being a clear beneficiary. Sales of the Apple Watch are believed to have surpassed those of the entire Swiss watch industry for the 2017 holiday season. CCS' latest report claims that ""Apple has become the market leader,"" while Canalys believes that the Apple Watch ""accounts for 51.6 percent of all smartwatches ever shipped.""Fitbit can console itself knowing that it remains a big deal in the fitness market, but that position is under threat. CCS believes that just two companies, Fitbit and Huami, account for 80 percent of the global activity tracker business. But Huami, the Chinese company that designed the Xiaomi MiBand, has now edged ahead in the market, thanks to its ""enormous home market advantage in China."" But even that battle may not be worth fighting for long, since the report believes that the tracker industry has shrunk by 18 percent. IDC analyst Francisco Jeronimo agrees, saying that the ""growth is now all in smartwatches"" rather than simpler, cheaper fitness trackers.Which is a shame, because Fitbit was arguably the best hope for a compelling alternative to Apple becoming a de facto smartwatch monopoly. Those who would suggest Google as a worthy opponent forget that the company has paddled around in circles. It doesn't publish figures on how many Android Wear smartwatches have been sold, but -- judging by downloads of the Android Wear app -- it only broke the 10 million sales threshold over the holidays.There is a hope that Fitbit's Ionic will help satisfy demand from a legion of consumers who have been holding out for a Fitbit-branded smartwatch. That may not be justified. During the 2017 holidays, investment bank Morgan Stanley found that boxes of the watch were piled up at retailers. When Fitbit publishes its figures, the most important number will be the volume of devices that it sold across 2017. ""Fitbit shipped 21 and 23 million units in 2015 and 2016,"" says Jijiashvili, but he expects the 2017 total to be 14 million trackers and just one million smartwatches.IDC's Jeronimo is more optimistic about Fitbit's sales, believing that the Ionic is likely to be ""one of the 10 best-selling wearable devices of 2017."" He says that's going to be a big achievement, especially since the watch only launched in October.Separate from the volume of devices that Fitbit has sold (or shipped) will be the amount of money it's been able to bring through the door. The company reported revenues of $574 million in the last three months of 2016, and it needs to do significantly better to avoid more red ink on its spreadsheets. Fitbit does, at least, appear to understand that its dominance in the consumer wearables market is under threat, and that it can't beat Apple at hardware alone. This explains its hard turn into the healthcare market, where its relatively affordable trackers can undercut more expensive medical devices.The company purchased Twine Health, a startup that created a platform for managing chronic conditions like diabetes and hypertension. It also invested cash in Sano, a company that is building a very small blood glucose monitor for continuously monitoring diabetic blood sugar. ""It's a risky strategy,"" says Jijiashvili, since the healthcare market is so fraught with necessary regulations to protect patients. Then again, the analyst believes that ""it is a necessary move,"" since the consumer wearables ""space continues to shrink.""Jeronimo is, again, more optimistic, saying that a move toward the healthcare business is a ""good way forward."" He feels that a Fitbit is a better alternative than a bulky, medical-grade tracker for patients with heart conditions or those who need to lose weight. The crucial element, however, is to win the trust of the medical community and also to ensure that it can survive making the transition away from being a purely consumer-focused brand. Jeronimo also believes that Fitbit needs to make more of the data it collects from users, so that its devices can warn you that something is going wrong. But even here, the analyst admits that Apple also has its sights on doing this, and its flagship product has ""far more functionality.""In many ways, Fitbit's struggles mirror those of GoPro, another brand that has found sustained success hard to come by. Both quickly rose to prominence by essentially defining a relatively niche category -- fitness trackers and action cameras -- that went mainstream at the right time. And both have suffered from the issues of serving that niche too well, and also being undercut by their competitors. A $149.95 Fitbit Alta HR has to compete with Xiaomi's MiBand 2, which does almost everything the pricier unit does for less than $40. Jijiashvili feels that Fitbit has to be hoping that ""its foray into adjacent products is more successful than GoPro's efforts with drones.""Not everyone believes that the company is doomed. Fitbit investor and writer George Kesarios is firmly behind the company. Last November, he wrote an article in finance journal Seeking Alpha that said he believes ""that this company has turned a page, and things will be different from now on."" Kesarios' belief is based on the increase in Fitbit's revenue, even though that increase is not enough to cover its losses. He adds that he believes that the Ionic will ""be the product that will sell year after year, like Apple's iPhone."" But his opinion is a minority amid the tide of folks who are increasingly pessimistic about Fitbit's long-term health.If the wearables industry -- as we understand it today -- is to succeed and grow, then it needs Fitbit to remain as a viable wearable alternative to Apple. But if Fitbit can't turn a profit during the launch of its first smartwatch, during a holiday season, no less, then it's hard to imagine a future.After training to be an intellectual property lawyer, Dan abandoned a promising career in financial services to sit at home and play with gadgets. He lives in Norwich, U.K., with his wife, his books and far too many opinions on British TV comedy. One day, if he's very, very lucky, he'll live out his dream to become the executive producer of Doctor Who before retiring to Radio 4.",What does Fitbit need to succeed?
9805,3810307,2018-02-26 14:00:25,"A file that Apple updated on its website last month provides the first acknowledgment that it's relying on Google's public cloud for data storage for its iCloud services.The disclosure is fresh evidence that Google's cloud has been picking up usage as it looks to catch up with Amazon and Microsoft in the cloud infrastructure business.Some media outlets reported on Google's iCloud win in 2016, but Apple never provided confirmation.Apple periodically publishes new versions of a PDF called the iOS Security Guide. For years the document contained language indicating that iCloud services were relying on remote data storage systems from Amazon Web Services, as well as Microsoft's Azure.But in the latest version, the Microsoft Azure reference is gone, and in its place is Google Cloud Platform. Before the January update, Apple most recently updated the iOS Security Guide in March.The latest update doesn't indicate whether Apple is using any Google cloud services other than core storage of ""objects"" like photos and videos. The document also doesn't make it clear when Apple started storing data in Google's cloud. Apple and Microsoft didn't respond to requests for comment.Earlier this month Google said its public cloud and its G Suite line of cloud-based productivity apps contribute $1 billion in revenue per quarter. In the fourth quarter, market leader Amazon Web Services brought in $5.11 billion in revenue.In addition to Apple, other Google public cloud customers include Kroger, PayPal, Snap and Spotify.",Apple confirms it uses Google's cloud for iCloud
9806,3810309,2018-02-26 00:00:00,"Back Pain May Be The Result Of Bending Over At The Waist Instead Of The Hips : Shots - Health NewsNo, we're not talking about squatting. We're talking about a way to bend over that's nearly disappeared in our culture. And it could be one reason why back pain is so common in the U.S.That little look down bends your spine and triggers your stomach to do a little crunch. ""You've already started to bend incorrectly — at your waist,"" Couch says. ""Almost everyone in the U.S. bends at the stomach.""In the process, our backs curve into the letter ""C"" — or, as Couch says, ""We all look like really folded cashews.""In other words, when we bend over in the U.S., most of us look like nuts!But in many parts of the world, people don't look like cashews when they bend over. Instead, you see something very different.I first noticed this mysterious bending style back in 2014 while covering the Ebola outbreak. We were driving on a back road in the rainforest of Liberia and every now and then, we would pass women working in their gardens. The women had striking silhouettes: They were bent over with their backs nearly straight. But they weren't squatting with a vertical back. Instead, their backs were parallel to the ground. They looked like tables.After returning home, I started seeing this ""table"" bending in photos all around the world — an older woman planting rice in Madagascar, a Mayan woman bending over at a market in Guatemala and women farming grass in northern India. This bending seemed to be common in many places, except in Western societies.""The anthropologists have noted exactly what you're saying for years,"" says Stuart McGill, at the University of Waterloo in Ontario, Canada, who has been studying the biomechanics of the spine for more than three decades.""It's called hip hinging,"" McGill says. ""And I've spent my career trying to prove it's a better way of bending than what we do.""'Table' Bending Versus 'C' BendingWhen you hip hinge (left), your spine can stay in a neutral position, while the hips and upper legs support your body weight. When you bend at the waist, the back curves, putting stress on the spine.When people bend with the cashew shape in their back — like we often do — they're bending their spine. ""That puts more stress on the spinal discs,"" McGill says.Discs are little rings of collagen found between each vertebra, which form a joint. But they aren't made for tons of motion. ""They have the mechanical characteristics of more like a fabric,"" McGill says.""If you took a cloth, and you kept bending and stressing it, over and over again, the fibers of the weave of the cloth start to loosen up and delaminate,"" he says.Eventually, over time, this fabric can fray, which puts you at risk of slipping a disc or having back pain.On the other hand, when you hip hinge, your spine stays in a neutral position. The bending occurs at the hip joint — which is the king of motion.""Hips are a ball and socket joints,"" McGill says. ""They are designed to have maximum movement lots of muscle force.""In other words, your boots may be made for walking, but your hips are made for bending.""Bending at the hip takes the pressure off the back muscles,"" says Liza Shapiro, who studies primate locomotion at the University of Texas, Austin. ""Instead, you engage your hamstring muscles.""And by ""engage the hamstrings,"" she also means stretching them.""Oh yes! In order to hip hinge properly, your hamstrings to have to lengthen,"" Shapiro says. ""If you have tight hamstrings, they prevent you from bending over easily in that way.""Tight hamstrings are extremely common in the U.S., Kennedy says. They may be one reason why hip hinging has faded from our culture: Stiff hamstrings are literally hamstringing our ability to bend properly.But hip hinging isn't totally lost from our culture, Shapiro says. ""I just saw a website on gardening that recommended it, and many yoga websites recommend bending at the waist, too.""And the hip hinging is sprinkled throughout sports. Weightlifters use it when they do what's called a deadlift. Baseball players use it when they bat. Tennis start Rafael Nadal does it when he sets up a forehand. And in football, players kneel at the line of scrimmage with beautiful hip hinging.Toddlers, less than three years old, are great hip hingers. They haven't learned yet from their parents to bend like a cashew.Whether or not hip hinging will prevent back pain or injuries, doctors don't know yet, says Dr. D.J. Kennedy, a spine specialist at Stanford University and a former weightlifter.""We don't have these randomized trials, where we have people lifting things hundreds of times and see how their body responds to hip hinging,"" Kennedy says. ""Still, though, Kennedy says he tries to hip hinge as much as possible.""I think hip hinging intuitively makes sense, just given how the spine functions,"" he says. ""So I try very hard to do it.""So how in the world do you do this mysterious bending? Back in Palo Alto at Jean Couch's Balance Center, she tells me the trick: Find your fig leaf.""Stand up and spread your heels about 12 inches apart, with your toes 14 inches apart,"" she says. ""Now, if you are Adam in the Bible, where would you put a fig leaf?""So I try it. I put my hand on my pubic bone as a pretend fig leaf. Then as I bend my knees a bit, I allow my fig leaf to move through my legs. A little crevice forms right at the top of my legs and my back starts to fold over, like a flat table.""Now you're using the large muscles of your hips, such as the glutes, to support the whole weight of you body, instead of the tiny muscles of your back,"" says Jenn Sherer, who co-owns the Balance Center with Couch.And she's right. My back relaxes, while my hamstrings start to stretch. And boy are they tight!""Wow! My hamstrings are stretching like crazy,"" I yell out, while I'm bent over like a table.""Yes,"" Couch chuckles. ""That's why we call it the world's best hamstring stretch. We find that the bend feels so good for some people, they never want to get back up.""Shots is the online channel for health stories from the NPR Science Desk. We report on news that can make a difference for your health and show how policy shapes our health choices. Look to Shots for the latest on research and medical treatments, as well as the business side of health. Your hosts are Scott Hensley and Nancy Shute. You can reach the Shots team via our contact form.",Lost Art Of Bending Over: How Other Cultures Spare Their Spines
9807,3810311,2018-02-22 00:00:00,"New Maps Reveal Global Fishing's 'Vast Scope Of Exploitation Of The Ocean' : The SaltResearchers have used radio transmissions to track the movement of fishing vessels and create stunning maps of fishing activity. The maps show that fishing covers most of the globe's oceans.New Maps Reveal Global Fishing's 'Vast Scope Of Exploitation Of The Ocean'A global map showing where all fishing vessels were active during 2016. Dark circles show the vessels avoiding exclusive economic zones around islands, where they aren't allowed. Global Fishing Watch hide captiontoggle captionGlobal Fishing WatchA global map showing where all fishing vessels were active during 2016. Dark circles show the vessels avoiding exclusive economic zones around islands, where they aren't allowed.Global Fishing WatchThe fishing industry has long been hard to monitor. Its global footprint is difficult even to visualize. Much fishing takes place unobserved, far from land, and once the boats move on, they leave behind few visible traces of their activity.But this week, the journal Sciencepublished some remarkable maps that help fill that gap. John Amos, president of an organization called SkyTruth, which helped produce them, issued a statement calling the maps ""a stunning illustration of the vast scope of exploitation of the ocean.""SkyTruth and its collaborators tracked most of the world's fishing vessels through an entire year by monitoring radio transmissions that most vessels now emit automatically in order to avoid collisions with each other. The researchers were able to distinguish between different kinds of vessels — trawlers that drag nets behind them, for instance, versus vessels that deploy drifting ""longlines"" that often are used to catch tuna.This map shows fishing by trawlers, which drag fishing nets behind them. They dominate fishing in coastal areas, such as fisheries near Europe and China. Global Fishing Watch hide captiontoggle captionGlobal Fishing WatchThe maps show the most intense fishing activity along the coasts of heavily populated areas like Europe and China. But fishing also covers much of the high seas. According to the researchers, commercial fishing operations covered at least 55 percent of the world's oceans. That area, it calculates, is four times larger than the area devoted to agriculture on land.The researchers also were able to distinguish between fishing vessels from different countries. According to the study, five countries — China, Spain, Taiwan, Japan, and South Korea — accounted for 85 percent of all high-seas fishing.This map shows activity of fishing vessels that use drifting longlines. They roamed the high seas, especially in tropical latitudes. Global Fishing Watch hide captiontoggle captionGlobal Fishing WatchIn addition to SkyTruth, researchers from Global Fishing Watch, the National Geographic Society's Pristine Seas project, University of California Santa Barbara, Dalhousie University, Google, and Stanford University collaborated on the study.With a pinch of skepticism and a dash of fun, The Salt covers food news from the farm to the plate and beyond. You can connect with senior editor and host Maria Godoy via our contact form or directly by email. You can also reach correspondent Allison Aubrey via email.",New Maps Reveal Global Fishing's 'Vast Scope Of Exploitation Of The Ocean'
9808,3810312,2018-02-26 12:00:00,"One of the key takeaways from Hot Chips last year was that Intel’s EMIB strategy was going to be fixed primarily in FPGAs to begin with. Intel instigated a bridge and chiplet strategy: with the right FPGA, up to six different chiplets can be added via EMIB in a single package. This is well beyond the single EMIB implementation coming to consumers. Intel has already been selling its Stratix 10 family of FPGAs that have additional functionality through EMIB for a number of months, however today Intel is announcing the latest member of that family, one that has the ability to include 58G transceivers: the Stratix 10 TX.Intel will offer different variants of the Stratix 10 TX, from 600k logic elements with two chiplets up to 2.8 million logic elements with six chiplets. Five of the tiles are capable of taking the new 58G transceivers, enabling up to 144 transceivers in a single package. The sixth tile is used for PCIe. While the central FPGA is built on Intel’s 14nm process technology, the transceivers will be built upon TSMC’s 16FF process, due to Altera’s history of using TSMC for its analog hardware. Intel states that the transceivers in play can go down to 1 Gbps if needed, enabling backwards compatibility with existing networks. These transceivers were first demonstrated back in March 2016, finally making it to market.Intel is calling the new transceivers dual-mode 58G, with 4-levels of pulse-amplitude modulation (PAM4) and 30 Gbps non-return-to-zero. Previously these were (and have been called) 56G, but Intel is using the term 58G because it believes that its solution will perform better than other 56G solutions on the market, enough to call it a 58G device.The aim for these new Stratix 10 devices is for driving networking bandwidth needs for centralized base stations and network virtualization. The latter here is critical for 5G implementation, where compute happens in edge networks and VMs are transferred around to enable compute that has to happen closer to the device, especially with software defined networking. The Stratix 10 also features a range of IP cores, for 100GbE MAC and FEC, and the transceivers are built through a range of Intel and third-party IP.We are told from Intel that using EMIB in conjunction with the new chiplets enables for a much lower power product than previous designs. Intel offers customers substantial training, but EMIB implementations are essentially transparent to the design: the upside being bandwidth related and the ability to create use cases from that functionality.Alongside the Stratix 10 TX, Intel will also be offering a TX version with HBM2 (high-bandwidth memory). Within the largest design, the package can have two 4GB HBM tiles paired with three 58G transceivers and one 28G transceiver. That last one is 28G because it also has a hard PCIe connection. The HBM2 memory can run at either 800 MHz or 1 GHz depending on the product.",Intel Launches Stratix 10 TX: Leveraging EMIB with 58G Transceivers
9809,3810315,2018-02-25 09:45:26,"Things I Learned Managing Site Reliability for Some of the World’s Busiest Gambling Sitestl;drFor several years I managed the 3rd line site reliability operation for many of the world’s busiest gambling sites, working for a little-known company that built and ran the core backend online software for several businesses that each at peak could take tens of millions of pounds in revenue per hour. I left a couple of years ago, so it’s a good time to reflect on what I learned in the process.In many ways, what we did was similar to what’s now called an SRE function (I’m going to call us SREs, but the acronym didn’t exist at the time). We were on call, had to respond to incidents, made recommendations for re-engineering, provided robust feedback to developers and customer teams, managed escalations and emergency situations, ran monitoring systems, and so on.The team I joined was around 5 engineers (all former developers and technical leaders), which grew to around 50 of more mixed experience across multiple locations by the time I left.I’m going to focus here on process and documentation, since I don’t think they’re talked about usefully enough where I do read about them.ProcessProcess is essential to running and scaling an SRE operation. It’s the core of everything we achieved. When I joined the team, habits were bad – there was a ticketing system, but one-journal resolutions were not uncommon (‘Site down. Fixed, closing.’).An SRE operation is basically a factory processing information and should act accordingly. You wouldn’t have a factory running without processes to take care of the movement of goods, and by the same token you shouldn’t have a knowledge-intensive SRE operation running without processes to take care of the movement of knowledge.A great book on this subject is ‘The Checklist Manifesto’, which inspired many of the changes we made, and was widely read within the team. It cites the examples of the aviation industry’s approach to process, which enables remarkable creativity under stressful conditions by mental automation of routine operations. There’s even a film about one incident discussed and the pilot himself cited checklists and routine as an enabler of his fast-thinking creativity and control in that stressful situation. In fact, we used a similar process ourselves: in emergency situations, an experienced engineer would dive into finding a solution, while a more junior one would follow the checklist.Another critique of process is that process can inhibit effective working and collaboration. It absolutely can if process is treated as an entity justified by its own existence rather than another living asset. The only thing that can guard against this is culture. More on that later.Process – ToolingThe first thing to get right is the ticketing system. Like monitoring solutions, people obsess over which ticketing system is best. And they are wrong to. The ticketing system you use you will generally end up preferring simply due to familiarity. The ticketing system is only bad if it drives or encourages bad processes. What a bad process is depends on the constraints of your business.It’s far more important to have a ticketing system that functions reliably and supports your processes than the other way round.Here’s an example. We moved from RT to JIRA during my tenure. JIRA offered many advantages over RT, and I would generally recommend JIRA as a collaborative tool. The biggest problem we had switching, however, was the loss of some functionality we’d built into RT, which was critical to us. RT allowed us to get real-time updates on tickets, which meant that collaboration on incidents was somewhere between chat and ticketing. This record was invaluable in post-incident review. RT also allowed us to hide entries from customers, which again was really hard to lose. We got over it, but these things were surprisingly important because they’d become embedded in our process and culture.When choosing or changing your ticketing system, think about what’s really important to operations, not specific features that seem nice when on a list. What’s important to you can vary from how nice it looks (seriously – your customers might take you more seriously, and your brand might be about good design), to whether the reporting tools are powerful.DocumentationAfter process, documentation is the most important thing, and the two are intimately related.There’s a book in documentation, because, again, people focus on the wrong things. The critical thing to understand is that documentation is an asset like any other. Like any business assets, documentation:If properly looked after, will return investment many times overRequires investment to maintain (like the fabric of a factory)If out of date, costs money simply because it’s there (like out-of-date inventory)If of poor quality, or not usable is a liability, not an assetBut this is not controversial – few people disagree with the idea that good documentation is useful. The point is: what do you do about it?Documentation – Where We WereWe were in a situation where documentation provided to us was not useful (eg from devs: ‘a network partition is not covered here as it is highly unlikely’. Well, guess what happened! And that was documentation they kindly bothered to write…), or we simply relied on previously-journalled investigations (by this time we were writing things down) to figure out what to do next time something similar happened.This was frustrating all of us, and we spent a long time complaining about the documentation fairy not visiting us before we took responsibility for it ourselves.Documentation – What I DidHere’s what I did.I took two years’ worth of priority incidents (ie those that triggered – or would have triggered – an out of hours call), and listed them. There were over 1700 of them.Then I categorised them by type of issue.Then I went through each type of issue and summarised the steps needed to either resolve, or get to a point where escalation was requiredThis took seven months of my full-time attention. I was a senior employee and I was costing my company lots of money to sit there and write. And because I had a clueful boss, I never got questioned about whether this was a good use of time. I was trusted (culture, again!). I would say it took four months before any dividends at all were seen from this effort. I remember this four-month period as a nerve-wracking time, as my attention was taken away from operations to what could have been a complete waste of my time and my employer’s money and an embarrassing failure.Why not give it to an underling to do? For a few reasons. This was so important, and we had not done it before so I needed to know it was being done properly. I knew exactly what was needed, so I knew I could write it in such a way that it would be useful to me at the very least. I was also a relatively experienced writer (arts grad, former journalist), so I liked to think that that would help me write well.We called these ‘Incident Models’ as per ITIL, but they can also be called ‘run books’, ‘crib sheets’, whatever. It doesn’t matter. What mattered was:They were easy to find/search forIt was easy to identify whether you got a matchThey were not duplicatedThey could be trustedWe put this documentation in plain text within the ticketing system, under a separate JIRA project.The documentation team got wind of what we were up to and tried to pressure us to use an internal wiki for this. We flat-out refused, and that was critical: the documentation system’s colocation with the ticketing system meant that searching and updating the documentation had no impedance mismatch. Because it was plain text it was fast, simple to update, and uncluttered. We resisted process that jeopardised the utility of what we were doing.Documentation and the Criticality of De-clutteringWhen we started, we designed a schema for these Incident Models which was a thing of beauty, covering every scenario and situation that could crop up.In the end it was almost a complete waste of time. What we ended up using was a really dumb structure of:Statement of problemSteps 1-n of what to doFurther/deeper discussion, related articlesThat was it. Attempts to structure it more thoroughly all failed as it was either confusing to newcomers, created too much administrative overhead, or didn’t cover enough. Some articles developed their own schema over time that was appropriate to the task, and new categories (eg the ‘jump-off’ article that told you which article to go to next) evolved over time. We couldn’t design for these things in advance because we didn’t know what would work or what would not.Call it ‘agile documentation’ if you want – agile’s what sells these days (it was ITIL back then). Again, what was critical was that simplicity and utility trumped everything else.There Is No Documentation FairyHaving spent all this time and effort a couple of other things became clear regarding documentation.First, we gave up accepting documentation from other teams. If they commented code, great, if there was something useful on the wiki for us to find, also great. But when it came to handing over projects we stopped ‘asking for documentation’. Instead we’d arrange sessions with experienced SREs where the design of the project would be discussed.Invariably (assuming they had no ops experience), the developer would focus on the things they’d built and how it worked – and these things were often the most thoroughly tested and least likely to fail.By contrast, the SRE would focus on the weak points, the things that would go wrong. ‘What happens if the network gets partitioned? What if the database runs out of disk? Can we work out from the logs why the user didn’t get paid?’We’d then go away and write our own documentation and get the engineer to sign off on it – the reverse of the traditional flow! They’d often make useful comments and give us added insights in the process.The second thing we noticed was that our engineers were still reluctant to update the docs that only they were using. There was still a sense that documentation should be given to them. The leadership had to constantly reinforce that this was their documentation, not tablets of stone handed down from on high, and if they didn’t constantly maintain this, they would become useless.This was a cultural problem and took a long time to undo. Undoing it also required the documentation changes to be reinforced by process.In the end, I’d say about 10% of the ongoing working time was spent maintaining and writing documentation. After the initial 7-month burst, most of that 10% was spent on maintenance rather than producing new material.Documentation – BenefitsAfter getting all this documentation done, we experienced benefits far in excess of the 10% ongoing cost. To call out a few:Easier onboardingBefore this process started we were reluctant to take on less experienced staff. After, onboarding became a breeze. Among other things the training involved following incidents as they happened and shadowing more experienced staff. New staff were tasked with helping maintain docs, which helped them understand what gaps they had in their knowledge.Better trainingThe docs gave us a resource that allowed us to identify training requirements. This ended up being a curriculum of tools and techniques that any engineer could aim to get a working knowledge of.Less stress through simpler escalationThese was a big one. Before we had the step-by-step incident models, when to escalate was a stressful decision. Some engineers had a reputation for escalating early, and all were insecure about whether they’d ‘missed something obvious’ before calling a responsible tech lead out of hours. SREs would also get called out for not escalating early enough as well!The incident models removed that problem. Pretty soon, the first question an escalated-to techie asked was ‘have you followed the incident model’? If so, and there something obvious was missed, then gaps in it became clear and quickly-fixed. Soon, non-SREs were busy updating and maintaining the docs themselves for when they were escalated-to. It became a virtuous circle.Better disciplineThe obvious value of documentation to the team helped improve discipline in other respects. Interestingly, SREs previously had the reputation for being the ‘loudest’ team – there was often a lot of ‘lively’ debate, and the team was very social – which made sense, as we relied on each other as a team to cover a large technical area, dealt with often non-technical customer execs, and sharing knowledge and culture was critical.As time progressed, the team became quieter and quieter – partly due to the advent of chatrooms, increased remote working, and international teams, but also due to the fact that so much of the work became routine: follow the incident model, when you’re done, or don’t understand something, escalate to someone more senior.AutomationAutomating the investigations this way meant that the way was clear to further automate them with software.Having metrics on which tickets were linked to which incident models meant that we knew where best to focus our effort. We wrote scripts to comb through log files in the background, make encoding issues quicker and simpler to figure out, automate responses to customers (‘Issue was caused by a change made by app admin user XXX’), and a lot more.These automations inspired an automation tool we built for ourselves based on pexpect: http://ianmiell.github.io/shutit/ But that’s another story. Basically, once we got going it was a virtuous circle of continuous improvement.Back to ProcessGiven you have all these assets, how do you prevent them from degrading in value over time? This is where process is critical.Two processes were critical in ensuring everything continued smoothly: triage and post-incident review.Process – Triage5%-10% of time was spent on the triage process. Again, it took a long time to get the process right, but it resulted in massive savings:Reduce the steps to the minimum useful stepsIt’s so tempting to put as much as possible into your triage process, but it’s vital to keep the value in the process over completeness. Any step that is not often useful tends to get skipped over and ignored by the triager.Focus on saving cost in the processLooking for duplicates, finding the relevant incident model, reverting quickly to the customer, and escalating early all reduced the cost per ticket significantly. It also saved other engineers the context switch of being asked a question while they’re thinking about something else. It’s hard to evaluate the benefits of these items, but we were able to deal with increased volumes of incidents with fewer people and less difficulty. Senior management and customers noticed.Recording the details of these efforts also saved time, as (for example) engineers given a triaged ticket could see that the triager searched for previous incidents with a string that maybe they could improve on. It also meant that more experienced staff could review the triage quality.When I moved to another operations team (in a domain I knew far less about), I cut the incident queue in half in about 3 days, just by applying these techniques properly. The triage process was there, but it wasn’t being followed with any thought or oversight, and was given to a junior member of staff who was not the most capable. Big mistake. Triage must be done – or overseen – by someone with a lot of experience, as while it looks routine and mechanical it involves a lot of significant decisions that rest on experience in the field.And yes, I was the new boss, and I chose to spend my first week doing the ‘lowly’ task of triage. That’s how important I thought it was.Rota the taskNo-one wants to do triage for long, so we rota’d it per week. This allowed some continuity and consistency, but stopped engineers from going crazy by spending too long doing the same task over and over.Process – Post-Incident ReviewThe mirror image of triage was the ‘post incident review’. Every ticket was reviewed by an experienced team member. Again, this was a process that took up about 5% of effort, but was also significant.A standard form was filled out and any recommendations were added to a list of backlog ‘improvement’ tasks which could be prioritised. This gave us a number for technical/process debt that we wanted to look at.CultureI’ve mentioned culture a few times, and it’s what you always return to if you’re trying to enact any kind of change at all, since culture is at root a set of conceptual frameworks that underlie all our actions.I’ve also mentioned that people often focus on the ‘wrong thing’. Time and again I hear people focus on tools and technology rather than culture. Yes, tools and technology are important, but if you’re not using them effectively then they are worse than useless. You can have the best golf clubs in the world, but if you don’t know how to swing and you’re playing baseball then they won’t help much.Culture requires investment far more than technology does (I invested over half a year just writing documentation, remember). If the culture is right, people will look for the right tools and technology when they need to.When given a choice about what to spend time and money on, always go for culture first. It cost me a lot of budget, but forcibly removing an ‘unhelpful’ team member was the best thing I did when I took over another team. The rest of the team flowered once he left, no longer stifled by his aggressive behaviour, and many things got done that didn’t before.We also built a highly effective team with a budget so small that recruiters would phone me up to yell at me what I was looking for was ‘impossible’, but by focussing on the right behaviours, investing time in the people we found, and having good processes in place, we got an extremely effective and loyal team that all went on to bigger and better things within and outside the company (but mostly within!).PoliticsA quick word on politics. You’ve got to pick your battles. You’re unlikely to get the resources you need, so drop the stuff that wont get done to the floor.Yes, you need a monitoring solution, better documentation, better trained staff, more testing… you are not going to get all these things unless you have a money machine, so pick the most important and try and solve that first. If you try and improve all these things at once, you will likely fail.After process, and documentation, I tried to crack the ‘reproducible environment’ puzzle. That led me to Docker, and a complete change of career. I talk about these things a little here and here.Recently, I’ve been working more than never in servers(AWS) and came to realize that having a defined structure when deploying helps me automating the whole process even more. Just as you did back there, I’ve been taking some time to document what I’ve done(and doing) so that later on it benefits myself and my fellow coworkers.Having a defined process and a clear documentation, in my opinion, really helps to get a reproducible environment.Appreciate documenting your experience and see a lot of similarities with past job where we made the same transition with very similar problems.I’d be interested more in how you built out documentation within JIRA itself rather than putting in Confluence.One thing I’ll recommend for SRE teams is capturing performance metrics from applications, services, and systems through graphite/influxdb/promethus/etc so 1) your SRE/NOC team can team/silo specific dashboards to see what’s going on easily and 2) your monitoring system can build alerts against the same data for notifications. We ended up embedding these dashboard within Confluence runbooks/playbooks followed by diagnosing/triaging, resolving, and escalation information. We also ended up associating these runbooks/playbooks with the alerts and had the links outputted into the operational chat along with the alert in question so people could easily follow it back.Definitely want to encourage others for automating as much of recovering from these alerts as possible but also the other operational processes within your organization. It’s embarrassing to tell management one of your team members caused an outage because they failed to copy and paste a command from the runbook in the proper order. Whether you use Ansible, Puppet, Chef, Salt stack, etc, invest in an automation framework.Lastly, I definitely recommend checking out and watching BigPanda’s Monitoringscape (https://bigpanda.io/monitoringscape/) page as they track new and mature technologies most SRE teams leverage.I am the editor of InfoQ China which focuses on software development. We like this article and wish to translate it.Before we translate it into Chinese and publish it on our website, I want to ask for your permission first! This translation version is provided for informational purposes only, and does not make any commercial use.In exchange, we will put the English title and link at the beginning of Chinese article. If our readers want to read more about this, they can click back to your website.Thanks a lot, hope to get your help. Any more question, please let me know.Also would love to hear more on this. I get a lot of pushback when talking about the importance of culture – a success story is always worth sharing. :) Thanks so much for taking the time to write out your thoughts.",Things I Learned Managing Site Reliability for Some of the World’s Busiest Gambling Sites
9810,3810553,2018-02-26 15:00:00,"TNW SitesGet your career on the right track with a year of professional business development for under $100Before you get the wrong idea — this is NOT an Excel training course offered at a ridiculously low price from TNW Deals. Well, it is…but it’s not JUST an Excel training course. Sure, as one of the most popular, widely-used software programs of all time, there’s a lot to be gained from learning all the bells and whistles under Excel’s impressive hood.Want to know how to master not just Excel, but the entire Microsoft Office Suite of applications? Courses here will get you proficient in all of Microsoft’s core disciplines. Looking to start up your own online business with all the growth potential of your current employer? You can dive into that prep as well. Or do you need to develop the mindset to prove you’ve got what it takes for advancement? There are even classes here to help organize and develop your approach to your profession.In all, you’ve got more than 60 courses to work your way through, offering up loads of business tips and ideas to take your career to a new pinnacle. Classes even come with exclusive members-only live courses, including Q and A sessions with industry-proven veterans.And just to show it isn’t all about being on the fast track, you can even take a few courses just for fun, including photography or chess.Once you’re finished, these courses (the business ones, not the chess) also count toward various career development programs like CPD UK Accredited Certificates of Attainment, offering verifiable proof that you’re taking your career seriously.It’s a veritable college education from Excel with Business, all available to you at any time at a level of access that would usually cost nearly $350. Get in on this limited time offer now to get one year of unlimited training for only $99.",Get your career on the right track with a year of professional business development for under $100
9811,3810554,2018-02-26 14:37:18,"About TNWTNW Sites5 iconic Nokia phones we desperately want to see remadeNokia licensor HMD has captured the attention of the phone-buying public by offering remakes of classic phones from the Finnish giant’s past. It started by refreshing the iconic (and indestructable) Nokia 3310 with a new colorful and capable version. Now, it’s the turn of the Nokia 8110, which is best known as the slider phone used by Neo in The Matrix.Nokia’s history is an almost bottomless well of handset brilliance. Its history prior to 2010 is of releasing iconic and intriguing phones. Some were roaring successes that ultimately defined the entire industry. Others, although undeniably ambitious, fell flat on their faces.Nokia 9000 CommunicatorLaunched in 1996, the Nokia 9000 Communicator was a fascinating phone. At first glance, it looked like a bog-standard “brick” phone. It was clad in grey, had big chunky buttons, and a postage stamp-sized LCD to see who’s calling. It wouldn’t look out of place in the hand of someone working in finance.However, it held a secret. Crack it open from its side, and you get access to a larger screen, and a miniature QWERTY keyboard.This was no dumb phone. It came with a small array of applications, including a web browser, a notepad and calendar, and the ability to send faxes (which was vital in 1996, I’m sure). Watching this commercial, it feels like an immediate ancestor to today’s smartphones. It also feels as though Nokia aimed the 9000 Communicator squarely at the “odious city trader” market.At the time, the Nokia 9000 Communicator was a hard sell to ordinary punters. You’d most likely find this in the pocket of a high-flying Gordon Gecko than anyone else. That’s because it came with a steep price tag, and retailed at $800. Adjusted for inflation, that’s nearly $1,250 in today’s money, or just slightly more than the base iPhone X.Technically the Nokia 9110, but close enough. The two phones looked very similar.So, what would the Nokia 9000 Communicator look like today? Well, you could expect a bigger, higher-resolution screen. It’d be able to do more, both in terms of sheer computation, but also as it’d have access to a wealth of third-party apps.Personally, I’m not too fussed about what it could do, or the specs. I just desperately want to see the return of a form-factor that has largely died out. As someone who prefers physical keyboards over virtual ones, there’s an undeniable appeal in being able to open (and use) a phone like a laptop.Fortunately, there’s some good news on that front. Remember Psion? In the 1990’s, the iconic British tech company offered a line of PDAs that looked and worked much like the Nokia 9000 Communicator, albeit without the phone element.Now, they’re coming back. The Gemini from Planet Computers is a modernized remake of the Psion Series 5, runs Android and Linux, and supports 4G data and voice calls. Gemini has successfully completed a run on Indiegogo, and should make its way to backers soon.Given the phone landscape has moved on, this is unlikely to achieve mass-market success. The Gemini caters to a small niche. But for those who want a more productivity-focused device, it’s certainly nice to have options, especially if HMD doesn’t revive the Nokia 9000 Communicator.N95The Nokia N95. What a phone.When you think about it, the Nokia N95 was a triumph of timing. It arrived in March 2007, right before smartphones became ubiquitous. It offered many of the must-haves offered by smartphones, including GPS navigation and a 5MP camera, but in the body of the then-familiar feature phone.It was a roaring success. Within one year, Nokia sold seven million handsets, including one million in the UK. It also had an impressive staying-power. Nokia discontinued the N95 after three years on the market. That’s pretty remarkable when you consider the short shelf-life of today’s phones.But it was also flawed. The N95 was slow. Symbian, compared to Android and iOS, was a clunky mess. Build quality was an issue. And at a time when people were joining Facebook and Twitter en-masse, old-school T9 keyboards simply weren’t cutting it, especially compared to the sleek keyboards found on Apple and Blackberry phones.I wonder what the N95 would look like, if remade for 2018’s audience. Personally, I’m thinking of a feature phone with a kick-ass camera. Hardware media buttons would be nice too, especially if there’s a Spotify (or equivalent streaming service) app to accompany them.Broadly speaking, I feel as though there’s a small market of people who want first-class phone experiences, but don’t really want to carry a smartphone. A revamped N95 would cater to this market. But the question remains if this market is big enough to justify a revival.Probably not. But it’s nice to imagine “what if,” right?N97Oh, the Nokia N97. What a pity this intriguing handset missed the mark.Nokia released the N97 two years after the N95. It offered a full (resistive) touchscreen, alongside a slide-out QWERTY keyboard. Users got the best of both worlds from this handset, which felt like an answer to the HTC Dream (G1) android flagship.Its strongest features were its battery life, storage space, and camera, but it struggled in other more crucial departments. The Symbian OS was a confusing mess, and the keyboard wasn’t nearly as pleasant to type on as BlackBerry’s phones.Sales were solid, but reviews were damning. The most memorable review of the N97 came from Gizmodo’s Matt Buchanan, not least because its title flatly declared “Nokia N97 Review: Nokia Is Doomed.” Other reviews were similarly unkind.I know I’m a minority, but I do love the hybrid touch-type form factor, and I wonder how the N97 would have performed if it came with less dreadful software, and more capable hardware.With HMD willing to revive Nokia’s more unusual phones (I mean, the 8110 is shaped like a banana and is best known from The Matrix), maybe it’s worth giving this beleaguered blower another chance?N-GageThe Nokia N-Gage was a pioneering phone. At a time when people were commonly carrying both a mobile and a hand-held console (like the GameBoy Advance, Nintendo DS, or PlayStation Portable), Nokia decided to combine the two into one pocketable device.Nokia released the short-lived N-Gage in 2003, and over its short life, managed to shift 3 million units. Along the way, it managed to attract some big-name developers, who launched mobile versions of existing console franchises.The Nokia N-Gage amassed a decent catalog of titles, with games like Splinter Cell: Chaos Theory, Tony Hawk’s Pro Skater, Red Faction, Rayman 3, and Call of Duty. But it wasn’t enough, and the N-Gage was a commercial failure.That’s partly because of the stiff, entrenched competition. Nokia wasn’t the only company to take on Sony and Nintendo at the time. There was the ill-fated Gizmondo, which crashed and burned after selling only 25,000 units (although there were other issues at play here).But also, the hardware was a bit naff. The N-Gage was an ugly looking phone, and suffered from several usability woes. Changing games was an ordeal that practically required the user to have a degree in mechanical engineering, and to make a call, you had to hold the phone awkwardly.What if HMD made another gaming phone under the memorable N-Gage marque? Perhaps with built-in hardware controls that gracefully slide away when not in use?There are some great games on Android, but I’m yet to see an incredible gaming phone. Perhaps a future N-Gage is that phone?Nokia 7280Unashamed product placement aside, the Nokia 7280 was an intriguing handset. It was a so-called “lipstick phone,” simply because it roughly matches the same profile and dimensions as a tube of lipstick. Unlike other phones, it didn’t have a keyboard. It instead used a iPod-esque clickwheel.To access the camera, you needed to slide one part of the phone out. And when not in use, the screen transformed into a mirror.The Nokia 7280 was a fashion statement first, and a phone second. It’s probably not the most usable phone ever, but people often willingly suffer for the sake of fashion.I never owned the Nokia 7280, nor was I (at the time, a high school student) part of its target audience, but I grudgingly respected the fact that Nokia was willing to experiment with something decidedly left-field and try something new.Overall, I’d love to see how a tech company (cough HMD cough) would approach designing a minuscule phone. One that lacks the most basic input devices. I bet it’d be really interesting.What’s next?HMD now has two old-school Nokia phones in its lineup. It’s calling these Nokia Originals, suggesting the company isn’t finished unearthing the best gems from Nokia’s past.With a storied company like Nokia, where do you start? Nokia confidently made its mark over several decades. At one point, Nokia was the leading phone manufacturer, setting the tone for an entire growth industry. Looking back, there’s so much stuff to admire, even if it wasn’t exactly appreciated at the time.And I know that by the phones I left out, simply because I ran out of time and word-count. My colleague Abhimanyu wanted me to mention the legendary Nokia N8 (with its weird camera), and the chunky yet quirky Nokia 6600.So, over to you. What retro Nokia handsets do you want to see reborn? Tweet me here and let me know.Exploiting feelings of nostalgia is one way to design pretty products, but our Design Thinking track at TNW Conference 2018 offers more. Find out about our flagship Amsterdam event here.",X retro Nokia phones we want HMD Global to remake.
9812,3812956,2018-02-26 00:00:00,"Tech takes overWith 7,500 firms employing 120,000 workers, New York is the sector's official second city. And the boom is just beginningPhoto: Buck Ennis EARLY ADOPTER: Ma knew launching in New York City would be key to Zola's success.In 2013 Shan-Lyn Ma, then head of the food division at flash-sales site Gilt Groupe, wanted to launch her own startup. For months, she and her eventual co-founder, Nobu Nakaguchi, considered what they had learned at Gilt about building customer demand, growing fast and running a retail-meets-tech company. Gilt founder Kevin Ryan invested seed money for their brainstorming sessions and the early months of running what was to become Zola, their online wedding registry. “Get started,” he told them.Ma grew up in Australia with Yahoo founder Jerry Yang’s picture on her wall. After college she enrolled at Stanford’s Graduate School of Business, then interned at Yahoo and eventually got a job there. As that company began its long decline, Ma started looking for a product- oriented position at a Silicon Valley startup but found herself shopping every day at New York–based Gilt. She applied for a product manager job but wrangled an interview only because she agreed to pay her own travel expenses: Gilt executives simply couldn’t believe that a rising tech executive would leave Silicon Valley to come to New York.Ma got the job as Gilt’s first product manager, launched its mobile app, became that division’s team leader, and eventually created the wine and food division Gilt Taste. Then she realized, I want to do it all over again. Ma briefly considered launching Zola in the Bay Area but soon found that New York had many more reasons to stay than she had accounted for. Five years later, downtown-based Zola has raised $40 million in capital, hired 100 employees and pushed annual sales above $120 million.article continues below advertisementMa is just one of scores of CEOs, engineers, product managers, marketers and branding experts driving a tech boom that has taken hold of the city, freeing the economy from dependence on Wall Street and making New York home to thousands of new companies in dozens of verticals. Some are early stage startups, others billion-dollar behemoths, but all are aiming to change how business is done, media is consumed, products are purchased and brands are built.New York has now hooked its economic future to the tech sector—the most dynamic aspect of the American economy—and is emerging as the second headquarters of the whole industry. The dividends have already been enormous. Tech firms have provided jobs for tens of thousands of millennial graduates from the nation’s best colleges. The companies are filling office space as other industries cut back. Amazon has New York on its HQ2 short list and announced in September that its $55 million Hudson Yards office would employ 2,000 by 2020. Companies announce new deals and venture capital rounds every day; last week Roche bought five-year-old health-tech firm Flatiron Health for $1.9 billion.There are now 7,500 New York–based tech companies—defined as those whose principal business is tech and its applications—fueled by $38.4 billion of investment in the past five years and supported by accelerators, incubators, meet-ups and universities. The companies employ 120,000 people, 60% more than a decade ago, according to the state comptroller.Photo: Buck EnnisMeet-ups, like this one for UX designers, help power the tech ecosystem.Despite that momentum, virtually all local start-ups are losing money, and only a few have achieved scale. Some of the biggest brands that launched here have faltered, none has become a household name, and initial public offerings remain few and far between.The first tech boom, in the 1990s, centered on DoubleClick, a software company that placed advertising on websites. It went public and its founder, Kevin O’Connor, briefly became a billionaire on his stock holdings. Hundreds of others launched, but almost all of them disappeared when the tech bubble burst in 2001.The second generation of entrepreneurs jumped back in two years later. Again, they built companies serving the city’s established industries, expanding to online media and financial tech. The 2008 meltdown inspired other fortune seekers to make the leap, especially from the newly tarnished financial sector. In the past five years, what used to be called Silicon Alley, a tiny sliver of the city’s economy, has transformed itself into a thriving ecosystem where founders, engineers, investors and talent of all kinds are determined to transform a host of industries. Each generation’s experience leads to the next generation’s innovations. If you trace the line that runs from Kevin Ryan, who succeeded O’Connor at DoubleClick, to Gilt, then to Zola and the three other local companies started by Gilt alums, you reach generation 3.0.Shopping spreeWhen a couple get married, there are a lot of home goods to buy, a lot of guests to buy them and a deadline for those purchases. Wedding registries are a $19 billion industry dominated by traditional department stores’ online sites. But most wedding registries are clunky to use, both for newlyweds and their benefactors. Looking at those facts, Ma saw an opportunity.She made Zola an easy-to-use platform for the digital aspects of modern wedding planning, including sharing reception details and collecting guests’ addresses. But the business makes a profit off markups on registry items sold. An upcoming “I do” drives demand as effectively as the impending end of a flash sale once did at Gilt. Other perks of the niche include ease of predicting behavior, freedom from inventory—vendors ship products straight to the newlyweds—and the elimination of returns since gifts can be exchanged virtually, before they leave the warehouse.Zola’s 2017 sales represent less than 1% of the total registry market, and the jury is still out on whether the five-year-old company will succeed. But Ma, a 2017 Crain’s 40 Under 40 honoree, has bigger goals than just crashing wedding registries.“What we’re really building is the back-end technology for the next generation of commerce,” she said.Consumer tech is particularly strong here because of the pool of founders who worked at the previous iteration of e-commerce companies or came up through traditional retail careers. These executives understand the sector’s mounting challenges and think they have solutions. The same holds true for investment bankers advising big companies and learning a great deal about their businesses—and the holes in them. “All that knowledge is sitting there,” said Andrew Mitchell, an investor at Brand Foundry, a venture capital firm that focuses on consumer products and has backed Birchbox, Harry’s and Warby Parker.Two years before Ma started Zola, New York native Neil Blumenthal and three classmates at Wharton were figuring out how to use the internet to do direct-to-consumer retail sales. Their product, inexpensive and fashionable eyeglasses, became the base for a recognizable brand that resonated with young people. Within a month they had met their first-year sales goals, sold out of their top 15 styles and stretched their waiting list to 20,000.Warby Parker’s business model is to do everything from design to fulfillment to managing a customer’s entire experience with the brand. That means its workforce spans an enormous range of skills. Warby employs software engineers to build applications that let customers “try on” glasses virtually and data scientists to scrutinize customer feedback for rapid marketing and design updates. There are brand managers, eyewear designers and experts in sourcing, transportation, shipping, merchandising, finance and fulfillment. Warby also has eight brick-and-mortar stores in New York City, which it stocks and staffs.“There is no other place Warby Parker could exist,” Blumenthal said. “We are at the intersection of the tech startup world, the fashion world, the retail world and the social enterprise world.”An endless series of consumer tech companies has followed in Warby’s vertically integrated footsteps. These local companies sell mattresses (Casper), razors (Harry’s) and meal kits (Blue Apron), as well as leather sneakers, electric toothbrushes, rentable wardrobes, additive-free tampons and frozen smoothie packs. Like Warby and Zola, many founders say they couldn’t have built their businesses anywhere else, and plenty of opportunities remain.“Health and wellness is one space that’s up-and-coming,” said Rebecca Kaden, a general partner specializing in consumer tech at Union Square Ventures, the city’s leading VC firm, who is in the process of moving back to New York after nearly a decade in the Bay Area. “Plus, commerce platforms that don’t feel like commerce—and especially don’t feel like Amazon.”Like Zola, they are all unprofitable. With $250 million in revenue, Warby Parker accounts for virtually all online glasses sales, but that’s only 3% of the market. While it claims its brick-and-mortar expansion is pioneering a complete new way to do retailing, the strategy is a concession that it can’t succeed as a purely tech company.I <3 New YorkWhile New York’s original tech verticals—fintech, adtech and media—remain healthy, new frontiers have opened in several big industries, including health care, real estate and nonprofits. That’s made insiders more optimistic than ever.“How can you not be?” said Adam Enbar, whose coding-education company, Flatiron School, was acquired by WeWork last year for an undisclosed amount. “Look at where we’re living. We’re the luckiest people in the world who have ever lived in the history of time.”New York’s tech sector is thriving in large part because the people who work in it simply want to live here. They can afford to. The average salary is $150,000, the state comptroller says. Engineers make more, and that figure rises quickly for specialists and managers.Some of the employees come to New York straight from college, while others switch careers by training at coding boot camps like Flatiron and still others come from Wall Street. East Coast natives and alumni of East Coast universities who went west to work in tech are flooding back from the Bay Area.“In San Francisco, tech is the only conversation,” said Matt Harrigan, who runs a no-equity accelerator called Grand Central Tech. “The richness and vibrancy of New York is calling to people.” Many are finding that vibrancy in the rapidly gentrifying areas of Brooklyn where they live.Talented tech employees are in such demand that they hear from recruiters daily, but most employers find them quite loyal. At Wal-Mart-owned jet.com, there are no referral bonuses for hires. Meals on the house tend to be occasional and relatively modest. “Most of us can’t afford to outspend Facebook and Google on free lunch,” said Daniel Chait, CEO of Greenhouse, a recruiting software company.BOOM TOWN7,500 Estimated number of New York–based tech companies120K Number of New Yorkers employed by those firms, a 60% increase over the past decade16.7M Square feet of Manhattan office space occupied by tech tenants in 2016$38.4B Venture capital investment in local tech startups in the past five years$150K Average salary of city tech employees, second only to Wall StreetVenture capitalists, founders and employees describe the New York tech culture as inclusive and tight-knit, perhaps because tech feels like the underdog twice over: New York still sees less than half the number of deals as the Bay Area, and Wall Street traders, investment bankers and hedge fund managers are still making multiples of product managers’ salaries. They also built it to be that way. The ecosystem has had many intentional hands shaping its personality, such as Michael Bloomberg, who put $200 million of city money into Cornell Technion on Roosevelt Island; David Tisch who imported the TechStars accelerator from Boulder, Colo.; Julie Samuels, who formed Tech:NYC; and Andrew Rasiej of Civic Hall and the New York Tech Alliance. Add to that the scores of venture capitalists who have realized that mentoring founders, hosting entrepreneurs in residence and posting thoughtful reflections on Medium are good investments.The culture attracts engineers who might otherwise work on Wall Street. After graduating from Columbia with a master’s degree in computer engineering in 2010, William Zhang wanted to stay in the city. He accepted a job at Credit Suisse but wasn’t always fully engaged and after two years moved to Goldman Sachs, which now employs 9,000 computer engineers, roughly one-quarter of its staff. After two years he had learned all he could and decided to join the startup world.Soon after he was recruited by Honest Buildings, one of the startups planning to revolutionize the staid world of real estate. He is now co-head of the engineering team.On Wall Street, new computer engineers get hired at $120,000, but senior managers can make $400,000 plus the bonuses Wall Street hands out every winter. Startups like Honest Buildings pay less, but they offer stock options. If Honest Buildings goes public and the stock soars, Zhang will do quite well.In the meantime, there are other rewards. “I work with all the teams and meet the clients,” he said. “I see a bigger picture in the startup world and have a much bigger impact. They value me here.”Big fishWhen California tech giants Google and Facebook first came to New York, they opened tiny offices and hired locals to run ad sales shops. Today they house the computer engineers developing the Google cloud and applying artificial intelligence at Facebook in building after building. Nearly 10% of tech workers in New York are at Amazon, Facebook or Google.In 2000 Tim Armstrong, one of Google’s first New York hires and now the CEO of Oath, began building an ad sales force from a townhouse at 86th and Broadway. Three years later he was the head of the sales division when Google paid $3.1 billion to acquire DoubleClick and its web advertising technology and hundreds of internet-savvy salespeople. It was about three times what the seller had paid for the company only two years before.Google has not bought many other city-based companies. But it did pay more than $2 billion to acquire the Chelsea Market building because it desperately needs more space. The company employs some 7,000 people in the city—40% more than its 2015 roster. Nearly two-thirds of them are engineers working on every major business line.If Google acted to take advantage of the business strengths of the city, Facebook’s decision to open an office in New York was driven by another common phenomenon: A New Yorker wanted to come home.Serkan Piantino grew up in the New York suburbs and wanted to return to the area in 2004, when he graduated with a degree in computer science from Carnegie Mellon in Pittsburgh. The best opportunities at the time were in financial services, so he joined a then-small hedge fund called Bridgewater Associates. But like Zhang, he didn’t feel fully engaged in the work. With the help of a friend at Facebook, he landed a job there and moved to California.“I was attracted because it was a small company with a crazy idea about building communication skills,” he remembered. “I reluctantly moved out there.”Photo: Buck EnnisMOVING MAN: Plantino wanted to come home, so Facebook came with him.Piantino was a big success at Facebook, building teams and developing products crucial to its explosive growth. Almost all the time he was badgering his bosses to let him move back to New York. When Facebook allowed someone to work remotely from Denver, Piantino won. In 2011 Facebook officially opened its New York office with a ceremony hosted by Bloomberg, who was mayor at the time.Today Facebook has more than 1,000 local employees, said Jeff Reynar, the company’s engineering director. They work on its news feed, particularly the story rankings, build the infrastructure that has allowed the messenger feature to scale and develop the local applications and infrastructure that connect consumers to businesses. In the past two years, half of Facebook’s Instagram team has been anchored in New York.Big companies, including Google and Facebook, are now rushing headlong into artificial intelligence, with their teams based primarily in New York to take advantage of the talent that got its start at Bell Labs in the 1990s.So much of the coverage of AI’s job-destroying potential is hype, said Ron Brachman, a Bell Labs alum now at Cornell Tech, but the work being done by New Yorkers in that sector will transform many industries. By centering artificial intelligence in the city, the California tech giants have made it clear they are here to stay.Out of the incubatorDespite the hundreds of startups, the billions of dollars in venture investment they have amassed and the tens of thousands of millennials they have attracted to the city, the tech sector here remains a promise only partially fulfilled.Venture-funded startups often grow revenue and head count fast, but they don’t make a profit. New York dominates online media, but with the possible exception of Business Insider, none is profitable. Vice, valued at almost $6 billion, missed last year’s $805 million revenue target by more than $100 million. It is the same story at BuzzFeed and the dozens of other verticals that have established themselves in New York.Some of the highest-profile companies have stumbled badly. Yahoo bought social media site Tumblr for $1.1 billion in 2013, an amount the company was forced to completely write off. Meal delivery service Blue Apron cut the price of its IPO to $10 a share amid weak demand, and the stock has fallen to about $3. Investors are hanging on, hoping it is acquired by Amazon to bolster Whole Foods. Online lender OnDeck is stuck at $5 a share, down 75% from its initial price. Even Mongo DB, the enterprise software company with bright prospects, is a couple of dollars off its IPO price. Critics say raising money is too easy, which can entice founders to take on too much too soon, overvaluing the company.The surprising lack of public awareness of New York’s tech scene is partly the consequence of not having a single homegrown company achieving enough scale to acquire other companies the way Google, Facebook and Apple have. Industry insiders like to cite Bloomberg L.P. as the exception, but a newer contender is WeWork, which acquired three local startups last year.In addition, there’s still not quite enough talent. A shortage of educated, experienced tech workers has led to a recruitment war for engineers, executives and specialists in data, machine learning and AI. Cornell Tech, a new NYU campus and Columbia’s expansion will churn out qualified people, but the pipeline still needs feeding. Some hope future workers will come out of New York City public schools and universities, but the de Blasio administration’s new “computer science for all” initiative amounts to $8 annually per student, which will not put many on track for a coding job. The city also needs to keep itself in shape: Companies whose employees have fanned out into northern Brooklyn are very worried about the looming L train shutdown.But the biggest gap between New York and the Valley are successful IPOs. Companies whose stocks soar after going public help create the virtuous cycle that drives a tech ecosystem. Executives, workers and investors cash in stock options and become wealthier than they ever expected. They take that money and use it to finance the next round of startups and spend on homes and philanthropy.Some people say IPOs are not as important for New York tech companies. Because they pioneer new ways of doing business, they say, they will eventually be acquired by legacy companies at sky-high prices. Earlier this month pharmaceutical giant Roche paid $1.9 billion for complete ownership of Flatiron Health, whose technology could transform how doctors treat cancer. The total value of the agreement was $2.1 billion.Even such big deals won’t be enough to trigger the virtuous cycle. “The scale of the exits isn’t of the magnitude sufficient to fund other generations of tech entrepreneurs,” said David Tisch, one of the most important backers of early stage companies in the city.By most counts, there are 25 homegrown companies that could launch IPOs within the next two years. How many people cash out and for how much could shape the future of the sector—and the city.",Tech takes over
9813,3812962,2018-02-26 08:00:01,"Y Combinator’s future as a Silicon Valley kingmaker is at a make-or-break momentWritten byShareWritten byDropbox on Feb. 23 announced plans to raise $500 million through an initial public offering (pdf), making the 11-year-old file-storage startup the first company in Y Combinator’s portfolio to make a market debut.It’s a big moment, and a potential inflection point, for the influential incubator. Y Combinator has a roster of big companies, including Stripe and Airbnb, that plan to go public eventually.Today, Y Combinator’s model is well-known: Its incubator program, held for three months each summer and winter, gives dozens of startup founders the chance to gain expert advice on refining and selling their products—and offers a $120,000 investment in exchange for a 7% stake in their companies.But when Y Combinator launched in 2005, the idea sounded dubious to a lot of people. The incubator’s cofounders—Paul Graham, Jessica Livingston, Trevor Blackwell, and Robert Morris—set out to buck conventional wisdom about smart startup investing. As Graham explains in a 2012 blog post, his thinking was that “investors should be making more, smaller investments, they should be funding hackers instead of suits, they should be willing to fund younger founders.” Most importantly, Graham notes, Y Combinator decided to fund companies “synchronously,” launching with a summer program for the undergraduate founders of eight chosen startups in which young people would receive feedback and support on refining and selling their products.The incubator program, described by the New York Times (paywall) as “a sleep-away camp for start-up companies,” and the incubator’s willingness to take risks on college students made it easy for critics to trivialize Y Combinator at first. “When people came to YC to speak at the dinners that first summer, they came in the spirit of someone coming to address a Boy Scout troop,” Graham writes. But among those first eight companies were some pretty choice picks, including Reddit, now valued at $1.8 billion.As Y Combinator companies proved successful, its incubator program gained gravitas. Soon investors were flocking to Demo Day, the grand finale of the program, in which founders have the chance to pitch their startups. Meanwhile, applications flowed in: The boot camp now boasts an acceptance rate of less than 3%, lower than Ivy League universities. Articles about how to aceY-Combinatorinterviews—and dealwithinevitablerejections—have become their own internet genre. (In fact, Y Combinator famously rejected Dropbox founder Drew Houston the first time around, when he applied with an idea for an SAT prep company in 2005.)It’s difficult to overstate Y Combinator’s impact on the startup scene. The prestige associated with the incubator made young people see startup life as both more exciting and potentially more lucrative than working for Microsoft or Apple. “By providing training and producing a group of successful alumni, YC has helped popularize the idea that startups are a viable career,” The Economist explains. The incubator also changed the relationship between investors and startups. Its alumni network functions as a kind of union, giving founders more bargaining power, while the incubator’s model has has increased the cost of early-stage investing.As of August 2017, Y Combinator had founded 1,430 companies. Plenty of those failed, but that’s normal for young businesses. And the potential upside is huge. The accelerator says that 64 of its companies are valued at over $100 million, while 10 are worth more than $1 billion—including Dropbox, valued at $10 billion by private investors.Under president Sam Altman, Y Combinator has kept expanding its ambitions, launching a free online Startup School to connect with thousands of aspiring entrepreneurs. As Wired explains, Altman is a true believer (paywall) in the power of startups to heal the world’s social ills, which means that launching them at a rate of 250 or 300 a year isn’t enough. That’s why his new goal “not just nurturing startups, but also fomenting startup-like innovation as a tonic to fix the world.”But even as Y Combinator spreads the global gospel of startups, it’s still got a backlog of unicorns—companies worth more than $1 billion—that have yet to go public. Now that Dropbox has taken the jump, other unicorns will be watching closely, including its siblings at Y Combinator. The success or failure of Dropbox will reflect on Y Combinator, too, and its status as a Silicon Valley kingmaker.",Y Combinator’s future as a Silicon Valley kingmaker is at a make-or-break moment
9814,3812963,2017-12-11 12:00:00,"In-Space Manufacturing Is About to Get a Big TestBy Mike Wall, Space.com Senior Writer | December 11, 2017 07:00am ETMOREMade In Space has partnered with the New Jersey company Thorlabs on a project to produce ZBLAN optical fiber in orbit. This machine, which is a bit bigger than a microwave, is scheduled to launch toward the International Space Station abroad a SpaceX Dragon capsule on Dec. 12, 2017.Credit: Made In SpaceA bold plan to rev up off-Earth manufacturing is about to get a big test.A small, privately built machine designed to make optical fiber is launching toward the International Space Station (ISS) aboard SpaceX's Dragon cargo capsule tomorrow (Dec. 12).If all goes according to plan, this little factory — which is owned by California-based startup Made In Space — will churn out stuff that's good enough to sell here on Earth, opening up space to greater commercial use. [3D Printing: 10 Ways It Could Transform Space Travel]""We believe that we're really on the verge of something truly amazing here, with kind of using space for industrial purposes for some of the very first times,"" Made In Space CEO Andrew Rush said during a news conference in late November.Made In Space is no stranger to the ISS. In 2014, a 3D printer built by the company made it to the orbiting lab, as part of a pilot project with NASA that aimed prove out the technology's use in space. That machine performed well, so the company launched a commercial version, known as the Additive Manufacturing Facility (AMF), to the station in 2016. The AMF continues to produce stuff for a variety of customers today. (NASA owns the 3D printer that launched in 2014.)The machine that will launch tomorrow, which Rush said is a bit bigger than a microwave, is more specialized. It will produce a type of exotic optical fiber known as ZBLAN, which Made In Space representatives said is difficult to make on Earth: The high-gravity environment here causes tiny crystalline flaws to emerge in the fiber.""In space, ZBLAN optical fiber can be produced without these crystals, providing superior data-transmission capabilities compared to both Earth-produced ZBLAN and traditional silica-fiber optic lines,"" Made In Space representatives wrote in a description of the project. ""This microgravity-produced fiber has numerous applications, including trans-Atlantic telecommunications, high-speed internet [and] lasers, as well as enhancing technologies in space.""The Made In Space machine will produce at least 330 feet (100 meters) of ZBLAN on this trial run, pulling the material like taffy from heated feedstock material. The fiber will then come back to Earth (aboard the same Dragon that took the feedstock up) for study and characterization, with the aid of project partner Thorlabs, a New Jersey-based company.If the space-made ZBLAN looks good, Made In Space will aim to sell it, Rush said. And that will be just the beginning.""We do intend to develop this into a commercial product that we sell in large quantities,"" he said. ""The progression of that will be kind of a step-by-step approach.""Launching raw materials to space is expensive, but ZBLAN is the sort of high-value product that makes on-orbit manufacturing economically feasible, Rush has said. And the company can make a lot of the material from a relatively small launch mass — about 2.5 miles (4 kilometers) of fiber per 2.2 lbs. (4 kilograms) of ZBLAN ""preform,"" Rush said during the news conference.There's a lot of other cool stuff going up on the Dragon tomorrow as well: for example, a super-sensitive space-junk sensor, an experimental new bone adhesive, and an instrument that will measure the amount of energy going into and out of the Earth system with incredible precision.And the launch itself will be a milestone in SpaceX's quest to develop reusable spaceflight systems, a key priority of the company's founder and CEO, Elon Musk. The Dragon flying tomorrow already has one space mission under its belt, as does the first stage of the Falcon 9 rocket that will carry the capsule aloft.Michael was a science writer for the Idaho National Laboratory and has been an intern at Wired.com, The Salinas Californian newspaper, and the SLAC National Accelerator Laboratory. He has also worked as a herpetologist and wildlife biologist. He has a Ph.D. in evolutionary biology from the University of Sydney, Australia, a bachelor's degree from the University of Arizona, and a graduate certificate in science writing from the University of California, Santa Cruz. To find out what his latest project is, you can follow Mike on Google+.",In-Space Manufacturing Is About to Get a Big Test
9815,3813120,2018-02-26 13:53:08,"Apple could be working on AirPods-like over-the-ear headphones0According to KGI analyst Ming-Chi Kuo and Apple Insider, Apple could be working on a brand new pair of wireless headphones for later this year. While the company already owns Beats, those over-the-ear headphones could be sold under the Apple brand.It seems like the AirPods are a success, and the company wants to capitalize on this market. The new headphones would feature the same convenience of the AirPods, but with better sound quality. Apple could ship them at some point in Q4 2018, just in time for the holiday season.Some users also can’t use the AirPods because of the shape of their ears. This new pair of headphones should solve this issue. Headphones are usually more expensive than wireless earbuds, so this new accessory could cost even more than $159.Kuo has heard that Apple is working with metal injection molding company SZS. That’s probably how KGI managed to learn about this project. It’s unclear how different they’re going to be from Beats-branded wireless headphones.In particular, you can already find Beats Solo3 with an Apple W1 wireless chip. It uses the same wireless chip as the one that you can find in the AirPods.But if you combine today’s rumor with previous rumors, Apple could unveil a brand new lineup of wireless accessories later this year. There could be new AirPods with always-on “Hey Siri” as well as those new headphones. They could run the same improved W chip.Apple also announced a wireless charging case for the current AirPods. Slowly but surely, Apple is building a new product line of audio accessories.",Apple could be working on AirPods-like over-the-ear headphones
9816,3813121,2018-02-26 13:41:33,"Pop-up cameras could soon be a mobile trend0There’s an interesting concept making its way around Mobile World Congress. Two gadgets offer cameras hidden until activated, which offer a fresh take on design and additional privacy. Vivo built a camera into a smartphone concept that’s on a little sliding tray and Huawei will soon offer a MacBook Pro clone that features a camera hidden under a door above the keyboard.This could be a glimpse of the future of mobile design.Cameras have long been embedded in laptops and smartphones much to the chagrin of privacy experts. Some users cover up these cameras with tape or slim gadgets to ensure nefarious players do not remotely activate the cameras. Others, like HP, have started to build in shutters to give the user more control. Both DIY and built-in options require substantial screen bezels, which the industry is quickly racing to eliminate.With shrinking bezels, gadget makers have to look for new solutions like the iPhone X notch. Others still, like Vivo and Huawei, are look at more elegant solutions than carving out a bit of the screen.For Huawei, this means using a false key within the keyboard to house a hidden camera. Press the key and it pops up like a trapdoor. We tried it out and though the housing is clever, the placement makes for awkward photos — just make sure you trim those nose hairs before starting your conference call.Vivo has a similar take to Huawei though the camera is embedded on a sliding tray that pops-up out of the top of the phone. This is in response to what Engadget’s Chris Velazco calls, “as close to a full-screen phone as I’ve seen.”At this point both of these options are silly alternatives to proven solutions but shows an attempt to evolve standard design. Being moveable devices, both options are more likely to break or fail. Yet I, for one, hope this design is iterated upon and will usher in a new age of gadget design to get us out of the boring age of slate designs.",Pop-up cameras could soon be a mobile trend
9817,3813122,2018-02-26 11:15:26,"Acorns, the financial management service for everyone else, adds 3 million users0Acorns, the financial management service focused on getting low- and middle-income households to invest and save responsibly, has reached 3 million users in the United States.The company has roughly $800 million in assets under management, with accounts coming from primarily low and middle income customers across all fifty states, according to Acorns co-founder and chief executive Noah Kerner.Unlike other savings and investment services — like the automated advisory services on offer from Betterment or Wealthfront, or the no-fee stock and cryptocurrency trading service from Robinhood — Acorns is trying to introduce responsible investing principles and savings methods to consumers who have never had the option before.“I’m not focused on people with money,” Kerner says. “I care about hard working Americans who are trying to build a better life every day.”There are a few mechanisms that have driven the growth of the service — from the “found money” offer where companies will pay a certain percentage of a purchase or a set dollar amount toward a into user’s Acorns account to boost savings.There are now over 200 brand partners in the company’s “found money” program including Nike, HotelTonight, Dollar Shave Club — as well as Apple, AirBNB and Zappos. The most recent addition — and the one that conceivably has the biggest reach among mom and pop consumers is Sam’s Club (which will automatically invest whenever an Acorns user shops at online or in a store). In all, Acorns has managed to invest $2 million into customer accounts through the partnerships.Acorns has also integrated its service with PayPal recently — so PayPal customers can access Acorns via its website and mobile app.All of this is contributing to a huge boost in new accounts. According to Kerner, Acorns added 270,000 accounts in January alone and added over 500,000 accounts in the last three months.Soon there will be even more ways for Acorns customers to invest — and with better tax advantages. The company will be rolling out an automated individual retirement account feature, which already counts over 300,000 people on the waiting list.An IRA tool will allow Acorns customers to deduct the contributions into the accounts from their taxes (the investments are treated as pre-tax income) and are only taxed as income on retirement — so investors don’t have to worry about any capital gains.“I really do believe that the next generation of great financial companies are going to be defined by integrity,” says Kerner. “We’re talking about hardworking Americans who are saving 5% to 7% … of their income. We’re not a wealth management platform for the 5%…I don’t give a fuck about people who have money. I’m focused on people who are working hard to achieve the American dream.”","Acorns, the financial management service for everyone else, adds 3 million users"
9818,3813123,2018-02-26 10:48:03,"Crooks launder money using real (and fake) Amazon ebooks0In a fascinating post, Brian Krebs and a group of security researchers have found a method for laundering money through expensive ebooks. It started when author Patrick Reames received an Amazon tax form saying he’d made $24,000 selling his books through Amazon’s CreateSpace, the company’s book-printing arm. His books didn’t sell nearly that many copies, he thought, and when he investigated he found a $555 book full of gibberish being sold under his name.The scam is simple: crooks use stolen credit cards to buy their own fake ebooks for inflated prices and pocket the 60% commission. Amazon and CreateSpace, obviously, also get their cut. Doing this a few times can rack up earnings with ease.But that didn’t stop someone from publishing a “novel” under his name. That word is in quotations because the publication appears to be little more than computer-generated text, almost like the gibberish one might find in a spam email.“Based on what I could see from the ‘sneak peak’ function, the book was nothing more than a computer generated ‘story’ with no structure, chapters or paragraphs — only lines of text with a carriage return after each sentence,” Reames said in an interview with KrebsOnSecurity.If you get charged for one of these books, however, don’t try searching how to contact Amazon customer service. In the process of trying to understand what was going on, Krebs discovered a trove of books dedicated to contacting Amazon customer support which, if read, most probably include phone numbers designed to phish personal information.Amazon, for its part, offered to help any authors caught in this scam. They write:Anyone who believes they’ve received an incorrect 1099 form or a 1099 form in error can contact us1099@amazon.com and we will investigate.A Krebs reader pointed out that this is an excellent scam because Amazon allows you to cash out directly to a bank account, bypassing all of the rigmarole associated with crypto exchanges. In short, it’s stolen credit cards in, in money out.",Crooks launder money using real (and fake) Amazon ebooks
9819,3813199,2018-02-26 16:38:12,"TNW SitesFor anyone who’s paying attention, it won’t come as a surprise that virtual reality (VR) is on the verge of becoming mainstream.Variety recently cited the Greenlight Insight’s report which forecasted VR revenues to total $75 billion by 2021. According to the same report, VR revenues were projected to reach $7.17 billion by the end of 2017. Which is why it’s important that businesses both small and large consider early exposure to this growing segment.Promising statistics, but how can business owners put this information to good use?MARK.SPACE, a blockchain-powered platform that allows for the creation of 3D websites compatible with VR, poses an interesting solution. MARK.SPACE aims to become the powerhouse of the next generation of VR internet.Changing the gameMARK.SPACE provides businesses and others with a tool to launch any type of 3D or VR project based on the ‘real world’ economy. Users will be able to create environments where they can carry out activities, such as selling customized furniture or decor. In addition, they can create their own ‘offices’ and in a city-like virtual environment, advertisers can take advantage of spaces like hoardings in common areas.The project uses ‘M1 technology’, which is based on ‘advanced mathematical algorithms’ that MARK.SPACE developed in order to give the best possible precision in 3D visualisation. The VR platform is able to digitize entire interiors and product lines of shopping malls complete with branded stores.Users will be able to create their own social VR environments as well. MARK.SPACE has created a working platform prototype that will support content like YouTube videos, Instagram posts as well as audio and text files.Tokenization of the virtual spaceThe VR environment created by MARK.SPACE consists of ‘Cities;’ including districts, blocks and units. The bridge between the virtual and the real world economy is the MRK token, which is the internal currency of the platform. This token will allow users to purchase and sell physical goods as well as services on the MARK.SPACE platform.Also, it enables users to create their own spaces and to customize them. In order to expand the capabilities of the platform, more funding is needed. Therefore, the company is organizing a phased token sale.Currently, the fourth round of the sale is being held, which started on February 15, 2018 and has already raised $9.12 million at the time of the writing of this article. The price of each MRT token during this round has been set at $0.10. There is a hard cap of $35 million. A whitepaper with details of the sale as well as the project has been released by MARK.SPACE.Fostering creativity from the get goMARK.SPACE recently announced a real estate contest to select the ‘Best Apartment’ on the platform, meaning all registered users can design a 3D or VR location from scratch to compete.The contest will allow users to experience the VR platform and illustrate its convenience and usefulness. It will stay open until February 28, 2018, when the main token sale ends.This post is brought to you by The Cointelegraphand shouldn't be considered investment advice by TNW. Yes, TNW sells ads. But we sell ads that don’t suck.",This blockchain-based platform creates entire cities in virtual reality
9820,3813200,2018-02-26 16:00:00,"About TNWTNW SitesGet a lifetime of critical SEO analysis and recommendations…for only $9.99With the right idea and execution, anyone can succeed on the Web. If you’ve got something other people want, and you know how to reach those audiences, you can build web traffic to rival any Fortune 500 company. But that even playing field can also seem pretty daunting when you’re trying to break through the online clutter.SEOPop essentially does all the legwork for you to help make sure your product or service is showing up at the top of search requests. SEOPop automatically analyzes your site and kicks back a full report on how you’re doing — whether you’re using the right keywords, generating the right content, and generally making your message as Google-friendly as possible.Once SEOPop scans your site, you’ll get all the action items you need to help increase traffic, improve your site performance, and make big leaps on competing with — and succeeding over — your competitors.SEOPop even looks at your social media presence, helping you optimize what you’re posting to Facebook, Twitter, and the rest of the social landscape to sharpen your message with those key audiences.A lifetime of access to SEOPop’s features in usually $149, but with this limited time deal of just $9.99, it’s too affordable NOT to at least try it.",Get a lifetime of critical SEO analysis and recommendations...for only $9.99
9821,3815883,2018-02-26 17:55:00,"FCC Chairman Ajit Pai: ‘We need smart networks, not dump pipes’After cancelling his CES 2018 appearance in January, reportedly due to death threats, FCC Chairman Ajit Pai today attended a panel at MWC 2018 in Barcelona. During the event, titled ""The Future of the Industry: Transatlantic Digital Policy and Regulation,"" Pai discussed his plans for 5G connectivity and, of course, his decision to repeal net neutrality in the US. He said that, despite public perception on the moves he's made since taking over the FCC as part of President Trump's administration, the internet will remain open and free and that ""no one gets a pass."" The ""no one"" he mentions is likely a reference to the idea that certain companies, particularly network carriers like Verizon, will get special treatment after net neutrality ends on April 23rd.Pai emphasized that his version of the internet won't be any different from what was in place during the the 1990s and until 2015, which he said is what allowed companies like Amazon, Facebook and other to thrive and become the tech titans that they are now. He also said that taking his market-based, light-touch regulation approach is key to making 5G a success in the States. ""To realize the promise of 5G, we will need smart networks, not dumb pipes,"" Pai said. ""Dumb pipes won't deliver smart cities. Dumb pipes won't enable millions of connected, self-driving cars to navigate the roads safely at the same time.""He added that these ""dumb pipes"" won't give US the networks needed to ""enable the 5G applications of the future,"" noting that he has the country's best interest at heart and this will allow the US to be a leader in the technology. ""The United States needs modern, flexible, light-touch network regulation,"" he said, ""not a one-size-fits-all utility model from the 1930s."" He said that model was what Obama and his FCC administration implemented in 2015: ""Without question, our most important move here was to reverse the previous administration's decision to subject our 21st century networks to 20th century utility-style regulation.""Pai knows how many people, and the majority of tech companies, feel about his decision to repeal net neutrality. And he knows there's a long legal battle ahead. Still, he said he's confident consumers will see how his plan will benefit them by looking ""at the facts,"" rather than be influenced by viral campaigns like Burger King's net neutrality ad -- which he referred to after the panel moderator said to him that his decision was so frowned upon that even a fast-food chain felt the need to mock him.Ultimately, Pai said, the government must work together with networks and tech firms to enable innovation and investment in new wireless technologies and services. ""If we do that, the earth will be connected (if not converted), and everybody will win."" Up to you if you agree with that.Edgar began hitting newsrooms as a young kid in the ’90s, when his dad worked at a regional newspaper. Growing up, he had two passions: technology and football (soccer). When he wasn't on the pitch scoring hat tricks, he could be found near his SNES or around the house, taking things apart. Edgar's also deeply in love with tacos, sneakers and FIFA, in no particular order. He lives in New York City with his better half.","FCC Chairman Ajit Pai talks 5G, net neutrality repeal at MWC 2018"
9822,3815884,2018-02-26 06:32:00,"Tesla's electric trucks may be more cost-effective than expectedWhen Tesla unveiled its Semi electric truck, it made audacious claims about the big rig's value -- namely, that companies would recoup the cost of the vehicle in 2 years thanks to the savings on fuel. As it turns out, that might have been conservative. DHL exec Jim Monkmeyer told Reuters in an interview that he expects the shipping company to reach that point in a year and a half, saving tens of thousands of dollars per year. The absence of fuel is only part of the equation, he noted. As EV motors are much less complex than the big diesel engines that power conventional trucks, the Semi shouldn't need as much maintenance.Appropriately, Tesla's Elon Musk has hinted that the Semi's performance might see an upgrade. He's ""feeling optimistic"" that Tesla can outdo the specs from the November 2017 debut without hiking the price. We'd take that with a grain of salt when Musk's companies tend to set overly ambitious goals, but it's not an idle claim when the first deliveries are slated to start in 2019.DHL's hopes depend on the Semi living up to initial expectations, of course, and there's a lot that could go wrong. It's no mean feat to produce an EV that large with a range of 300 to 500 miles (the battery will be gigantic), and a lot of Tesla's math is predicated on the assumption that diesel prices remain the same. If there are unforeseen technical hurdles or diesel becomes cheap, the Semi's value may go down the tubes. However, it's not often that customers suggest a vehicle maker is underselling its product -- while caution is definitely warranted, this bodes well for the future of electric trucks.",Tesla's electric trucks may be more cost-effective than expected
9823,3815885,2018-02-26 17:12:00,"BlackBerry thinks shipping 850,000 KEYOne phones is a successTCL unveiled the Blackberry KEYOne at MWC 2017 and now, a year later, we have an idea of how well it sold as well as a glimpse into the company's outlook going forward. IDC's Francisco Jeronimo says that just 850,000 BlackBerry devices shipped last year, a very low number that's well behind those of companies like Apple and Samsung. But according to The Verge, who spoke to two members of the BlackBerry Mobile team, the company considers last year's sales a success.Blackberry and TCL are apparently coming into this year ""with a feeling of mission accomplished,"" which has less to do with actual sales numbers and more to do with how available their devices are to consumers. With the KEYOne, having it be available in many direct sales channels was important, and in that regard, BlackBerry is pretty pleased with how 2017 went. The company plans to launch two new phones this year.As CNET reports, BlackBerry Mobile's chief commercial officer, Francois Mahieu, would like to see the company grab three to five percent of the market for premium smartphones. ""It doesn't have to be a niche business,"" he said. ""I would not be satisfied with market share in premium (phones) that is sub-one percent forever."" And that would require sales of around 10 million units per year, according to Counterpoint Research analyst Neil Shah. One factor holding the company back is that many consumers don't yet know that BlackBerry phones now run on Android, not BlackBerry's own OS, says The Verge, and the company has had to put in a more concerted effort to make that more widely known. BlackBerry plans to shutter its app store at the end of 2019.","BlackBerry thinks shipping 850,000 KEYOne phones is a success"
9824,3816140,2018-02-26 16:56:36,"Of course, the promotional video was a lot more dramatic than what the company was actually demoing at the show itself. And while the company insisted to us that the dog in the video was, indeed real and not the result of a few clever film cuts, the poor confused pup was never actually at risk during its trials.At its MWC trials, the company brought along big cutouts of a dog, cyclist and soccer ball (okay, okay, football). The company invited us for a quick ride along to test what was essentially a clever promotion for the smartphone’s baked in AI processing capabilities. If nothing else, it was an excuse to go for a quick ride in a Porsche Panamera during our time in Barcelona.Essentially the phone’s neural processing unit (NPU) is being used to identify objects and telling the car how to react when they’re in the way. The demo consisted of two short drives. The first was roughly five miles an hour, with the camera identifying objects. Here we were asked to assign one of three different reactions: swerve left, swerve right and stop, each corresponding to a different cutout.With the second trip, which ramped things up to school zone unfriendly 30 miles an hour, we drove directly at one of the objects choose by the team, who picked it up and walked in directly into the path of the car. As promised, the system spotting the cyclist and swerved to the right.It’s a simple demo, but it was flashy enough to get the point across. The NPU utilizes that sort of image recognition to help take better images — kind of like the new version of the LG V30. The system was built out in around five weeks, according to the company. It certainly looks jury-rigged together, with a cable snaking from the phone sitting on the dashboard to the equipment on top of the room.As Global Senior Product Marketing Manager Peter Gauden was understandably quick to point out during our conversation today, this shouldn’t be taken as any sort of indication that the company is planning to get into the driverless car business.",We went for a ride in a Huawei smartphone-controlled self-driving Porsche
9825,3816141,2018-02-26 15:39:05,"Circle acquires cryptocurrency exchange Poloniex0Circle just announced that it is acquiring U.S.-based cryptocurrency exchange Poloniex. According to Fortune, Circle is paying $400 million for the acquisition. Poloniex has been around for years and used to be one of the biggest exchanges out there — there are now many exchanges competing with Poloniex.Circle is an interesting startup because it’s hard to keep track of what it does. The company first pitched itself as a bitcoin company that wants to make bitcoin more accessible. Circle wanted to become the PayPal of bitcoin. You could buy and sell bitcoins quickly and easily without any technical knowledge.But that was in 2013 during the first bitcoin boom. Shortly after that, Circle called itself a social payment company, a Venmo competitor. The words bitcoin and blockchain were gone from the company’s website.“We never thought of ourselves as a Bitcoin startup. The media certainly classified us that way because we were involved with the technology. From the day we founded the company three years ago we’ve focused on trying to build a new consumer finance company. And one that makes money work the way the Internet works,” co-founder and CEO Jeremy Allaire told TechCrunch’s Natasha Lomas in 2016.More recently, Circle got back into the cryptocurrency game and came full circle.The peer-to-peer payment service that was called “Circle” is now called Circle Pay. It also runs Circle Trade, an over-the-counter trading desk for large cryptocurrency investors and exchanges.In other words, Circle Trade fosters liquidity between a handful of fiat currencies and cryptocurrencies. Circle Trade also powers Circle Pay behind the scene. According to Fortune, Circle Trade manages $2 billion a month in transactions and generated $60 million in revenue in just three months.The company now wants to expand beyond those two products with Circle Invest, an easy-to-use investment app to start buying cryptocurrencies, and Poloniex, a full-fledged exchange.Fortune also says that Circle is also working on Centre, a protocol that is going to make Alipay, PayPal, Circle and other digital wallets interoperable. And if you’re a Poloniex user, Circle says that the transition should be smooth.",Circle acquires cryptocurrency exchange Poloniex
9826,3816219,2018-02-26 17:54:48,"TNW SitesYouTube now lets Live viewers control streamers’ lights and pet feedersYouTube today announced it’s introducing new features to its Live streams to bring its communities closer together.Among the changes is chat replay, which allows anyone watching saved livestreams to see the chat — an indispensable feature when rewatching streams as you might otherwise wonder who or what the streamers are responding to. Twitch introduced a similar feature two years ago.Other changes include tagging your location in livestreams, and the ability to search streams by location. YouTube is also bringing its auto-generated captions to streams, which means you’ll probably see more than a few screenshots of streams with humorously inaccurate captions — though a YouTube spokesperson says they’ll be working to improve the accuracy.The most interesting change is the connection of YouTube Live Super Chats — highlighted, pinned chat messages that viewers pay for. Last year, YouTube announced an update to Super Chats which would let those who bought them trigger real world events. Now YouTube’s expanding that to include IFTTT triggers. Super Chats can now be connected to hundreds of devices.As an example, YouTube cites pet feeders, lights, and confetti cannons — all things I’d be terrified to let strangers on the internet control, but I guess I’m just not gutsy enough to be a YouTube livestreamer.The features start rolling out today, with improvements coming over the next several weeks.TNW Conference 2018 features a track on more kinds of creativity in tech called Creative Commons. For more information, check out our event page.",YouTube updates Live streaming with captions and location tagging
9827,3816220,2018-02-26 17:41:48,"About TNWTNW SitesIt has been more than 10 days since numerous Coinbase users cried out the popular cryptocurrency exchange desk has erroneously charged them multiple times for past purchases, but it seems the situation is far from over – and customers are still waiting on refunds from Visa and their banks.“Still have not received refund on [C]oinbase triple charge,” one user wrote on February 25. The poster went on to explain that after being refunded for a wrong purchase that occurred on February 1, the customer was again wrongly charged on February 14. Unfortunately, he has yet to receive a refund for the second mishap.To this date, it remains unclear who is responsible for this massive blunder – or what precisely caused it. And from the looks of it, things are likely to stay this way.Shortly after confirming the issue on February 15, Coinbase blamed the multiple charges on Visa. While the financial service giant initially denied any fault, essentially shifting the responsibility back to Coinbase, it eventually released another statement to make it clear that the exchange desk made no mistake.“This issue was not caused by Coinbase,” the company wrote in a joint-statement with payment processing service Worldpay. “Worldpay and Coinbase have been working with Visa and Visa issuing banks to ensure that the duplicate transactions have been reversed and appropriate credits have been posted to cardholder accounts.”We asked Coinbase to clarify which banks are yet to handle unprocessed refunds, but the exchange desk was unfortunately unable to provide this information. However, according to a statement the company posted on Reddit on February 22, Scotiabank is one of these institutions.“We are investigating a potential issue for Scotia Bank [sic] customers leading to incorrect charges,” a company spokesperson said. “We believe this to be isolated at this point, and separate from the Worldpay and Visa issue outlined above. We will post an update here as we learn more.”Browsing through complaints on Reddit, it appears that customers of the Singapore-based United Overseas Bank are facing the same issue.On the bright side, there is a growingselection of users reporting that their banks have successfully reversed the erroneous transfers. US-based Landmark Credit Union Bank has purportedly already begun issuing refunds, according tocommenters.One slightly annoying detail is that wrongly charged users appear to have incurred additional fees for the multiple charges. Speaking to Coinbase over the phone, the exchange desk vehemently insisted that banks are responsible for any unexpected fees resulting from wrongful charging. There is no mention if overdraft fees incurred due to this inconvenience will be refunded – or who is responsible for such expenses.A spokesperson for Coinbase further told TNW that any impacted users still waiting to receive refunds ought to get in touch with their customer support team, so they can give them further directions on how they can contact their banks and claim their cash back.In the meantime, we would like to encourage affected users (who are yet to receive refunds) to contact us with details about their card-issuing banks. We are trying to compile a list of banks that are yet to reverse the inaccurate charges. Please get in touch at dimitar@thenextweb.com.Even though blockchain and cryptocurrency might not be perfect yet, we’re exploring the possibilities atTNW Conference2018. Find more infohere.",Overcharged Coinbase users should contact banks
9828,3818521,2018-02-26 18:30:00,"Bitcoin miners turn Quebec's cheap energy into cold cashBitcoin mining is a weird industry. Vast banks of dedicated computers solve complex equations to generate hashes worth a fraction of a coin, consuming huge amounts of power in the process. For such operations to be economically feasible nowadays, miners need the cheapest electricity possible.Energy is cheap in certain regions of Asia but many nations there are becoming increasingly hostile to cryptocurrency. China, for instance, is concerned that a sudden Bitcoin collapse could cause economic chaos. More importantly, it wants to cut off cryptocurrency operations near coal-fired energy plants that cause extra pollution in an already-polluted country.The answer, for many bitcoin miners, is to be found in Quebec. The Canadian province has some of the lowest electricity prices in North America, and produces an energy surplus that amounts to 100 terawatt hours over a decade -- enough to supply 6 million homes in a year, according to Global News. As such, rather than turning away miners like China is, the province's power agency, Hydro Quebec, has been encouraging them to come.Quebec generates nearly 97 percent of its electricity from hydro projects and produces around 1,245 tonnes of carbon per TWh of power, between 50 and 240 times less than the industry average in North America. Cryptocurrency mining is thus relatively clean and cheap in Quebec, making it interesting for both the province and miners. ""Of the world's top five largest blockchain players, we have at least three or four,"" David Vincent of Hydro Quebec told Reuters.Just because the hydroelectric dams are carbon free, doesn't mean they're controversy-free, however. Many projects built between 1970 and 2000 negatively affected the lives of Innu, Cree and Inuit tribes in the province, flooding their traditional trapping and fishing lands.And while hydroelectric power is plentiful in Quebec, that situation might change in the near future. Experts keep revising EV sales upwards, for one thing, which will soon stress electrical grids around the world. Hydro Quebec might make more money by exporting its excess power to the US, meaning miners won't see cheap prices forever.Bitfarms is one of the biggest players in Quebec, and the above photo was snapped by Reuters photographer Christinne Muschi at one of its four Quebec sites. The company plans to open another three sites in Quebec, and many other players are interested in coming to the province, too. It's even drawing in industries that have nothing to do with Bitcoin, like forestry and paper producers who could rent out space in their factories. ""They want space and cheap power,"" said Resolute Forest Products CEO Chad Wasilenkoff.Steve should have known that civil engineering was not for him when he spent most of his time at university monkeying with his 8086 clone PC. Although he graduated, a lifelong obsession of wanting the Solitaire win animation to go faster had begun. Always seeking a gadget fix, he dabbles in photography, video, 3D animation and is a licensed private pilot. He followed l'amour de sa vie from Vancouver, BC, to France and now lives in Paris.",Bitcoin miners turn Quebec's cheap energy into cold cash
9829,3818523,2018-02-26 18:13:00,"I found a Gemini PDA running Sailfish OS, and it was wildAt CES, a certain portion of Engadget's staff fell in love with the Gemini, a reimagining of the Psion PDA from the late 1990s. If the promise of a dual-booting Android and Linux phone had you drooling at the mouth, strap in, because I've just seen the same hardware running Sailfish OS. That's right, the alternative mobile operating system that Jolla has been working on since 2011. It's one of the strangest and most surprising phone collaborations of 2018, and I can't help but applaud the audacity of it all. The only problem is that the phone isn't very fun to use -- not yet, anyway.Gallery: Gemini PDA running Sailfish OS | 12 PhotosIf you haven't used Sailfish OS in a while, let me bring you up to speed. The Linux-based platform is a successor to the MeeGo project that then-Nokia CEO Stephen Elop abandoned in favor of Windows Phone. It's heavily reliant on swipes; the original Jolla Phone didn't ship with a home button or navigation keys. Instead, you have to slide your finger in from the left or right-hand side of the display to go home. From there you can swipe left to see a notification center of sorts, or up to browse the colorful app drawer.At first, it's a confusing interface to wrap your head around. Thankfully, I've used the Jolla Phone a couple of times before and quickly remembered how to traverse between the various software layers. If you're a newbie, though, expect a few frustrating hours as you try to recall how to switch apps or find the system settings. On the Gemini, you'll also need to grapple with its landscape-oriented display. The swipes are all the same, but it's tricky to open the app drawer because the keyboard tends to get in the way. It's also nearly impossible to do any of the gestures one-handed.Performance was generally smooth as I opened apps and webpages in the browser. That's no surprise given the deca-core MediaTek processor and 6GB of RAM powering the Gemini PDA. As far as I can tell, it's the best mobile hardware that Sailfish OS has ever been paired with (Sony's Xperia X, which also supports Sailfish OS, has a Snapdragon 650 and 3GB of RAM.) Typing, though, felt like a slog. The keyboard is cramped and the stock note-taking app missed some of my keystrokes. I also found it difficult to reach the keys in the center of the keyboard with my little thumbs.A Jolla spokesperson stressed, however, that I wasn't using final software. The company has a major update in the works, called Sailfish 3, that will introduce a new multitasking interface, a light theme, and ""full cloud integration"" including bookmark, note and photo syncing. That could eliminate some of the typing problems and add some Gemini-exclusive functionality. The Planet key, for instance (the Gemini is being built by a British company called Planet Computers) doesn't do anything right now. A Jolla employee also hinted at keyboard shortcuts -- pressing C while you're on the home screen, for instance, could open the camera or calendar app.I love this idea. It would turn the Gemini into a pocket-sized laptop with Spotlight and Alfred-style key commands. I suspect it would be impractical -- again, this sort of input requires both hands and a flat surface -- but in a way, it would be faithful to the classic PDA and its power computing ambitions. For now, though, that sort of functionality is purely hypothetical. The device I was shown at Mobile World Congress was ultimately cumbersome and unintuitive. If you plan on buying a Gemini, you're probably better off sticking with the Android and Linux software.Nick is a technology journalist at Engadget, covering video games, internet culture and anything else that takes his fancy. Before joining Engadget he was a reporter at The Next Web. He has a degree in multimedia journalism from Bournemouth University and an NCTJ certificate. He lives in Greenwich, London.","I found a Gemini PDA running Sailfish OS, and it was wild"
9830,3818617,2018-02-26 14:41:06,"We are thrilled to announce that as of Monday, February 26th, Visual China Group (VCG), a visual-content licensing and visual data technology company based in Beijing, has acquired 100% of 500px shares. VCG is among the top image licensing companies in the world, the go-to choice for creative and media professionals in China, and an award-winning leader in copyright protection.Most importantly, VCG has been an invaluable partner to us since they became our lead investor in 2015, and we’ve since watched both 500px.com and 500px.me (the China-based counterpart of 500px.com) flourish.For 500px photographers, this means a lot of exciting things are on the way. By joining forces with VCG, we’ll be able to consistently deliver more innovative features to help you amplify your personal brand, connect with millions of like-minded creatives, and inspire you to improve your skill set as a photographer, as well as reward your talent and creativity with new incentives.New products and services rolling out this year include:Better statistics to improve and understand the reach and impact of your photos;Top charts and badges that will help expand your exposure and reward your achievements;Private messaging to support meaningful communication between members;Sub-communities that allow photographers to connect around shared interests, styles, and locations.If you’re selling photos with us, you’ll be excited to hear that we will be announcing new partnerships that will give your photos an even greater reach, and new markets in which to sell.We’re extremely proud of where we’ve taken 500px in recent years. We truly believe this acquisition is a great opportunity, and can’t wait to see the 500px community grow and prosper even more. We’re so honored to share the journey with all of you.Related PostsAbout 500px ISO500px ISO is home to the best photo stories on the web. We feature the unique, crazy, and beautiful stories behind the photos you see on 500px. You'll meet inspirational photographers, and discover how they capture the images that blow your mind every day. From tutorials to collections and beyond, 500px ISO is your go-to source for everything photography.","Exciting news, 500px Community!"
9831,3818622,2018-02-26 18:24:09,"Share this articleThe compiler tool chain is one of the largest and most complex components of any system, and increasingly will be based on open source code, either GCC or LLVM. On a Linux system only the operating system kernel and browser will have more lines of code. For a commercial system, the compiler has to be completely reliable—whatever the source code, it should produce correct, high performance binaries.So how much does producing this large, complex and essential component cost? Thanks to open source not as much as you might think. In this post, I provide a real world case study, which shows how bringing up a new commercially robust compiler tool chain need not be a huge effort.How much code?An analysis by David A Wheeler’s SLOCCount shows that GCC is over 5 million lines. LLVM is smaller at 1.6 million lines, but is newer, supports only C and C++ by default and has around one third the number of architectures included as targets. However a useful tool chain needs many more components.Debugger: Either GDB (800k lines) or LLDB (600k lines)Linker: GNU ld (160k lines), gold (140k lines) or lld (60k lines)Assembler/disassembler: GNU gas (850k lines) or the built in LLVM assemblerIn addition the tool chain needs testing. In most GNU tools, the regression test suite is included with the main source. However for LLVM, the regression tests are a separate code base of 500 thousand lines. Plus for any embedded system, it is likely a debug server will be needed to talk to the debugger to allow tests to be loaded.What is involved in porting a compiler?Our interest is in a port of the tool chain that is robust for commercial deployment. Many PhD students round the world port compilers for their research, but their effort is dedicated to exploring a particular research theme. The resulting compiler is often produced quickly, but is neither complete, nor reliable—since this is not the point of a research program.This article is instead concerned with creating a set of tools which reliably produce correct and efficient binaries for any source program in a commercial/industrial environment.Fortunately most of this huge code base is completely generic. All mainstream compiler tools go to considerable efforts to provide a clean separation of target specific code, so the task of porting a compiler tool chain to a new architecture is a manageable task. There are five stages in porting a compiler tool chain to a new target.Proof of concept tool chain. Initial working ports of all components are created. This prototype is essential to identify areas of particular challenge during the full porting process and should be completed in the first few months of the project. At this point it will be possible to compile a set of representative programs to demonstrate the components will work together as expected.Implementation of all functionality. All functionality of the compiler and other tools is completed. Attributes, builtin/intrinsic functions and emulations of missing functionality are completed. All relocations are added to the linker, the full C library board support package is written and custom options are added to tools as needed. At the end of this process, the customer will have a fullly functional tool chain. Most importantly, all the regression test suites will be running, with the great majority of tests running.Production testing. This is often the largest part of the project. Testing must pass in three areas:regression testing, to demonstrate that the tool chain has not broken anything which works on other architectures;compliance testing, often using the customer’s tests to show that all required functionality is present; andRoll out. This is primarily about helping users understand their new compiler and how it differs from the previous tools, and usually involves written and video tutorials. While there will be real bugs uncovered in use, invariably there will also be numerous bug reports which amount to “this new compiler is different to the old one”. This is particularly pronounced where GCC and LLVM replace old proprietary compilers, because the step up in functionality is so great. Where there is a large user base, phased roll-out is essential to manage the initial support load.Maintenance. LLVM and GCC are very active projects, and new functionality is always being added, both to support new language standards in the front end and to add new optimizations in the back end. The compiler will need to be kept up to date with these changes. Plus of course there will be new functionality specific to the target required and on such a large project bugs reported by users.How much effort: the general caseLet us consider the general case. A new architecture with a large external user base, which must support C and C++ in both bare metal and embedded Linux targets. In this case it is likely that the architecture provides a range of implementations, from small processors used as bare metal or with RTOS in deeply embedded systems, to large processors capable of supporting a full application Linux environment.Overall first production release of a such a tool chain takes 1-3 engineer years. The initial proof of concept tool chain should be completed in 3 months. Implementation of all the functionality then takes a further 6-9 months, with a further 3 months if both bare metal and Linux targets are to be supported.Production testing takes at least 6 months, but with a large range of customer specific testing this can be as large as 12 months. Initial roll-out takes 3 months, but with a large user base, phased general release can take up to 9 months more.Maintenance effort depends hugely on the size of the customer base reporting in issues and the number of new features needed. It can be as little as 0.5 engineer months per month, but is more usually 1 engineer month per month.It is important to note that a complete team of engineers will work on this: compiler specialists, debugger experts, library implementation engineers and so on. Compiler engineering is one of the most technically demanding disciplines in computing, and no one engineer can have all the expertise needed.How much effort: the simpler caseNot everyone needs a full compiler release for a large number of external users. There are numerous application specific processors, particularly DSPs which are used solely in-house by one engineering company. Where such processors have proved commercially successful they have been developed and what was a tiny core programmed in assembler by one engineer has become a much more powerful processor with a large team of assembler programmers. In such cases moving to C compilation would mean a great increase in productivity and reduction in cost.For such use cases, the tool chain need only support C, not C++ and a minimal C library is sufficient. There may well be a pre-existing assembler and linker that can be reused. This greatly reduces the effort and timescales to as little as one engineer year for a full production compiler.The proof-of-concept still takes 3 months, but then completing full functionality can be achieved in as little as 3 more months. Production testing is still the largest effort, taking 3-6 months, but with a small user base 3 months is more than sufficient for roll out.The tool chain still needs to be maintained, but for this simpler system with a small user base, an effort of 0.25 engineer months/month is typically enough.For the smallest customers, it can be sufficient to stop after completing full functionality. If there are only a handful of standard programs to be compiled, it may be enough to demonstrate that the compiler handles these well and efficiently without progressing to full production testing.Case StudyIn 2016, Embecosm was approached by an electronic design company, who for many years had used an in-house 16-bit word addressed DSP designed to meet the needs of their specialist area. This DSP was now on its third generation and they were conscious that they needed a great deal of assembler programming effort. This was aggravated by the standard codecs on which they relied having C reference implementations. They had an existing compiler, but it was very old and porting it to the new generation DSP was not feasible.Embecosm were tasked with providing a LLVM based tool chain capable of compiling their C codecs and delivering high quality code. There was an assumption that this code would then be hand-modified if necessary. They had an existing assembler/linker, which worked by combining all the assembler into a single source file, resolving cross references and generating a binary file to load onto the DSP. The customer was also keen to build up in-house compiler expertise, so one of their engineers joined the Embecosm implementation team and has been maintaining the compiler since the end of the project.In the first 3 months, we created a tool chain based on their existing assembler/disassembler. In order to use newlib, we created a pseudo-linker, which would extract the required files from newlib as source assembler to combine with the test program. Because silicon was not yet available, we tested against a Verilator model of the chip. For this we wrote a gdbserver, allowing GDB to talk to the model. In the absence of ELF, source level debugging was not possible, but GDB was capable of loading programs and determining results, sufficient for testing purpose. In the absence of 16-bit char support in LLVM, we used packed chars for the proof-of-concept. This meant many character based programs would not work, but was sufficient for this stage.This allowed us to compile representative test programs and demonstrate that the compiler tool chain would work. It became clear that there were two major obstacles to achieving full-functionality: 1) lack of ELF binary support; and 2) lack of proper 16-bit character support.For phase two, we implemented a GNU assembler/disassembler using CGEN, which required approximately 10 days of effort. We also implemented 16-bit character support for LLVM as documented in this blog post. With these two features, completing the tool chain functionality became much more straightfoward and we were able to run standard LLVM lit and GCC regression tests for the tool chain, the great majority of which passed. The DSP has a number of specialist modes for providing saturating fixed-point arithmetic. To support these we implemented specialist builtin and intrinsic functions.At this point we had a compiler which correctly compiled the customer’s code. The ELF support meant techniques such as link-time optimization (LTO) and garbage collection of sections were possible, leading to successful optimization of the code so it met the memory constraints of the customer. With an investment of 120 engineer days, they had achieved their goal of being able to compile C code efficiently for their new DSP.The customer decided they had all the functionality they needed by this point and decided no further work was required. Should they decide to make the compiler more widely available they have the option to continue with full production testing of the compiler tool chain.Lessons learnedTwo factors made it possible to deliver a fully functional compiler tool chain in 120 engineer days.Using an open source compiler. The tools used in this project represent a cumulative effort of thousands of engineer years by the compiler community over three decades. Our customer was able to take advantage of this to acquire a state-of-the-art compiler tool chain.An expert compiler team. Although this was a 120 days project, a team of five were involved, each bringing years of specialist expertise. No one individual could know everything about emulation, GDB, CGEN assemblers, the GNU linker and the LLVM compiler. But within Embecosm we have the skill set to deliver the full project.If you would like to know more about bringing up a compiler tool chain for your processor, please get in touch.",How Much Does a Compiler Cost?
9832,3818860,2018-02-26 19:13:52,"About TNWTNW SitesCryptocurrency News Feb 26 – Crypto Is For RealGood morning everyone, I’m here live from cryptocurrency news HQ with some shocking news that I didn’t make up all of. Prepare to gorge yourself on news chow.Poloniex now part of company you haven’t heard of before todayCircle, a company that does something, acquired cryptocurrency exchange Poloniex today for $400 million, or roughly three Bitcoin. The deal has people swirling with excitement over the potential of Goldman Sachs having a cryptocurrency exchange desk. Also, what the hell is Circle? Nobody seems to want to say because I am not confident anyone knows. They claim on their website to be “crypto without the cryptic,” which is great because it also has the phrase “invest in digital currency without investing a lot of time deciphering the market. ” Yeah dude, just send a bunch of money without thinking about it into volatile assets. Circle’s main product appears to be Venmo but without the features of Venmo, or being Venmo.“Anyone can learn about cryptocurrencies if they’re willing to spend 70 to 80 hours researching every source until they find a couple sources that make sense,” said Andrew, who lives with his parents and younger sister in Wellesley, Massachusetts. “The whole book, in the simplest terms, is very easy to read and simple to understand.”Anyone can learn about this normal thing by spending a normal amount of time that a normal 11-year-old would.Andrew said he owns .00222 bitcoin (about $22.50), which he bought just before Christmas when it was higher, and a little bit of ethereum, but that he’s not buying anymore and would rather own Amazon shares.So this whole story gets to this point where you find out the kid basically spent 70 to 80 hours researching this topic (maybe) and probably spent like $40 to own $22.50 of Bitcoin. He’s now realized that he’d rather own Amazon shares. His dad is an investor at Matrix Partners, investors in Oculus. So I guess he must have some great advice on investing early in something that you’ll look at once intently then forget ever existed! Folks.Yeah dude, get a loan from the bank to invest in Bitcoin. It’s totally reasonable to expect to double your money on a currency that in the last two months has barely gone up once.John McAfee update: the drunken masterSome folks are perturbed because some of our team drink or even toke weed on the job. We have no rules. But ….. If anyone becomes less than the best, we ask them to leave. If you want to shoot heroin and yet continue to be the best programmer on the planet, then have at it.",Cryptocurrency News Feb 26 - Crypto Is For Real
9833,3818863,2018-02-26 18:08:57,"TNW SitesGoogle’s DeepMind teaches AI to predict deathDeepMind wants to solve the problem of patient deterioration in hospitals. The Google sister-company fed its AI the historical medical records of about 700,000 US veterans in hopes it will learn to predict changes in patient condition that, unchecked, lead to death.The partnership between DeepMind and the Veterans Administration (VA) brings some of the top minds in artificial intelligence research together with “world-renowned clinicians and researchers” working for the government.Basically, the US government is turning to, arguably, the smartest computer on the planet in order to find a cure for human-error. According to the laws of 1980s movies the robots will be attacking by the time you finish reading this sentence.All kidding aside, AI is transforming the medical field – as we’ve written before – and this is another example of that sweeping change.Traditionally, nurses are responsible for monitoring patients. Since it isn’t feasible to place every patient under constant direct care, the vast majority of patient monitoring is done remotely through electronics and sensors like EKGs and respirators. Nurses and doctors make rounds, checking in on each patient, and listen for alarms at a central station, but there’s really no one watching most patients the majority of the time.If DeepMind can teach AI to figure out why patients deteriorate then machines can, theoretically, take over monitoring duties. And it’s absolutely feasible for AI to continuously watch every patient all the time — computers don’t take breaks or get tired. This might not be the instant solution to human-error in the medical field – the third leading cause of death in the US – but it’s a start.It’s worth mentioning that all the data gleaned from service members’ records was scrubbed of personal information in order to maintain the privacy of the veterans they belonged to.The team focused on a specific problem in order to lay the groundwork for further work in the field. According to a DeepMind blog post:We’re focusing on Acute Kidney Injury (AKI), one of the most common conditions associated with patient deterioration, and an area where DeepMind and the VA both have expertise. This is a complex challenge, because predicting AKI is far from easy. Not only is the onset of AKI sudden and often asymptomatic, but the risk factors associated with it are commonplace throughout hospitals. AKI can also strike people of any age, and frequently occurs following routine procedures and operations like a hip replacement.DeepMind may be the smartest team in artificial intelligence research. There’s no better place to focus its energy and effort at the cutting-edge than in saving human lives. Solving the human-error problem in hospitals could dramatically lengthen our species expected lifespans. It would be huge.But, we’d better have Matthew Broderick standing by just in case.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",Google’s DeepMind teaches AI to predict death
9834,3821545,2018-02-25 18:31:00,"Russia hacked the Olympics and tried to pin it on North KoreaNow that the 2018 Winter Olympics are over, we're now learning who was responsible for hacking the games' systems... and the culprit won't surprise you at all. US intelligence officials speaking anonymously to the Washington Postclaimed that spies at Russia's GRU agency had compromised up to 300 Olympics-related PCs as of early February, hacked South Korean routers in January and launched new malware on February 9th, the day the Olympics began. They even tried to make it look like North Korea was responsible by using North Korean internet addresses and ""other tactics,"" according to the American sources.It's unclear if Russia was directly responsible for the chaos during the opening ceremony, which disrupted internet and broadcast systems to the point where some guests couldn't even print their tickets. However, the new evidence suggests Russia was in a prime position for such an attack. It also has the motivation: the country hasn't been shy about voicing its anger over its ban following revelations of state-organized doping, and it leaked Olympic athletes' health data following the 2016 Summer Olympics. North Korea is considered an unlikely candidate as it was using the games to improve relations with South Korea.The same unit responsible for these hacks is also believed to be responsible for the NotPetya attacks that wiped computers in numerous countries during 2017.Neither US officials nor Olympics overseers have publicly blamed Russia, and it's not certain they ever will. Olympics representatives hinted that they knew who perpetrated the opening ceremony attack but were keeping it a secret. And it's easy to see reasons why they wouldn't want to lay blame even now. Organizers risked stoking tensions at the same time as they were running an event promoting world harmony. It's virtually certain that Russia would deny any involvement, too, even if there was smoking gun evidence. As it stands, the hacking campaign suggests that future games' overseers will have to treat cybersecurity as a top priority.",Russia hacked the Olympics and tried to pin it on North Korea
9835,3821549,2018-02-26 20:32:00,"Michael B. Jordan burns all the books in 'Fahrenheit 451' trailerLast year, HBO announced that it was adapting Ray Bradbury's Fahrenheit 451 into a movie and today we get its first trailer. In it, we see Captain Beatty, played by Michael Shannon, telling underling fireman Guy Montag (Michael B. Jordan) about the dangers of books and why they have to be burned. ""A little knowledge is a dangerous thing,"" he says. ""News, facts, memoirs, internet of old -- burn it."" We also see Jordan's character spreading those ideas to school children, lighting room-fulls of books ablaze and saying with barely restrained fury, ""I want to burn.""Fahrenheit 451 will premiere on HBO in May. You can check out the trailer below.",Michael B. Jordan burns all the books in 'Fahrenheit 451' trailer
9836,3821885,2018-02-26 21:25:43,"About TNWTNW SitesGovernment contractor claims it can unlock any iPhoneNew reports today surfaced revealing that government organizations might have access to all iPhones currently on the market. A government contractor apparently figured out how to crack iOS 11.According to Forbes, Cellebrite, an Israeli company that specializes in accessing hard-to-access digital information, has developed the new method. Anonymous sources claimed the company was already shopping it around to customers.The company makes the claim themselves in a pamphlet on “Advanced Unlocking and Extraction Services” saying the company’s services are available for “Apple iOS devices and operating systems, including iPhone, iPad, iPad mini, iPad Pro and iPod touch, running iOS 5 to iOS 11.”Elsewhere in the same pamphlet, the company described how it was able to access locked phones:Cellebrite Advanced Unlocking Services is the industry’s only solution for overcoming many types of complex locks on market-leading devices. This can determine or disable the PIN, pattern, password screen locks or passcodes on the latest Apple iOS and Google Android devices.Forbes adds that the Department of Homeland Security accessed an iPhone X last December, with the help of a specialist who was trained in Cellebrite tech.Apple didn’t immediately return requests for comment.The FBI infamously searched for ways to unlock the iPhone owned by the San Bernardino shooter, demanding Apple assist before mysteriously backing down and reporting it’d managed to unlock it. Former director James Comey eventually revealed the organization had spent over a million dollars to unlock the phone.Also, this isn’t the first time the question of the iPhone X’s security has come up. When the iPhone X was announced, Senator Al Franken asked Apple in a letter “How will Apple respond to law enforcement requests to access Apple’s faceprint data or the Face ID system itself?” While Apple did respond to the letter in general, it didn’t address that particular point.TNW’s 2018 conference is just a few months away, and we want to talk about tech, security, and the government. Find out all about our trackshere.","Federal agents can now unlock your iPhone -- yes, even yours"
9837,3821886,2018-02-26 21:19:14,"It looks pretty sleek from the leaked image, if a little nondescript. It’s not clear exactly how small those bezels are, as the screen is turned off and there’s no front-facing fingeprint reader (unless it’s hidden under the screen). Still, not too shabby.Keep in mind the device isn’t made by BlackBerry itself in the traditional sense; the BlackBerry name is licensed out to Optiemus Infracom. This is the first BlackBerry phone by that company; other recent models have been made by TCL and BB Merah Putih.No word on when the phone will arrive, but it’d be nice to see BlackBerry put up more of a fight in the smartphone market. You’re definitely not getting a mechanical keyboard on this one though.",BlackBerry is making a bezel-less phone too
9838,3821887,2018-02-26 20:12:21,"About TNWTNW SitesApple to introduce a whopping 6.5 inch iPhone later this yearApple is preparing to release its largest iPhone ever this fall. According to Bloomberg, the company intends to release three new models: an upgraded (and roughly the same size) iPhone X, a less expensive model with some of the flagship’s key features, and a new device with a whopping 6.5 inch screen.Apple is seemingly scratching itches on both sides of the market. The larger model appeals to those seeking better multi-tasking and split screen capabilities, while the less expensive option appeases those who can’t stomach the $1,000 price tag of the iPhone X — but still want some of its features.The larger device is reportedly already running production tests with major suppliers.Codenamed D33, the new offering is expected to look a lot like the iPhone X. It’s said to occupy the same footprint of the iPhone 8 plus, utilizing edge-to-edge design to squeeze more screen into less real estate, much like the X. It’ll rely heavily on the love-it-or-hate-it Face ID scanner and utilize the same OLED technology in Apple’s current flagship, at about the same resolution.Under the hood, we should see Apple’s next-generation A12 processors in all but the cheapest model, which could rely on the A11 “Bionic” chip from current devices.Apple is reportedly considering adding a gold option to his year’s lineup again, and perhaps even offering a dual-SIM card option in the larger model.A 6.5 inch screen would best last year’s iPhone X (5.8 inches) and move Apple firmly into the “phablet” category of phones — but don’t expect Apple to use the term. First introduced in 2010 to describe the Dell Streak, Samsung has since popularized the word by introducing a series of ever-larger phones — including last year’s Note 8 and its 6.3 inch screen.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.","Apple to reveal trio of new smartphones, including its largest iPhone to date"
9839,3821888,2018-02-26 19:52:21,"TNW SitesBaidu’s voice cloning AI can swap genders and remove accentsChinese AI titan Baidu earlier this month announced its Deep Voice AI had learned some new tricks. Not only can it accurately clone an individual voice faster than ever, but now it knows how to make a British man sound like an American woman.You can insert your own joke here.The Baidu Deep Voice research team unveiled its novel AI capable of cloning a human voice with just 30 minutes of training material last year. And since then it’s gotten much better at it: Deep Voice can do the same job with just a few seconds worth of audio now.The team revealed two separate training methods in a recently published white paper. In one of the models a more believable output is generated, but it takes additional audio input. The second model can generate cloned audio much faster but at lower quality.Both are nominally faster than Baidu’s previous attempts with Deep Voice and, according to the researchers, could be upgraded even further with tweaked algorithms and broader datasets. The researchers claim, in a company blog post:In terms of naturalness of the speech and similarity to the original speaker, both demonstrate good performance, even with very few cloning audios.The purpose of the research is to demonstrate that machines can learn complex tasks with limited datasets, just like people. Imitating voices may be a specific use-case, but it’s important for researchers to find ways to minimize footprints through fine-tuning or replacing unwieldy algorithms.According to the team:Humans can learn most new generative tasks from only a few examples, and it has motivated research on few-shot generative models.Research that furthers the abilities of AI systems while simultaneously reducing the processing power required are what’s propelling the field forward.The world already has Deep Fakes, the controversial AI that can swap one person’s face onto another’s body – and of course it was immediately used for porn. And Nvidia’s AI can generate startlingly realistic photographs of people that don’t even exist. We’re inching ever closer to a world where you can’t believe your own eyes or ears.Deep Voice isn’t perfect, of course, you’ll notice the AI’s voice sounds a bit robotic. But, let’s keep in mind that a year ago this was barely possible at all.Now, we can’t be too far from hearing Kurt Kobain’s voice sing new music or learning what Queen Elizabeth would sound like as a male politician from Alabama.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",Baidu's voice cloning AI can swap genders and remove accents
9840,3824196,2018-02-26 19:49:00,"Water could be extracted all over the Moon, not just at its polesIf we're ever going to colonize another world, we can't rely solely on the supplies we bring with us. We'll have to make use of the resources available at our destination, with water being one of the most important. Not only can we drink it, but add a bit of science and voilà! You've got oxygen to breathe or rocket fuel to fly. Figuring out what resources are where is extremely important, and we've been honing this craft by looking at our celestial ally, the Moon. A study published last year suggested that water may exist in high quantities in the lunar interior, and now researchers have found evidence of water being distributed across the entire satellite, which is at odds with the widely held theory it's concentrated in the colder spots at the Moon's poles.The problem with studying water on the Moon is that we're largely relying on spectrometry. The way sunlight bounces of the lunar surface tells us about its chemical composition, but the Moon can also heat up enough to emit infrared light of its own, which is thought to mess with the readings. And so the Space Science Institute in Colorado went about combining two data sets -- temperature readings from NASA's Lunar Reconnaissance Orbiter and spectrometer measurements from India's Chandrayaan-1 orbiter -- to try and mute the impact the Moon's own light has on results.The institute's research, published in Nature Geoscience, disagrees with the accepted theory that water is concentrated at the poles, and that it travels before settling in these colder regions. The study of cleaned-up spectrometry data suggests water is present all over the Moon, but unfortunately, it doesn't make a lunar base any more viable. The researchers believe the majority of what they're seeing is probably hydroxyl (OH), not actual water molecules (H2O). In order to make use of the hydroxyl, you'd have to mine, extract and process it -- not nearly as simple as stumbling across a big deposit of ice, then.While the study may have given us a better understanding of our moon's water reserves, the techniques used could also help us learn more about potential sources of water that exist in our wider solar system and beyond.","Water could be extracted all over the Moon, not just at its poles"
9841,3827478,2018-02-26 20:11:55,"The leaked BlackBerry ‘Ghost’ is reportedly a high-end Android phone built for India0The clicky-clacky, full keyboard devices that spring to mind when you hear “BlackBerry” might be a thing of the past… but the brand lives on.Famed gadget-unearther Evan Blass dug up a picture of an upcoming BlackBerry device said to be known as “Ghost.”For those finding themselves saying “Wait, they still make BlackBerries?,” the answer is: yeah, sort of. BlackBerry Limited — the company formerly known as Research In Motion — stopped designing and building its own phones back in 2016, instead licensing the brand out to others. In most of the world, that’d be China’s mega-manufacturer, TCL.In this case, though, the “Ghost” is being designed/built by the Delhi-based telecom, Optiemus, following a licensing agreement announced early last year. The catch there: there’s a pretty good chance this one will be exclusive to India.Blass says that the device, which appears to have a rather slim bezel, will run Android — but that’s about all there is to know at this point.",The leaked BlackBerry ‘Ghost’ is reportedly a high-end Android phone built for India
9842,3829972,2018-02-24 11:49:08,"Except for her family and closest friends, no one in Jennifer’s various circles knows that she is on the spectrum. Jennifer was not diagnosed with autism until she was 45 years old—and then only because she wanted confirmation of what she had figured out for herself over the previous decade. Most of her life, she says, she evaded a diagnosis by forcing herself to stop doing things her parents and others found strange or unacceptable. (Because of the stigma associated with autism, Jennifer asked to be identified only by her first name.)Over several weeks of emailing back and forth, Jennifer confides in me some of the tricks she uses to mask her autism—for example, staring at the spot between someone’s eyes instead of into their eyes, which makes her uncomfortable. But when we speak for the first time over video chat one Friday afternoon in January, I cannot pick up on any of these ploys.She confesses to being anxious. “I didn’t put on my interview face,” she says. But her nervousness, too, is hidden—at least until she tells me that she is tapping her foot off camera and biting down on the chewing gum in her mouth. The only possible “tell” I notice is that she gathers up hanks of her shoulder-length brown hair, pulls them back from her face, and then lets them drop—over and over again.In the course of more than an hour, Jennifer, a 48-year-old writer, describes the intense social and communication difficulties she experiences almost daily. She can express herself easily in writing, she says, but becomes disoriented during face-to-face communication. “The immediacy of the interaction messes with my processing,” she says.“Am I making any sense at all?” she suddenly bursts out. She is, but often fears she is not.To compensate, Jennifer says she practices how to act. Before attending a birthday party with her son, for example, she prepares herself to be “on,” correcting her posture and habitual fidgeting. She demonstrates for me how she sits up straight and becomes still. Her face takes on a pleasant and engaged expression, one she might adopt during conversation with another parent. To keep a dialogue going, she might drop in a few well-rehearsed catchphrases, such as “good grief” or “go big or go home.” “I feel if I do the nods, they won’t feel I’m uninterested,” she says.Over the past few years, scientists have discovered that, like Jennifer, many women on the spectrum “camouflage” the signs of their autism. This masking may explain at least in part why three to four times as many boys as girls are diagnosed with the condition. It might also account for why girls diagnosed young tend to show severe traits, and highly intelligent girls are often diagnosed late. (Men on the spectrum also camouflage, researchers have found, but not as commonly as women.)Nearly everyone makes small adjustments to fit in better or conform to social norms, but camouflaging calls for constant and elaborate effort. It can help women with autism maintain their relationships and careers, but those gains often come at a heavy cost, including physical exhaustion and extreme anxiety.“Camouflaging is often about a desperate and sometimes subconscious survival battle,” says Kajsa Igelström, an assistant professor of neuroscience at Linköping University in Sweden. “And this is an important point, I think—that camouflaging often develops as a natural adaptation strategy to navigate reality,” she says. “For many women, it’s not until they get properly diagnosed, recognized, and accepted that they can fully map out who they are.”Even so, not all women who camouflage say they would have wanted to know about their autism earlier—and researchers acknowledge that the issue is fraught with complexities. Receiving a formal diagnosis often helps women understand themselves better and tap greater support, but some women say it comes with its own burdens, such as a stigmatizing label and lower expectations for achievement.Because so many more boys are diagnosed with autism than girls are, clinicians don’t always think of autism when they see girls who are quiet or appear to be struggling socially. William Mandy, a clinical psychologist in London, says he and his colleagues routinely used to see girls who had been shuffled from one agency or doctor to another, often misdiagnosed with other conditions. “Initially, we had no clue they needed help or support with autism,” he says.Over time, Mandy and others began to suspect that autism looks different in girls. When they interviewed girls or women on the spectrum, they couldn’t always see signs of their autism but got glimmers of a phenomenon they call “camouflaging” or “masking.” In a few small studies starting in 2016, the researchers confirmed that, at least among women with high intelligence quotients, camouflaging is common. They also noted possible gender differences that help girls escape clinicians’ notice: Whereas boys with autism might be overactive or appear to misbehave, girls more often seem anxious or depressed.Last year, a team of researchers in the United States extended that work. They visited several school yards during recess and observed interactions among 48 boys and 48 girls, aged 7 or 8 on average, half of each group diagnosed with autism. They discovered that girls with autism tend to stay close to the other girls, weaving in and out of their activities. By contrast, boys with autism tend to play by themselves, off to the side. Clinicians and teachers look for social isolation, among other things, to spot children on the spectrum. But this study revealed that by using that criterion alone, they would miss many girls with autism.Typical girls and boys play differently, says Connie Kasari, a researcher at UCLA who coled the study. While many boys are playing a sport, she says, girls are often talking and gossiping, and involved in intimate relationships. The typical girls in the study would flit from group to group, she says. The girls with autism appeared to be doing the same thing, but what was actually happening, the investigators learned, was different: The girls with autism were rejected repeatedly from the groups, but would persist or try to join another one. The scientists say these girls may be more motivated to fit in than the boys are, so they work harder at it.Delaine Swearman, 38, says she wanted badly to fit in when she was about 10 or 11, but felt she was too different from the other girls in her school. She studied the girls she liked and concluded, “If I pretended to like everything they liked and to go along with everything, that maybe they would accept me,” she says. Her schoolmates were avid fans of the band New Kids on the Block. So Swearman, who says she had zero interest in the band, feigned a passion she did not feel. She made a few more friends, but felt she was never being herself. Swearman, like Jennifer, was not diagnosed until adulthood, when she was 30.Even when teachers do flag girls for an autism evaluation, standard diagnostic measures may fail to pick up on their autism. For example, in a study last year, researchers looked at 114 boys and 114 girls with autism. They analyzed the children’s scores on the Autism Diagnostic Observation Schedule (ADOS) and on parent reports of autism traits and daily living skills, such as getting dressed. They found that even when the girls have ADOS scores similar to those of boys, they tend to be more severely impaired: The parents of girls included in the study had rated their daughters lower than the boys in terms of living skills and higher in terms of difficulties with social awareness and restricted interests or repetitive behaviors. The researchers say girls with less severe traits, especially those with high IQs, may not have scored high enough on the ADOS to be included in their sample in the first place.These standard tests may miss many girls with autism because they were designed to detect the condition in boys, says the lead researcher, Allison Ratto, an assistant professor at the Center for Autism Spectrum Disorders at Children’s National Health System in Washington, D.C. For instance, the tests screen for restricted interests, but clinicians may not recognize the restricted interests girls with autism have. Boys with autism tend to obsess about things such as taxis, maps, or U.S. presidents, but girls on the spectrum are often drawn to animals, dolls, or celebrities—interests that closely resemble those of their typical peers and so fly under the radar. “We may need to rethink our measures,” Ratto says, “and perhaps use them in combination with other measures.”Before scientists can create better screening tools, they need to characterize camouflaging more precisely. A study last year established a working definition for the purpose of research: Camouflaging is the difference between how people seem in social contexts and what’s happening to them on the inside. If, for example, someone has intense autism traits but tends not to show it in her behavior, the disparity means she is camouflaging, says Meng-Chuan Lai, an assistant professor of psychiatry at the University of Toronto in Canada, who worked on the study. The definition is necessarily broad, allowing for any effort to mask an autism feature, from suppressing repetitive behaviors known as stimming or talking about obsessive interests to pretending to follow a conversation or imitating neurotypical behavior.To evaluate some of these methods, Mandy, Lai, and their colleagues in the United Kingdom surveyed 55 women, 30 men, and seven individuals who are either transgender or “other” gendered, all diagnosed with autism. They asked what motivates these individuals to mask their autism traits and what techniques they use to achieve their goal. Some of the participants reported that they camouflage in order to connect with friends, find a good job, or meet a romantic partner. “Camouflaging well can land you a lucrative job,” Jennifer says. “It helps you get through social interaction without there being a spotlight on your behavior or a giant letter A on your chest.” Others said they camouflage to avoid punishment, to protect themselves from being shunned or attacked, or simply to be seen as “normal.”“I actually got told by a couple of my teachers that I needed to have ‘quiet hands,’” says Katherine Lawrence, a 33-year-old woman with autism in the United Kingdom. “So I had to resort to hiding my hands under the table and ensuring my foot tapping and leg jiggling remained out of sight as much as possible.” Lawrence, who was not diagnosed with autism until age 28, says she knew that otherwise, her classmates would think she was strange and her teachers would punish her for distracting others.The adults in the survey described an imaginative store of tools they call upon in different situations to avoid pain and gain acceptance. If, for example, someone has trouble starting a conversation, she might practice smiling first, Lai says, or prepare jokes as an icebreaker. Many women develop a repertoire of personas for different audiences. Jennifer says she studies other people’s behavior and learns gestures or phrases that, to her, seem to project confidence; she often practices in front of a mirror.Before a job interview, she writes down the questions she thinks she will be asked, and then writes down and memorizes the answers. She has also committed to memory four anecdotes she can tell about how she met a challenging deadline. The survey found that women on the spectrum often create similar rules and scripts for themselves for having conversations. To avoid speaking too much about a restricted interest, they may rehearse stories about other topics. To hide the full extent of her anxiety when she is “shaking inside” because, say, an event is not starting on time, Swearman has prepared herself to say, “I’m upset right now. I can’t focus; I can’t talk to you right now.”Some women say that, in particular, they put in a great deal of effort into disguising their stimming. “For many people, stimming may be a way to self-soothe, self-regulate, and relieve anxiety, among other things,” Lai says. And yet these motions—which can include flapping hands, spinning, scratching, and head banging—can also readily “out” these people as having autism.Igelström and her colleagues interviewed 342 people, mostly women and a few trans people, about camouflaging their stimming. Many of the participants had self-diagnosed, but 155 women have an official autism diagnosis. Nearly 80 percent of the participants had tried to implement strategies to make stimming less detectable, Igelström says. The most common method is redirecting their energy into less visible muscle movements, such as sucking and clenching their teeth or tensing and relaxing their thigh muscles. The majority also try to channel their need to stim into more socially acceptable movements, such as tapping a pen, doodling, or playing with objects under the table. Many try to confine their stimming to times when they are alone or in a safe place, such as with family. Igelström found that a few individuals try to prevent stimming altogether by way of sheer will or by restraining themselves—by sitting on their hands, for example.For Lawrence, her need to fidget with her hands, tap her foot, or jiggle her leg feels too urgent to suppress. “I do it because if my brain doesn’t get frequent input from the respective body parts, it loses track of where in space that body part is,” she says. “It also helps me concentrate on what I am doing.”All of these strategies call for considerable effort. Exhaustion was a near-universal response in the 2017 British survey: The adults interviewed described feeling utterly drained—mentally, physically, and emotionally. One woman, Mandy says, explained that after camouflaging for any length of time, she needs to curl up in the fetal position to recover. Others said they feel their friendships are not real because they are based on a lie, increasing their sense of loneliness. And many said they have played so many roles to disguise themselves through the years that they have lost sight of their true identity.Igelström says some of the women in her study told her that suppressing repetitive movements feels “unhealthy” because the stimming helps them to regulate their emotions, sensory input, or ability to focus. Camouflaging feels unhealthy for Lawrence, too. She has to spend so much effort to fit in, she says, that she has little physical energy for tasks such as housework, little mental energy for processing her thoughts and interactions, and poor control over her emotions. The combination tips her into a volatile state in which “I am more likely to experience a meltdown or shutdown,” she says.Lawrence says that if she’d been diagnosed as a child, her mother might have understood her better. She might have also avoided a long history of depression and self-harm. “One of the main reasons I went down that route was because I knew I was different but didn’t know why—I was bullied quite badly at school,” she says.The vast majority of women diagnosed later in life say that not knowing early on that they have autism hurt them. In a small 2016 study, Mandy and his colleagues interviewed 14 young women not diagnosed with autism until late adolescence or adulthood. Many described experiences of sexual abuse. They also said that, had their condition been known, they would have been less misunderstood and alienated at school. They might have also received much-needed support sooner.Others might have benefited from knowing themselves better. Swearman completed a master’s degree to be a physician assistant, but ultimately stopped because of issues related to her autism. “I was actually very good at what I did,” she says. But “it was too much social pressure, too much sensory stimulation, a lot of miscommunication and misinterpretation between myself and supervisors, due to thinking differences.” It was only after she stopped working that her counselor suggested she might have autism. She read up on it and discovered, “Oh, my gosh, that’s me!” she recalls. It was a major turning point: Everything started to make sense.It’s only after a diagnosis that a woman may ask, “Which parts of myself are an act and which parts of me have been hidden? What do I have that’s valuable inside myself that can’t be expressed because I’m constantly and automatically camouflaging my autistic traits?,” Igelström says. “None of those questions can be processed without first getting diagnosed, or at least [self-identifying], and then replaying the past with this new insight. And for many women, this happens late in life after years of camouflaging in a very uncontrolled, destructive, and subconscious way, with many mental-health problems as a consequence.”A diagnosis leads some women to abandon camouflaging. “Realizing that I am not broken, that I simply have a different neurology from the majority of the population and that there is nothing wrong with me the way I am means that I will not hide who I am just to fit in or make neurotypical people more comfortable,” Lawrence says.Related StoriesOthers learn to make camouflaging work for them, mitigating its negative effects. They may use masking techniques when they first make a new connection, but over time become more authentically themselves. Those who feel that camouflaging is within their control can plan to give themselves breaks, from going to the bathroom for a few minutes to leaving an event early or forgoing it entirely. “I learned to take care of myself better,” Swearman says. “The strategy is self-awareness.”Jennifer concedes that knowing about her autism earlier would have helped her, and yet she is “torn” about whether it would have been better. Because she didn’t have a diagnosis, she says, she also had no excuses. “I had to suck it up and deal. It was a really difficult struggle, and I made loads of mistakes—still do—but there was simply no choice,” she says. “If I had been labeled as autistic, maybe I wouldn’t have tried so hard and achieved all the things I’ve achieved.”She has achieved a great deal. During our video chat that snowy afternoon in January, it’s clear that one of her most significant accomplishments has been finding a balance in life that works for her. Her camouflaging skills allow her to put on a warm, personable exterior, one that has helped her build a successful career. But thanks to a few friends and a husband and son who love her for who she is, she can let that mask drop when it becomes too heavy.Most PopularThe emphasis placed on whether the Trump team colluded with Russia to interfere in the election threatens to overshadow the scandal in plain sight.There was a time when the White House’s frequent denials of collusion with Russia appeared largely defensive. Over time, however, their primary purpose has morphed. These days, the denials serve instead to distract from the ever-clearer picture of a president surrounded by crooks and liars.“Consistently we have said there was no collusion,” Ivanka Trump told NBC News Monday. “There was no collusion. And we believe that Mueller will do his work and reach that same conclusion.” That echoes her father and a White House statement from February 16, after Special Counsel Robert Mueller indicted a group of Russians for interfering in the election. “President Donald J. Trump … is glad to see the Special Counsel’s investigation further indicates—that there was NO COLLUSION between the Trump campaign and Russia and that the outcome of the election was not changed or affected,” the press secretary wrote.They weren’t the first mass-shooting victims the Florida radiologist saw—but their wounds were radically different.As I opened the CT scan last week to read the next case, I was baffled. The history simply read “gunshot wound.” I have been a radiologist in one of the busiest trauma centers in the United States for 13 years, and have diagnosed thousands of handgun injuries to the brain, lung, liver, spleen, bowel, and other vital organs. I thought that I knew all that I needed to know about gunshot wounds, but the specific pattern of injury on my computer screen was one that I had seen only once before.In a typical handgun injury, which I diagnose almost daily, a bullet leaves a laceration through an organ such as the liver. To a radiologist, it appears as a linear, thin, gray bullet track through the organ. There may be bleeding and some bullet fragments.By “camouflaging” their condition, many women on the spectrum learn to fit in—and risk psychological harm.Except for her family and closest friends, no one in Jennifer’s various circles knows that she is on the spectrum. Jennifer was not diagnosed with autism until she was 45 years old—and then only because she wanted confirmation of what she had figured out for herself over the previous decade. Most of her life, she says, she evaded a diagnosis by forcing herself to stop doing things her parents and others found strange or unacceptable. (Because of the stigma associated with autism, Jennifer asked to be identified only by her first name.)Over several weeks of emailing back and forth, Jennifer confides in me some of the tricks she uses to mask her autism—for example, staring at the spot between someone’s eyes instead of into their eyes, which makes her uncomfortable. But when we speak for the first time over video chat one Friday afternoon in January, I cannot pick up on any of these ploys.The politicization of the public sphere is compelling nonpartisan companies to take one partisan stand after another.After the most recent high-school massacre in Parkland, Florida, left 17 students and teachers dead, the National Rifle Association (NRA), the nonprofit gun-rights advocacy group, was rebuked by a surprising group of liberal activists: American corporations. Pressured by Parkland high-school students and others to boycott the NRA, more than 20 companies have cut ties with the pro-gun group.The NRA exodus includes major airlines like United, six rental-car firms including Hertz and Avis Budget Group, and MetLife, the insurance giant. These companies are not rescinding NRA donations, nor are they refusing service to NRA members. Rather, they’re ending discount programs, which companies routinely offer to groups and companies, like the NRA or the AARP. For example, United Airlines offered discounts on flights to the NRA annual meeting, and MetLife auto insurance offered a $50 benefit to members for each year of claim-free driving.A high-carb diet, and the attendant high blood sugar, are associated with cognitive decline.In recent years, Alzheimer’s disease has occasionally been referred to as “type 3” diabetes, though that moniker doesn’t make much sense. After all, though they share a problem with insulin, type 1 diabetes is an autoimmune disease, and type 2 diabetes is a chronic disease caused by diet. Instead of another type of diabetes, it’s increasingly looking like Alzheimer’s is another potential side effect of a sugary, Western-style diet.In some cases, the path from sugar to Alzheimer’s leads through type 2 diabetes, but as a new study and others show, that’s not always the case.A longitudinal study, published Thursday in the journal Diabetologia, followed 5,189 people over 10 years and found that people with high blood sugar had a faster rate of cognitive decline than those with normal blood sugar—whether or not their blood-sugar level technically made them diabetic. In other words, the higher the blood sugar, the faster the cognitive decline.The Senate candidate's allies believed party activists were trying to change bylaws to exclude him—but a last-minute amendment preserved his eligibility.In a Saturday-morning meeting outside of Salt Lake City, a hardline faction of conservative activists and agitators gathered to change the Utah Republican Party’s bylaws in a way that could have resulted in Senate candidate Mitt Romney being expelled from the state GOP and ejected from the ballot. They abandoned the effort at the last minute with a hastily written provision that spared the state’s most famous republican, but could further imperil the already dysfunctional state party.The Parkland shooting is yet another reminder of America’s dangerous model of masculinity—but that model could be changing.In the aftermath of a school shooting, one question always stands out: Why did he—it’s almost always a he—do it?Such an event, and its male perpetrator, draws attention to an awful truth lurking behind the “crazy” outburst: Male violence isn’t a one-off, anomalous occurrence, but one more event in a steady drone of violence in homes, schools, and neighborhoods.In 2014, the University of Alabama criminologist Adam Lankford examined a database of mass killings that occurred from 2006 to 2012. Of the 308 killers, 94 percent were male. Separately, Mother Jones compiled a list from 1982 to today; they found that of 93 shooters in 2014, 97 percent were male. In other violence categories, boys have a higher rate of assault than girls and a suffer higher rate of injury from assault. They are also more likely to report being in a fight in the past year and far more likely to be a homicide victim. In fact, homicide has become the leading cause of death for young African American males.The revolutionary ideals of Black Panther’s profound and complex villain have been twisted into a desire for hegemony.The following article contains major spoilers.Black Panther is a love letter to people of African descent all over the world. Its actors, its costume design, its music, and countless other facets of the film are drawn from all over the continent and its diaspora, in a science-fiction celebration of the imaginary country of Wakanda, a high-tech utopia that is a fictive manifestation of African potential unfettered by slavery and colonialism.But it is first and foremost an African American love letter, and as such it is consumed with The Void, the psychic and cultural wound caused by the Trans-Atlantic slave trade, the loss of life, culture, language, and history that could never be restored. It is the attempt to penetrate The Void that brought us Alex Haley’s Roots, that draws thousands of African Americans across the ocean to visit West Africa every year, that left me crumpled on the rocks outside the Door of No Return at Gorée Island’s slave house as I stared out over a horizon that my ancestors might have traversed once and forever. Because all they have was lost to The Void, I can never know who they were, and neither can anyone else.Many seniors are stuck with lives of never-ending work—a fate that could befall millions in the coming decades.CORONA, Calif.—Roberta Gordon never thought she’d still be alive at age 76. She definitely didn’t think she’d still be working. But every Saturday, she goes down to the local grocery store and hands out samples, earning $50 a day, because she needs the money.“I’m a working woman again,” she told me, in the common room of the senior apartment complex where she now lives, here in California’s Inland Empire. Gordon has worked dozens of odd jobs throughout her life—as a house cleaner, a home health aide, a telemarketer, a librarian, a fundraiser—but at many times in her life, she didn’t have a steady job that paid into Social Security. She didn’t receive a pension. And she definitely wasn’t making enough to put aside money for retirement.Decades before he ran the Trump campaign, Paul Manafort’s pursuit of foreign cash and shady deals laid the groundwork for the corruption of Washington.The clinic permitted Paul Manafort one 10-minute call each day. And each day, he would use it to ring his wife from Arizona, his voice often soaked in tears. “Apparently he sobs daily,” his daughter Andrea, then 29, texted a friend. During the spring of 2015, Manafort’s life had tipped into a deep trough. A few months earlier, he had intimated to his other daughter, Jessica, that suicide was a possibility. He would “be gone forever,” she texted Andrea.His work, the source of the status he cherished, had taken a devastating turn. For nearly a decade, he had counted primarily on a single client, albeit an exceedingly lucrative one. He’d been the chief political strategist to the man who became the president of Ukraine, Viktor Yanukovych, with whom he’d developed a highly personal relationship.",The Struggles of Women Who Mask Their Autism
9843,3830130,2018-02-26 21:20:19,"Fitbit posted a weaker-than-expected quarter and its shares are crashing0Fitbit, which has increasingly had to fend off competition from devices like the Apple Watch and is increasingly making moves in the healthcare space, still hasn’t seemed to nail things down quite yet as it posted weaker-than-expected financial results for its fourth quarter.The quarter was more or less a complete whiff, falling short of Wall Street estimates for earnings, revenue generated and the company’s outlook for the first quarter next year. That may be just a factor of the difficulty wearable device companies face going forward, as even though they can come out with new products and find niches, it’s not clear if users are going to continue adopting them. That explains some of the moves from Fitbit to further get into healthcare, but it looks like there’s still some ways to go as investors recalibrate their expectations.The stock price, as a result, is crashing this afternoon after the company posted its results. It’s down more than 10 percent following the report, sending its shares back below $5.Fitbit, for example, recently said it would acquire Twine Health, a cloud-based health management platform. As Apple looks to increasingly lock up the general consumer market — and also make its own moves into healthcare — Fitbit has had to try to make some aggressive moves in order to justify its existence in a wearable space that’s getting more and more crowded with Android devices and the Apple Watch. It can do that by finding more niches to exploit, though it still releases products like the Fitbit Ionic watch.",Fitbit posted a weaker-than-expected quarter and its shares are crashing
9844,3835645,2018-02-26 09:00:00,"Insights about the first three years of the Right To Be Forgotten requests at GoogleThe ""Right To Be Forgotten"" (RTBF) is the landmark European ruling that governs the delisting of personal information from search results. This ruling establishes a right to privacy, whereby individuals can request that search engines delist URLs from across the Internet that contain “inaccurate, inadequate, irrelevant or excessive” information surfaced by queries containing the name of the requester. What makes this ruling unique and challenging is that it requires search engines to decide whether an individual's right to privacy outweighs the public's right to access lawful information when delisting URLs.Since this ruling came into effect a little over three years ago (May 2014), Google has received ~2.4 millions URLs delisting requests. 43% of these URLs ended-up being delisted. Each delisting decision requires careful consideration in order to strike the right balance between respecting user privacy and ensuring open access to information via Google Search.To be as transparent as possible about this removal process and to help the public understand how the RTBF requests impact Search results, Google has documented this removal process as part of its Transparency report since 2014.This initial RTBF transparency report was a great first step toward detailing how the RTBF is used in practice. However inside Google we felt we could do better and that we needed to find a way to make more information available. A key challenge was ensuring that we were able to respect users’ privacy and avoid surfacing any details that could lead to de-anonymization or attract attention to specific URLs that were delisted.So in January 2016, our RTBF reviewers started manually annotating each requested URL with additional category data, including category of site, type of content on page, and requesting entity. By December 2017, with two full years of carefully categorised additional data, it was clear that we now had the means to deliver an improved transparency dashboard -- which we made publicly available earlier this week. Together with the data that we have previously published about the Right To Be Forgotten, the new data allowed us to conduct an extensive analysis of how Europe’s right to be forgotten is being used, and how Google is implementing the European Court’s decision. The result of this analysis was published in a paper that we release alongside with the improved transparency dashboard earlier today (official annoucement). This blog post summarizes our paper’s key findings.Who uses the right to be forgotten?89% of requesters were private individuals, the default label when no other special category applied. That being said, in the last two years, non-government public figures such as celebrities requested to delist 41,213 URLs; politicians and government officials requested to delist another 33,937 URLs.89% of right to be forgotten requests originate from private individuals, but public figures use the RTBF too. In the last two years gov. official requested to delist ~33k URLs; celebrities requested to delist ~41k URLs.The top 1,000 requesters, 0.25% of individuals filing RTBF requests, were responsible for 15% of the requests. Many of these frequent requesters are in fact not individuals themselves, but law firms and reputation management services representing individuals.A minority of requesters (0.25%) are responsible for a large fraction of the Right To Be Forgotten requests (~15%).What is the RTBF used for?Breaking down removal request by site type revealed that 31% of the requested URLs related to social media and directory services that contained personal information, while 21% of the URLs related to news outlets and government websites that in a majority of cases covered the requester's legal history. The remaining 48% of requested URLs cover a broad diversity of content on the Internet.The two dominant intents behind the Right To Be Forgotten delisting requests are removing personal information and removing legal history.What type of information is targeted?The most commonly targeted content related to professional information, which rarely met the criteria for delisting: only 16.7% of the requested URLs end-up being delisted. Many of these requests pertained to information that was directly relevant or connected to the requester’s current profession and was therefore in the public interest to be indexed by Google Search.Different countries, different usagesThe way the RTBF is exercised through Europe varies by country. Variations in regional attitudes toward privacy, local laws, and media norms strongly influence the type of URLs requested for delisting. Notably the citizens of France and Germany frequently requested delisting of social media and directory pages, while requesters from Italy and the United Kingdom were three times more likely to target news sites.The Right To Be Forgotten use is country specific. French citizens frequently request social media delisting whereas UK requesters are 3x more likely to target news site.RTBF requests mostly target local contentOver 77% of the requests are for URLs that are hosted on domains that have the top-level domain associated with the country of the requester. For example peoplecheck.de lives under the TLD .de which is the German top-level domain. At least 86% of the requests targeting the top 25 news outlets were from requesters from the same country.Right To Be Forgotten delisting requests are mostly used to remove local content.Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and llearn how the Right To Be Forgotten is used through Europe.To get notified when my next post is online, follow me on Twitter, Facebook, Google+, or LinkedIn. You can also get the full posts directly in your inbox by subscribing to the mailing list or via RSS.",Insights about the first three years of the Right To Be Forgotten requests at Google
9845,3835888,2018-02-27 01:57:45,"About TNWTNW SitesDuolingo data shows this is the fastest way to learn a new languageLanguage-learning app Duolingo recently combed through a mountain of user data to find the best times each day to practice a foreign language. Its findings were published today at Quartz.The findings essentially mirrored a 2016 study that showed the positive effect on sleep between practice sessions. Researchers split 40 participants were into two groups: one that practiced in the morning and reviewed in the evening, and another that practiced at bed time and reviewed the lesson the next morning. After a week, the researchers found retention rates much higher among participants that slept between sessions — indicating sleep leads to better long-term retention.To find out if this was true for its own customers, researchers at Duolingo applied the same concepts to its user base. After first identifying 14 groups based on when they most commonly used the app, the group color-coded the subsets based on activity level during specific hours of the day.The group then analyzed users’ performance with an additive factor model, a tool used to estimate language ability while controlling for the difficulty of each exercise. Using this model, the team was able to rank each group according to proficiency level.Those practicing at bed time ranked higher than 52.9 percent of users on average, the highest performing group measured. Those who practiced without a routine (the “arbitrary” group) were significantly less likely to retain what they had learned, but still not quite as bad as those who only practiced on weekdays (or weeknights) without practicing over the weekend.Overall, the study offers confirmation that consistent practice and sleep between sessions is the most effective way to learn a language.",Duolingo data shows this is the best way to learn a new language
9846,3838533,2018-02-27 04:54:39,"TNW SitesApple confirms it runs iCloud on Google’s cloud, and you shouldn’t be surprisedEver wondered where Apple stores all your iCloud backups? Between photos, files, contacts, and other data for millions of customers, that’s a whole lot of storage space that the company would have to make room for and manage. Now, it’s confirmed that iCloud actually lives on Google Cloud servers.CNBC noted that the revelation came in the form of an update to Apple’s publicly released iOS Security Guide document (PDF). In previous editions, it stated that iCloud content was stored on Amazon Web Services and Microsoft’s Azure cloud, but the latest version point mentions Amazon S3 and Google Cloud.It shouldn’t really come as a surprise: managing such massive troves of data is no mean feat, and it’s perfectly understandable that Apple chooses to work with third-party vendors instead of spinning up its own servers.To that end, it’s roped in two of the best vendors in the business. AWS earned Amazon $5.1 billion in Q4 2017, while Google’s cloud operations raked in more than a billion dollars for the company during the same period. What’s interesting is that Google is a smaller player in this field than Microsoft, which Apple has dropped for its iCloud service.The other iCloud development worth noting is that Apple reportedly plans to store encryption keys for its data service for users in China within that country itself. The problem with that is the Chinese government could coerce the state-owned firm holding said keys to hand them over for investigation, and perhaps even routine surveillance efforts. It remains to be seen if this will affect sales of the company’s products in the country, now that its data is more susceptible to oversight by government authorities.TNW’s 2018 conference is just a few months away, and we want to talk about tech, security, and the government. Find out all about our trackshere.","Apple confirms it runs iCloud on Google's cloud, and you shouldn't be surprised"
9847,3841314,2017-11-09 15:34:18,"The forgotten art of squatting is a revelation for bodies ruined by sittingSentences that start with the phrase “A guru once told me…” are, more often than not, eye-roll-inducing. But recently, while resting in malasana, or a deep squat, in an East London yoga class, I was struck by the second half of the instructor’s sentence: “A guru once told me that the problem with the West is they don’t squat.”This is plainly true. In much of the developed world, resting is synonymous with sitting. We sit in desk chairs, eat from dining chairs, commute seated in cars or on trains, and then come home to watch Netflix from comfy couches. With brief respites for walking from one chair to another, or short intervals for frenzied exercise, we spend our days mostly sitting. This devotion to placing our backsides in chairs makes us an outlier, both globally and historically. In the past half century, epidemiologists have been forced to shift how they study movement patterns. In modern times, the sheer amount of sitting we do is a separate problem from the amount of exercise we get.Our failure to squat has biomechanical and physiological implications, but it also points to something bigger. In a world where we spend so much time in our heads, in the cloud, on our phones, the absence of squatting leaves us bereft of the grounding force that the posture has provided since our hominid ancestors first got up off the floor. In other words: If what we want is to be well, it might be time for us to get low.To be clear, squatting isn’t just an artifact of our evolutionary history. A large swath of the planet’s population still does it on a daily basis, whether to rest, to pray, to cook, to share a meal, or to use the toilet. (Squat-style toilets are the norm in Asia, and pit latrines in rural areas all over the world require squatting.) As they learn to walk, toddlers from New Jersey to Papua New Guinea squat—and stand up from a squat—with grace and ease. In countries where hospitals are not widespread, squatting is also a position associated with that most fundamental part of life: birth.It’s not specifically the West that no longer squats; it’s the rich and middle classes all over the world. My Quartz colleague, Akshat Rathi, originally from India, remarked that the guru’s observation would be “as true among the rich in Indian cities as it is in the West.”But in Western countries, entire populations—rich and poor—have abandoned the posture. On the whole, squatting is seen as an undignified and uncomfortable posture—one we avoid entirely. At best, we might undertake it during Crossfit, pilates or while lifting at the gym, but only partially and often with weights (a repetitive maneuver that’s hard to imagine being useful 2.5 million years ago). This ignores the fact that deep squatting as a form of active rest is built in to both our evolutionary and developmental past: It’s not that you can’t comfortably sit in a deep squat, it’s just that you’ve forgotten how.“The game started with squatting,” says author and osteopath Phillip Beach. Beach is known for pioneering the idea of “archetypal postures.” These positions—which, in addition to a deep passive squat with the feet flat on the floor, include sitting cross legged and kneeling on one’s knees and heels—are not just good for us, but “deeply embedded into the way our bodies are built.”“You really don’t understand human bodies until you realize how important these postures are,” Beach, who is based in Wellington, New Zealand, tells me. “Here in New Zealand, it’s cold and wet and muddy. Without modern trousers, I wouldn’t want to put my backside in the cold wet mud, so [in absence of a chair] I would spend a lot of time squatting. The same thing with going to the toilet. The whole way your physiology is built is around these postures.”Reuters/StringerIn much of the world, squatting is as normal a part of life as sitting in a chair.So why is squatting so good for us? And why did so many of us stop doing it?It comes down to a simple matter of “use it or lose it,” says Dr. Bahram Jam, a physical therapist and founder of the Advanced Physical Therapy Education Institute (APTEI) in Ontario, Canada.“Every joint in our body has synovial fluid in it. This is the oil in our body that provides nutrition to the cartilage,” Jam says. “Two things are required to produce that fluid: movement and compression. So if a joint doesn’t go through its full range—if the hips and knees never go past 90 degrees—the body says ‘I’m not being used’ and starts to degenerate and stops the production of synovial fluid.”A healthy musculoskeletal system doesn’t just make us feel lithe and juicy, it also has implications for our wider health. A 2014 study in the European Journal of Preventive Cardiology found that test subjects who showed difficulty getting up off the floor without support of hands, or an elbow, or leg (what’s called the “sitting-rising test”) resulted in a three-year-shorter life expectancy than subjects who got up with ease.In the West, the reason people stopped squatting regularly has a lot to do with our toilet design. Holes in the ground, outhouses and chamber pots all required the squat position, and studiesshow that greater hip flexion in this pose is correlated with less strain when relieving oneself. Seated toilets are by no means a British invention—the first simple toilets date back to Mesopotamia in the fourth millennium B.C., while the ancient Minoans on the island of Crete are said to have first pioneered the flush—but they were first adopted in Britain by the Tudors, who enlisted “grooms of the stool” to help them relieve themselves in ornate, throne-like loos in the 16th century.The next couple hundred years saw slow, uneven toilet innovation, but in 1775 a watchmaker named Alexander Cummings developed an S-shape pipe which sat below a raised cistern, a crucial development. It wasn’t until after the mid-to-late-1800s, when London finally built a functioning sewer system after persistent cholera outbreaks and the horrific-sounding “great stink” of 1858, that fully flushable, seated toilets started to commonly appear in people’s homes.Today, the flushable squat-style toilets found across Asia are, of course, no less sanitary than Western counterparts. But Jam says Europe’s shift to the seated throne design robbed most Westerners of the need (and therefore the daily practice) of squatting. Indeed the realization that squatting leads to better bowel movements has fueled the cult-like popularity of the Lillipad and the Squatty Potty, raised platforms that turn a Western-style toilet into a squatting one—and allow the user to sit in a flexed position that mimics a squat.“The reason squatting is so uncomfortable because we don’t do it,” Jam says. “But if you go to the restroom once or twice a day for a bowel movement and five times a day for bladder function, that’s five or six times a day you’ve squatted.”While this physical discomfort may be the main reason we don’t squat more, the West’s aversion to the squat is cultural, too. While squatting or sitting cross legged in an office chair would be great for the hip joint, the modern worker’s wardrobe—not to mention formal office etiquette—generally makes this kind of posture unfeasible. The only time we might expect a Western leader or elected official to hover close to the ground is for a photo-op with cute kindergarteners. Indeed, the people we see squatting on the sidewalk in a city like New York or London tend to be the types of people we blow past in self-important rush.“It’s considered primitive and of low social status to squat somewhere,” says Jam. “When we think of squatting we think of a peasant in India, or an African village tribesman, or an unhygienic city floor. We think we’ve evolved past that—but really we’ve devolved away from it.”Avni Trivedi, a doula and osteopath based in London (disclosure: I have visited her in the past for my own sitting-induced aches) says the same is true of squatting as a birthing position, which is still prominent in many developing parts of the world and is increasingly advocated by holistic birthing movements in the West.“In a squatting birthing position, the muscles relax and you’re allowing the sacrum to have free movement so the baby can push down, with gravity playing a role too,” Trivedi says. “But the perception that this position was primitive is why women went from this active position to being on the bed, where they are less embodied and have less agency in the birthing process.”Reuters/Carlos BarriaChildren in the West squat with ease. Why can’t their parents?So should we replace sitting with squatting and say goodbye to our office chairs forever? Beach points out that “any posture held for too long causes problems” and there are studiesto suggest that populations that spend excessive time in a deep squat (hours per day), do have a higher incidence of knee and osteoarthritis issues.But for those of us who have largely abandoned squatting, Beach says, “you can’t really overdo this stuff.” Beyond this kind of movement improving our joint health and flexibility, Trivedi points out that a growing interest in yoga worldwide is perhaps in part a recognition that “being on the ground helps you physically be grounded in yourself”—something that’s largely missing from our screen-dominated, hyper-intellectualized lives.Beach agrees that this is not a trend, but an evolutionary impulse. Modern wellness movements are starting to acknowledge that “floor life” is key. He argues that the physical act of grounding ourselves has been nothing short of instrumental to our species’ becoming.In a sense, squatting is where humans—every single one of us—came from, so it behooves us to revisit it as often as we can.","To solve problems caused by sitting, learn to squat"
9848,3841322,2018-02-26 22:14:14,"Life Exists in the Driest Desert on Earth. It Could Exist on Mars, Too.In BriefIn the search for extraterrestrial life, scientists often look for liquid water on exoplanets. One new study uses desert life on Earth to show that life could exist in some of the driest conditions on the planet Mars.Scientists have known that microbes exist in these extremely arid conditions, but they have been previously unsure of whether the microbes actually reside in this environment or are simply temporarily moved there by weather patterns. Within this study, the researchers concluded that this desert actually supports permanent microbial life.The research team visited the Atacama in 2015 following an extremely rare rainfall, and detected a veritable boom of microbial life in the soil afterwards. When the team returned over the next two years, their samples showed the same microbial communities were still there, but had begun to dry out and go dormant, awaiting the next rain.Martian Conditions“It has always fascinated me to go to the places where people don’t think anything could possibly survive and discover that life has somehow found a way to make it work,” said Washington State planetary scientist Dirk Schulze-Makuch, in a press release. Schulze-Makuch led the study as part of his research into Earth’s most extreme organisms, which could tell us something about life through our universe.“Jurassic Park references aside, our research tell us that if life can persist in Earth’s driest environment there is a good chance it could be hanging in there on Mars in a similar fashion,” he said.This doesn’t mean that Mars is secretly teeming with life. However, it does point to the possibility. Because the Red Planet once held liquid water, Mars bacteria could have developed and then, as the planet dried out, evolved to adapt to a niche below the surface. Because Mars is so much colder, similar communities would likely have to live off the occasional melting of soil ice or snowfall on the surface.“There are only a few places left on Earth to go looking for new lifeforms that survive in the kind of environments you would find on Mars,” Schulze-Makuch said. “Our goal is to understand how they are able to do it so we will know what to look for on the Martian surface.”","Life exists in the driest desert on Earth. It could exist on Mars, too."
9849,3841560,2018-02-27 06:59:30,"TNW SitesAlthough Samsung may had made the biggest splash at MWC this year with its highly anticipated Galaxy S9 and S9+, I’m a lot more excited about what Vivo’s getting up to with its Apex concept phone, which goes far beyond what any other manufacturer has managed thus far when it comes to ditching bezels around the display.The Apex’s display has a 1.8mm bezel at the top, and a 4.3mm chin at the bottom – leaving no room for a front camera or even a speaker. However, those are still part of the package, thanks to some clever design and engineering feats.First off, the 8-megapixel front snapper pops up from behind the screen in 0.8 seconds, above the top edge of the device – so there’s no need for a notch like on the Essential handset. As for the speaker that lets you hear calls with your ear to the phone, the company has done away with it and instead uses an exciter to turn the entire display into an earpiece.The Apex also hides a fingerprint sensor beneath the display. Vivo had already pulled this off with ultrasound tech on the X20 Plus UD back in January, but it’s now improved upon that idea by increasing the on-screen sensor field to a space that’s about a quarter of the display. That negates the need to aim before placing your finger on the screen to unlock your device.Pretty neat, huh? We’ll have to wait and see if any of these innovations actually make it to the production line, but it’s nice to see Vivo thinking out of the box and ahead of the competition.",Vivo's Apex concept phone hides a front camera behind its borderless display
9850,3841561,2018-02-27 05:49:11,"Kleiman, who died in 2013, was a forensic computer analyst, author of multiple books on IT security, and supposedly involved in the invention of Bitcoin. In 2011, he established a company called W&K Info Defense Research LLC in Florida with Wright. The duo allegedly held 1.1 million Bitcoins between them at the time.According to the suit, filed by Kleiman’s family, Wright attempted to backdate and forge contracts, so as to transfer Kleiman’s assets to his own holdings. It alleges that Wright had designs on $5 billion worth of Kleiman’s Bitcoins and intellectual property pertaining to blockchain technologies.In addition, it states that documents detailing the W&K company’s workings point to Kleiman being the sole member of the firm, and as such, his estate would be entitled to all 1.1 million Bitcoin mined there – which works out to a little over $10 billion. Naturally, Kleiman’s family wants all of it back.In emails with Kleiman’s brother, Ira, Wright admitted that he was holding at least 300,000 Bitcoins that belonged to Kleiman.It’ll be interesting to see how this plays out, as it could potentially shed light on the shady origins of the world’s most popular cryptocurrency. If Wright is found guilty, it could also affect nChain – the blockchain development firm where he serves as chief scientist – as well as Bitcoin Cash, the currency created last year by forking off from Bitcoin.TNW’s 2018 conference is just a few months away, and we want to talk about tech, security, and the government. Find out all about our trackshere.",Bitcoin 'creator' Craig Wright is being sued for allegedly swindling $5 billion in cryptocurrency
9851,3843968,2018-02-26 17:18:43,"A British publisher is releasing 1,000 facsimiles of the two notebooks in which Shelly scrawled her iconic novelMary Shelley, as the famous story goes, first conceived of Frankenstein on a stormy night in 1816, while vacationing at the Lake Geneva villa of Lord Byron. The poet challenged his guests to “each write a ghost story,” as Shelley later explained in the introduction to her iconic novel, and she would spend the following months scribbling her tale of the “Modern Prometheus” and his monster into two large notebooks.In honor of the 200th anniversary of Frankenstein’s publication, the British publisher SP Books is releasing a facsimile of Shelley’s original manuscript. According to Roslyn Sulcas of the New York Times, the limited run will produce 1,000 copies of the facsimile, which will be available for purchase starting March 15.Most copies of Frankenstein derive from an 1831 edition that was heavily revised, reports Alison Flood of the Guardian. SP Book’s facsimiles are based on Shelley’s original notebooks, which are held today by Oxford’s Bodleian Library. These manuscripts offer unique insight into how Shelley’s novel evolved as she revised the text. The facsimile shows, for instance, that the author softened her portrayal of Frankenstein’s monster. In one sentence, she scratches out the word “creature” and replaces it with ""being."" In another, the ""fangs"" that Victor imagines gripping his neck become ""fingers.""The facsimiles also preserve notes made by Percy Shelley, Mary’s husband and a prominent Romantic poet. He suggests, for example, that Mary add ""lustrous black"" to her description of the monster’s hair. In one passage, he corrects her spelling of “enigmatic, which Mary had written as ""igmattic."" ""[E]nigmatic o you pretty Pecksie!"" Percy chastises Shelley teasingly. (According to Graham Henderson, who runs a blog focusing on Shelley and the Romantics, Shelley ""was prone to double the letter 'm' while her husband had an ie/ei problem with words like 'viel' and 'thier​.'"")Jessica Nelson, a founder of SP Books, tells Flood that the notations reveals another layer of the manuscript. “What’s really moving about this manuscript,"" she says, ""is that you can see the literary work mixed with something tender and emotional–literature and love inside the pages of the manuscript. Their two handwritings are very similar, which is bizarre and sweet at the same time.""Shelley was just 18 years old when she wrote Frankenstein, and in her introduction to the 1831 edition, she writes that many people had asked her “how a young girl, came to think of, and to dilate upon, so very hideous an idea?” Shelley, downplaying her work, chalked it up to “imagination, unbidden, possessed,” but the facsimiles show that thoughtful writing and meticulous revisions played an important role in creating one of the most enduring horror stories of all time.Like this article?SIGN UP for our newsletterAbout Brigit KatzBrigit Katz is a freelance writer is based in Toronto. Her work has appeared in a number of publications, including NYmag.com, Flavorwire and Tina Brown Media's Women in the World.",‘Frankenstein’ Manuscript Shows the Evolution of Mary Shelley’s Monster
9852,3844206,2018-02-27 08:01:34,"TNW SitesThis company made a $150 iPhone X ripoff and called it the S9So you can’t afford the iPhone X – or the upcoming Samsung Galaxy S9; welcome to the club. Or, you know what, screw that. Just get the best of both worlds for $150, from Leagoo.The oddball Chinese gadget maker unveiled its S9 handset at MWC 2018, complete with narrow bezels, a familiar name, and an iPhone-inspired notch above the display.For its low asking price, it comes with a 5.85-inch “HD+” screen, an unnamed octa-core processor, 4GB RAM, 32GB of onboard storage, dual 13-megapixel rear cameras, and a 3,300mAh battery. The notch houses the front camera that also allows for facial recognition to unlock your phone.I’m not a fan of these gimmicks, but I’m interested to know if people actually care for such blatant rip-offs (besides Samsung and Apple’s lawyers, that is).",Leagoo puts a notch in your Android experience for $150
9853,3846992,2018-02-27 08:47:39,"I Wanna Go Fast: Why Searching Through 500M Pwned Passwords Is So Quick27 February 2018In the immortal words of Ricky Bobby, I wanna go fast. When I launched Pwned Passwords V2 last week, I made it fast - real fast - and I want to talk briefly here about why that was important, how I did it and then how I've since shaved another 56% off the load time for requests that hit the origin. And a bunch of other cool perf stuff while I'm here.Why Speed Matters for Pwned PasswordsFirstly, read the previous post about k-Anonymity and protecting the privacy of passwords to save me repeating it all here. I've been amazed at how quickly this has been adopted since I pushed it out very early on Thursday morning my time. Perhaps most notably is 1Password's use of the service having pushed out integration within 27 hours. They had no prior noticed of this either, they just got down to business and did it as soon as I launched.Read through the comments on that original blog post and you'll see a heap of other integrations too. Perhaps my favourite though is this one:Beauty of @Cloudflare Workers is that it was easy to integrate @troyhunt's new pwned password service to add a header indicating whether a POSTed password is pwned or not. Then the server can warn the user.https://t.co/SmCgwV1tpnJohn is the CTO of Cloudflare and his model uses a Cloudflare Worker. This makes it possible to add that little bit of code to Cloudflare's ""edges"" (effectively the 122 data centres they have around the world sitting between browsers and servers), grab the password that's submitted by, say, a login request then check if it's been pwned and return the result in a response header. It's a beautifully elegant solution because there's no code to run on the server side and it can execute async whilst the original request is being processed by the origin server. Of course, you still need to do something with the newly added header indicating pwn status, but you get the idea.If people are adding this service into their sites, it has to be fast. Super-fast. It can't block requests whilst searching through a large DB of half a billion records, if it's to be successful then responses must be lightening quick. There's also a direct line between execution time and the cost I incur because I'm using Azure Functions which are priced as follows:If I can keep execution time and resource consumption down, I can keep costs down. But further to that, if I can keep the number of executions down then that too will reduce my bill. Let's talk about how I'm doing that:Reducing Executions and Load Time with Cloudflare CacheWhen data in Pwned Passwords is queried using the k-Anonymity model, the request looks like this:https://api.pwnedpasswords.com/range/21BD1The last part of the path is the first 5 characters of the SHA-1 hash of the password being searched for which in this case, was ""P@ssw0rd"". The response to that request is very aggressively cached at Cloudflare's end such that in the last 24 hours, 47% of the requests have been served directly by their cache and never actually hit my Azure Function:I actually thought this number would be higher, steadily increasing as people did queries that gradually exhausted the 16^5 range for hash prefixes 00000 through FFFFF. Eventually, I theorised, there'd be 1,048,576 entries sitting in Cloudflare's cache, only refreshing themselves from the origin after they expired a month later. In practice, however, cache eviction logic turns out to be a little more nuanced than that and after many discussions with Junade at Cloudflare (he came up with the k-Anonymity model for Pwned Passwords), it seems that things are as well-optimised as they're going to get for the moment. That may change in the future, but for now I needed to look further for more optimisation.(A quick tangential footnote on this: the password search page on HIBP is really aggressively cached at Cloudflare. When I've had sudden, large influxes of traffic due to media coverage, only low single-digit percentages of it has hit my origin site then the search itself has gone to the Azure Function meaning almost no requests hitting the HIBP service even when things ramp right up.)Table Storage Versus Blob StorageI've been a massive proponent of Azure's Table Storage for years now. This is the construct that underlies all the breached email addresses that presently sit in HIBP and it's worked absolutely flawlessly. (Side note: we also use Azure Table Storage for all the reports in Report URI.) One of the observations I've long made about Table Storage is that it provides an alternate or even complementary storage model to the classic RDBMS approach. Over many years in a corporate environment, I'd constantly hear this:Oh, we're building a web application so we'll be needing a SQL Server databaseSometimes, this was true, but more often than not it was a reflection of doing things the way they'd always been done as opposed to choosing the most optimal technology for the job. In my post from a few years ago on the architecture of HIBP, I showed how I used both Table Storage and SQL Azure to compliment each other. I picked the best bits from both and even today, that's still precisely the same storage choice I'd make. But on reflecting about the decisions I made for Pwned Passwords V2, I found myself having fallen into that same trap of ""doing what I've always done"".Here's what V2 of Pwned Passwords looks like in Table Storage:The first 5 chars of the hash are the partition key, the remainder of the hash is the row key and then there's a count attribute (Azure adds in the Timestamp automatically). This made it very fast to look up by individual hash (just segment it into two parts and query by partition and row key), but also very easy to pull a complete hash range for the k-Anonymity model as I just dragged in the entire partition. When I analysed the function execution time, here's what I found:Very linear performance that barely changes with volume. Those 403k requests executed in an average of 122ms which is predominantly comprised of the function reaching out to table storage, grabbing the partition and deserialising it before returning it in the response. The execution time also had a pretty well-distributed range with 50% of them running in 100ms or lower:But then, I started thinking about it more: why am I using Table Storage? I mean why sit the data in a queryable construct when ultimately, that range search simply pulls back one of 1,048,576 different results that don't change! Yes, they'll change when I release a V3, but I went 6 months between V1 and V2 and I don't expect anything to change again any time soon. Plus, when it does change, it'll be a complete rollover in one go; I'll still have 1,048,576 different ranges just with, say, 750M hashes. Would it be faster if I just exported the data into 1,048,576 separate files, dropped them into Blob Storage then pulled back the exact file I needed from the function? Kinda like this:And then the file has precisely the contents you get in the existing API response:tl;dr - can I pull a pre-prepared file from a blob faster than I can pull a complete partition from a table? Actually, yes, a lot faster:That's just shaved 56% off the execution time! 122ms down to only 54.2ms. That's with no change to the publicly facing API and an identical response body returned. The distribution of those times is where it gets particularly interesting:Half of the queries now execute in 46ms or less! The interesting bit is that the distribution of execution times is a lot more erratic so the performance of this model isn't as consistent, but it's an awful lot faster. Still, look at that cluster of requests at only 10ms - wow!For now, I can switch between the two back end repositories with a simple config setting so if I find, for example, that perf degrades on blob storage when larger volumes occur, I can always roll back in an instant.Let me touch on a couple of other really neat perf things.ArgoWe all know the internet is a collection of interconnected nodes with traffic bouncing around between them in order to carry your request to the origin server and then return you a response. But what's the most optimal route for the traffic? The shortest distance? Not necessarily because there's factors such as congestion and the reliability of particular nodes or connections to deal with. But typically, that's not something you really have control over, I mean in terms of deciding which route your traffic should take.Cloudflare’s Argo is able to deliver content across our network with dramatically reduced latency, increased reliability, heightened encryption, and reduced cost vs. an equivalent path across the open Internet. The results are impressive: an average 35% decrease in latency, a 27% decrease in connection errors, and a 60% decrease in cache misses. Websites, APIs, and applications using Argo have seen bandwidth bills fall by more than half and speed improvements end users can feel.Picture it like this:Cloudflare can do this because they ""route more than 10% of all HTTP/HTTPS Internet traffic, providing Argo with real-time intelligence on the fastest network paths"". Traffic routing in this fashion is only possible because they see such a significant portion of what's flying around the internet and can make intelligent decisions about how it should be routed. Let's look at what that actually looks like for Pwned Passwords:The first thing you see is that clearly, there's a massive improvement here; response times have been slashed by more than 60%. The next thing is you might start to wonder why even the 278ms figure is so high. What we're seeing here is Time To First Byte (TTFB) which is the delay between Cloudflare sending a request to the server and receiving the first byte in response. That figure therefore includes all the server processing time as well and whilst we know that's now only about 20% of the TTFB number, I've still got traffic traveling long distances. Remember, this is the time between one of those 122 Cloudflare edge nodes making a request and the Azure Function in Microsoft's West US data centre actually returning a response. Consider that in the context of the following map:That biggest green dot is on Munich which is a 17,000km round trip away from Seattle. Longer distances, higher latency, greater volumes, more chance to improve things. Folks in Europe are picking up the biggest gains here whilst those on the west coast of the US are getting the smallest (but they're closer with lower latency in the first place).BrotliHere's another great performance boost that's dead easy to enable:Brotli compression can work much more efficiently than gzip for certain content types. For example, Scott Helme found a 33% reduction for HTML when he wrote about it a couple of years back. So what does it look like for HIBP? Let's load the Pwned Passwords search page via Fiddler first with no ""br"" encoding then with brotli and we'll compare the results:It's not quite at the level Scott found, but it's still 8% off the response size by doing nothing more than toggling a switch. However, you might have noticed that this is the search page on HIBP rather than the API request itself; as of today, Cloudflare isn't applying brotli to ""text/plain"" content types. However, I'm told that in the very, very near future that'll be turned on so I decided to stick it into this post anyway.But what's not said in the image above is that brotli is HTTPS only (do read that post from Sam Saffron on brotli, it has a heap of good info). As if you needed another good reason to roll over to secure communications...FutureI'm really happy with the basic philosophy of this whole thing: serve as much as possible from edge node cache, go as fast as possible across the network when the request needs to go to the origin then process that request as quickly as possible. On the one hand, that's all web performance 101 but on the other hand, it's not always that readily achievable, especially at both scale and price points.There's certainly more opportunity for improvement though, for example by distributing the data and API endpoints in a more globally-accessible fashion. Putting another Azure Function in Europe with another copy of the data in Blob Storage wouldn't be hard or expensive. Cloudflare traffic manager could help geo-steer requests and folks in places like Europe could realise performance gains on requests that go all the way to the origin.Another idea I'm toying with is to use the Cloudflare Workers John mentioned earlier to plug directly into Blob Storage. Content there can be accessed easily enough over HTTP (that's where you download the full 500M Pwned Password list from) and it could take out that Azure Function layer altogether. That's something I'll investigate further a little later on as it has to potential to bring cost down further whilst pumping up performance.Ultimately though, it's all a work in progress and I'm enormously happy with the adoption and performance of the service so far. My intention is to keep working on that speed component so it's as efficient as possible for those who want to use it and hopefully, it actually makes a difference to the security posture of individuals and companies alike.",I Wanna Go Fast: Why Searching Through 500M Pwned Passwords Is So Quick
9854,3847237,2018-02-27 10:48:35,"TNW SitesThis AI checks NDAs for free – and offers a grim glimpse of the futureNDA Lynn, an AI that can evaluate confidentiality agreements for free, is a perfect example of the role artificial intelligence will probably play in our life.For the past decade or so, Arnoud Engelfriet has been the Netherland’s go-to guy for any question regarding internet and the law. Also his last name translates roughly to ‘Angelic Fries,’ which is awesome.One of the services Engelfriet offered was checking NDAs if they should be signed. Non-disclosure agreements (NDAs) are pretty standard business contracts used to keep confidential information under wraps. They are quite common, but do require some scrutiny to establish what exactly establishes a breach and that they don’t gag one forever.“A businessman who needs his NDAs checked, but doesn’t have the budget to have them all checked, only needs to know which one really needs to looked at by a professional,” he tells me“And if something is too expensive to do manually, but people need it a lot, it’s a good candidate for automation,” he concludes.Checking NDAs pretty standard work, but nonetheless time-consuming. So he put his computer science degree to good use and started building an AI that could help him out.“Way back in 1993 I studied computer sciences at university. Back then there was no machine learning, or at least not in this way. For this project I used BigML, a cloud solution for machine learning that aims at making it more accessible to the masses.”All Engelfriet needed was a good dataset, which he happened to have as a lawyer. “To start out with, I uploaded 300 NDAs from my archive, and labeled them by hand,” he tells me.The dataset consisted of about 10,000 sentences, and to train the AI, each of those had to be marked if they contained something untoward for an NDA.“I put all that in the system, and that produced an AI. All it needed afterwards was a nice wrapper.”Engelfriet is very realistic about how revolutionary NDA Lynn is. “It’s not very innovative, but that’s exactly what makes it powerful. AI has become a commodity.”NDALynn itself is a machine learning system that has been trained to identify clauses in NDAs that don’t seem right. It then returns a clause-by-clause report on which ones seem fine, and which ones seem to require too much or are unclear.It sounds trivial, but reviewing NDAs requires a significant amount of work. “Drafting or reviewing a non-disclosure agreement is an art and therefore requires significant amounts of work to get right.”At the moment NDA Lynn achieves an accuracy of 94,8 percent in the latest tests, which is remarkably high. A study published this week by LawGeex, a competing NDA AI, pitted lawyers against their system to check NDAs, with lawyers scoring a paltry 85 percent accuracy on average.I tried out a few NDAs collected from colleagues, and most of them did contain some fishy clauses, which the system labels as ‘dealbreakers.’ But that doesn’t necessarily mean they’re not ok, Engelfriet admits.“Imagine a reporter, for a story they look for sources, reach out to a few of them. You could train an AI to do that. But every reporter has certain biases. Some like talking to scientists instead of politicians or PR representatives, and that skews the AI in a certain direction.”The same goes for NDA Lynn. Because it was trained by Engelfriet, who has certain biases against certain clauses, it takes over those biasses and applies them to the NDA. The AI assigns more weight to clauses Engelfriet as a lawyer would assign more weight.Moreover, since it’s a machine that learns by association, Engelfriet had to weed out quirks that it picked up along the way.He tells me that the machine consistently labeled NDA written under California law as too strict, even though some of them were quite permissible. In the end, it turned out that his original training set contained a disproportionate amount of strict NDAs written under California law, so the AI simply concluded all NDAs from California were too strict.This silly quirk sadly also illustrates another characteristic of AIs: before they take shitty work out of our hands, we’ll have to do quite some shitty work for them. To teach an AI to do a task we think is shitty, we have to do that shitty task A LOT.One example of this is something a maker of interactive sex toys once told me. To make their toy react to the movements in porn videos, they hired a sweatshop full of workers to physically jerk off sensor-laden dildos to the beat of the people fucking on screen. Later, once AI tech had progressed enough, they trained it to take over that unfortunate job.Training AI could be the sweatshop job of the futureTraining an AI to take over is an investment, that should pay out in the end in alleviating the number of hours we have to do shitty tasks. At the same time, we’re polarizing work into the fun stuff AI can’t do and the boring stuff we want an AI to take over.In a way, this is exactly what Engelfriet also did with NDA Lynn: the NDAs that look fine – and would thus be a boring waste of time to review – get a thumbs-up, and the NDAs that contain irregular clauses get flagged and could be passed on to him for a human review – and to further train Lynn.He’s also just released an enterprise version of Lynn that can be trained by companies to incorporate their own specific biases. Unlike Engelfriet, some companies might be fine with perpetual NDAs, and this version would allow them to train the AI to let those pass.On the one hand automating boring tasks to leave more time for interesting work is great news. But on the other, I can’t shake the image of a corporate headquarter basement filled with ‘AI trainers’ – basically glorified data entry grunts, endlessly feeding information into systems and telling them what is right and what is wrong.In the grimmest version of our AI future, the polarization of work could lead to a divide between doing the shitty work of teaching AI how to do shitty work, and all the cool human work that will be able to be done by less people – leading to more competition for those human jobs.In a slightly less grim version, the models become better and better at training themselves, requiring less and less input from humans – leaving the grunts without a job.But however it turns out, someone make me that source contacting AI plz.The Machine:Learners track at our flagship TNW Conference 2018 in May offers more deep learning on the future of AI. Find out who’s speaking and more about the themes here.",This AI checks NDAs for free - and gives a grim glimpse of the future
9855,3849639,2018-02-27 12:00:19,"German court backs city bans on diesel carsLEIPZIG, Germany (Reuters) - A top German court ruled on Tuesday in favor of allowing major cities to ban the most heavily polluting diesel cars, a move set to hit the value of 12 million vehicles in Europe’s largest car market and probably force carmakers to pay for costly modifications.There has been a global backlash against diesel-engine cars since Volkswagen (VOWG_p.DE) admitted in 2015 to cheating U.S. exhaust tests, meant to limit emissions of particulate matter and nitrogen oxide (NOx), known to cause respiratory disease.While other countries are also considering restrictions on diesel cars, a ban in the birthplace of the modern automobile is a new blow for the car industry, and an embarrassment for Chancellor Angela Merkel’s government.Merkel’s government, which has come under fire for its close ties to the car industry, had lobbied against a ban, fearing it could anger millions of drivers and disrupt traffic.The ruling by the country’s highest federal administrative court came after German states had appealed against bans imposed by local courts in Stuttgart and Duesseldorf in cases brought by environmental group DUH over poor air quality.Related CoverageThe court on Tuesday rejected the appeals by the state governments and ordered Stuttgart and Duesseldorf to amend their anti-pollution plans, saying that city bans can be implemented even without nationwide rules.“This is a great day for clean air in Germany,” DUH managing director Juergen Resch said.DIESEL CAR SALES DECLINEThe shares of German automakers Volkswagen, Daimler (DAIGn.DE) and BMW (BMWG.DE) slipped after the ruling, trading down 1.7 percent, 0.4 percent and 0.8 percent respectively.The environmental lobby DUH sued Stuttgart and Duesseldorf to force them to implement driving bans, after about 70 German cities were found to exceed European Union NOx limits.Slideshow (6 Images)Sales of diesel cars have been falling fast in Europe since the Volkswagen scandal, with fears of driving bans sending demand sharply down in Germany in the last year.Paris, Madrid, Mexico City and Athens have said they plan to ban diesel vehicles from city centers by 2025, while the mayor of Copenhagen wants to ban new diesel cars from entering the city as soon as next year.The share of diesel cars in overall vehicle production in Europe could be cut to 27 percent by 2025 from 52 percent in 2015, Barclays forecasts.COSTLY IMPROVEMENTSGermany’s government and the auto industry have been seeking to avert driving bans. Carmakers have pledged to overhaul software on 5.3 million diesel cars and are offering trade-in incentives for older models to help improve air quality.But environmental groups have lobbied for costly improvements to exhaust cleaning systems for cars with Euro-6 and Euro-5 emissions standards.“Once it is clear there will be driving bans the car industry will end its resistance against technical upgrades,” DUH’s Resch earlier told public broadcaster Bayern 2.The government has begun work on legal changes to permit driving bans on certain routes on an emergency basis, transport ministry documents seen by Reuters showed.It is also considering making public transport free in cities suffering from poor air quality.Cities will probably target models with older emission controls. Of the 15 million diesel cars on Germany’s roads, only 2.7 million have the latest Euro-6 technology, data by the KBA motor vehicle watchdog showed.Retrofits would cost billions of euros and an outright ban could slash resale prices, which are used to price leasing and finance contracts. German auto safety group ADAC said carmakers should bear any costs of upgrading old diesel models.",German court backs city bans on diesel cars
9856,3849885,2018-02-27 12:38:36,"About TNWTNW Sitesbridging blockchain’s gaps is the next major hurdleThe enormous number of solutions we see being brought to market under the moniker of blockchain is absolutely astounding. Yet, it’s not inconceivable when considering the amazing potential of the underlying architecture. The use cases for blockchain span across industries and geographical boundaries, helping push its popularity. Most importantly, now that the ecosystem is beginning to mature, new services and products are capitalizing on the industry’s existing weaknesses.Take, for instance, cryptocurrency transactions. If you’re an avid cryptocurrency investor, chances are you’ve experienced more than your fair share of frustration associated with processing times and clearing trades. Enter second layer solutions like the Lightning Network, which are attempting to solve this critical issue, helping us use cryptocurrency as it was intended.Although blockchain delivers significant benefits, namely trustless transactability and immutability which help guarantee us a more secure environment for interacting, integrating and connecting these decentralized systems is concurrently growing in difficulty. If we continue to focus exclusively on valuation and ignore compatibility, blockchain could prove to be its own Achilles’ heel, limiting the architecture’s potential in relation to its ambitions of banking the under-banked and ushering in a new era of democratic systems.Imagine, for a second, moving assets from one chain to the next, a complex process that is fraught with many risks. Whether loss of value due to volatility or confusion over similar wallet addresses—as evidenced by the Bitcoin and Bitcoin Cash debacle—trying to exchange values between blockchains is a headache in the best of cases. It often means that you have to take an asset off one chain, find an intermediary to move the asset from one venue to the next, and then finally onto another chain.Apart from lacking connectivity, the length of time it takes to record, process, and clear transactions means that most of blockchain’s legacy currencies have not cleared the hurdles needed to unseat government issued currencies or gain enterprise adoption. Even so, these inefficiencies have motivated innovative solutions for overcoming compatibility issues, improving connectedness across the entire ecosystem.merging the best of blockchainBlockchain has sparked the introduction of decentralized applications that can touch upon nearly every aspect of our daily lives, whether commerce, logistics, finance, and even travel. The high level of security in Bitcoin’s chain has catalyzed a wave of innovations that use the Bitcoin Core foundation as the as the base for other services.By comparison, ethereum’s construction as a platform for building decentralized applications (dApps), tokenization, and smart contracts sparked a second wave of innovation, helping us access services all while reducing the need to pay intermediaries and cutting costs. Nonetheless, despite these notable advances, Bitcoin has struggled to meet its original objective due to scalability problems and slow transactability, and ethereum suffers from negative publicity surrounding its high profile smart contract losses and security blunders.Credit: Pinterest/OTNEthereum’s smart contracts have paved the way for the creation of decentralized eco-systems based on different blockchains and protocols, but can these eco-systems communicate and collaborate?One of the key beneficiaries of these disadvantages, NEO, has stormed onto the scene, building the infrastructure necessary to support a future smart economy. NEO is in the headlines quite a lot thanks to it’s soaring price. While it mirrors ethereum’s advantageous functionalities like hosting dApps and smart contracts, NEO has conquered the scalability problem plaguing predecessors thanks to its capacity to process 10,000 transactions per second compared to Bitcoin’s paltry 3. Instead of waiting minutes or even hours for your transaction confirmation and clearance, NEO can accomplish the same feat in seconds. In addition, its unique architecture adds to security, with NEO believed to be able to withstand attacks from quantum computers.Other innovations are tackling weaknesses from another angle. Bringing together the best features of different chains has long been a goal, with the early entrants into the space of interoperability like Qtum conquering perceived limitations. By integrating the Bitcoin Core chain with the ethereum virtual machine, Qtum built a better smart contract solution for businesses, benefiting from added security and functionality. The company is already one of the biggest players in the field thanks especially to its modularity and mobility. Its focus on lightening the load means you can even access applications and execute smart contracts directly from your mobile phone.Credit: Qtum Whitepaperdeploying the Ethereum Virtual machine on the Bitcoin blockchain is a big technical challenge. QTUM’s Account Abstraction Layer addresses this challenge and converts the blockchain’s outputs into account balances and facilitates the transfer of information between them.adding scalability to the interoperability frayOther companies are also taking intriguing approaches towards solving Bitcoin and ethereum’s interoperability and scalability problems. Their goal is to facilitate cross-chain transactability and ease the flow of information. While second-layer solutions like the Lightning Network will help us alleviate some of Bitcoin’s sluggishness by promoting off-chain transactability, scalability is just one part of this equation.Ardor, the brainchild of Jelurida, has taken a unique step towards opening the blockchain ecosystem for us even further through the introduction of child chains. Ardor has effectively built its own blockchain on which other chains, referred to as child chains, can be integrated to benefit from the main chain’s infrastructure and security.Just like ethereum, if you are a child chain developer, you can issue coins and build smart contracts. The one unique feature is that you can spend these coins on services within the network, unlike ethereum-based applications which require you to pay for the gas required to run services in Ether instead of your application’s native token. By improving fungibility, the case for migrating decentralized applications to Ardor becomes stronger despite it being a lesser-known blockchain.Credit: Ardor WhitepaperA diagram that describes Ardor’s child chain system where the main chain deals with finding the blocks while the child chain takes care of the of the transactions and paying for the fees by the child chain genesis account.Furthermore, the major advantage of Ardor and accompanying child chains is the processing efficiency available from proof of stake consensus instead of proof of work. In effect, Ardor is unleashing a highly cost-effective blockchain solution that is already prepared to host your applications thanks to its smart construction and engrained interoperability.driving interconnectedness in a decentralized environmentThe idea of decentralizing activities to reduce the scope of middlemen is noble in theory, but unfortunately takes on a different reality in most blockchain ecosystems. Despite the idea of eliminating gatekeepers, the fragmentation that currently defines blockchain-based solutions effectively means that intermediaries continue to disturb our push for greater decentralization. However, these artificial walls and barriers are gradually being broken down by solutions that plan to ameliorate these characteristics, enabling greater fungibility thanks to improved interoperability. FUSION, a public blockchain, which recently raised $50m in under 24h, is launching a platform that plans to centralize decentralized cryptofinance by enabling the creation of multi-currency smart contracts.Credit: Fusion WhitepaperBy the distributed control management of a digital asset, the asset can freely interactwith different types of other digital assets, making the digital asset become a financialproduct and the object of trading, with the ability to make agreements in thefinancial market. This adds the possibility of developing financial services into all digitalassets, extending their potential beyond their current capabilities.By building a hub that can effectively operate with a large array of digital assets and ease their use in smart contracts to provide wide-ranging financial services, blockchain can effectively move us one step closer to servicing under-banked communities. Thanks to its architecture, you can issue a loan to another user in one cryptocurrency via the platforms smart contracts and be repaid in the future with another cryptocurrency, building upon FUSION’s promise of inclusivity. This degree of interoperability overcomes one of the most significant hurdles, which is helping us exchange value across assets that are not necessarily equivalent or pegged to a fiat currency.Interoperability as a driver of adoptionUltimately, the biggest factor that will drive our adoption of blockchain is not only is accessibility, but our ability to build bridges between disconnected systems. The solutions that are incorporating the concept of interoperability are advancing our collective cause, providing a tangible answer to the question of how to unify numerous decentralized environments while appealing for more commercial adoption.Newer entrants to the blockchain environment are promoting the solutions necessary to broaden the scope of services while helping build the trust and security needed for enterprise-level solutions to modernize our outdated approaches. As a result, blockchain’s future will closely mirror our demands for greater scalability, better transactability, and improved interoperability to ensure that the entire ecosystem can finally realize the goals outlined at inception.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",bridging blockchain's gaps is the next major hurdle
9857,3852191,2018-02-27 13:01:00,"Electric cars could soon have as much range as petrol and diesel cars -- and recharge in a matter of minutes -- thanks to what researchers are calling a ""breakthrough"" in energy storage technology. Teams from Bristol University and Surrey University have created a new material for supercapacitors, which store electric charge, that could see EVs recharge in as little as 10 minutes compared to the eight hours it can take for EVs with lithium-ion batteries. And according to the researchers, it boasts enough energy density to see EVs surpass even the top range of current leading models, such as Teslas. Elon Musk himself has previously said a breakthrough in EV technology would likely come from supercapacitors, rather than batteries.The technology was originally being developed for mobile devices -- researchers wanted to create a transparent polymer for Google Glass-like applications -- but once the team discovered the energy storage potential of the material it refocused its efforts. Now it believes the polymer could be more energy-dense than lithium ion, holding 180 watt-hours per kilogram, while lithium ion holds around 100-120 watt-hours per kilogram.However, the technology has some drawbacks. Its capacity to charge quickly means it loses charge relatively rapidly, too. Leaving a supercapacitor car on your driveway for a month would see it lose most of its charge, for example. So it's likely the first cars offering this tech would come with a small conventional battery, too. Nonetheless, while it may still be early days, this technology has the potential to eradicate some of the biggest barriers to EV take-up. According to Dr. Donald Highgate, research director for company Superdielectrics, which worked with the universities on the project, ""It could have a seismic effect on energy, but it's not a done deal.""",Energy storage breakthrough could boost EV range and slash charge time
9858,3852192,2018-02-27 12:00:00,"AiFi replicates Amazon Go's checkout-free shopping in any storeBricks-and-mortar shopping could be about to change forever. Amazon pioneered the automated convenience store with its newly-opened Go supermarket, and now computer vision company AiFi is introducing the first scalable, checkout-free system for stores outside of Amazon's sphere of influence. The technology has the potential to turn enormous retail spaces and small mom-and-pop operations alike into shopping environments of the future, allowing customers to ""grab and go"" without the hassle of physically checking out.The system works in largely the same way as Amazon's, with sophisticated camera technology, AI algorithms and sensors. The difference is its scalability -- it doesn't require any major retrofitting, which makes it a feasible option for almost all stores. Plus, it offers store owners useful understanding into shopping behavior and gestures. This, according to AiFi CEO Steve Gu, makes things ""easier for shoppers and [gives] more insights and real-time statistics for stores so they can better serve their customers and manage overall operations."" Gu says the pilot will roll out in one large store, ""orders of magnitude bigger than the Amazon Go store"" at the end of this year, with more stores to follow. And then it might be time to say goodbye to checkout small talk forever.",AiFi replicates Amazon Go's checkout-free shopping in any store
9859,3852193,2018-02-27 13:12:00,"Apple will open its own medical clinics for employeesApple is launching its own medical clinics called ""AC Wellness,"" in a move that will allow it to take employee healthcare into its own hands. Following similar news about Amazon's venture with Warren Buffet's Berkshire Hathaway, Apple plans to offer what it calls the ""world's best health care experience"" to workers. It quietly published a website for the venture with a careers page seeking a primary care doctors, nurses, an exercise coach, ""care navigator,"" and on-site lab test personnel.The company will build two clinics in Santa Clara, describing them as ""multiple, stunning state-of-the-art medical centers"" in the job listings. One will be built inside its ""spaceship"" Apple Park headquarters, while the other will be located just north it. Several employees who previously worked at Stanford Health Care have already started working for AC Wellness, according to CNBC.Apple said it will provide a ""unique concierge-like healthcare experience ... enabled by technology."" It has already gone heavily into the industry via its Apple Watch health tracking, and creating its HealthKit and ResearchKit platform to aid researchers and doctors in tracking employee and patient health.On top of providing doctors, nurses and other medical staff, it looks like Apple will focus equally on prevention, via diet and exercise programs for employees. As such, it's looking for ""designers"" who will put together programs aimed at promoting healthy living.Apple has over 120,000 employees, and US tech firms spend more per employee on healthcare than most other industries. At the same time, US medical spending tops every other nation in the world by a long shot. Apple should thus be able to save a lot of money and increase the quality of care by bringing its medical services in-house. There are very few companies with the means to launch their own private medical systems, so it's not surprising that among the first to do it are Amazon and Apple.",Apple will open its own medical clinics for employees
9860,3852194,2018-02-26 21:58:00,"Tesla Powerwall systems help some Hawaii schools beat the heatTesla shipped Powerwall batteries to Puerto Rico last fall -- and to Australia last December -- and now it's helping Hawaii. Again. Specifically, it supplied equipment to the island state to help schools combat Hawaii's tropical temperature and relative humidity. Roadshow reports that Tesla shipped some 300 batteries and solar panels to the island as a way of keeping schools cool using renewable energy. This was after state government challenged the local department of education to cool an additional 1,000 classrooms without bumping electricity usage.It wouldn't be the first time Tesla has provided ways to mitigate Hawaii's energy problem, either. Tesla already has a network of solar panels and batteries that keep Kauai lit up at night, for instance. Hawaii's solar grid contained approximately 55,000 cells spread over 45 acres as of last March.In late January, Governor Ige announced that the original 1,000-classroom goal had been surpassed and that over 1,200 rooms had sustainably powered air conditioning. Last September, Pacific Solar installed 955 solar panels and eight power inverters at Oahu's Kamaile Academy, a public charter school. So while Tesla certainly has the most name recognition, local companies are doing their part as well.",Tesla Powerwall systems help some Hawaii schools beat the heat
9861,3852195,2018-02-27 11:00:00,"Snapdragon 700 brings AI acceleration to lower-cost phonesThere's long been a conspicuous feature gap between Qualcomm's mobile processors: You typically have to choose between a do-it-all beast like the Snapdragon 845 or settle for the good-enough features of midrange parts like the Snapdragon 600 series. The company is trying to bridge that divide today. It's introducing the Snapdragon 700 series, which promises some of the features from the 800 line without the pricey hardware. It's providing only a handful of details as we write this, but they'll give you an idea of what to expect.Most notably, the 700 range will include the 800's AI Engine and Spectra image processing. If you're using an AI-savvy device or thrive on mobile photography, you could have the same experience as someone toting the latest flagship phone. Qualcomm is also promising recent (if expected) additions like Bluetooth 5.0 and Quick Charge 4.0 support, as well as new variants on its Kryo CPU cores and Adreno graphics. You can expect as much as a 30 percent power-efficiency boost over a Snapdragon 660, Qualcomm said, in addition to unspecified speed advantages.It's likely to take several months before you find the new Snapdragon in shipping hardware. Samples don't reach Qualcomm's clients until sometime in the first half of 2018, and it'll take longer than that for manufacturers to build chips into finished products. When the 700 series does arrive, though, it should spark a shift in the Android phone market. You may see more in-between devices that outperform typical midrange smartphones, but don't carry the stratospheric prices associated with the latest flagships.",Snapdragon 700 brings AI acceleration to lower-cost phones
9862,3852288,2018-02-27 14:17:30,"California to allow testing of self-driving cars without a driver present0California’s Department of Motor Vehicles established new rules announced Monday that will allow tech companies and others working on driverless vehicle systems to begin trialling their cars without a safety driver at the wheel. The new rules go into effect starting April 2.Until now, the DMV has allowed companies approved for autonomous vehicle testing to run their cars on the roads, with autonomous driving systems engaged, provided that there’s a trained safety driver behind the wheel ready and able to take over manual control. Now, the regulators are updating their rules to allow for fully driverless test, which is a key step along the route towards actually deploying self-driving vehicles in a commercial capacity.This doesn’t mean test vehicles will be out there on the roads without any kind of human intervention backup – the DMV will require that those testing autonomous cars without a driver present have a dedicated communications channel that ties the car to a remote operator, who can take over if needed. The cars will also need to be hardened against cyber attacks and be able to provide their owner and operator info to any other parties in the event of an accident.Any companies wishing to test this way will need to secure a permit to do so from the DMV, just as with driver-present testing, and the new rules only apply to consumer passenger vehicles (not semi trucks, for instance).Alphabet-owned Waymo and GM’s Cruise will likely welcome this news, since the former is already testing totally driver-free versions of its Chrysler Pacifica test car in Arizona, and Cruise recently showed off its fully driverless autonomous vehicle, a modified version of the Bolt EV with no steering wheel and no pedals for brake and acceleration. GM hopes to field those cars by 2019, and this will help them do that in San Francisco, the site of its current largest test pilot.",California to allow testing of self-driving cars without a driver present
9863,3852451,2018-02-27 14:22:58,"Rating the big smartphone makers at MWC 20180Greetings for snowy Barcelona. Sure we’ve still got several days of Mobile World Congress ahead of us, but we’ve entered the point in the week when all of the big names have already wrapped up their press conferences and revealed their hands.With that in mind, this seems like the perfect opportunity to take a good look at how the industry’s big names fared at the show. Barring any sort of unforeseen circumstances, here’s a list of this week’s biggest winners and losers.HMD (Nokia): HMD scored a coup for a second year in a row, led by another nod to Nokia’s former successes. This time out, it was a return for the much loved Nokia 8110 Matrix slider, featuring 4G, a handful of big name social media apps and, yes, Snake. The company also unveiled a quartet of new Nokia Android phones and announced its intentions to “lead the charge” with Google’s Android One initiative.Score: 9/10HTC: The Taiwanese company is struggling more than most to stay relevant in an ever-shifting smartphone landscape. It’s telling, then, that HTC’s big announcements this week revolved around investing more of its existing resources in its VR play. It was crickets on the smartphone front, as the company made the Vive the centerpiece of its booth at the show.Score: 3/10Huawei: After a messy CES, Huawei really could have used a big win at MWC. Of course, a new flagship phone launch would have helped. Instead, the company announced two new moderately interesting tablets and a MacBook competitor with an awkwardly placed webcam. Just in case you were worried, though, it parked a giant truck outside the convention center noting that its upcoming P20 was, indeed, coming soon. The device is set to launch next month.Lenovo: Lenovo was extremely quiet on the mobile front, but the company did announce a slate of fairly compelling laptops. Most interesting of all is the $219 100e. The Chromebook has fairly middling specs, but it looks to be nigh indestructible, as evidence by the vaguely upsetting image above. Meanwhile, a leaked render of the Moto E5 served as this week’s reminder that Motorola’s still kicking.Score: 6/10LG: LG’s big announcement was the very definition of more of the same. The company announced this week that it will be reissuing a sort of director’s cut edition of its V30 flagship sporting the decidedly clunky V30S ThinQ name. The product features AI-based image recognition designed to improve photos. It’s a cool trick that may ultimately prove more novel than it is useful.More importantly, however, the device marks a new six month refresh cycle for the company that will offer a slightly souped-up version of its latest flagship between major upgrades.Score: 5/10Samsung: Samsung was declared MWC’s big winner well in advance of the actual event. The company happily took some of the wind out of its own sails back at CES, when it announced that its next big thing would be arriving this month, offering up teasers all along the way. A few days in, few major competitors have emerged for the title. Even if it wasn’t an off-year, it’s tough to imagine anyone being able to compete with a new flagship for the world’s largest smartphone maker.The Galaxy S9 isn’t a revolutionary device on most accounts, but it bring enough solid upgrades to keep the company’s fanbase happy and remain competitive with Apple’s everything and the kitchen sink iPhone X. As promised, most of the big upgrades are camera related, bringing improved low light through a dual-aperture camera and some really neat real-time translation tricks with Bixby vision.The device’s AR Emojis are admittedly kind of creepy, but a deal with Disney was a nice poke in Apple’s eye after the launch of Animoji. Samsung also upgraded DeX and continued kicking the ball forward with the promise of a new version of Bixby around the corner.Score: 8/10Sony: The Japanese electronics giant launched the XZ2 and XZ2 Compact at this years show. The devices will more than likely run up against the same sort of problems that have plagued the Xperia for as long as anyone can remember, never really becoming the kind of smartphone competitor the company is truly capable of. But the devices do mark a step toward Sony taking these devices more serious as consumer products, rather than simple showcases for its camera tech.During its press conference, the company reintroduced us to the Xperia Ear Duo and teased Samsung by noting that it had released its own Super Slow Motion camera tech last year. Of course, none of that means a whole lot if nobody’s buying. Sony also teased its own upcoming take on low-light shooting, but wasn’t ready to offer a timeline.Xiaomi: While, the smartphone maker has been having a pretty solid last couple of years, it didn’t have much to offer this week at the show. Sure Xiaomi’s got a booth, but it’s saving the big announcements for another day. That said, a company spokesperson did take to Twitter to note that a big announcement is, indeed, around the corner, teasing Snapdragon 845-sporting Mi Mix 2S.Score: 3/10ZTE: ZTE had an all around decent week, announcing a new mid-tier Blade device, featuring some premium-level features, including a dual-lens camera. The company also revealed that its Tempo Go handset will be the first phone to arrive in the States sporting Android Go, Google’s mobile operating system designed for low cost phones with serious hardware limitations.Score: 6/10The rest:-Apple’s never been a player here, but the company did grab some residual points as the iPhone X was invoked with nearly ever mention of Samsung’s new phone.-Crickets from TCL’s BlackBerry branding initiative. The KeyOne was a big hit from last year’s show. This year, nothing.",Rating the big smartphone makers at MWC 2018
9864,3852531,2018-02-27 14:26:23,"EU’s new copyright law will effectively create censorship machinesLast week, the European Parliament’s MEP in charge of overhauling the EU’s copyright laws did a U-turn on his predecessor’s position. Axel Voss is charged with making the EU’s copyright laws fit for the Internet Age, yet in a staggering disregard for advice from all quarters, he decided to include a obligation on websites to automatically filter content.In 2016 the European Commission proposed a new Directive on Copyright in the Digital Single Market. While there are other serious concerns about the proposals, Article 13, which sets out how online platforms should manage user-uploaded content appears to have the most dangerous implications for fundamental rights. Since then, European Parliament committees have done some good work improving the draft law — which makes Voss’ 180° spin all the more alarming.Never mind that the new Article 13 proposal runs directly contrary to an existing EU law — the eCommerce Directive — which prohibits member states from imposing “general monitoring” obligations on hosting providers, Voss’ own party colleague and predecessor, Therese Comodini Cachia (who left to take up a seat in her national parliament in Malta), was against the idea.The European Parliament committee for Justice and Civil Liberties (LIBE) also tried to come up with alternatives (again, drafted by Voss’ own party colleagues) that would safeguard fundamental rights by removing reference to “content recognition technologies,” and no fewer than six national governments are so concerned about the idea that they have requested legal advice.The aim of the rule, which is in line with the European Commission’s proposals more than a year ago, is to strengthen the music industry in negotiations with the likes of YouTube, Dailymotion, etc. While laudable — of course creators should be paid if someone is making money off their work — it is ridiculous to suggest a method that would effectively undermine Europe’s guaranteed Freedom of Expression and Information, and create a new justification for widespread censorship.According to Pirate Party MEP Julia Reda, Voss appears to have “learned nothing from the year-long debate,” and describes the move as a “green light for censorship machines.” That phrase “censorship machines” will anger (gaslight?) those in favor of stronger copyright controls, but it’s no exaggeration.The six countries — Belgium, the Czech Republic, Finland, Hungary, Ireland, and the Netherlands — that sought advice from the Council’s Legal Service last July, asked specifically if “the standalone measure/obligation as currently proposed under Article 13 [would] be compatible with the Charter of Human Rights” and queried “are the proposed measures justified and proportionate?” Well if you have to ask…Under Voss’ revised Article 13, websites and apps that allow users to upload content must acquire copyright licenses for EVERYTHING, something that is in practice impossible. If they cannot, those platforms must filter all user-uploaded content.In the six countries’ original query it was pointed out that “the proposal does not provide for appropriate measures that would enable these users to actually benefit from public interest copyright exceptions. It is important to point out that certain exceptions to copyright, such as e.g. parody or the quotation right are the embodiment in copyright of fundamental rights other than the right to property.” Nothing in Voss’ so-called “compromise” will allay those concerns.Pro-filtering advocates will say that the process is only trying to find copyright-infringing material, not monitoring all uploaded content, without explaining how you can find something without looking for it.The truth is that this latest copyright law proposal favors the rights-holders above anyone else. But it is worth remembering, as the debate is framed as copyright Davids against big tech Goliaths, that rights-holders are very often NOT the original creators. In fact, they are hugely well funded middlemen who skim off everyone in the value chain, and will stop at nothing to protect their business model even as it becomes obsolete thanks to technology that allows genuine creators to earn money in different ways.Ironically, rights-holders claiming to act on behalf of the creator can actually prevent the creator from making a living — the impressively-named Miracle of Sound had his own work taken down by filters set up to act on his behalf by a global rights administration network. With no humans in the loop to check, the filter didn’t realize he was posting his own work!Copyright laws are frequently the thin end of the wedge for the erosion of fundamental rights online, so even if you don’t care who wins in the corporate David versus Goliath battle, remember the Fundamental Right to Freedom of Expression applies to EVERY citizen — let’s not let that get trampled underfoot.",Beware unlucky Article 13… if you care about freedom of expression
9865,3852532,2018-02-27 13:52:36,"TNW SitesBitcoin maximalists have long counted on the Lightning Network and its off-chain transaction solution to fix the network’s increasing scaling issues and exorbitant transfer costs – but it seems this dream might not be as close to reality as the blockchain community wishes.Bitcoin Core developer Peter Todd has taken to Twitter to share his first impressions from playing around with the Lightning testnet and the results are not particularly encouraging, to say the least.Among other things, Todd reported that C-lightning – the Lightning Network implementation written in C – runs into segmentation faults pretty frequently and “when it’s not crashing payments fail more often than not.”For those unfamiliar, so-called ‘segfaults’ occur when poorly written programs falsely attempt to access memory locations that are otherwise out of scope – or have been modified to be accessed in a forbidden way, like writing to a read-only property.One of the reasons for such hurdles is that Lightning was written in C – a programming language Todd argues might not have been the best fit for the task at hand. By contrast, he suggested Rust fits the intended function of the network much closer.Initial impressions of Lightning on testnet: c-lightning segfaults a lot, and when it's not crashing payments fail more often than not. Writing it in C – a notoriously dangerous language – doesn't strike me as a good idea.The developer further noted that the current iteration of the Android-based Eclair wallet for Lightning is also flawed and could lead to a loss of funds.More worryingly though, Todd went on to predict the Lightning protocol could very well “prove to be vulnerable to DoS [denial of service] attacks in its current incarnation.” According to the cryptographer, this poses danger to both the peer-to-peer as well as the blockchain level of the project.As for the Lightning protocol, I’m willing to predict it’ll prove to be vulnerable to DoS attacks in it’s current incarnation, both at the P2P and blockchain level.While bad politics, focusing on centralized hub-and-spoke payment channels first would have been much simpler.Instead, Todd contended, Lightning should have opted for a more centralized approach to setting up its payment channels.Responding to criticism that Lightning is already centralized, Todd said that it “is obviously a decentralized protocol” in its current form. However, he critiqued their approach for “biting off more than they can chew by going for the moon shot of a fully decentralized protocol first.”One thing to mention is that Lightning is still a work in progress. In fact, the company has repeatedly warned non-technical users against toying around with its solution until it is more commercially-ready.We don’t recommend mainnet usage of *any* lightning implementation yet. It has become an unnecessary distraction for our devs.We strongly advise people to wait until the lnd mainnet beta, which will have additional safety and security features. ✅Co-founded by Elizabeth Stark and Olaoluwa Osuntokun, Lightning was conceived as an additional layer to the Bitcoin blockchain that leverages a network of many small nodes to facilitate cheap, fast, and private transactions – a much-needed off-chain alternative to Bitcoin’s congested network.Meanwhile, Bitcoin continues to struggle with high transaction fees and slow transactions – though the network is admittedly much more stable now than it was a couple of months back.Indeed, Bitcoin.org (not to be mistaken with the Bitcoin Cash-associated Bitcoin.com) recently updated its website to reflect this reality.",Bitcoin developer warns Lightning Network is flawed and vulnerable to DoS attacks
9866,3855210,2018-02-27 15:14:00,"Bitcoin 'creator' slapped with $10 billion lawsuitCraig Wright, the Australian who has previously claimed to be Bitcoin creator Satoshi Nakamoto, is the subject of a multi billion dollar lawsuit. Wright is being sued by the estate of David Kleiman, who was thought to have co-created the cryptocurrency with the Australian. Kleiman passed away in 2013, but Kleiman's brother Ira claims that Wright somehow appropriated his former partner's bitcoin hoard.According to Motherboard, either Wright and Kleiman were involved in the creation of Bitcoin, or were there moments after the platform was created. Consequently, they apparently had access to mining tools at its earliest time, enabling them to rack up a small fortune in cryptocurrency. The pair are thought to have controlled anything up to 1.1 million BTC, although it's clear that nobody knows the figures for sure.The case itself centers on Wright's conduct shortly after Kleiman died in 2013, and Wright subsequently contacted Kleiman's elderly father. Wright stands accused of fraudulently claiming that David Kleiman had signed over ownership and control of W&K, a company Kleiman ran, to Wright. He supported this with documents that, as far as Ira Kleiman is concerned, were fraudulent, and signed with a fake signature.The lawsuit contends that the value of the contentious Bitcoin, and any associated intellectual property, is worth anything between $5 billion and $10 billion. It also claims that Wright once admitted that the signature used on the documents was computer-generated. The document also highlight's Wright's history of problems with the Australian Tax Authorities that included the back-dating of crucial documents.In the subsequent fallout, Wright was subject to intensive scrutiny that painted the computer scientist as something of a fabulist. His Ph.D. credentials were found to be questionable, and companies he claimed to have worked with professed to having never worked with him The achievements on his LinkedIn profile were subsequently erased and questions remain as to whether he was the creator of Bitcoin, or simply had too much free time on his hands.",Bitcoin 'creator' slapped with $10 billion lawsuit
9867,3855211,2018-02-27 14:57:00,"Amazon made an escape room powered by AlexaDuring a glitzy Amazon showcase along Barcelona's seafront, the company held two escape room experiences to drill home how very, very excited it is about its action series, Tom Clancy's Jack Ryan, coming to Prime Video later this year. Now, escape rooms are really popular. So is Amazon's Alexa assistant and all those Echo gadgets it likes to call home. The two things make the perfect storm for 2018. So we tried to solve the (not much of a) mystery.Gallery: Amazon's Alexa escape room | 8 PhotosDuring the company's earnings a few weeks ago, Jeff Bezos noted that a lot of its recent growth was due to increased Alexa take-up (""We've reached an important point where other companies and developers are accelerating adoption of Alexa,"") and a boom in Prime membership. Amazon saw more new paid members join last year than in any previous year. That explains why it can afford to splash out on action shows -- and Superbowl ads to promote them.I've missed out on the escape room wave until now, so I took along seasoned escape room... escaper Cherlynn Low to ensure I didn't screw it up. ""It's kinda small,"" she scoffed. It was also kind of classy: all strip lighting and steel fittings. Oh, and filled of Amazon's range of smart devices (Echo Spot, Echo Show and good ole original flavor Echo). Hopefully, Amazon doesn't shoehorn quite as many product placements into the Jack Ryan showitself.Cherlynn Low, EngadgetThe mission? To crack a code, through a back-of-the-cereal box numbers to letters cipher and some interactive Alexa tricks that showcase what Amazon is capable of. Yes, the room was small, and the mystery kinda simple (it still took us twice as long as it should have), but it would be cool to see voice assistants integrated into fully-fleshed escape rooms -- I might be desensitized to smart speakers for the most part, but they still add a nice touch of futurism to everything.Mat once failed an audition to be the Milkybar Kid, an advert creation that pushed white chocolate on gluttonous British children. Two decades later, having repressed that early rejection, he completed a three-year teaching stint in Japan with help from world-class internet and a raft of bizarre DS titles. After a few years heading up Engadget's coverage from Japan, covering high-tech toilets and robot restaurants, he heads up our UK bureau in London.",Amazon made an escape room powered by Alexa
9868,3855213,2018-02-27 14:45:00,"Qualcomm's Snapdragon 845 VR kit needs software to make it shineWell, the hardware is there...When Qualcomm announced its new Snapdragon 845 Mobile VR platform earlier this year, it threw around terms like ""6 DoF SLAM"" and ""foveated rendering"" as highlights. While it's easy to understand the benefits of these tracking and graphics improvements in theory, seeing them in practice is what could get you excited about upcoming headsets. At its booth in MWC 2018, the chip maker had a demo on its reference design headset to showcase some of the changes, which we checked out briefly.Frankly the demo was quite limited. I didn't get to check out foveated rendering, as it wasn't live yet, and it's not like I can tell if the battery life is indeed longer from my five to 10-minute trial. Still, I was able to experience the benefits of SLAM (simultaneous localization and mapping), which tracks your body with reference to the room you're in.This not only helps the system better integrate obstacles into the virtual environment, but also provides a more grounded experience when exploring the simulated world. But the obstruction avoidance system hasn't been implemented yet, so I can't evaluate how effective or smooth it is. What I can say, is that I felt noticeably less nauseated chasing and shooting extra terrestrial bugs in a spaceship's hangar on the reference design headset than some other goggles I've tried. The entire experience was smooth, and the headset was lightweight and comfortable enough for me to duck and hide from swooping vermin.I couldn't tell if the resolution was noticeably improved from Snapdragon 835 headsets, since few of them are actually available yet, but the visuals during my experience were fluid and sharp. My mortal enemies (bugs) and little robot helpers ran around the scene without frames stuttering, and they had smooth borders. I also used the companion touch controller to shoot down the flying creatures, and didn't notice any lag.It would have been nice to see how well the foveated rendering can keep up with my frantic gaze as I hunted down a flurry of winged creatures, but unfortunately, we'll have to wait a bit longer to check out that demo. Qualcomm suggested that there might be more to share come Games Developers Conference in mid-March, so stay tuned to see what happens then. For now, the Snapdragon 845 Mobile VR reference headset is a sweet, if limited, preview of what upcoming standalone VR goggles can do.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.",Qualcomm's Snapdragon 845 VR kit needs software to make it shine
9869,3855309,2018-02-27 11:00:11,"New York’s tech scene is growing up. Venture capital investment is ballooning, tech giants like Google have added thousands of jobs in the city, and homegrown startups are beginning to find multi-billion dollar exits.Yes, the city is a long way from fulfilling the tired trope of becoming the “next Silicon Valley.” But New York techies like it that way.Investors, founders and employees point to all the things that make New York one-of-a-kind as the reason tech is growing in America’s biggest city: the presence of other industries like finance, media and advertising, much more gender and racial diversity and the metropolis’s centuries-old status as a center of global commerce. New York provides a contrast to Silicon Valley, which has been criticized for tunnel vision, being insular, out of touch with the rest of the country and overly homogeneous–both company employees and the people for whom they create products.Instead of being dragged down or drowned out by New York’s well-established mega-businesses, startups are feeding off them, hiring their disaffected employees, building products to make them more efficient and partnering with them in ways that help both sides.The new campus of Cornell Tech on Roosevelt Island in New York City.Photographer: Drew Angerer/Getty Images“We wouldn't have been able to start our company with this scale anywhere other than New York,’’ says Ryan Williams, the chief executive officer of Cadre, a real estate investment startup backed by Andreessen Horowitz. Being in the center of finance gives the firm access to top Wall Street talent and helped it land $250 million to invest on behalf of Goldman Sachs Group Inc. “You have literally every major bank as well as a ton of private equity firms and hedge funds here,” Williams says. “A lot of our early adopters and users are here.”It’s not just finance. It’d be hard to imagine a better place for Artsy, the leading online art platform, to build connections with the world’s top artists and dealers. Oscar Insurance Corp., a startup that sells individual health plans and was co-founded by a brother of senior White House adviser Jared Kushner, found its feet by focusing on the city, one of the largest individual health insurance markets in the country.The number of New York tech jobs has grown 30 percent in the last 10 years, twice the rate of overall economic growth in the city, according to a study from Tech:NYC, which advocates for the local industry. Last year, New York startups raised $11.5 billion in venture capital, quadrupling the $2.6 billion raised in 2012, according to PWC. That puts the city second only to the entire Bay Area, and ahead of Silicon Valley if the gigantic startups based in San Francisco proper like Uber and Airbnb are stripped out.“The ecosystem here has evolved and is a real market,” says Beth Ferreira, a managing director at New York-based venture firm FirstMark Capital and former Etsy Inc. executive. “If the number of times I’m competing with west coast firms for east coast deals is any indication, everyone else has woken up on that too.”Whereas companies like Google, Facebook and Amazon invented, developed and dominated entire new industries like internet advertising and e-commerce, the next wave of major tech companies are disrupting long-standing industries with a strong presence in New York, such as finance and healthcare, says Julie Samuels, the executive director of Tech:NYC.Shan-Lyn MaSource: ZolaShan-Lyn Ma came to New York from a job in Silicon Valley in 2008 to help develop new products for fast-growing online luxury retailer Gilt Groupe Inc. When she left the company in 2012, Ma was one of many former Gilt executives who had the choice to stay in the city or decamp for California.She stayed, taking seed funding from Gilt founder and New York tech godfather Kevin Ryan for her wedding planning and registry startup Zola. She’s now raised more than $40 million and netted a valuation of more than $220 million. “In retrospect it was the best decision ever,” Ma says.The proximity to major offices of many of the brands Zola sells through its site was a major benefit to being in the city, she says. Having the headquarters of top media and advertising companies a ten-minute subway ride away went a long way too, Ma says, especially since one of Zola’s investors is the venture capital arm of Comcast Corp.New York’s tech scene still has room to grow. Even though it leads the country outside of California in terms of number of tech jobs, they only make up 7 percent of the city’s total, compared with 13 percent in Boston.Politicians, keen to brand New York as a major tech hub, are starting new tech training programs and welcoming tech giants to the city. Mayor Bill de Blasio announced funding to double the number of tech graduates from the City University of New York system by 2022. A new tech-focused campus of Cornell University and Technion-Israel Institute of Technology just opened in the city with a government grant of $100 million. (Michael R. Bloomberg, the founder and majority owner of Bloomberg News parent Bloomberg LP, has donated to the project through Bloomberg Philanthropies.)The cycle of startups growing up, going public and former employees spinning out their cash and expertise into new ventures that built Silicon Valley over 40 years is being repeated in New York. The acquisition of DoubleClick by Google, Tumblr by Yahoo and Jet.com by Wal-Mart all boosted the city’s ecosystem. Though New York has yet to produce a Goliath like Facebook or Amazon, the steady stream of IPOs—Etsy and Blue Apron among them—and high-value acquisitions is encouraging.Beyond the funding raises and promises from politicians, the city seems to be ahead of Silicon Valley in one crucial area. “New York is a much better place for fostering women entrepreneurs and women in tech,” says Samuels, who worked in the industry on both coasts. Both Ma and Ferreira agree.The flexibility of a city that incorporates industries outside of tech and New York’s status as a world city are factors too, Ferreira says. She could have found a job in Silicon Valley but doubted her lifestyle would improve with a move to California. “Even with all the wineries.”",New York Will Never Be Silicon Valley. And It's Good With That
9870,3855471,2018-02-27 14:17:30,"California to allow testing of self-driving cars without a driver present0California’s Department of Motor Vehicles established new rules announced Monday that will allow tech companies and others working on driverless vehicle systems to begin trialling their cars without a safety driver at the wheel. The new rules go into effect starting April 2.Until now, the DMV has allowed companies approved for autonomous vehicle testing to run their cars on the roads, with autonomous driving systems engaged, provided that there’s a trained safety driver behind the wheel ready and able to take over manual control. Now, the regulators are updating their rules to allow for fully driverless test, which is a key step along the route towards actually deploying self-driving vehicles in a commercial capacity.This doesn’t mean test vehicles will be out there on the roads without any kind of human intervention backup – the DMV will require that those testing autonomous cars without a driver present have a dedicated communications channel that ties the car to a remote operator, who can take over if needed. The cars will also need to be hardened against cyber attacks and be able to provide their owner and operator info to any other parties in the event of an accident.Any companies wishing to test this way will need to secure a permit to do so from the DMV, just as with driver-present testing, and the new rules only apply to consumer passenger vehicles (not semi trucks, for instance).Alphabet-owned Waymo and GM’s Cruise will likely welcome this news, since the former is already testing totally driver-free versions of its Chrysler Pacifica test car in Arizona, and Cruise recently showed off its fully driverless autonomous vehicle, a modified version of the Bolt EV with no steering wheel and no pedals for brake and acceleration. GM hopes to field those cars by 2019, and this will help them do that in San Francisco, the site of its current largest test pilot.",California to allow testing of self-driving cars without a driver present
9871,3855472,2018-02-27 14:00:58,"Google’s Flutter app SDK for iOS and Android is now in beta0Flutter is Google’s open source toolkit for helping developers build iOS and Android apps. It’s not necessarily a household name yet, but it’s also less than a year old and, to some degree, it’s going up against frameworks like Facebook’s popular React Native. Google’s framework, which is heavily focused around the company’s Dart programming language, was first announced at Google’s I/O developer conference last year.As the company announced today, Flutter is now officially in beta and a number of developers have already used it to build and publish apps that have hit top spots in both the Google Play and Apple App Store.Seth Ladd, Google’s product manager for Flutter, told me that it’s no surprise that the company is making this announcement during MWC. The company wants to use this opportunity to engage with mobile developers and to highlight the advances it made over the course of the last year. For the most part, that means better tooling, like support for Android Studio and Visual Studio Code for writing Flutter apps.Since launching its alpha, the Flutter team added support for new phones like the iPhone X, a number of accessibility features, right-to-left text support and worked on localization and internationalization, as well as the ability to run Flutter code in the background.What’s probably even more interesting for developers, though, is Flutter’s support for stateful hot reloads. That means you can make changes to your source code and within a second, you can see that change reflected in the app on your phone. As Ladd noted, that not only makes the development process faster, but also reduces the need for prototyping tools.With its focus on Dart, Flutter relies on what is still a bit of a niche programming language. Ladd, however, argues that Dart is the just the right language for Flutter. “We didn’t find another language that hit this sweet spot of fast development cycle plus the standard stuff devs expect and love like object orientation, a rich core library and very easy onboarding. With this beta, Flutter now supports the pre-release version of Dart 2, which offers better support for client-side development, too.Ladd also noted that unlike some rival frameworks like React Native, Flutter uses its own GPU-accelerated graphics and rendering engine and not a web view. “There is a huge benefit to this in that the design that your designers envision and what they delivered to your developers are the exact some pixels and designs that your users will experience,” said Ladd. “By shipping our own graphics engine, we offer consistent design as your designers envisioned.”The Flutter team also stresses that Flutter plays nice with existing parts of an app. You don’t need to write your complete app in Flutter. Indeed, many of the developers that have already used it have simply added new Flutter-based screens to their existing apps. As for apps that are fully based on the new toolkit,Google notes that the Hamilton app is among the most popular app to have been built with Flutter.",Google’s Flutter app SDK for iOS and Android is now in beta
9872,3855473,2018-02-27 13:52:23,"Rakuten will roll its $9B loyalty program into a new blockchain-based cryptocurrency, Rakuten Coin0Back in 2016, Amazon’s Japanese rival Rakuten acquired Bitnet, a bitcoin wallet startup that it had previously invested in, to help it work on blockchain technology and applications. Today, one of the first fruits of that deal has come to light. The company is planning a new cryptocurrency called Rakuten Coin — built on blockchain technology and the company’s existing loyalty program, Rakuten Super Points — which it plans to use to encourage loyalty services globally and to help customers to buy goods across different Rakuten services and markets.The news was announced by Rakuten’s CEO Hiroshi “Mickey” Mikitani on stage at Mobile World Congress in Barcelona, where he described Rakuten Coin as a “borderless” currency. Neither he nor a Rakuten spokesperson we followed up with would give a launch date for the service.The news comes on the heels of a big wave of companies trying to figure out their cryptocurrency strategies, to tap into the current hype around decentralised financial services and the seemingly endless appetite of people to hear about and buy into them as their price skyrockets in the absence of much regulatory control.Crypto is being used for a wide range of reasons — as a new funding platform, for currency speculation, international remittances, as a new payment currency and more. In the case of Rakuten, it seems that there are two things at play here.First, the company wants to see if it can drive more transactions from people internationally by cutting out some of the exchange rate fees and other issues if they buy in fiat currencies. Second, there is simply the buzz of crypto today: people who might not have been all that interested in loyalty programs before might turn on to them if they see their reward as blockchain buy-in.Even before Rakuten has launched Rakuten Coin, it’s notable and interesting to see a major e-commerce company — which has billions of users globally and reported $8.8 billion in revenues in 2017 — coming out with a move into how it might use cryptocurrency on its platform.Interestingly, while Amazon has yet to make any significant moves in the area of cryptocurrency, there some speculate the company could get more involved, based on some recent domain purchases and bigger trends.There have been over 1 trillion Super Points awarded to users since the program was launched 15 years ago, equivalent to $9.1 billion, and the idea will be to now give users more ways of applying those loyalty points to more purchases, as a way of driving more purchasing to collect them in the first place.Points currently are collected each time you buy — or, in certain markets where Rakuten runs marketplaces, sell — items or services on the site. As with its rival Amazon, Rakuten has a payment also has an MVNO mobile service with plans to launch its own full-blown mobile carrier — all of which become ways of spending more money as part of the loyalty program.A spokesperson said that there is a decent funnel of people who are already interested in buying items across regions. “People want special items that you can’t get anywhere else,” she said. There are some 44,000 merchants selling goods on Rakuten in Japan, its biggest market. Other holdings include PriceMinister in France (which is now rebranding to Rakuten) and Ebates, the rebates website operator in the US that Amazon acquired for $1 billion in 2014. The logic will be to add Rakuten Coin to all of Rakuten’s businesses — some of which today have loyalty programs, and some of which do not.Mikitani used his appearance at MWC to run through a range of other developments at the company, including yesterday’s news that the company planned to apply to become Japan’s fourth mobile operator, and an expansion of the social features on messaging app Viber, which now has around one billion registered users.Mickitani stressed that Rakuten was “very different from Amazon.”“Basically, our concept is to recreate the network of retailers and merchants,” he said. “We do not want to disconnect [them from their customers] but function as a catalyst. That is our philosophy, how to empower society not just provide more convenience.”0CrunchbaseOverviewRakuten is a tech company that offers electronic commerce solutions in Japan and internationally. The company operates in three segments such as internet services, finance services, and others. The internet services segment comprises businesses running various e-commerce sites, including Rakuten Ichiba, an internet shopping mall; travel booking sites; portal sites; and e-book businesses as well …","Rakuten will roll its $9B loyalty program into a new blockchain-based cryptocurrency, Rakuten Coin"
9873,3855474,2018-02-25 17:20:37,"Samsung Galaxy S9 vs iPhone XTwo killer flagships comparedThe iPhone X is one of the most high-end and desirable phones on the planet, but it’s now got some serious competition in the form of the Samsung Galaxy S9.The two phones have a number of similarities, including their OLED screen technology and focus on photography, but there’s a whole lot more that’s different.So, to help you decide which of these to buy, we’ve put the two handsets head to head, comparing their design, screen, specs, cameras and battery.Watch our hands-on video with the Samsung Galaxy S9 below.Samsung Galaxy S9 vs iPhone X designThe Samsung Galaxy S9 has a high-end but familiar design, with a glass back, a metal frame and a curvy screen, similar to the Galaxy S8. It has small bezels above and below the display and it’s IP68 certified, meaning it can be submerged up to 1.5 meters deep in water for up to 30 minutes.The iPhone X meanwhile was a completely new design for Apple and in some ways it arguably looks more modern than Samsung's effort, with even less bezel on the front, and instead just a small notch jutting into the screen.Its screen is flat though and it too has a glass back and a metal frame. The iPhone X is also water resistant, but it’s only IP67 certified, meaning it can be submerged up to 1 meter deep for up to 30 minutes.The Samsung Galaxy S9 is slightly larger and thicker than the iPhone X, but not as wide, at 147.7 x 68.7 x 8.5mm to the iPhone X’s 143.6 x 70.9 x 7.7mm. At 163g, the Galaxy S9 is lighter than the 174g iPhone X. One thing that might work in the Galaxy S9’s favor depending on whether or not you’re happy to go wire-free is that it has a headphone port, which the iPhone X doesn’t.Samsung Galaxy S9 vs iPhone X displayBoth the Samsung Galaxy S9 and iPhone X have 5.8-inch Super AMOLED screens with wide aspect ratios (18.5:9 in the case of the S9 and 19.5:9 in the case of the iPhone X), but they also have a lot of differences.For one, the Galaxy S9’s screen is slightly curved while the iPhone X’s is flat, but the iPhone X has less bezel around its display, relying instead on a notch to house the front-facing camera and sensors.The iPhone X has a big, almost bezel-free displayThe Samsung Galaxy S9 has a higher resolution screen at QHD+, while the iPhone X’s is a little below QHD at 1125 x 2436. Though Apple’s screen does sport 3D Touch, allowing it to judge different levels of pressure so it can respond differently to a light press or a hard press.It’s a great screen overall as our review attests, and the Galaxy S9’s sounds great too but we’ll let you know exactly how good it is once we’ve had a chance to fully test the phone.Check out our overview video of the iPhone X below.Samsung Galaxy S9 vs iPhone X OS and powerThe Samsung Galaxy S9 has 4GB of RAM and either an octa-core Exynos 9810 chipset with four cores clocked at 2.7GHz and four running at 1.7GHz, or an octa-core Snapdragon 845 chipset with similarly fast cores.Which chipset you’ll get depends on where you are – it’s Snapdragon in the US and Exynos in most other places, but both of them are top-end chipsets.The iPhone X on the other hand has 3GB of RAM and a 2.39GHz hexa-core A11 Bionic chipset, which is also top-end. It’s unknown yet which is the most powerful but neither of these phones should feel slow at all.As for the OS, you get Android Oreo on the Samsung Galaxy S9 (overlaid with Samsung’s own interface) and iOS 11 on the iPhone X. The differences between Android and iOS are well-documented, but in both cases you’re getting the latest, most polished version of the operating system.There are also some differences in storage – the Galaxy S9 comes with 64GB built-in along with a microSD card slot, while the iPhone X comes with a choice of 64GB or 256GB, but there’s no microSD card slot.Samsung Galaxy S9 vs iPhone X camera and batteryIn our review we praised the iPhone X’s cameras and there’s a good chance we’ll be doing the same with the Samsung Galaxy S9, because the company has clearly put a lot of effort into them.The S9 has a 12MP camera on the back and while it has just a single lens it can switch between two different apertures. There’s f/2.4 which is what you’ll probably want to use in daylight, and f/1.5 which is better for low light scenes. That latter aperture is the highest ever on a smartphone, and the camera also has optical image stabilization (OIS), to help keep shots blur-free.The iPhone X meanwhile has a dual-lens 12MP camera with one f/1.8 lens and one f/2.4 lens. It also sports OIS and we’ve found it to be a capable low light shooter.Another thing Samsung has focused on with its camera is slow motion shooting, allowing you to shoot at up to 960fps, compared to just 240fps on the iPhone X.The iPhone X has Animoji while the Samsung Galaxy S9 has 'AR emoji'The Galaxy S9 also has a similar-ish feature to Apple’s Animoji called AR Emoji, letting you create an emoji of your face and have it mimic your expressions. That’s powered by the 8MP f/1.7 front-facing camera, while the iPhone X has a 7MP f/2.2 one.Battery-wise you get a 3,000mAh unit in the Samsung Galaxy S9, capable of both wireless charging and fast charging. The iPhone X meanwhile has a smaller 2,716mAh one, and also supports wireless and fast charging.In our review we found its life was average, but it remains to be seen whether the slightly larger battery in the Galaxy S9 will translate to longer life.TakeawayIf it lives up to its specs the Samsung Galaxy S9 looks set to be one of the best phones around, just like the iPhone X, but choosing between them could come down to what you want from your handset.The choice of Android or iOS is a big one of course, but expandable storage, a variable aperture and a curvy screen among other things could push you towards the Galaxy S9, while the absence of bezels and the extra camera lens are both things in the iPhone X’s favor.One thing’s for sure, you’re going to have to spend a lot either way, £739 for the S9 ($719, AU$1,199) - though we expect a lower official price) so you might want to wait for our full Samsung Galaxy S9 review before making your final decision.",Samsung Galaxy S9 vs iPhone X
9874,3855475,2018-02-26 02:00:34,"This phone of the future has a pop up selfie camera and half-screen fingerprint scannerSo many tricks that you can't haveIt has an in-display smartphone fingerprint scanner that covers almost half the screen, and a selfie camera that pops out the top when required.This is Chinese manufacturer Vivo's glimpse of future phones, and we checked out the Apex Concept at MWC 2018.First is the handset's claim of the ""world's first half-screen in-display fingerprint scanning technology,"" allowing you to place your digit anyway on the bottom half of the screen to have your print successfully read.It's an advancement of the technology we witnessed on the Vivo X20 Plus UD - the first phone to feature an in-display scanner - although you had to place your finger on one particular spot towards the bottom of the screen.With the Vivo Apex, using the fingerprint scanner built into the display is easier, which will be an especially useful feature in large phone as you won't have to awkwardly shuffle the device in-hand to reach the scanner.Image 1 of 4We had trouble registering our fingerprint, and you have to press hard on the screenImage 2 of 4You can see the area of the screen where it's able to scan your fingerprintImage 3 of 4Press your finger anywhere on the highlighted area, at any angle...Image 4 of 4...and the Apex Concept unlocks!It doesn't quite take up half of the display though, instead the scanner covers a reasonable portion of the bottom half of the screen. It's still a much larger landing area than we've previously enjoyed though - and it's something we'd love to see on a phone that's actually going on sale.Yes, that's right, you won't be able to buy this phone. The clue is in the name... 'Concept'. Rather, Vivo is showing us the sort of technology that's nearing a consumer roll out, but isn't quite ready for the big time yet.While the fingerprint scanning area is impressive, during our demo of this development device it didn't always behave itself. It struggled to register our print, and you have to apply a reasonable amount of force on the screen for it to scan you properly.The technology will continue to improve, but it's fair to say that it's not ready for a commercial launch just yet.Two at a timeThere's another party trick up the sleeve of this in-display fingerprint scanner though, with Vivo saying the technology ""enables new usage scenarios, such as the new dual-fingerprint scanning feature... for even greater strengthened security.""What it means is you can opt to use two fingerprints instead of one to unlock the phone, make a payment of unlock certain apps, folders or features. What's even more interesting is the fingerprints don't both have to be yours.It means you and your partner/friend/colleague/pet chimpanzee can each proffer a finger to the phone, and only when you're both present will you be able to access the device as a whole, or certain areas of it.This would be useful in a shared device scenario where a large number of people may be using the same device, but you wish to keep some sensitive or private information secure on it.Image 1 of 4The demo app on the phone offered a variety of different fingerprint scanning optionsImage 2 of 4The dual fingerprint registration is a clear, if potentially rarely used featureImage 3 of 4Holding a finger on each of the blue on-screen pads will register your dual-fingerprintImage 4 of 4Then you can lock down that saucy snaps folder for you and your partner to enjoy togetherPeek-a-booThe tricks on the Vivo Apex Concept phone don't stop there either. You may have already noticed the incredible bezeless design on the front of the phone.On the top and down both sizes there's a gap of just 1.8mm, while the bottom bezel isn't that much thicker at 4.3mm. However, there's not notch encroaching onto the display like on the iPhone X or Essential Phone - so where are the sensors, cameras and earpiece?The proximity sensor and earpiece are embedded under the display, and the latter uses vibrations to generate the sound through the screen and frame. It also directs the volume forward, into your ear, so those around you still won't be able to listen in to your calls.Meanwhile the light sensor is located on the top of the handset, but it's the front facing camera - or the visual lack of one - which is the really fun part.Going up!It's not located under the screen, rather it's hidden from view until it's needed.As you can see in the GIF above, the 8MP front facing camera rises out of the top of the phone when you switch from rear to front camera in app. It takes 0.8 seconds to reveal itself, and it has a smooth motion.Image 1 of 4You wouldn't know the selfie camera was thereImage 2 of 4But switch to the front facing snapper in the camera app and it'll appearImage 3 of 4It takes 0.8 seconds to rise out of the metal frameImage 4 of 4When you exit the app, or switch to the rear camera, it'll lower itself back into the phoneThere's currently no spec sheet for the Vivo Apex, but the firm also claims it will offer hi-fi quality audio through the two down-firing speakers on the base of the handset - and we also know that it has a dual-camera on the back.As we've already mentioned, the Vivo Apex is a concept smartphone which means you won't be able to buy it - but you can expect the fingerprint scanning technology, hidden sensors and pop up camera to appear in future phones in the coming years.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",This phone of the future has a pop up selfie camera and half-screen fingerprint scanner
9875,3855550,2018-02-27 12:34:26,"TNW SitesWant some protection against the rollback of net neutrality? We’ve got an option…Net neutrality isn’t dead. In fact, the fight to promote an even playing field for all users of the world wide web may only be just beginning. Despite the recent removal of those protections by the Federal Communication Commission, efforts to get Congress to act on overturning the decision persist. Either way, expect the fight to rage on for months or (more likely) years to come.Against a backdrop with so much uncertainty, it can’t hurt to take some precautions against any eventuality…and a VPN service from an industry stalwart like Private Internet Access may be just the ticket to assure at least some protection. Right now, you can even get a one-year subscription to PIA for just $34.95 (a 58 percent savings) from TNW Deals.Under the current unregulated rules, internet service providers can legally throttle web traffic that doesn’t warrant preferential treatment, presumably including anyone who doesn’t pay for the privilege. With a VPN, all your web activity is fully cloaked, making it nearly impossible for an ISP to determine what traffic they should be restricting.Nothing says an ISP can’t eventually throttle ALL your traffic…but that’s an extra step that no provider has even threatened to take yet.Meanwhile, PIA also offers full encryption to protect you from everything from data mining to cyber attacks, while also allowing you to get around infuriating international geo-blocking content restrictions.And with an offer this tasty on the table, you may be tempted to double-down and max out your VPN protection. In addition to the 1-year, $34.95 deal, right now, you can also get a 3-year ($89.95, 64 percent off) or 5-year ($139.95, 66 percent off) plan.",Want some protection against the march of net neutrality? We’ve got an option...
9876,3855551,2018-02-27 15:12:51,"TNW SitesThere’s a major lack of quality VR/AR content — this startup wants to fix thatIn the past few years, advances in hardware and software have helped push augmented and virtual reality (AR/VR) from test labs into the hands of millions of consumers. Headsets such as Oculus Rift, HTC Vive and Microsoft HoloLens and the newer versions of Apple’s iOS and Google’s Android operating systems have made it easier for everyone to enjoy a rich AR and VR experience.With platforms such as Apple’s ARKit, Google’s ARCore and Facebook’s AR Studio, creating AR and VR applications has become easier than before, pushing the technologies beyond gaming and entertainment into ecommerce, retail, healthcare, professional work and many other domains.However, the growth of the AR and VR market has been constrained by the limited access to high-quality 3D content. Currently, users must create their own objects and environments to populate their AR and VR applications, which can be a lot of work.Cappasity, a California-based blockchain startup, has launched a decentralized content generation and distribution platform to bridge the gap between the creators of 3D content and the users and businesses who want to deploy AR and VR applications.How does it work?Cappasity’s platform will connect the creators and consumers of AR/VR experiences from across the world. The creators of 3D content and developers of AR/VR applications can put their products on display in the Cappasity marketplace and specify the pricing models for different purposes such as commercial and personal use.Cappasity provides tools for creating 3D content, but it is also compatible with other popular engines such as Unity, Unreal Engine, ARKit and ARCore. The startup also provides tools for integrating 3D objects into web and mobile AR/VR apps and a set of APIs to build custom applications on top of the platform.Businesses and individuals can discover and rent or purchase content on Cappasity’s platform through its app. Cappasity will also sport a “sandbox” feature, an environment in which users can test-drive content and application on different devices.Cappasity already has a working platform that serves more than a million views of 3D content every month. Among its customers are online retailers, fashion brands and the New York Academy of Art. The company will also be partnering with 30 educational institutions in 2018.The CAPP tokenThe Cappasity platform is based on the Ethereum blockchain and its own cryptocurrency, the CAPP token. Buyers spend their CAPP tokens to purchase or rent content and app from sellers. Users can purchase CAPP tokens from online exchanges or earn some by renting or selling their own 3D models and apps, or by helping in moderating the content that is hosted on the platform.Transactions are registered on the blockchain and performed through smart contracts, bits of software that run on the blockchain.There are distinct benefits to running a marketplace for 3D content on the blockchain. A clear challenge that every digital content provider faces is the handling and protecting copyrights. Cappasity stores creates a cryptographic hash of every piece of content uploaded on its platform and stores it on the blockchain along with the information of the person who holds the rights to that content.Blockchain transactions are immutable and can’t be manipulated or altered through data breaches and server hacks. This means that users will have undeniable proof of their ownership of digital content. Other creative industries such as music, arts, film and gaming industries are also making use of blockchain to protect the digital rights of content creators.Another advantage of Cappacity’s use of blockchain is the immediacy and transparency of payments. Blockchain enables secure, peer-to-peer payments, which means the owners of 3D content can be rewarded immediately without going through middlemen that delay their payments or take commission cuts from their earnings.Smart contracts, on which Cappasity relies to process payments and delivery of content, make it possible to provide to implement complex features without the need for a centralized application server. For instance, if several artists or developers are involved in the creation of a model or application, they can set up a smart contract that will automatically distribute dividends between them every time a customer purchases or rents their content.Adoption plansCappasity is currently conducting the second phase of its initial coin offering (ICO), in which it will make $20 million worth of CAPP tokens available to buyers. Several things are worth noting about the ICO:The token sale will only be available to pre-registered users, but others will be able to acquire CAPP tokens from online exchanges once the ICO is over.Cappasity will reward users who hold on to their CAPP tokens for half a year with an “airdrop” or 7-25% of their tokens. This is a positive measure to discourage “pump and dump” schemes, in which users collude to buy and sell tokens at the same time to manipulate prices.The company also encourages large holders to contribute part of their holdings to the token sale to become eligible for the “airdrop” bonus. This too is positive since a fairer distribution of tokens will protect the platform from manipulation by large holders.Cappasity will also launch an innovation fund after the second phase of its ICO, which will allocate $4 million worth of CAPP tokens to support developers who use the platform to create 3D content and AR/VR experiences. The company’s goal is to attract talented teams to employ the platform and help it achieve mass adoption.This post is brought to you by The Cointelegraphand shouldn't be considered investment advice by TNW. Yes, TNW sells ads. But we sell ads that don’t suck.",There’s a major lack of quality VR/AR content — this startup wants to fix that
9877,3855552,2018-02-27 14:36:00,"TNW SitesApple is bringing employee healthcare in-house with a new line of clinics, called AC Wellness.On its website, the venture describes itself as “an independent medical practice dedicated to delivering compassionate, effective healthcare to the Apple employee population.” From what we can tell, it seems it aims to cover the functions of a primary care clinic, but with all the high-end trappings you’d expect from Apple.The About page of AC Wellness promises “high-quality care,” and a “unique patient experience.” It also states care will be “enabled by technology,” which isn’t particularly surprising, given health-tech has been an area of interest for Apple recently.According to CNBC, AC Wellness will launch two clinics in Santa Clara, California. One is in close proximity to Apple’s Cupertino HQ, while the other will conveniently be located within the quirky new Apple Park HQ.AC Wellness is actively recruiting healthcare personnel. On its career page, it’s actively searching for both primary and acute care physicians, nurses, and testing staff. The venture is also recruiting coaches and “behavioral health partners,” which will offer guidance and preventative health tips.For tech companies, health insurance is an essential employment benefit. That’s particularly true in the US, where the cost of seeing a physician can be astronomical. In order to attract talent, companies distinguish themselves through the quality of insurance they offer.With AC Wellness, Apple goes one step further. By creating its own healthcare company, it’s able to control every element of the experience. If this experiment works, it could be a real draw to work at Apple.There are a couple of other benefits to Apple, too. Firstly, by moving primary healthcare in-house, AC Wellness could ultimately save the company a significant amount of money. It could also serve as a laboratory for Apple’s future endeavours in health-tech, and be the place where the company’s future healthcare products are pioneered.","Apple launches in-house employee healthcare service, called AC Wellness"
9878,3857848,2018-02-27 14:30:00,"'Into the Breach' is monsters, mechs and a reset for strategy gamesSubset Games' 2012 space command simulator FTL wasn't the first roguelike indie game to come out during the subgenre's renaissance, but it stood out from the rest. Players guide their single ship against a galaxy of enemies, and the challenge and high skill ceiling earned legions of fans and financial success. Last February, the studio teased its second game, Into The Breach, a grid-based strategy game where the player's trio of mechs must fight off an invading onslaught of colossal bugs while saving as many people as possible. The game comes out today -- and anyone that loved the studio's tough-but-rewarding first game will be equally charmed by its sophomore release.Into The Breach is a turn-based strategy game where mistakes are costly and character death is permanent. It takes cues from similarly unforgiving titles like XCOM and Fire Emblem while preserving the deceptive simplicity of Advance Wars. As in FTL, the graphics are 16-bit sprites, and musician Ben Prunty returns to score a swelling-yet-somber soundtrack. But what sets Subset's new game apart is brilliant novelty: You know exactly how and where every enemy is going to attack. This turns the game into a puzzle, since your first priority is to preserve buildings and the people inside them, only obliterating the bug scum when possible.Subsequently, your toolkit focuses more on shifting the battlefield to your favor, redirecting attacks away from dwindling human infrastructure and, preferably, toward another enemy. Thus, Into The Breach probably won't please folks who just want to clobber giant monsters without consequences. But anyone who likes strategy and isn't afraid to unlearn everything they know about the genre will find the game a refreshing challenge. And just like FTL, it will probably make you rage quit a few times.""As with FTL, we appreciate it when your choices have consequences, and it's not just a power fantasy entirely on its own. There are obviously elements of the power fantasy [in Into The Breach] in that you are just being a hero in giant mechs, but having a piece of the game requires you to make difficult decisions, and that there are consequences to your decisions, I think makes for interesting gameplay,"" Matthew Davis, the game's programmer and half of Subset Games, told Engadget. ""If it's just a case of you win, win, win, win, win then it gets kind of boring.""Unlike FTL's reliance on randomness, Into The Breach gives players all the information they need to solve every encounter. Like Advance Wars, there are no hit percentages -- you hit what you aim at -- and you can see how much damage an attack will cause before you strike. In practice, this prophetic knowledge is both captivating and weighty, as every mistake is preventable. When an error kills a pilot or ends the whole run, it's all your fault. This is humbling -- I've quit in embarrassment after realizing a dumb move many turns ago doomed my squad -- but there are a few mechanics to ease the sting of failure, like a once-per-battle rewind that lets you start a turn over.The encounters themselves are small-scale (often eight-by-eight grids) but complex, less like three-dimensional chess than a 3D puzzle where changing one part of the battle shifts others. Occasionally you'll luck out, and a forgotten enemy will get shoved in the way of another's attack; other times, you'll ignore a crucial element, like terrain or a disabling status, and inadvertently cripple your chances. It's a bit like turning over a Rubik's cube to find you misjudged how a sequence of moves would play out, and you feel silly for essentially outsmarting yourself.When an error kills a pilot or ends the whole run, it's all your faultBut the other big catch-up feature ties in with the game's conceit: If (and when) your mech squad is completely destroyed, you can choose one of your pilots to send back in time, effectively restarting the game. Narratively, all is lost and the bug-like Vex invaders have won, but you can still send one of your skilled heroes into the past to change that new timeline's future, Terminator-style.Starting a new run with an experienced pilot is a nice leg up (though this advantage is probably more psychological than impactful, Davis said.) But the longer you dwell on this mechanic, the more the game world's grim tone sinks in: A tide of colossal monsters barely restrained by a handful of humans endlessly hurling themselves against the onslaught across infinite timelines.You'll have to read into the minimal text and flavor for Into The Breach's world to open up, as Subset Games is a firm believer in less-is-more. The studio consists of artist Justin Ma and programmer Matthew Davis, who hammered out the game's concept for nearly two years before bringing on more professionals to help development. Burnt out on space combat after finishing FTL: Advanced Edition, Ma and Davis decided on a tactics game. The pair noticed that movies like Man of Steel and other blockbuster films habitually disregard the human cost of combat carnage. They wanted to create the opposite -- an experience where players labor to save lives as they clash with unyielding threats.Into The Breach is an intentional rebuttal to FTL in many ways, such as ditching hit-and-evasion percentages for perfect information. And in striving to create the opposite, Ma and Davis explained, they found a way to include even more of the standard rogue-like experience -- the end of one run, the beginning of another -- into the game's overall narrative using the time-travel mechanic. This brings the player completely inside the gameplay loop, enveloping the intermission between play sessions into the narrative.""So, in FTL, the story is this lone ship on a suicide mission, and succeed or fail, it's try your best... you bash your head against the wall 100 times, and the game story resets each time, and so it's a sort of incompatibility with the player's experience versus what's supposed to be going on in the game,"" Ma said. ""So that's how we came up with the time travel, sending the pilots back, and it's sort of to build the actual game experience into the meta narrative.""By making the strategy game Ma and Davis would want to play themselves, Into The Breach ends up being a response to the genre, too. Not just rejecting annoyances (XCOM's ridiculous miss chance comes to mind) but also chopping out all slow and tedious gameplay. Battles can't be won by attrition, and they shift enough to prevent a single strategy from dominating every encounter. To do this, Subset made most missions last around five turns, streamlining combat to the most exciting moments. Aside from brief introductions from each island's reigning governor, a corporate CEO, there are no cutscenes. This brevity and minimalism reinforces the game's scarcity -- the lack of time, resources and hope.""That's the reason why each battle is a set time limit rather than until victory,"" Ma said. ""That keeps it short. Knowing that you don't actually have to eliminate all the enemies, you technically just have to survive, and that sort of tone, in itself, really sort of sets up the way we want the player to feel in the entirety of the game.""The idea to give players foresight into enemy plans came when the pair were fiddling with units, and due to some wonky code in an early game build, moving a certain foe shifted its prepared attack accordingly. ""We were sort of running on a single enemy being able to show their predicted actions, and then we decided that was fun, so we pushed the whole game in that direction,"" Ma said.""Games, to be interesting to us, need to have a stronger core where you can completely ignore all the setting, and absolutely everything, and it still be an interesting game.""What was always central to the game's conceit was straining -- and sacrificing -- to save lives. Enemies alternate between targeting the player's mechs and buildings containing civilians and power. If too many structures go down, you'll lose. The energy grid becomes the campaign's life gauge, and it's prominently displayed up top. But trying to encourage players to save people became a thorny problem -- grant players items or resources for every life, and they'll rescue humans purely for the reward. So the civilian count is just a cosmetic number, a high score to count at the end of your run.I find that disappointing -- to have labored on this exquisitely balanced strategy system and then pull punches on the moralistic core -- but, as Ma and Davis reminded me, the core isn't the story or the world. It's the thrill of the mechanics playing out in an encounter. ""Games, to be interesting to us, need to have a stronger core where you can completely ignore all the setting, and absolutely everything, and it still be an interesting game,"" Ma said. ""We had to balance this whole giving as much theme as possible whilst still letting you play as if there is no theme whatsoever, and it's just a board with pieces.""Into The Breach comes out for Windows today for $15, and then will come the Mac and Linux versions down the line. Subset has no plans to port it to iPad or consoles (including the Switch); they'll revisit that topic after they've seen how the game is received.Including the time travel mechanic means your mission is never over. When you win, one pilot bravely volunteers to venture to the past anyway to give another timeline a fighting chance. There are infinite worlds to save. Whether you fail or succeed, it's always time to jump, once more, into the breach.","'Into the Breach' is monsters, mechs and a reset for strategy games"
9879,3858107,2018-02-27 17:00:44,"Google Clips review0A couple of things to know about Google Clips: First, it is absolutely, unequivocally not a life-logging camera. Sure, it may be called Clips and it does, in fact, have a clip on the back. But do not clip it to your clothing.Second, it is not an action camera. There are plenty available, if you’re in the market. Google Clips is not that. Don’t attach it to your skydiving helmet or motorcycle handlebars. Third, Clips is unequivocally not a security camera. Again, there are plenty of those. Here’s one you can buy for $20.So then what, precisely, is Clips? A “smart camera,” according to Google. It’s a new category, of sorts. One that really couldn’t have existed in this form without the current on-board technology. The device is actually a deceptively sophisticated collection of tech wrapped up in an adorable little package that looks like an Instagram icon that a fairy godmother turned into a real boy.It’s not exactly the “set it and forget it” device that we thought we were getting when the company first announced it way back when at its Pixel 2 event. As advertised, Clips is intended to capture little moments it might otherwise be tough to photograph. The system uses a combination of AI and ML to identify familiar people and animals and determine the moments worth capturing.All of that happens through a robust combination of AI and ML, all processed directly on the device itself — rather than sending it off to a server for processing. This helps cut down on the processing time and maintains some privacy, with nothing you shoot leaving the camera until you choose to transfer it.It’s an interesting concept — and one that could certainly appeal to parents looking to capture those little moments without having to keep a smartphone or camera screen at the ready all the time. But is it $249 worth of interesting?Fixed lensA lot of thought clearly went into making Clips as simple as possible to operate. The result is an extremely minimalist object, roughly the size of a silver dollar. On the front is a large, fixed lens. Twisting it turns the camera on an off, while a trio of white lights let you know that it’s on. Google says it purposely designed the Clips to be immediately recognizable as a camera, so people are aware that they’re being recorded if they’ve never seen one before. And, indeed, it looks like a real-world shorthand for the basic idea of a camera.Below the lens is a single button — the only one on the device itself. This is the shutter. Something the company apparently only added after getting feedback from users. Sure, the system is built around the notion of using machine learning to take the perfect photos and videos, but sometimes you want to override that and not leave it up to chance.On the bottom is a USB-C port for charging the battery and a small hole used to manually reset the system. The back of the device, meanwhile, is empty, save for a Google “G” logo. There’s no built-in viewfinder, for several reasons, including, but not limited to: price, battery life and the desire to simplify the product as much as possible.That means you’ll have to sort of eyeball the ideal spot to place the camera to get the right shot. Given the fact that it’s got a fixed focal length, there isn’t a ton of wiggle room there. Google recommends placing it around three to eight feet from your subject. You also can tap into Live Preview on the accompanying app to get a better idea of what the camera is seeing. The feature also will let you know whether the camera is on a flat surface for better shots. Of course, staring at that sort of defeats the whole “not having to keep a smartphone or camera screen at the ready all the time” thing.A river runs through itThe Clips app is similarly simple, to a fault. By default, everything is captured in a seven-second video called… wait for it… a Clip. If you’ve ever used Vine, Apple Live Photos or Google Motion Photos, you know the deal here. In fact, the videos shot on device can be saved as either of the latter two and, even more handily, an animated GIF.Everything captured by the device is served up in a river. All of the shots appear as static images, until you scroll over them, at which point you can view the full seven seconds of action.Tap on a Clip to edit it either as a still image or motion video. The device actually saves every Clip as a series of stills. Using the Photo option, you can scroll through to grab the perfect frame and save that as a JPEG. The video editing option, meanwhile, has built-in scrubbers that let you edit the length of the Clip.Machine learningOnce you start capturing shots, you’ll notice that some sport a sparkle icon in the upper-right corner. That shows up on “Suggested Clips” — when the system identifies the face of a familiar person or pet. If you want to utilize the functionality, you can connect your device to your Google Photos account and the system will cull facial information from there.You also can just put the camera to use and it will pick up faces pretty quickly. That aspect did seem to be pretty hit or miss in our testing. There wasn’t really much consistency with regard to which Clips of a particular subject the system identified as “suggested.” So far, it’s been trained to capture humans, dogs and cats.I also can confirm that it does a pretty decent job on rabbits. The Google team wasn’t really sure how it would fare on that front, given some of the complexities of the artificial intelligence on board. For example, the system is trained to identify certain aspects, like standing on four legs, whereas rabbits tend to sit on their hind legs. But Google’s AI managed to prevail here, so kudos to them, and congrats to the dozen or so people looking to pick up a Clips to grab photos of their bunny.One perfect shotClips mostly does a good job capturing key moments. It’s not perfect, of course. And really, it requires a bit of good-old-fashioned human curation. That’s where you come in. Odds are you’re only going to end up sharing a fraction of the shots the camera ends up capturing.The device has 16GB of storage on board, so it’s going to take you a while to fill the thing up with all of those 1080p videos — meaning it will be a while before you have to worry about going through and deleting things. The battery, meanwhile, should last around three hours on a charge. It also will go into sleep mode if it gets bored and doesn’t spot any action.The videos are captured in 1080p and the stills are roughly what you’d expect from a standard smartphone camera. And as with many smartphones, Clips struggles in low-light situations. Images come back grainy and motion is blurry. The device will auto-adjust color settings like the default on a handset. You can’t futz with those levels in the camera app — and honestly that level of control probably runs counter to the sort of plug and play functionality Google’s shooting for here. Though extra control is never a bad thing.Money clipsThe first time I tested a smart security camera in my home, I ended up disabling the notifications. I was just getting way too many false positives. Every time my rabbit would move at home, the app would alert me, even if I was on the other side of the world. It was really annoying — but thankfully, the company added AI functionality that was able to distinguish animals and other movement from people.Clips sort of works on the opposite principle, operating on the idea that the right combination of image smarts can scientifically capture the perfect moment in time. It’s a new piece of technology designed to free you from being constantly tethered to another piece of technology. It’s an odd proposition, and ultimately one that will likely be searching to find its audience for a generation or two.Sure, there’s appeal for parents looking to capture as many perfect moments with their kids before they get too old. And certainly plenty of pet owners need another excuse to fill their social media feeds full of adorable action shots. Clips certainly delivers as a GIF delivery service, but $249 is a steep price to pay for such novelty in an era when we’ve all got a camera within arm’s reach 24 hours a day.",Google Clips review
9880,3860870,2018-02-27 17:00:00,"It’s an interesting, if niche device with loads of potential.ClipsGet more infoEngadgetCriticUsersA few months ago, I met my favorite dog. I was standing in my friend's living room when Tassie, a little black and white chonzer puppy, came sniffing by my feet. She looked up at me, got up on her hind legs and placed her two front paws on my shin. A second later, she leaned forward, crossing her paws behind my calf and hugged my leg.I froze, my heart stopped and I melted into a puddle on the floor. I didn't dare to move in case she stopped hugging me. But then I thought, ""This is a moment I want to memorialize!"" So I reached for my phone, and as I shifted my weight slightly, Tassie walked away. I tried to get her to hug me again, but to no avail. My heart had been won, but the moment was lost.Gallery: Google Clips review | 16 PhotosIf I had had a Google Clips with me then, that story might have ended differently. I'd have had a shot at recording that moment for posterity. It's kind of like Snap's Spectacles in the body of a GoPro, being told when to capture bursts of video by built-in AI. Google is calling it a smart camera, and named it Clips, because it records ... clips. Get it?It's a $250 accessory designed to help you capture moments you might otherwise miss when reaching for a phone. Google is quick to emphasize this isn't meant to be a wearable or always-on camera; it's intended for a pretty specific audience -- parents of human babies and fur-babies.Cherlynn Low / EngadgetHardwareKids and pets tend to be careless with gadgets, so it's a good thing Clips is pretty durable. It survived a few falls during my testing from between 3 and 5 feet high, thanks to its sturdy build and the Gorilla Glass 3 covering its lens.Twisting that lens turns the camera on, and one of three white LEDs blinks to indicate it's watching. When it thinks something interesting is happening, it'll capture a seven-second burst of video. There's also a shutter button you can press to take a shot, but the idea is that you shouldn't have to use it often. Google's built-in AI is supposed to be smart enough to recognize scenes that are exciting.Cherlynn Low / EngadgetGoogle includes a sturdy clip case that doubles as a stand so you can attach the camera to the back of a chair or your diaper bag or set it on an uneven surface to get more angles. I found myself using it as a stand most often.There's also a reset button on the camera's bottom, as well as a USB-C charging port, but there's no screen on the device. You can use your phone as a viewfinder, though. That's it physically -- a lens, some lights and a shutter button -- super simple.In useThe setup process is relatively fuss-free, too. The camera is compatible with both Android and iOS. The main difference between the two is that you'll have to press a button on your iPhone to initiate a file transfer, while that happens automatically with Android. Pairing your phone to Clips is straightforward -- download and launch the app, press the camera's shutter button when instructed, and that's it.I had significant connectivity issues when using Clips with a Huawei Mate 10 Pro, and it turns out only iPhones, Pixels and the Galaxy S7 and S8 work with the smart camera right now. Google will add more compatible phones over time, although which ones and how long that will take isn't clear. When I switched to a Pixel 2, my experience was much smoother. If you're not using an iPhone, Pixel or Galaxy, you might want to hold off on buying a Clips.Once you're done setting up, you can start having fun! The system will recognize people, dogs and cats, and when it notices something interesting happening, it will trigger a recording. To be clear, this seven-second clip is actually a series of still images stitched together. The camera shoots at 15 frames per second and does not record sound (no mic onboard). This not only keeps file size relatively small, it also makes it easy to pull out individual frames that you can export as a picture.I didn't use that feature much, though -- I prefered the videos. I especially liked the app's built-in editor that lets you crop into a specific part of the frame and control when to start and end the video. It was supereasy to focus on what I wanted and export just those parts. When it comes to sharing these clips, Google's also made it very simple. You can save your selection as a GIF, an MP4 or a Live photo, which you can then publish on basically any platform.Cherlynn Low / EngadgetThe AISo Google has nailed the basics. But, what ultimately sets Clips apart is the AI. And this is where it gets a little tricky.It's not easy to tell exactly what the AI finds interesting, or why. But after dancing around in front of the camera endlessly trying to figure out what exactly makes Clips tick, I've noticed some patterns. You can teach Clips who is important by training it on your Google Photos library or taking a photo of that person or pet using the onboard (or in-app) shutter button. Over time, it'll also learn who's important based on how often you hang with them.When it sees a familiar face, the camera is more likely to snap. But even then, it doesn't capture everything they do. Movement alone, even with an important face in the frame, won't trigger the camera. When the face is smiling, though, Clips is more likely to be set off. One of the demos that Google showed us at a briefing was of a laughing child jumping off a stool. I tried to replicate that scene by hopping around in front of my Clips, but couldn't get it to record me.Gallery: Google Clips screenshots | 10 PhotosI set the camera in front of a group of friends as we played a few rounds of Jackbox. Clips seemed content to only snap when either me or my friend Sam, whom it recognizes from my Google Photos history, moved. It failed to capture a moment of intense laughter when my friend Jill and I made an (awful) joke about Mother Teresa.Clips was just as unpredictable when it came to capturing one of its intended subjects -- adorable doggos. I set the camera on the couch and let it run for an hour. During that time, Hudson, a terrier mix pup, sniffed around the Clips curiously. I immediately thought, ""Yes! We got something good."" But I was disappointed to find that it did not capture that moment. In fact, it only snapped about a dozen clips, and Hudson wasn't doing much in any of them. What's worse, I had actually focused the camera on Hudson's face and pressed the shutter to tell Clips I wanted more shots of him, but it still barely recorded his adorable mug. Most of the shots it got were of Hudson's cute little butt.Bottom line: Even if you have Clips set up and switched on when a special moment happens, there's no guarantee the device caught it. At this point, only God and Google know what the AI catches, and you'll have to live with the unpredictability.Google says it didn't design Clips to be ""set it and forget it,"" but reps said they noticed that some parents left it in a room after playing with their kids and forgot about it. I've almost left it behind at meetings and restaurants, and there isn't an option to set a distance alert in the app, which would solve this issue.If the AI notices nothing is happening after a while (Google didn't say how long), it will automatically shut itself off. Clips is not a security camera; it's not always always-on. Its battery will last about three hours if it's switched on and actively looking out for something to record. Even if you keep recording for three or four hours straight like I did several times, it's almost impossible to fill up the device's 16 GB of storage. After collecting about 70 Clips, I had used only about 8 percent of the space.That's in part because once you save a Clip to your phone, it's removed from the internal memory. This is a minor complaint I had with the whole setup. I wanted to save all my clips to my phone to make sure I didn't lose them in case I had to reset my camera, but I also wanted them to stay in the app's stream so I could still tweak them from Clips' editor.Wrap-upThat's a relatively small gripe compared to the many things I appreciate about the device. I like its premise, its simplicity, its design and most of its results. I wish the price were a little lower and that the AI was more reliable, but ultimately it lives up to most of its promises.Most of all, I am intrigued by its potential. Sure, this was designed for parents, but think of all the ways the rest of us could use this. And I don't just mean for selfies. Athletes could record themselves performing slam dunks or back-to-back 1080s. Chefs could get closeups of their chopping prowess; circus performers could show off their mad juggling skills. Clips is being marketed to a surprisingly limited audience, but with a few tweaks it could be pretty compelling for the rest of us nonparents.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.","Google Clips review: A smart, but unpredictable camera"
9881,3861135,2018-02-27 17:33:18,"Motorola CEO just dropped a major hint at a Razr return, complete with foldable screenExclusive: Imagine a new Motorola RazrSharesRetro phones may not be the talk of MWC like they were last year (all we have seen is the return of the Nokia 8110) but Motorola's CEO just hinted to TechRadar that its iconic Razr design will soon make a return.Lenovo (which now owns the Motorola brand) CEO Yang Yuanqing spoke to a select group of journalists at MWC 2018 and we took the opportunity to ask him about a potential Razr revival.Last year, he told CNBC that Motorola shouldn't rule out the phone's return, but nothing has been mentioned since. But that doesn't mean the idea of a new Razr has been cut, as Yuanqing explained to us.""With the new technology, particularly foldable screens, I think you will see more and more innovation on our smartphone design,"" he told TechRadar.""So hopefully what you just described [the Motorola Razr brand] will be developed or realized very soon"".He was referring to our question about a possible return for the well-remembered Razr design.We followed up to confirm he was talking about the Razr in particular, and Yuanqing replied: ""I think I have already answered the question"".Cryptic commentsYuanqing's comments have got us thinking just how Motorola could return to the Razr name.He may have been hinting at an updated version of the Motorola Razr (like how the Nokia 3310 2017 returned) but his comment about foldable displays also suggests it may be an entirely new handset instead. Then there could be the possibility that we will get both.If Motorola is experimenting with flexible display technology - like what we've seen rumored for the Samsung Galaxy X - it may be the company will make a new phone line and use the Razr branding for it.The Motorola Razr V3 had an iconic design that many remember fondly, so it'd make sense for the company to lean into the affection tech fans have for the phone from 2005.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.","Motorola CEO just dropped a major hint at a Razr return, complete with foldable screen"
9882,3861210,2018-02-27 17:50:03,"The label is found on physical games, like the ones you might purchase in Gamestop or WalMart. This move from the ESRB follows several horror stories where kids have inadvertently racked up massive credit card bills in games like FIFA 18 and Jurassic World.By creating this label, the ESRB recognizes that in-app purchases are no longer exclusively found in crap “freemium” games. These microtransactions are increasingly present in so-called “triple-A” titles, and in many cases, are required to fully complete a game.That was especially true with EA’s despised Star Wars: Battlefront 2, where players who wanted to unlock certain characters were initially faced with a choice: either “grind” for 40 hours, or open their wallets.These labels warn parents that the game includes additional paid-for elements, and they should be wary with handing over their credit cards to their offspring. It also warns the general game-buying public that there’s almost certainly an annoying upsell somewhere.In addition to the label, the ESRB has launched an campaign aimed at parents, which warns about the dangers of in-app purchases. This includes a website and a PSA video, which you can watch below.",The ESRB just launched an in-app purchases warning label
9883,3863903,2018-02-27 19:40:00,"ASUS’ ZenFone 5 stretches the limits of the term ‘AI’Too bad no one can agree on a definition.As expected, ASUS officially revealed its new ZenFone 5 in Barcelona today, and if you don't mind I'm going to skip my usual meandering intro. The company talked up a handful of AI features in its latest midrange smartphone, but I think ASUS is throwing around the word ""AI"" haphazardly. More on that later though: Let's get the usual hands-on stuff out of the way first.Yes, with its notched 6.2-inch screen and its vertically mounted dual camera, the ZenFone 5 looks quite a bit like an iPhone. (Just, you know, bigger and with a striking finish on its rear.) The 19:( screen is particularly nice (and that's not a typo): Colors appear much brighter and punchier than I had hoped, and it's longer than the standard 18:9 displays so videos played at full resolution aren't obscured by the notch. All told, generally a pleasure to gaze at, as long as you're not looking for a super high-resolution panel.Inside, you'll find one of Qualcomm's Snapdragon 636 chipsets with either 3GB of RAM and 32GB of storage or 4GB RAM and 64GB storage. While it isn't the fastest machine I've tried at MWC, navigating was mostly painless -- I'm willing to chalk up the handful of performance issues I encountered to our phone's non-final hardware. If the truly complete ZenFone 5 runs as smoothly as this one did most of the time, people searching for a midrange phone would do well to keep this thing in mind. And while I couldn't offload any of the sample images I took, the 12-megapixel main sensor seemed to capture sufficiently detailed images with respectable dynamic range. It has an f/1.8 aperture too, which should help out a lot in low light (which I didn't get to test) -- too bad the 120-degree, secondary wide-angle camera was mostly just OK.Chris Velazco/EngadgetASUS hasn't told us how much the ZenFone 5 will cost, but everything about it screams above-average midrange. There's nothing wrong with that, but the way ASUS describes some of its features rubs me the wrong way. When the company introduced us to the ZenFone, a spokesperson proudly talked about its ""10 AI features."" The problem is, ASUS is playing pretty hard and fast with the way it defines artificial intelligence. This is nothing new: While AI has become more accessible and more relevant, it has become clear that there isn't one true definition of artificial intelligence.ASUS has staked its position inside that gray area. Some of those AI features it spoke about to us didn't seem to rely on AI as we know it at all, and I'm concerned that ASUS is overselling things by banking on the general confusion that comes with talking about artificial intelligence. The company's position is that the definition of AI has shifted in recent years toward one that involves some level of machine learning. In applying the AI label to many of the ZenFone 5's features, ASUS is simply sticking with an older, broader definition and hoping the average consumer won't notice (or care about) the difference.Gallery: Hands-on with ASUS's Zenfone 5 | 14 PhotosConsider the ZenFone 5's ability to change its screen's color temperature in different situations. The same feature is called Truetone on Apple's most recent iPhones, and ASUS concedes there's no machine learning going on. Ditto for another feature that keeps the phone's screen on and unlocked while you're looking at it. If that sounds familiar, it's because Samsung Galaxy S phones have been doing that since the GS3. Again, the company was clear: No machine learning involved here either.Meanwhile, a feature called Power Boost automatically manages the ZenFone 5's performance and provides an extra dose of oomph in certain intense situations. We've seen similar features in action in devices like Huawei's Mate 10 Pro, which leaned on an algorithm that was trained to help the phone understand how to best tweak performance over time. In the ZenFone 5, ASUS conceded that the Power Boost feature didn't rely on machine learning. Actually, ASUS wouldn't confirm how the feature worked at all, aside from saying that it isn't maintaining a white list of apps that are allowed to push the processor harder than others. Well, what is it then?Chris Velazco/EngadgetTo be clear, I'm not saying these features aren't helpful. The ones that actually worked in the preproduction sample we tested actually held up well -- the screen never turned off while my eyes were on it, and Power Boost did seem to work for certain applications. What's really odd is that ASUS actually built some features into the ZenFone 5 that rely on machine learning to improve performance over time. Consider the camera. After taking photos for between one and three weeks, the ZenFone will start to offer edited versions you might like.If you find them pleasing, you can accept those changes, and that feedback will help shape the way the camera processes images in the future. If you don't like them, you can dismiss them and never worry about them again. Also in the camera is an intelligent-scene mode that interprets what's in the frame and fires up the correct scene mode. Since we were inside an office on a chilly Barcelona afternoon, there weren't many sweeping vistas to test this feature with. The phone quickly launched its sunset, food and flower modes when pointed at printed pictures of, well, sunsets, food and flowers. Squeezing this kind of functionality into a midrange smartphone is good news for consumers, and for that, at least, ASUS deserves credit.Ultimately, whether or not these are actually AI-powered features is a philosophical question. As long as these features work -- and my current sense is that they do -- most people probably won't care about the distinction. Even so, I got the impression that ASUS wanted a nice, round number of buzz-inducing AI features it could tout at its very first smartphone launch at Mobile World Congress and applied the AI label too liberally. I don't think that ASUS is necessarily pulling a fast one, but I do think it's being at least a little disingenuous.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",ASUS’ ZenFone 5 stretches the limits of the term ‘AI’
9884,3863997,2018-02-26 22:26:44,"Ryzen Boards Reign at Embedded World 2018AMD just released its new embedded line that includes the server-based EPYC Embedded and the APU Ryzen Embedded(Fig. 1). The EPYC Embedded 3000 has up to 16 Zen cores, while the Ryzen Embedded is available with up to four cores that can run a pair of threads. This pairs up with AMD’s VEGA GPU. It’s able to drive up to four 4K displays, which suits it for use in a variety of embedded multimedia applications from medical to gaming.The company worked with its partners to deliver boards, systems, and modules with the Ryzen Embedded V1000 APUs for this year’s Embedded World conference. Presented below are a few of the platforms we saw at the show.Axiomtek’s GMB-140 Mini-ITX motherboard (Fig. 2) runs a V1000 SoC with 11 graphic compute units (GCUs). The motherboard has a x16 PCI Express socket and Mini-PCIe socket. The back panel exposes the dual Gigabit Ethernet ports as well as audio, four video ports, and USB ports. The motherboard can handle up to 32 GB of DDR4 DRAM.2. The GMB-140 Mini-ITX motherboard from Axiomtek links the V1000 SoC to a x16 PCI Express socket and a Mini-PCIe socket. It exposes the dual Gigabit Ethernet ports and handles up to 32 GB of DDR4 DRAM.The DPX-E140 (Fig. 3) from Advantech is an all-in-one, cabinet-ready casino gaming platform. It’s designed to handle four 4K DisplayPort 1.4 DP++ displays for advanced casino slot machines. The system supports 2.5-in. SATA SSD/HDD, SATA DOM, C-Fast, and USB drives. It comes in passive-cooled configurations for SoCs up to 25 W, and fan-cooled configurations for 54-W chips. The system uses a single 12-V dc or ATX power-supply connection. It also has ports for CCTALK, SAS, GPIO, I2C, meter connect, ID003, RS-485, and 5.1 surround-sound.3. Advantech’s DPX-E140 targets casino gaming. Passive cooling works with 25-W SoCs; active cooling is available for the 54-W SoCs.AOpen’s DE6340 media player is designed for large-screen TVs. It sports an AMD Ryzen Embedded V1807B with four HDMI 2.0 and UltraHD (4K2K) support (Fig. 4). The system takes advantage of the Ryzen’s HEVC/H.265 and VP9 high-definition playback support. AOpen bundles the system with AOpen’s intelligent control unit (AiCU) technology for monitoring and management of remote devices. This energy-efficient solution is designed for 24/7 operation.The Congatec Conga-TR4 is a COM Express Type 6 module that supports the family of Ryzen Embedded V1000 processors (Fig. 5). It targets applications such as medical imaging, professional broadcasting, infotainment, digital signage, and video surveillance. It can also be used in applications where SWAP-C is critical, such as UAVs. The module supports up to 32 GB of DDR4 memory with optional ECC support. It has a x8 PCI Express Graphics (PEG) interface and PCI Gen 3 interfaces. It exposes one Gigabyte Ethernet interface along with three USB 3.1 Gen 2, one USB 3.1 Gen 1, and eight USB 2.0 interfaces. There’s a pair of SATA interfaces along with SD, SPI, LPC, I²C, and two serial ports. The module also has high-definition audio support.5. Congatec’s Conga-TR4 is a COM Express Type 6 board that works with the full range of V1000 SoCs. It is available with active- and passive-cooling options.The ABOX-5100 fanless series developed by Sintrones targets vehicle applications and is part of its Artificial Intelligence (AI) GPU Computer line (Fig. 6). The system pairs an AMD Radeon Embedded E9260 or Nvidia GTX-1050TI GPU with an AMD Ryzen Embedded V1000 processor. The separate GPU provides the heavy lifting for AI applications. The system has three full-size Mini-PCIe slots that have become popular for embedded peripheral expansion. Moreover, the motherboard has an M.2 socket, and support for A-E Key 2230 for Wi-Fi/GPS/4G LTE is also available.6. The ABOX-5100 designed by Sintrones targets vehicle applications. It has expansion slots for LTE/3G, Wi-Fi, Bluetooth, GPS, and CANbus.The system uses a 9- to 48-V dc input with Intelligent Power Management. It also incorporates an eight-port Gigabit Ethernet switch that has optional Power-over-Ethernet (PoE 802.3) support.Sapphire Technology’s 5- by 5-in. board is ideal for small form factor video solutions (Fig. 7). It exposes the V1000’s dual Gigabit Ethernet. There’s an M.2 expansion slot, and DDR4 ECC memory can plug into the pair of SO-DIMM sockets.The Seco COMe-B75-CT6 is another COM Express Type 6 module (Fig. 8). It targets infotainment, gaming, digital signage, and medical devices. It can handle four 4K displays at 60 frames/s. It has a pair of SO-DIMM slots for DDR4 ECC memory.",Ryzen Boards Reign at Embedded World 2018
9885,3864000,2018-02-27 19:07:09,"Machine Learning Plus101 NumPy Exercises for Data Analysis (Python)The goal of the numpy exercises is to serve as a reference as well as to get you to apply numpy beyond the basics. The questions are of 4 levels of difficulties with L1 being the easiest to L4 being the hardest.# Input url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' iris = np.genfromtxt(url, delimiter=',', dtype='float', usecols=[0,1,2,3]) # Solution 1 np.corrcoef(iris[:, 0], iris[:, 2])[0, 1] # Solution 2 from scipy.stats.stats import pearsonr corr, p_value = pearsonr(iris[:, 0], iris[:, 2]) print(corr) # Correlation coef indicates the degree of linear relationship between two numeric variables. # It can range between -1 to +1. # The p-value roughly indicates the probability of an uncorrelated system producing # datasets that have a correlation at least as extreme as the one computed. # The lower the p-value (<0.01), stronger is the significance of the relationship. # It is not an indicator of the strength. #> 0.871754157305Output contains 10 columns representing numbers from 1 to 10. The values are the counts of the numbers in the respective rows. For example, Cell(0,2) has the value 2, which means, the number 3 occurs exactly 2 times in the 1st row.",101 NumPy Exercises for Data Analysis (Python) - Machine Learning Plus
9886,3864242,2018-02-27 20:14:56,"TNW SitesBye bye black box: Researchers teach AI to explain itselfA team of international researchers recently taught AI to justify its reasoning and point to evidence when it makes a decision. The ‘black box’ is becoming transparent, and that’s a big deal.Figuring out why a neural network makes the decisions it does is one of the biggest concerns in the field of artificial intelligence. The black box problem, as it’s called, essentially keeps us from trusting AI systems.The team was comprised of researchers from UC Berkeley, University of Amsterdam, MPI for Informatics, and Facebook AI Research. The new research builds on the group’s previous work, but this time around they’ve taught the AI some new tricks.Like humans, it can “point” at the evidence it used to answer a question and, through text, it can describe how it interpreted that evidence. It’s been developed to answer questions that require the average intellect of a nine year old child.According to the team’s recently published white paper this is the first time anyone’s created a system that could explain itself in two different ways:Our model is the first to be capable of providing natural language justifications of decisions as well as pointing to the evidence in an image.The researchers developed the AI to answer plain language queries about images. It can answer questions about objects and actions in a given scene. And it explains its answers by describing what it saw and highlighting the relevant parts of the image.It doesn’t always get things right. During experiments the AI got confused determining whether a person was smiling or not, and couldn’t tell the difference between a person painting a room and someone using a vacuum cleaner.But that’s sort of the point: when a computer gets things wrong we need to know why.For the field of AI to reach any measurable sense of maturity we’ll need methods to debug, error-check, and understand the decision making process of machines. This is especially true as neural networks advance and become our primary source of data analysis.Creating a way for AI to show its work and explain itself in layman’s terms is a giant leap towards avoiding the robot apocalypse everyone seems to be so worried about.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",Bye bye black box: Researchers teach AI to explain itself
9887,3864243,2018-02-27 20:07:02,"Perhaps more interesting, however, is Wacom’s new Cintiq Pro Engine. It’s basically a mini computer that attaches to the back of your Cintiq to turn it into a all-in-one, semi-portable workstation.The engine includes Nvidia’s new Quadro P3200, a mobile workstation GPU that is powerful enough to both create and run VR content. I’ve not seen any real benchmarks yet, but based on the specs, it seems to be somewhere between a 1050 Ti and 1060, which should certainly be enough to run current VR content.There are two versions of the engine. While both include the P3200, the $2,500 model uses an Intel Core i5 HQ, 6 GB of RAM, and 256 GB of storage, while the $3,300 comes with a Xeon CPU, 32 GB of RAM, and a 512 GB SSD. Thankfully, the RAM and storage are user upgradeable, so the CPU is the greatest factor to consider.I’ve been using Wacom’s MobileStudio Pro for the past couple of months and have seen the benefits to an all-in-one solution. Even though I still have my desktop for my most intensive work, it nice to have a system I can move around as needed as well. I can’t take my desktop with me to a meeting, for instance.Granted, lugging around a 24-inch display isn’t easy, either – plus, it needs to be wired – but it’s possible. It’ll also just save a lot of space and clutter over a traditional desktop setup.The new Cintiq will be available in March starting at $1,999. But if 24 inches isn’t big enough, fret not – there’ll be a 32-inch model later this year. You’ll just need a bigger wallet too.",Wacom's new 'Engine' turns your Cintiq Pro into a workstation
9888,3864244,2018-02-27 19:46:54,"TNW SitesMeet Vero, the barely-functional app gunning for Instagram’s audienceIn the last few days a fledgling social media app shot to the top of the free app charts. Vero is a bit of Facebook, Twitter, and Instagram all rolled into one and it’d undoubtedly be over the moon dethroning any or all of those apps.Spoiler alert: it won’t.Vero has been on the market for three years. Recently, certain contingents of vloggers and ‘Grammers (is that what Instagram stars are called? If not, it should be) have started advertising their moves to Vero. This has caused such a dramatic upswing in user numbers that the app crashes repeatedly — I can’t even search for new users to add without encountering a server error.While it has many, many similarities to Instagram, the differences are what put Vero on the radar: The news feed is arranged chronologically, rather than by algorithm; there are no advertisements; and you can also post text statuses, links, and music recommendations. If your problem with any of the aforementioned apps is ads or the fact that you’ll see 20 day-old posts from the same twelve people even if you follow 200, then you’re the kind of person Vero is trying to grab.Supposedly, the app will be free for the first million users, and everyone who signs up after that milestone will have to pay a subscription. Considering the app was created by a billionaire, I’m sure it’ll be able to stay afloat until that point.So what’s the problem?Let’s start by saying the app design is rather repellent. That’s personal preference, I know, and I don’t want to shit on the small fish in the pond for that reason alone — but the translucent teal GUI lacks Instagram’s je nai’s se quoi.I tried out Vero, and its design says ""what if linkedin designed a shitty nightclub?""Additionally, a few of the people who have advertised their Vero profiles have also said their primary reason for joining the app is just to claim their names, rather than out of any fondness for the app itself.It’s just a snowball effect. People like us want to see what it’s like or even just claim our names in case it gets successful. Tbh I saw big flaws in it immediately and haven’t touched it since the first day but hey! I have my name just in case roflLet’s be honest: You know Facebook is going to chew up Vero and spit it out, just like it’s done with Ello, Mastodon, and a dozen other apps. Vero tastes like the flavor of the moment, and having users join just to keep control of their branding or to stick it to Instagram isn’t the same as building a dedicated userbase who appreciates the app’s peculiarities.And the problem with subscriptions is that I doubt very much that there’s a group of people who care enough about social media to pay for it. That’s the beauty of social media: It requires little effort or investment.So I don’t recommend anyone get a Vero account, even for the dubious enticement of being one of the first million. The signs that we’ve got another Snapchat on our hands are just not there.TNW’s 2018 Conference is coming up soon, and we’ll be talking about social media, influencers, and app development. For more information, visit our event page.","Meet Vero, the barely-functional app gunning for Instagram's audience"
9889,3866555,2018-02-27 20:53:00,"Amazon acquires Ring's smart doorbell businessAmazon acquires Ring's smart doorbell businessIf it wasn't already evident that Amazon wants a stronger foothold in the smart home space, it is now. Amazon has acquired Ring, the device maker best known for its smart doorbells. The terms of the deal aren't clear, but it's expected that Amazon will keep the core Ring business intact while finding ways to integrate its work into the Amazon ecosystem. Ring already supports Alexa voice control, so we'd expect more than just obvious tie-ins.The two companies aren't specific about their plans. In a statement, Amazon told Engadget it was ""excited"" to work with Ring and aid in its ""mission to keep homes safe and secure."" Ring said it could ""achieve even more"" by allying itself with Amazon as it focuses on its ""vision for safer neighborhoods.""This is the latest (and arguably biggest) in a string of moves to strengthen Amazon's connected home strategy. It purchased Blink in December, giving it both smart doorbells and wireless security cameras. It also launched a Key service for Prime members that uses smart locks and cameras to let couriers into your home. And to no one's surprise, the Cloud Cam is quickly becoming a cornerstone of Amazon's smart home business.The Ring purchase should help Amazon's overall hardware business, but it's also a not-so-subtle attempt to counter Google's Nest team. Nest has been gradually branching out, most recently introducing its Hello video doorbell. If Amazon doesn't have an effective answer to the Nest group, it doesn't just risk losing out on device sales -- it risks Google Assistant gaining ground on Alexa. Whatever Amazon pays now may be easily justified if it helps the company dominate the voice assistant world.",Amazon acquires Ring's smart doorbell business
9890,3866556,2018-02-27 20:00:00,"Dear HMD, the world is finally ready for the Nokia CommunicatorThe phone that stole the show at last year's MWC wasn't an Android, didn't have a Samsung badge on it, could barely connect to the internet and didn't have a touchscreen. The relaunched Nokia 3310 traded on one key thing: nostalgia. But it was enough for it to grab most of the headlines, and even see a re-rerelease later that year with souped up ""3G"" data speeds. That wasn't it though, just a month ago, we finally got the 4G version we presume some people were asking for.This year, Nokia (or, rather, brand-owner HMD) is at it again, reviving the iconic ""Banana phone."" But, one day into the show proper, and the chatter about it had faded. I think new-Nokia missed a trick. Instead of rehashing iconic feature phones, it should revive one of Nokia's most prescient franchises: the Communicator.While the 3310 is iconic, it's a 16 years old ""dumb"" phone, which means that it's not really all that much use to today's tech-savvy crowd. Even with a color screen, a camera (unheard of back then) and a few other tweaks, it was always going to be something for users with simple needs. Don't get me wrong, there are situations where phones like this can flourish. Younger/older users might enjoy the simplicity, for one. But these aren't the people raving about retro revivals.The Communicator series (my favorite is either the 9110i or the E90), is ripe for a 2018 remake. If you never saw one of these beasts, they were extra chunky phones (back when phones were already chunky), that flipped open to reveal a second, full-length display and a QWERTY keyboard, almost like a mini laptop. The Communicator screamed ""I do business"" and came with all manner of apps for the suit on the go. This was the defining ""smart"" phone of its day (if you're asking me that is).The Communicator line started early, around 1996. Showing that, even then, Nokia knew the future was email and internet on your phone. There were several iterations that grew smaller and smarter until the name fizzled out around 2007. Nokia experimented with similar products, like the innovative, but ultimately lacking N800 internet tablet. And the 2011 E7 borrowed much of the Communicator's DNA with a slide out keyboard (but lost the executive charm of its forbearers).The E90 Communicator was the last hurrah of the franchise, but by then the iPhone had just arrived, Android was around the corner, and apps that didn't need a degree to configure were about to consign it to the history books. Nokia was then destined to wander into to sunset hand-in-hand with Microsoft (and Windows Phone). And here we are.You'd think that a clamshell phone with a physical keyboard would be redundant in 2018, but BlackBerry fans still pine for physical keys, and when Planet Computers showed off its -- Psion reboot -- Gemini PDA/Android hybrid at CES this year, the Engadget crew was polarized (I'm team Gemini, obviously). Anecdotally, a writer sat next to me at MWC as I type this is pecking out missives on an old Huawei phone and a fold-out Bluetooth keyboard, I'm pretty sure she'd love a modern Communicator too.New Nokia has the ability to make a decent Android phone, if the latest Scirocco is anything to go by, so a higher-spec, more capable Android Communicator isn't beyond the company's abilities. It might feel niche, but the combination of nostalgia and actual functionality would be intoxicating enough to reach new and nostalgic buyers alike. What's more, the average user wasn't ready for what the Communicator offered back then. Buyers today, however, expect and rely on the very features -- email, browsing and productivity -- that ultimately made phones like the E90 too exotic back then.James began writing for music magazines in the UK during the '90s. After a few failed attempts at a DJ career, he carved out a living reviewing DJ and music production gear. Now he lives in the Bay Area, covering drones, fitness tech and culture, though he keeps his DJ gear plugged in and on show. You never know.","Dear HMD, the world is finally ready for the Nokia Communicator"
9891,3866653,2018-02-27 21:46:09,"Amazon has reached an agreement to acquire Ring, the Santa Monica, Calif.-based maker of video cameras, doorbells and other smart home technologies, GeekWire has learned. The companies are expected to announce the acquisition this afternoon.The surprise acquisition marks the latest move by the Seattle-based tech giant into the smart home technology market. Financial terms were not disclosed, but Reuters puts the deal at more than $1 billion. Amazon is expected to treat the Ring deal similar to past acquisitions such as Zappos, Twitch and Audible, pursuing product and feature integrations where appropriate but maintaining the Ring brand and largely allowing the company to continue operating as it has in the past.“Ring is committed to our mission to reduce crime in neighborhoods by providing effective yet affordable home security tools to our neighbors that make a positive impact on our homes, our communities, and the world,” a Ring spokesperson said in a statement. “We’ll be able to achieve even more by partnering with an inventive, customer-centric company like Amazon. We look forward to being a part of the Amazon team as we work toward our vision for safer neighborhoods.”Amazon is positioning its Echo smart speaker, with the embedded Alexa voice assistant as a home technology hub.“Ring’s home security products and services have delighted customers since day one,” an Amazon spokesperson said in a statement. “We’re excited to work with this talented team and help them in their mission to keep homes safe and secure.”Ring makes WiFi enabled doorbells equipped with cameras that detect when someone is at the door. The smart doorbell rings users’ tablets or smartphones through an app that lets them see audio and video and talk to the person at the door, wherever they are.The company’s mission is to reduce crime in neighborhoods while providing affordable options that work for every home. It started with a single video doorbell, now priced at $179, and has since expanded to offer several different versions as well as security systems and cameras.Just last month Ring acquired a startup called Mr. Beams that makes WiFi-enabled LED lighting with motion-sensing capabilities.Amazon previously invested in Ring via its Alexa Fund, which helps companies building skills and products for the tech giant’s digital brain.This past October Amazon unveiled its own $119.99 home security camera, Cloud Cam, with a companion Amazon Key app that works with smart locks to let Amazon Prime members give house cleaners, dog walkers, package delivery companies and other service providers access to their homes. Amazon sells an Amazon Key In-Home Kit, including an Amazon Cloud Cam and a compatible smart lock, for $249.99.The Cloud Cam competes with a variety of security cameras on the market, including Ring, Arlo and Google’s Nest Cam. It can be used with subscription plans for extended storage and other extra features, ranging from $7 to $20 a month, or a free plan offering 24 hours of cloud storage.The technology addresses the ongoing challenge of package theft and provides an additional benefit for Amazon Prime members. The concept, however, is not popular with some people concerned about Amazon controlling access their front doors.Ring rolled out integration with Amazon’s Alexa voice platform in June, connecting select products to the Echo Show and Fire TV devices. “By working with Alexa, Ring is making home security even simpler and more convenient. Now, you not only have the power of home security at your fingertips, you can also have it at the sound of your voice,” Ring noted in a blog post. The skill has a 2-star rating.Ring CEO and Chief Inventor James Siminoff appeared on the show Shark Tank in 2013 to raise money for his product, which was called Doorbot at the time. He failed to make a deal on the show, but since then, the company has raised more than $209 million, according to CrunchBase, including a $109 million round in January 2017.In an update to the show last November, Siminoff said the company is valued at more than $1 billion and employed at the time more than 1,300 people and offered 10 core products, sold in more than 16,000 stores.“I am an investor in Ring, and while the product is exciting and its mission to reduce crime in communities is important; what I found so interesting was the story of the young entrepreneur – James Siminoff – behind it. Like many others at the start of their journey, James tried and failed to raise money for his idea – for him it was on the popular US show Shark Tank. Unbowed, he went on to develop the product and the business and just raised a substantial fundraising which I joined.”In December Amazon acquired Blink, a startup based in Andover, Mass. that makes wire-free smart security cameras and video doorbells.","Amazon to acquire Ring video doorbell maker, cracking open the door in home security market"
9892,3866892,2018-02-27 21:59:58,"About TNWTNW SitesIBM’s Watson is going to spaceIBM yesterday announced it would be providing the AI brain for a robot being built by Airbus to accompany astronauts aboard the International Space Station (ISS). When only the best of the best will do, it looks like Watson has the right stuff.The robot, which looks like a flying volleyball with a low-resolution face, is being deployed with German astronaut Alexander Gerst in June for a six month mission. It’s called CIMON, an acronym for Crew Interactive Mobile Companion, and it’s headed to space to do science stuff.It’ll help crew members conduct medical experiments, study crystals, and play with a Rubix cube. Best of all, just like “Wilson,” the other volleyball with a face and Tom Hanks’ costar in the movie Castaway, CIMON can be the astronauts’ friend. According to an IBM blog post:CIMON’s digital face, voice and use of artificial intelligence make it a “colleague” to the crew members. This collegial “working relationship” facilitates how astronauts work through their prescribed checklists of experiments, now entering into a genuine dialogue with their interactive assistant.It’s fun to think of a five kilogram robot flying around, taking pictures, offering advice and going over procedures with astronauts like a useful version of ED-E, the friendly Eyebot from Fallout 3. But it’s not as gimmicky as it might sound.The procedures the astronauts will engage in can have dozens or hundreds of steps and it’s not always feasible to have manuals or computers on hand. Not to mention Watson’s voice processing suite makes it a perfect assistant for researchers who have their hands full.Furthermore the system could be developed to work as an alert system, according to IBM, and we all know that no spaceship is complete without an AI computer that repeats the word “alert” for the entire duration of a catastrophe.","I'm sorry Dave, I can't do that: The International Space Station is getting AI"
9893,3869677,2018-02-16 21:03:35,"Carta for Public CompaniesIn 2017 we started supporting public companies. This is a risky bet for Carta. The cap table market for private companies is an easy market to enter. The incumbent is Excel and the competition is paper. In addition, the market itself is growing quickly with new companies being formed everyday, adding customers to the top of the funnel.In contrast, public market equity management is crowded with vendors. Plus the public market as a whole is shrinking. In 1996 there were more than 8,000 publicly listed companies in the US. Today there are less than 4,000. It is a negative sum game. The overall pie is shrinking. Even the winners are losing.So why did we enter the public market? For three reasons.Onramp to IPOIf you believe our thesis that private companies and their investors will converge on Carta, the path to IPO is through Carta. The 100–150 IPOs each year will be Carta Private customers. CFOs won’t have to worry about implementing new plan administration and stockholder tracking software when going public.Onramp to IPOToday, we have 15–20 Carta Private companies in the pipeline preparing to go public in 2018. Almost certainly, the companies that IPO in 2020, 2022, and 2025 are on Carta today as early stage customers. Our job is to seamlessly transition them into the public capital markets as they accelerate towards IPO.2. Transfer Agent + Equity Plan AdministrationWhen a private company goes public their cap table is split in half. The stock ledger goes to a Transfer Agent like Computershare or American Stock Transfer. The other half of the cap table, the employee equity plan, goes to a broker like E-Trade or Schwab.Transfer Agent + Equity Plan AdministrationIn this fractured world we have an advantage. We are an SEC Registered Transfer Agent and a FINRA Regulated Broker/Dealer. We also track the full capital structure of the company including the stock ledger and equity plans. We are the only platform for public companies to manage their entire shareholder base. Like private companies keep their full cap table on Carta, for the first time public companies can too.There is administrative convenience to consolidating solutions. Managing two different systems to track shareholders and employee equity is painful. For shareholders and employees, logging into two different systems is a terrible user experience.There are also cost savings. The transfer agent industry is an oligopoly that price gouges companies. Brokerages that offer employee equity software do the same. By combining services on one platform we can cut total costs in half.But there is one more reason for owning the entire shareholder registry that is far more valuable.3. Investor Relations 2.0There are many differences between being a public company CEO and a private one. The biggest difference between private and public companies is that private company CEOs know their shareholders. Public company CEOs don’t.Investor Relations 2.0This simple difference has large consequences. There is no such thing as an activist investor in the private market. Hostile takeovers and LBOs don’t exist. Neither do day-trading, market making, short-selling, and other business models that make money on short-term price changes.Now that Carta is both the Transfer Agent, Broker/Dealer and Plan Administrator we can know the entire investor base. This means we can do two things:First, we can give the CEO a 360 degree view of their shareholders. We can tell her if Carl Icahn has recently taken a large position. We can tell her what percentage of stock is institutionally owned versus retail. We can tell her who the biggest shareholders are and how long they have been holding the stock. We can even ask shareholders how long they plan to hold it.Second, we can give the CEO a direct, secure, and compliant communication channel to all shareholders. Today, CEOs communicate to shareholders with 10Ks and quarterly calls. On Carta, a CEO can proactively communicate directly with shareholders. Shareholders can subscribe to the CEOs updates. Shareholder meetings can be conducted through online video conferencing. Voting can all be performed on mobile devices. And CEOs can know every shareholder that’s involved.It will be a new way to do public company Investor Relations. Stay tuned…*Securities transactions offered through Carta Securities, LLC a broker-dealer registered with the SEC, member of FINRA and SIPC.",Carta for Public Companies
9894,3869838,2018-02-27 21:02:36,"0After acquiring Blink just two months ago, Amazon is now acquiring Ring, makers of the self-titled Ring doorbell (plus a bunch of other security gear, like solar security cameras, floodlight cams and an in-home alarm system).GeekWire broke the rumor this afternoon, and we’ve just received independent confirmation.Details on the deal are still pretty light; the financial terms of the deal, for example, haven’t trickled out just yet. Update: Reuters is reporting, via tweet, that the sale price was more than $1 billion. The company had raised around $209 million to date, according to Crunchbase.This acquisition makes plenty of sense. Amazon has already built a few connected cameras of its own — but hardware is, as they say, hard, and that’s not going to change. With nearly a dozen solid products to its name, the Ring team has proven themselves more than capable of building hardware (and I’m sure its array of patents doesn’t hurt, either.) With Amazon, Google, Apple et al. all duking it out for physical space in and around your home, someone was going to make a big offer — and I’d be surprised if Amazon was the only bidder in the mix. Plus, who on earth is responsible for more doorbell presses than Amazon?(Fun bit of trivia: Ring debuted to the world on Shark Tank back in 2013, then known as “DoorBot.” They wanted $700,000 for 10 percent of the company, but no one took the deal.)",Amazon is buying smart doorbell maker Ring
9895,3869918,2018-02-27 23:16:54,"About TNWTNW SitesClassic SNES game Chrono Trigger gets godawful PC portSquare Enix surprised fans today by releasing Chrono Trigger on Steam, making it the first time the cult classic has been ported to PC.Fans should be cheering. Instead, they’re groaning over the game’s terrible quality.The Steam release appears to be a port of the seven-year-old mobile version of the game. Let’s just say this particular version doesn’t look good on Windows. It has the tile-based interface, intended to go hand-in-hand with touch controls, an ugly font, and buttons which stay in the corner of your screen the whole game.Oh, and you apparently can’t remap the controls, which is anathema to PC gamers.For the record, Chrono Trigger has been released multiple times since its original 1995 release on the Super Nintendo. It’s been released for the original PlayStation and the Nintendo DS, before finally getting mobile ports to iOS and Android in 2011.I only mention this because you have to be really trying to get a game with this many ports and remakes to look this bad. Granted, I’ve only seen videos and screenshots, but it looks plenty ugh.I know Square Enix can port handheld games successfully. Final Fantasy Type-0 — a PSP port — suffered from muddy textures, but was otherwise a passable port. I’m not sure why the infinitely more popular Chrono Trigger was given short shrift.",Classic PS game Chrono Trigger gets godawful PC port
9896,3872489,2018-02-28 00:28:48,"New, new, new TechCrunch0Then there was new TechCrunch. And, if you’ve been reading TechCrunch for the last five years or so, you’re used to seeing new, new TechCrunch. Our last redesign, launched in 2013, was conceived when iOS was still Skeuomorphic and responsive web design was forward thinking. It’s served us well, but the internet never stands still and neither do we.Today, we’re launching new, new, new TechCrunch to about 10 percent of our readers. If you’re in that group, you’re going to see a very, very 1.0 version of our new site that we’re going to build on over the next few years to deliver what we hope is an experience that aligns perfectly with what we cover as well as how we cover it.The premise of TechCrunch has always been built around seeing things first. If you want to know about the companies that will be profiled next year in the weekend tech sections of big newspapers, or see and hear from the founders that will trot out onto the stage of a vanity project design conference 18 months down the line, you read TechCrunch now and you come to our Disrupt conferences now.We make it our business to wade into the ocean of new companies, technologies, founders and investors that make up the worldwide community of early stage startups and absorb as much as we can as early as we can. We’re the ‘tip of the spear’ in many of these cases, and it’s our job to parse these topics, give you context and have an ongoing conversation in clear, plain language with you.When we know it, you know it. Back in 2009, TechCrunch founder Michael Arrington made the case for what has been coined process journalism and Jeff Jarvis summed it up succinctly, saying that “newspaper people see their articles as finished products of their work. Bloggers see their posts as part of the process of learning.”No matter what you call it, the shape of it is really a conversation between the reader and the writer. As the word blogging has become a pejorative and the impact of decisions made about technology have been made dramatically and frighteningly clear, I believe that the overall shift has been away from having conversations and towards pontification. Over the past couple of years, the tech journalism meter has been pegged all the way over at cynical, knowing smugness, which doesn’t do much to invite people in and encourage discussion about the way that technology is built, funded and developed.Latest Crunch ReportInstead, our way is to approach technological topics from a place of healthy skepticism, but genuine optimism as well. Technology is not going to magically stop or disappear. It will only get more powerful and more important. So it’s our job to examine the power structures that bring it into existence and examine the sociological and business advantages that greater awareness, more organic cultural connections to the societies that they will eventually serve and a diverse and inclusive engineering culture can make for better tech. You can’t do that from on top of the mountain. You have to get out here and wade through it with the builders.The TechCrunch philosophy is to stay close enough to feel the heat without getting hypnotized by the flames. We’re not always perfect or right, but we’re willing to be here, to really understand it and to make an honest attempt to help others to.All newsrooms are or should be evaluating how to handle the rapidly shifting sands of tech news, but the core fact is that these conversations have no end. Even when a company closes up shop or gets acquired the diaspora of talent, money and technology may have reverberations down the line.And the product of journalism has not kept up with the need to have these conversations. When we began designing the new, new, new TechCrunch, the desire to have the site support rapid-fire, stream-of-understanding journalism in a way that could be presented as a conversation supported by context was the theological underpinning. Everything else was designed around that idea.Just like the companies that we cover here at TechCrunch, what we’re shipping isn’t perfect. I’m sure a ton of people reading this are probably taking a break from coding, launching, building, shipping and tweaking products.That’s why we’re launching with what we consider to be an MVP version of TechCrunch. We want you to check it out, let us know what you think and stress test it. This is a beta, a beginning. But we think it’s pretty cool. We’ll have more to say about the site and all of its features, as well as how we went about building it, in the future.For now, here are some things to look out for as more and more of you begin to see it.Exactly what needs to be there and no moreThe new, new, new TechCrunch is an empty bucket. We’re going to fill it up over time with the things that we need, but mostly with pure content. We’ve stripped away all of the extra visual crud that tends to wash up onto the shores of a website and stick to it. There’s the main feed, some simple navigation and a couple of modules that we can play with. Otherwise it’s there to support the content, not to impress you with our CSS skillz.There are no pages on TechCrunchWell, almost no pages. Page views are a BS metric favored by people who love to game the system. TechCrunch is focused instead on engaging our readers by shipping genuine scoops, analysis and context and compelling conversations that keep you glued to the site. We have a huge amount of readers that visit us monthly, weekly and even daily because they know that we’re pushing out a real-time feed of the things that fascinate us about tech inside and outside of Silicon Valley.This is all delivered in a single stream that will allow you to jump into and out of articles without losing your place in the overall conversation. You’ll never be out of context. And when they’re done, no infinite scroll onto some other random article, because you’ve never left the home page of TC where the newest stuff lives.A conversational structureAs I mentioned above, a key desire for us was to be able to have conversations with readers about stories that are developing in real time. To that end, we’ve developed story groups that allow us to link topics together into clusters and threads that will give readers the ability to jump into the middle of an ongoing conversation and be instantly provided with the context that they need to catch up and forge ahead.There are more features on the site that help readers to parse things like funding news and to see Disrupt and Battlefield event experiences that morph depending on whether you’re visiting them before, during or after they take place. The project has gone more than skin deep, we’ve also modernized the technologies we’re using, capitalizing on advancements in the WordPress platform to use it as a headless CMS powering an all-new React frontend. We’re excited about the potential for innovation that this creates. Our head of product, Nicole Wilke, will speak more to this later.For now, the lucky 10 percent gets a preview and we’ll roll it out to more of you over time. It’s a work in progress, just like tech, but we hope you like where it’s going.","New, new, new TechCrunch"
9897,3880656,2018-02-27 00:00:00,"The Feds Can Now (Probably) Unlock Every iPhone Model In ExistenceDespite Apple's improvements to iPhone security, the cops now have a way into any device, thanks to Israeli company Cellebrite. (Photo by Hitoshi Yamada/NurPhoto via Getty Images)In what appears to be a major breakthrough for law enforcement, and a possible privacy problem for Apple customers, a major U.S. government contractor claims to have found a way to unlock pretty much every iPhone on the market.Cellebrite, a Petah Tikva, Israel-based vendor that's become the U.S. government's company of choice when it comes to unlocking mobile devices, is this month telling customers its engineers currently have the ability to get around the security of devices running iOS 11. That includes the iPhone X, a model that Forbes has learned was successfully raided for data by the Department for Homeland Security back in November 2017, most likely with Cellebrite technology.The Israeli firm, a subsidiary of Japan's Sun Corporation, hasn't made any major public announcement about its new iOS capabilities. But Forbes was told by sources (who asked to remain anonymous as they weren't authorized to talk on the matter) that in the last few months the company has developed undisclosed techniques to get into iOS 11 and is advertising them to law enforcement and private forensics folk across the globe. Indeed, the company's literature for its Advanced Unlocking and Extraction Services offering now notes the company can break the security of ""Apple iOS devices and operating systems, including iPhone, iPad, iPad mini, iPad Pro and iPod touch, running iOS 5 to iOS 11."" Separately, a source in the police forensics community told Forbes he'd been told by Cellebrite it could unlock the iPhone 8. He believed the same was most probably true for the iPhone X, as security across both of Apple's newest devices worked in much the same way.Though it's always wise to take the claims of profit-focused vendors with a pinch of salt, whatever flaws Cellebrite found in Apple's tech in the last half year, they're likely significant; just last year, the company warned about a decline in its ability to break into iPhones.To take advantage of the Cellebrite service, which ""can determine or disable the PIN, pattern, password screen locks or passcodes on the latest Apple iOS and Google Android devices,"" cops have to send the device to Cellebrite first. In its labs, the company then uses whatever secret exploits it has to crack the lock and either hands it back to investigators so they can take data from the device, or Cellebrite can do that for them. As Forbes previously detailed, this can be relatively inexpensive, costing as little as $1,500 per unlock. Given there's a $1 million price tag for a single iPhone vulnerability, that's cheap.Cellebrite could put its latest iPhone unlocking tech into the software it sells to customers. But that would mean Apple could test the tool and potentially figure out a way to stop it working, explained Don Vilfer, a partner at private forensics firm VAND Group, who welcomed the new services. Vilfer said his company has already had some success with the iOS 11 service, in a case where a client's employee wouldn't give over their passcode for their work iPhone, though he recalled it was an iPhone 6 model, not one of the most recent devices.Neither Apple nor Cellebrite had provided comment at the time of publication.iPhone X examinedIt also appears the feds have already tried out Cellebrite tech on the most recent Apple handset, the iPhone X. That's according to a warrant unearthed by Forbes in Michigan, marking the first known government inspection of the bleeding edge smartphone in a criminal investigation. The warrant detailed a probe into Abdulmajid Saidi, a suspect in an arms trafficking case, whose iPhone X was taken from him as he was about to leave America for Beirut, Lebanon, on November 20. The device was sent to a Cellebrite specialist at the DHS Homeland Security Investigations Grand Rapids labs and the data extracted on December 5. (Saidi's case is due to go to trial on July 31. His legal team didn't respond to requests for comment).ForbesThe government turned to a Cellebrite-trained expert to get data from an iPhone X.But the ability to crack open almost any iDevice on the market is a significant moment for law enforcement, not just in America, but across the globe. Police have been left scrambling for ways into iPhones ever since Apple started improving its security with each new release. Layers of encryption have become increasingly difficult to penetrate, as highlighted in the now-infamous tussle between the Cupertino giant and the FBI in San Bernardino, where the feds wanted an unwilling Apple to help them access the iPhone 5C of San Bernardino murderer Syed Rizwan Farook.'Hoarding iPhone vulnerabilities'Cellebrite has no doubt benefited from the cat and mouse game being played out by the government and Silicon Valley giants. Many U.S. policing and intelligence agencies, including the FBI and the Secret Service, are customers. As detailed by Forbes last year, the company scored record contracts with a variety of agencies, most notably the Immigration and Customs Enforcement branch of the DHS, which spent $2 million on one deal alone. Customs and Border Protection is also a client.At the time of the ICE contract, civil rights activists raised concerns over the use of such powerful technology to search Americans' devices. Speaking about the latest developments, Electronic Frontier Foundation senior staff attorney Adam Schwartz said the way in which the government did business with the likes of Cellebrite was ""of great concern."" He said it was clear that Cellebrite was hoarding vulnerabilities rather than disclosing them to vendors like Apple, which would lead to patches and better security for the general public. ""All of us who're walking around with this vulnerability are in danger,"" he added.""When it comes to the international border, as the EFF has argued in court and in Congress, the government really needs to get a warrant before it searches our phones. It's all the more true when we see the ever expanding power of governments to get into those phones.""",The Feds Can Now (Probably) Unlock Every iPhone Model In Existence
9898,3880893,2018-02-28 06:05:18,"Amazon acquires Ring’s home security business for $1 billionThat will see the ecommerce giant push further into the home security business: it previously acquired Blink, which makes similar products. Plus, it already offers a Cloud Cam that’s designed for use inside your house, and to allow couriers into your home to drop off parcels as part of its Key program.GeekWire notes that while neither firm has shared details on the way forward, it’s expected that the Ring brand will continue to exist, as other Amazon-owned labels like Zappos and Twitch have, post-acquisition.Ring devices already play nice with Amazon’s Alexa assistant, and it’s likely that the companies will work on further integrations between them. With its two recent acquisitions, Amazon will have a fighting chance at taking on Google, which also has an assistant and a smart home device brand in the form of Nest.Fun fact: Ring was previously called DoorBot, and CEO James Siminoff appeared on the TV show Shark Tank in an effort to raise money for it – and failed. His tenacity, and the decision to stick with the product he originally built, has paid off in a big way.TNW’s 2018 conference is just a few months away, and we want to talk about tech, security, and the government. Find out all about our trackshere.",Amazon acquires Ring's home security business for $1 billion
9899,3880894,2018-02-28 02:42:54,"About TNWTNW SitesAspiring YouTube star? It’s time for a reality check.There’s something to be said for those with the courage to follow their dreams. But if your dream is to make it big on YouTube you should probably take a seat. This is going to hurt a little.Choosing a far-fetched career path isn’t new. From a young age, many of us aspired to be actors, pop stars, or professional athletes. This generation, though, has a new plan: taking over the internet.In a British survey last year, one in three children professed a desire to be full-time YouTubers. That is to say, the children wanted to be the next Jake Paul, PewDiePie, or Zoella — internet celebrities making a hefty income by posting a steady stream of video content. And whether you love them or hate them, there’s no arguing YouTubers have become role models for the next generation.The average YouTuber, however, is no star. In fact, 96.5 percent of all vloggers never make enough to crack the poverty line, according to analysis by Mathias Bärtl, a professor at Offenburg University of Applied Sciences in Germany. Even breaking into the top three percent of the platform’s most-viewed channels could bring in as little as $16,800 a year. And even to reach the rarified air of Bärtl’s top three percent, you’d have to bring in around 1.4 million views per month.If you think it’s looking grim, it’s about to get worse.The average ad rates earn vloggers abut $1 per 1,000 views. But that’s just the rule of thumb. Some earn much less, as little a third of that. according to Harry Hugo of The Goat Agency, a Lond0n-based influencer marketing firm. Hugo told Bloomberg:I’ve seen as low as 35 cents per 1,000 views and work with some YouTubers who can earn $5 per 1,000.It’s a rosy outlook assuming you can be part of the latter group. That’s typically reserved for the top one percent of all YouTube channels, those that snag between 2.2 million to 42.1 million views each month. For those just starting out, or vlogging in a category without a lot of advertiser interest, it’s almost certain to be the latter, 35 cents.In 2006, the top three percent of YouTubers accounted for 63 percent of all views, according to Bärtl. In 2016, the platform’s top channels received nine of every 10 views.Worse, YouTube itself is making it more difficult to earn revenue for newer accounts. Announced last month, the platform now requires viewers to soak in 4,000 hours of an account’s content and earn 1,000 subscribers before they’re eligible to collect any advertising revenue.If there’s anything positive to be gleaned from Bärtl’s research it’s this: gaming YouTubers are fourteen times as likely to earn a substantial living as those in other categories. And the research itself overlooks significant benefits like outside sponsorships, movie deals, or potential job offers from any number of companies looking for video-savvy young influencers.It also overlooks revenue earned from other means, such as subscriptions, product sales, or revenue generated on sites that aren’t YouTube, but rely on the platform as a means of driving traffic or building interest.There’s hope, but I wouldn’t suggest quitting your day job. And if you need inspiration, remember that there’s a six-year-old that made $11 million last year by reviewing toys. It’s possible, just not likely.",Aspiring YouTube star? It's time for a reality check.
9900,3886568,2018-02-28 08:49:53,"Amazon’s Prime Music streaming service lands in IndiaThe service is part of Amazon’s Prime subscription, which grants access to fast and often free shipping, as well as Prime Video, for Rs. 999 ($15) a year. That’s a sweet deal, considering that rivals like Saavn, Gaana, Apple Music, and Google Play Music cost between Rs. 1,200 ($15) and Rs. 1,440 ($22) annually.The company hasn’t said exactly how large its music catalog is in India; while it offers over 40 million tracks to subscribers of its Prime Unlimited service in the US, Prime Music unlocks only about two million songs in the various markets where’s it’s available.That being said, Amazon does offer local music here in India; you can choose from film soundtracks and regional artists in Hindi, English, Punjabi, Tamil, Telugu, Kannada, Malayalam, Marathi, Bengali, Bhojpuri, Gujarati, and Rajasthani.That could certainly help keep Prime customers in the country hooked to the subscription plan, and to Amazon’s ecosystem of services and products. It recently launched its Echo speakers in India, and its Alexa assistant now boasts more than 12,000 skills there.Alexa is also built into the Prime Music app, so you can use your voice to search for music and stations, as well as surface songs and albums you’ve previously played through the service’s beta with your Echo hardware.In my brief test, Prime Music feels rather basic, and seems to be geared towards people who listen to whatever’s on the radio rather than to folks who enjoy curating and discovering new tunes.The home screen is littered with recommendations of the most popular tracks from around the world (yep, some of you are still into Ed Sheeran’s ‘Shape of You’ from last January); songs from the Top Rock station date back as far as the early 2000s (Staind, anyone?), and the ‘Music for Every Mood’ section features playlists with regional-language music, even though I didn’t select any of those languages during the setup process. There’s also no way to generate a station or playlist with tracks similar to stuff from a song, artist, or album you like.Ultimately, Prime Music will likely find fans among casual listeners here in India, and give them another reason to consider renewing their Prime subscription. But in its current state, it’s hard to see it competing directly with other established streaming services.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our tracks, including our Music Summit that focuses on innovations in audio,here.",Amazon's Prime Music streaming service lands in India
9901,3886569,2018-02-28 06:57:41,"TNW SitesGoogle’s Flutter framework for building iOS and Android apps launches in betaAfter being announced last year, Google took to the stage at MWC 2018 to announce a beta release of Flutter, its free and open-source framework for building iOS and Android apps with a unified codebase.The SDK lets devs code their apps in the Dart programming language, and packages them along with a rendering engine, as well as the native code needed to run those apps on Android or iOS. It’s designed to offer top-shelf performance, while also making it easy to create interfaces suitable for both platforms, thanks to included UI widgets for each.Flutter plays nice with a range of IDEs including Android Studio, Visual Studio Code, and XCode, as well as some 1,000 packages like Firebase and Facebook Connect. It also offers Hot Reload, which lets you see changes in your app as you make them, without losing your current state while testing.Since it’s been in alpha from last year, Flutter has already been used to build several apps, including one for Google’s AdWords platform.Apps built using FlutterAs Ars Technica notes, Flutter may be about more than just Google trying to make mobile developers’ lives easier: the company has been working on an open-source mobile OS called Fuchsia for a couple of years now, and it’s been written in Flutter using Dart. As such, software built for Android and iOS in Flutter should also work on Fuchsia, should it ever see the light of day.It’ll be interesting to see if Flutter takes off and overshadows React Native. Google will likely try to draw more devs in at this year’s I/O conference in May, and hopefully reveal a few more advancements at the time as well. Exciting times, indeed.",Google's Flutter framework for building iOS and Android apps launches in beta
9902,3889349,2018-02-23 07:09:29,"African elephants are migrating to safety—and telling each other how to get thereWritten byShareWritten byThe Chobe National Park in northern Botswana is a 11,700 sq km (4,517 sq mile) sanctuary for animals. Families of endangered African elephants wander freely through the park, which is bordered by Namibia in the north and Zimbabwe in the east. The animals visit watering holes, feed on the lush vegetation, and play with the youngest of their groups.African elephants in nearby countries such as Zimbabwe, Zambia, and Namibia are migrating to parks like Chobe, where strict anti-poaching policies allow them to thrive. Botswana is now home to roughly 130,000 elephants—a third of Africa’s entire elephant population. This is an increase of over 30,000 elephants in Botswana since 1995 against a backdrop of declining numbers across the continent.In 2016 the Great Elephant Census published the first-ever continent-wide, standardized survey of African savannah elephants. The census found that savannah elephant populations in Africa fell by an estimated 30%—or 144,000 animals in total—between 2007 and 2014. Today there are only around 350,000 left on the entire continent. The survey authors blamed poaching as the primary reason for the decline.The 2016 survey did not include forest elephants, whose populations are difficult to discern because they live in dense, forested areas and would require labor-intensive ground counts to survey. Estimates suggest there are only 100,000 left today. In total, it’s believed that there were over 1 million elephants in Africa in the 1970s, and possibly more than 2 million at the beginning of the 20th century.Botswana, however, has become a safe haven for the world’s largest land mammals. “Elephants are using well-known migratory routes into Botswana to flee threats from neighboring countries,” says Mark Hiley, cofounder of the UK-based nonprofit National Park Rescue. “The systematic movement of elephants into Botswana is linked to their survival.”Elephants like these in the Chobe National Park are developing survival mechanisms in response threats. (Kanika Saigal)Communicating for survivalResearchers believe this migration is just one survival mechanism elephants have developed in response to poaching, conflict, urbanization, agriculture, and other pressures in Africa.In 2016, one elephant made a treacherous 209 km (130 mile) journey over three weeks from the relative safety of Kenya to conflict-ridden Somalia, all under the cloak of darkness. Morgan, as the researchers called him, remained in Somalia for just a day and a half before turning back.“We don’t know the precise reason for his migration into Somalia,” says Iain Douglas-Hamilton, founder of Save the Elephants, a UK charity headquartered in Nairobi that conducts research on elephant behavior and ecology, “but we suspect it was to mate.”“Moving by night was an extreme form of survival in a region where elephants are under threat from poaching,” adds Douglas-Hamilton. “He was the first elephant on record to visit in Somalia in 20 years.”Elephants also have developed sophisticated gestures, sounds, infrasound, and even chemical secretions to relay messages to one another for survival purposes. “Through various means, elephants can suggest that the group moves on, that they sense danger, or that they are in distress,” says Douglas-Hamilton.Elephants also have developed sophisticated gestures, sounds, infrasound, and even chemical secretions to relay messages to one another for survival purposes.Indeed, it’s not just where elephants are going that’s of interest to researchers, but when, and how they’re communicating about it within groups, between herds, and across generations. In 1977, Mozambique gained independence from Portuguese colonial rule; two years later the country collapsed into civil war. By the time the war ended in 1992, over a million people had been killed and 5 million others had been displaced. During the conflict, soldiers ate elephant meat and traded the animals’ ivory for weapons and ammunition, says Joyce Poole, cofounder and scientific director of ElephantVoices, an organization that studies the social behavior and communication of African elephants. Mozambique’s elephant population was completely decimated: 90% of the 4,000 elephants that lived in the greater Gorongosa area in central Mozambique, where much of Poole’s research has focused, had been killed.“Twenty-five years later and many elephants in Gorongosa are still scared of humans,” says Poole. “When family groups of elephants come into contact with people, older, experienced females typically communicate to their family that there is a threat and will charge or flee depending on the situation. Calves and juveniles learn how to react from their mothers.”Researchers have also found that African elephants can differentiate between human languages in order to identify potential threats. For instance, elephants in Kenya’s Amboseli National Park, near the Tanzanian border in the south, have shown the ability to distinguish between the voices and scents of potentially threatening spear-carrying Maasai warriors with those of less threatening farming tribes.To a deeper understandingResearch on elephant populations increasingly illustrates the complex ways they communicate and, consequently, their intelligence. But we have only scratched the surface. Although researchers have identified a number of general alarm calls used by elephants, they haven’t distinguished calls or gestures that specifically warn of poaching. The type of work required to figure out the nuances of elephant communication would force researchers to get extremely close to the elephants, Douglas-Hamilton says, which could put the humans at risk.Research on elephant populations increasingly illustrates the complex ways they communicate and, consequently, their intelligence.Studying elephants in general poses a number of other challenges. Habitat, past interaction with humans, and even differences in elephants’ personalities all affect the way research can be carried out. Poole describes working in Amboseli, Kenya, “within a few meters of calm elephants, which offered great visibility and good quality recordings and a deep understanding of how elephants behave,” she says. “In Gorongosa, Mozambique, the habitat is thick and the elephants can be both fearful of people and aggressive toward them, which makes studying them there much harder.”Researchers want to maintain harmony between the elephants and human communities living near them, to create a ready environment to study the animals. “Issues with crop raiding and poaching for example mean that people near elephant populations can get very frustrated with proximity with the animals,” explains Lucy Bates, a research fellow in the school of Psychology at the University of Sussex, UK, who has spent a number of years studying African elephants. “This can in turn create political issues for the researchers and block important studies.”Elephant researchers and advocates hope that as technology and our understanding of these pachyderms develops, challenges will dissipate. “As time goes on and scientists gain access to more sophisticated technology, we are likely to find that the calls elephants produce hold very specific meanings,” says Poole. “Humans have represented a primary threat to elephants for many thousands of years. I feel certain that elephants have a specific call to refer to our species—and likely several different calls to communicate to one another whether the specific humans represent a threat or not.”In the meantime, current observations still offer incredible insight: In times of heavy poaching, elephants appear to gravitate towards safe havens and hunker down. And even after the threat is gone, it can still take a generation or two for elephants to relax and move beyond the boundaries of these safe spaces.",African elephants are migrating to safety—and telling each other how to get there
9903,3889355,2018-02-28 11:04:42,"PwC: Weak pound hurt MaplinThey also say that Maplin was hurt by the fall in sterling – which slumped against rival currencies after the Brexit vote in June 2016.Zelf Hussain, joint administrator and PwC partner, says:“The challenging conditions in the UK retail sector are well documented. Like many other retailers, Maplin has been hit hard by a slowdown in consumer spending and more expensive imports as the pound has weakened.“Our initial focus as administrators will be to engage with parties who may be interested in acquiring all or part of the company. We will continue to trade the business as normal whilst a buyer is sought.“Staff have been paid their February wages and will continue to be paid for future work while the company is in administration.”Despite losing significant market share to competitors such as Amazon and Argos, Toys R Us failed to implement an effective E-commerce model with expedited shipping options and also continued to rely on its large warehouses. Opened in the 1980s and 90s, these have proven highly costly to run, with difficulties around managing stock levels, and have been outperformed by its new smaller stores.Toys R Us’ relationships with key suppliers also hit the headlines over the last year, with the chain demanding long payment terms and exclusivity on certain ranges despite no longer being the key player in the market.So toys will sell increasingly on hype - ads, or stuff like fidget spinners that go viral - not on picking stuff up & seeing how you could play with it. Less chance to see that plastic tat looks less appealing in real life than on telly.Poncey boutique toy shops selling adorable overpriced wooden toys that parents like & children don't will survive. Ditto The Christmas Circling Of Things In The Argos Catalogue. But a consumer skill gets lost I think. (end of boring thread)City analyst Neil Wilson of ETX Capital says retailers are suffering from “structural game-changers and a softening in consumer confidence”, which can be fatal for firms who haven’t adjusted.“Toys R Us has officially entered administration, with stores to remain open for the time being. The company failed to adapt and was stuck with a large portfolio of large warehouse shops. No experience for customers, no compelling online presence. A £15m VAT bill did for Toys R Us in the end after another poor Christmas, but it was a systemic failure to move with the changes in the retail sector that really did for it.Maplin has also gone into administration in what’s now a pretty torrid day for UK retail. In both cases the Amazon effect is all too clear to see, but there is more to it than that – there are retailers out there who are adapting and prospering.He also points out that Mothercare’s shares have fallen today (currently down 7%, having been 12% lower earlier).Labour: Devastating news for UK workersRebecca Long-Bailey, Labour’s Shadow Business Secretary, is urging the government to help Maplin and Toys R Us’s employees:“It’s devastating that over 5,500 High Street jobs risk being lost. This latest shock in the retail sector continues a worrying trend for our shopping streets and centres.“The Government must urgently meet with both the unions and the companies to ensure that these jobs are safeguarded.“Workers are suffering stress and anxiety not knowing what the future holds for them. In the event of job losses, the Government must act quickly to ensure all workers receive swift redundancy payments and are properly supported.“The Government must also urgently address problems across the retail sector.”Maplin falls into administrationBREAKING: Electrical goods chain Maplin has also fallen into administration, in an increasingly dark morning for UK retail.This puts around 2,500 jobs at risk.Maplin, which has over 200 stores in the UK, was forced into administration after failing to find new funding -- less than an hour after Toys R US suffered the same fate.Graham Harris, CEO of Maplin, has blamed the slump in the pound since the EU referendum, saying:“I can confirm this morning that it has not been possible to secure a solvent sale of the business and as a result we now have no alternative but to enter into an administration process. During this process Maplin will continue to trade and remains open for business.The business has worked hard over recent months to mitigate a combination of impacts from sterling devaluation post Brexit, a weak consumer environment and the withdrawal of credit insurance. This necessitated an intensive search for new capital that in current market conditions has proved impossible to raise. These macro factors have been the principal challenge not the Maplin brand or its market differentiation.We believe passionately that Maplin has a place on the high street, and that our trust, credibility and expertise meets a customer need that is not supported elsewhere.We will now work tirelessly alongside Zelf Hussain, Toby Underwood and Ian Green, from PWC, who have been appointed as the as Joint Administrators of Maplin Electronics Limited, to achieve the best possible outcome for all of our colleagues and stakeholders.”“We have informed employees about the process this morning and will continue to keep them updated on developments. We are grateful for the commitment and hard work of employees as the business continues to trade.”“It’s undoubtedly distressing news for employees of Toys ‘R’ Us especially because their fates have been uncertain for so long. Unfortunately they aren’t the only retailer that has been struggling.“Anyone who is worried about the future of their job should prepare themselves now. Check what redundancy rights you have and dig out any income or mortgage protection policies you hold just in case.“Toys ‘R’ Us have reported they will continue trading for now, however it’s currently unknown whether they will still accept gift vouchers and offer refunds. If the administrators decide to suspend gift vouchers and refunds and you paid in cash you could be in for a lengthy battle to get your money back.”Here’s her guide explaining how credit card spending is protected in the UK:Richard Lim, chief executive of Retail Economics, says Toys R Us is another victim of the “seismic structural changes” hitting the UK high streets (such as rising operating costs and the threat from online retailers).He adds:“Put simply, the retailer was too slow to embrace omnichannel, were burdened with too many stores and failed to deliver a retail ‘experience’ good enough to stand out from their competitors.”Toys R Us’s administrators haven’t given up hopes of finding a buyer, for at least some of its 100+ outlets.In the meantime, existing stock will probably be offered at a sharp discount:Joanna Partridge (@JoannaPartridge)Toys R Us has gone into administration after it wasn't possible to sell it as a going concern. Moorfields Advisory Ltd have been appointed administrators. All 100 UK stores will continue trading for now & lots of the stock will be discounted.From Toys R Us administrators Moorfields: ""We will make every effort to sell all or part of the business. The newer, smaller, more interactive stores in the portfolio have been out-performing the warehouse-style stores""Toys R Us administration: Instant reactionToys R Us details: All stores will trade until further notice, customers should use gift cards/vouchers asap, administrators will run orderly wind down of UK business with closing down sales. @BBCNews@BBCBusinesspic.twitter.com/ozYxHLSkfcExecutives had been battling to raise cash to pay a tax liability that fell due this week, but the efforts collapsed after a number of private equity funds and restructuring specialists walked away.Insolvency specialists at Moorfields have begun the process of closing its British operations, which employ nearly 3,000 people and has an estimated funding shortfall of at least £25m in its pension scheme.Retail analyst Fiona Paton points out that several other retailers have gone under recently:Fiona Paton (@fiona_GDretail).@ToysRUs has entered administration after months of battling to stay afloat. This follows Warren Evans, East and Multiyork - demonstrating the tough UK retail climate in 2018 and the need to stay relevant against online. https://t.co/hG9ih9B0W5",Thousands of jobs at risk as Maplin and Toys R Us fall into administration - business live
9904,3891993,2018-02-21 12:00:00,"Why we made this changeVisitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access.There’s never been data available on as many people’s genes as there is today. And that wealth of information is allowing researchers to guess at any person’s chance of getting common diseases like diabetes, arthritis, clogged arteries, and depression.Doctors already test for rare, deadly mutations in individual genes. Think of the BRCA breast cancer gene. Or the one-letter mutation that causes sickle-cell anemia. But such one-to-one connections between a mutation and a disease—“the gene for X”—aren’t seen in most common ailments. Instead, these have complex causes, which until recently have remained elusive.Here’s the breakthrough: a new way to guess your chance of serious disease from your DNA. Any drawbacks? You bet. The technology could lead to a society where people get genetic grades at birth.The day I visited him, Khera was constructing what is called a polygenic score—“poly” because his calculations involve thousands of genes, not just one. This particular score predicted a person’s chance of developing atrial fibrillation, or irregular heartbeat. It’s a common disorder but is often diagnosed only after someone has been rushed to the ER with a stroke.Khera pointed to his screen. There, seven-digit numbers, each representing an anonymous DNA donor, appeared alongside their scores. The outliers had a risk four times the average.Khera, who works in the laboratory of heart doctor and gene hunter Sekar Kathiresan at the Broad Institute, in Cambridge, Massachusetts, says the new scores can now identify as much risk for disease as the rare genetic flaws that have preoccupied physicians until now.Cardiologist Amit Khera is part of a team at the Broad Institute predicting disease from DNA.SIMON SIMARD“Where I see this going is that at a young age you’ll basically get a report card,” says Khera. “And it will say for these 10 diseases, here’s your score. You are in the 90th percentile for heart disease, 50th for breast cancer, and the lowest 10 percent for diabetes.”Such comprehensive report cards aren’t being given out yet, but the science to create them is here. Delving into giant databases like the UK Biobank, which collects the DNA and holds the medical records of some 500,000 Britons, geneticists are peering into the lives of more people and extracting correlations between their genomes and their diseases, personalities, even habits. The latest gene hunt, for the causes of insomnia, involved a record 1,310,010 people.The sheer quantity of material is what allows scientists like Khera to see how complex patterns of genetic variants are tied to many diseases and traits. Such patterns were hidden in earlier, more limited studies, but now the search for ever smaller signals in ever bigger data is paying off. Give Khera the simplest readout of your genome—the kind created with a $100 DNA-reading chip the size of a theater ticket—and he can add up your vulnerabilities and strengths just as one would a tally in a ledger.Such predictions, at first hit-or-miss, are becoming more accurate. One test described last year can guess a person’s height to within four centimeters, on the basis of 20,000 distinct DNA letters in a genome. As the prediction technology improves, a flood of tests is expected to reach the market. Doctors in California are testing an iPhone app that, if you upload your genetic data, foretells your risk of coronary artery disease. A commercial test launched in September, by Myriad Genetics, estimates the breast cancer chances of any woman of European background, not only the few who have inherited broken versions of the BRCA gene. Sharon Briggs, a senior scientist at Helix, which operates an online store for DNA tests, says most of these products will use risk scores within three years.“It’s not that the scores are new,” says Briggs. “It’s that they’re getting much better. There’s more data.”Tiny InfluencesWhen they launched the first modern gene searches a decade ago, following the completion of the Human Genome Project, medical researchers still hoped that a few major genetic culprits would explain common diseases like diabetes. “I expect there are about 12 genes involved [in diabetes], and that all of them will be discovered in the next two years,” Francis Collins, now the head of the US National Institutes of Health and one of the leading players in sequencing the human genome, confidently declared in 2006.If that had turned out to be true, the small list of genes would have given drug designers clear, tangible targets. That would easily have justified the whole enterprise, financed with hundreds of millions of US tax dollars. In the case of a few diseases, like macular degeneration, the searches paid off. Mostly, though, geneticists drew in empty nets. By 2009, Collins and others had begun to talk glumly about “the missing heritability.”Where were the disease-causing genes? Everywhere, it turns out. And by 2014, genetic studies were finally big enough to prove it. As the number of people with diabetes who enrolled in the gene search studies rose from 661 to 10,128 to 81,412, the “hits” began rolling in. Instead of 12 genes, we now know, type 2 diabetes is influenced by at least 400 locations in our DNA, and probably many more—each with only a tiny, hard-to-detect effect.To scientists seeking the ultimate cause of common diseases, that’s a huge disappointment. If the causes of diabetes, depression, or schizophrenia are sprinkled around the genome like so much powdered sugar, it means we’re far from understanding or curing them. “No one wanted that to be the answer,” says Mark Daly, a geneticist at the Broad Institute. “But it is what it is.”While the scattershot nature of inheritance may make disease hard to comprehend, though, the same data is making it much easier to predict. To create their models, Khera and Kathiresan use 6.6 million positions in a person’s genome. Each position is a single DNA letter. It could be A for you and G for me. From big genetic studies, Khera can now look up how much more likely a person with a G in that position is to have a heart attack. Maybe it raises the chances by 0.1 percent. That’s a negligible amount. Maybe a G in another position reduces the risk by 0.2 percent. But if you add up all the tiny genetic influences, the effect can become substantial.When they built a predictor for coronary heart disease, for instance, Kathiresan’s team discovered that the people it predicted to have the very highest risk, the top 2.5 percent, had four times the average chance of developing clogged arteries. That’s about equal to the risk of clogged arteries caused by familial hypercholesterolemia, a condition marked by sky-high cholesterol levels and caused by a single critical gene. If doctors worry about that—which they do—why not also pay attention to the high end of genomic risk scores?“That’s the thing that convinced me,” says Kathiresan. And the number of people whose genome predictions raise a red alert will also be much larger. Familial cholesterolemia affects only about one in 250 people. Genome scores would identify about eight times as many people at high risk for heart disease, he believes.What he’s not yet sure about, Kathiresan says, is how to get the new risk information into people’s hands. He has considered launching an app or selling the statistical model to a diagnostics company. “Everyone wants to get their score. Everyone is asking where is the product for heart disease,” he says. “I tell them, we are working on it.”Heart disease is, in some ways, a best-case scenario for using risk scores. That’s because you can change your real-life risk—say, by going on a diet or taking a cholesterol-lowering statin pill. What’s more, probabilities are already a big part of heart medicine. Khera, who dons a white coat once a week to treat patients at Massachusetts General Hospital, uses a combination of a person’s age, weight, cholesterol levels, and habits like smoking to guess the chance of a heart attack in the next 10 years. Now genetic scores could be added to those models, making them more accurate.Genetic studies are becoming more powerful as they involve more people. Here, a robot on rails searches among frozen DNA samples donated by 500,000 British volunteers as part of the UK Biobank.UK BiobankWhat’s powerful about DNA predictions is that they are measurable at any time of life, unlike most risk factors. “If you line up a bunch of 18-year-olds, none of them have high cholesterol, none of them have diabetes. It’s a zero in all the columns, and you can’t stratify them by who is most at risk,” says Khera. “But with a $100 test we can get stratification at least as good as when someone is 50, and for a lot of diseases.”Dangerous knowledgeDrug companies have started to notice. Last year Anders Dale, a brain researcher at the University of California, San Diego, announced his intention to market a risk calculator for Alzheimer’s disease. It will guess whether a person will develop the disease and, if so, at what age.The service won’t launch until this summer, but Dale says drug companies immediately got in touch. Now he is helping three of them test the DNA of people in clinical trials for Alzheimer’s drugs (he declined to name them). Despite the billions spent developing such drugs, every one tried so far has flopped. The problem is that when no one knows who will get the disease, it’s difficult to know whether a preventive drug is working. If companies could test the drugs only on people with a high risk of Alzheimer’s, it would be much easier. It’s possible future drugs will be labeled “Recommended for those with polygenic scores 90 and above.”Dale is working with commercial partners to put his Alzheimer’s predictor online and charge as little as $99 to anyone who wants to use it. More than ten million people already have their DNA data because they signed up for 23andMe or Ancestry.com to research their family trees. Dale says they will be able to upload the data with a click and receive a report. I asked him why people would want to know ahead of time about a disease that’s currently untreatable. “They may want to make plans,” he said.DNA-based IQ tests are likely to become available in coming years. Critics fear “some truly dreadful social policies” could result.Other doctors believe risk scores will give people the push they need to think harder about their well-being. “I love the idea of polygenic risk scores because the future is health, not medicine,” says Steven Tucker, a physician who practices in Singapore. He likes his patients to use wearable devices and trackers, and risk scores could be combined with those. Someone at high risk for atrial fibrillation, for instance, might wear a smart watch with a heart monitor built into it. “My patients want to manage the future,” says Tucker. “If you can define it more accurately, there is a better chance you can do something about it.”Even so, the value of the new genomic future-gazing is hotly disputed. That’s because the scores are not individual certainties; they are merely rough probabilities derived from large populations. Of people given high scores by Khera’s atrial fibrillation predictor, for example, only a small minority, 7 percent, would actually develop the condition by age 55.Dueling neural networks. Artificial embryos. AI in the cloud. Welcome to our annual list of the 10 technology advances we think will shape the way we work and live now and for years to come.This uncertainty matters because if people are given risk scores, they’ll base decisions on them. Last fall, Myriad Genetics became the first large diagnostics company to introduce a polygenic risk test to the US market. Called riskScore, it measures 81 variants to estimate a woman’s chance of breast cancer. Women with a high score might undergo extra mammograms; those at low risk might skip them. What no one yet knows is whether those decisions will lead to fewer cancer deaths. Finding out will require expensive long-term studies that Myriad, which is selling the test, hasn’t yet done.One physician who finds all this troubling is Patrick Sullivan at the University of North Carolina, Chapel Hill, where he leads the Psychiatric Genomics Consortium. The group has DNA data from more than 900,000 people with confirmed mental illness, including more than 60,000 with schizophrenia. This disease is known to be highly influenced by genes. If one identical twin develops schizophrenia, for example, there is a 50 percent chance the other one will too.But Sullivan says it would be reckless to tell apparently healthy people whether their DNA score for schizophrenia is high or low. Just think of those twins, he says: they have the same DNA and the same score, yet it is even odds a schizophrenia prediction would be wrong. Giving such a flawed forecast to someone “is a terrible idea,” says Sullivan. “What you want it to do is distinguish who has it and who doesn’t, and we aren’t there yet.”A DNA IQ testIn addition to predicting disease, geneticists can build models to predict any human trait that can be measured, including behaviors. Is this person destined for a life of crime and recidivism? Will that one be neurotic, depressed, or smarter than average?The scoring technology, scientists say, will soon shed uncomfortable light on such questions. In January, two leading psychologists argued that direct-to-consumer DNA IQ tests will soon become “routinely available” and will predict children’s ability “to learn, reason, and solve problems.” They believe parents will test toddlers and use the results to make school plans.To some, using foggy genetic horoscopes to decide who goes to college and who ends up in trade school sounds like an extraordinarily bad idea. On his blog Gloomy Prospect, Eric Turkheimer, a prominent psychologist at the University of Virginia, says the danger is that the scores will be overinterpreted to “recommend some truly dreadful social policies.” That, he thinks, would be “the worst possible kind of biologically determinist discrimination.” To Turkheimer, polygenic scores are “less than meets the eye” and about as fair as “predicting your IQ from a cousin you haven’t met.”Such views aren’t stopping the rapid pace of genetic exploration. Until last year, no gene variant had ever been tied directly to IQ test results. Since then, studies involving more than 300,000 people’s DNA have linked 206 variants to intelligence. It means genetic scores can now account for 10 percent of a person’s performance on an IQ test. That could reach 25 percent within a few years, as more data accumulates. One US company, Genomic Prediction, even says it wants to test IVF embryos for intelligence, so parents can discard those expected to be mentally unfit.Dystopia, dubious medicine, or a breakthrough in prevention? Genomic prediction may well be all three. What is clear is that, with the data needed to create predictors becoming freely available online, 2018 will be a breakout year for DNA fortune-telling.Tech Obsessive?Become an Insider to get the story behind the story — and before anyone else.TaggedI am the senior editor for biomedicine for MIT Technology Review. I look for stories about how technology is changing medicine and biomedical research. Before joining MIT Technology Review in July 2011, I lived in São Paulo, Brazil,… More where I wrote about science, technology, and politics in Latin America for Science and other publications. From 2000 to 2009, I was the science reporter at the Wall Street Journal and later a foreign correspondent.You've read of three free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Subscribe now for unlimited online access. This is your last free article this month. Subscribe now for unlimited online access. You've read all your free articles this month. Subscribe now for unlimited online access. You've read of three free articles this month. Log in for more, or subscribe now for unlimited online access. Log in for two more free articles, or subscribe now for unlimited online access.",Forecasts of genetic fate just got a lot more accurate
9905,3895021,2018-02-28 17:48:35,"MNIST Handwritten digits classification using Keras (part – 1)Hello everyone, this is going to be part one of the two-part tutorial series on how to deploy Keras model to production. In this part, we are going to discuss how to classify MNIST Handwritten digits using Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. However, for our purpose, we will be using tensorflow backend on python 3.6. It was developed with a focus on enabling fast experimentation. I have divided the tutorial series into two parts:MNIST Handwritten digits classification using KerasDeploying Keras model to production using flaskBy the end of the tutorial series, you will be able to build and deploy your very own handwritten digit classifier that looks something like this:Without further ado, let’s get started. First, let’s create a virtual environment and install all the necessary dependencies.Create Virtualenv and install necessary dependenciesWhy python virtual environment is needed has already been discussed in this another post here so I’m not going to do that here. Let’s get started with how we can set up virtualenv and install necessary dependencies in python 3.6. The easiest way is installing through python pip package. To install virtualenv through pip, simply type:pip3 install --upgrade virtualenvOnce the virtualenv is installed, you can create separate virtual environments for each of your projects. Simply go to the project directory and type:virtualenv kerasenvYou will see a message in your terminal like:Installing setuptools, pip, wheel…done.In a newly created virtualenv there will be an activate shell script. This resides in /bin/, so you can run:source kerasenv/bin/activateNow, we are ready to install necessary dependencies. The list of dependencies we will be needing for our project are as follows:tensorflow (1.5.0)Keras (2.1.4)Flask (0.12.2)h5py ( 2.7.1)You can install these all at the same time using the command:pip3 install tensorflow keras Flask h5pyComputation is much faster if you have a GPU but you’ll need to use GPU version of tensorflow. If you plan on using tensorflow-gpu instead, you can follow our other article here to learn how to install it.Our other required dependencies such as scipy, numpy etc. should automatically be installed while installing these dependencies.Introduction to Convolutional Neural Network (CNN)Now, we are ready to build a Convolutional Neural Network (CNN) to classify MNIST handwritten digits. But first, we must understand what a CNN is. We will only be covering the basic theory of CNN in this article. I highly recommend you refer to materials of course CS231n, if you want a deeper understanding of how CNN works.In machine learning, a Convolutional Neural Network (CNN, or ConvNet) is a class of deep, feed-forwardartificial neural networks that has successfully been applied to analyzing visual imagery. Convolutional Neural Networks are a type of neural network that makes the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. There are three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer. We will stack these layers to form a full ConvNet architecture.Image source: WikipediaCONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume, POOL layer will perform a downsampling operation along the spatial dimensions (width, height) and FC (i.e. fully-connected) layer will compute the class scores, ( resulting in volume of size [1x1x10] in our case), where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. All this may seem very confusing to you right now. So I highly recommend you refer to materials of course CS231n if you want a deeper understanding of how CNN works. However, for now, all we need to understand is that CNNs are one of the best available tools for machine vision and we will be using it for our purpose for classification of MNIST handwritten digits.MNIST Handwritten digits classification using KerasNow that we have all our dependencies installed and also have a basic understanding of CNNs, we are ready to perform our classification of MNIST handwritten digits. The Keras github project provides an example file for MNIST handwritten digits classification using CNN. (link: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) The classifier gets to 99.25% test accuracy after 12 epochs. We will not go with the approach of re-inventing the wheel and simply modify the example file to meet our necessity. The code is well commented and pretty much self-explanatory, however, we will be going through each portion of the code to understand what is being done.Here, we define the batch size of 128 data per epoch. Batch size defines the number of samples that going to be propagated through the network at each epoch. It requires less memory and is especially important in case if you are not able to fit dataset in memory. Since MNIST handwritten digits have a input dimension of 28*28, we define image rows and columns as 28, 28. The function mnist.load_data() downloads the dataset, separates it into training and testing set and returns it in the format of (training_x, training_y),(testing_x, testing_y). Some of you might get an error saying “IOError: CRC check failed 0xc187cf56L != 0x14c5212fL” in this function when running the file. For those of you who get the error, open the file kerasenv/lib/python3.6/site-packages/keras/utils/data_utils.py and just below the import statements, add:The actual design of neural network is done in this portion of the code. First, we define the model to be a sequential model. We stack Convolutional Layer and Pooling Layer on top of each other along with Dropout layer. Dropout layers provide a simple way to avoid overfitting by randomly dropping components of neural network (outputs) from a layer of neural network. This results in a scenario where at each layer more neurons are forced to learn the multiple characteristics of the neural network. The last layer of the neural network will have number of node equal to the number of output class i.e. 10 and the activation function we will be using is “softmax”.Now there are a lot of things like activation functions, (eg: relu, sigmoid; loss, categorical crossentropy; optimizer, adadelta) which I will not be explaining in this post. We don’t need it for our purpose right now but I do suggest you learn about those to get a better understanding of how to design neural networks.The model.fit function trains the model for a fixed number of epochs (iterations on a dataset).You will also find the files “model.h5″ and “model.json” in the working directory. We will be loading these file later using flask to make predictions. That’s all for this part of the tutorial. In the next part, we will learn how to deploy Keras model to production using flask and make predictions using the model files we created in this part of the tutorial. Like us on facebook and we will post when part two of the series is out.",MNIST Handwritten digits classification using Keras (part - 1) | Python 3.6
9906,3895023,2018-02-28 00:00:00,"As of Tuesday morning local time (CST), China had lifted its ban on N again.The letter is used in China to represent unknown numerical values, like the letter X in algebra.Professor Victor Mair, a China expert at the University of Pennsylvania, said in a Monday blog post it was ""probably out of fear on the part of the government that 'N' = 'n terms in office,' where possibly n > 2.""Chinese state media, for its part, has been trying to play down China's latest crackdown on internet communication.In a Tuesday editorial, state-run newspaper the Global Times accused Western critics of ""hysteria"" over the government's latest round of censorship.""The biggest reason for all this is that the rise of China has reached a critical point where some Westerners cannot psychologically bear it any longer. They wish to see misfortune befall the country,"" the Global Times said.",China banned the letter N from the internet after people used it to attack Xi Jinping's plan to rule forever
9907,3895260,2018-02-28 13:48:09,"Here’s another example: A few years ago, I paid my local branch a visit to buy some Arduino gear. At the time, I was in the middle of a Computer Science degree, and was studying physical computing. Maplin admittedly had a decent selection of official Arduino components, but it was vastly more expensive than Amazon. In the end, I went on eBay and ordered some Chinese knock-off boards for a fraction of the cost.Another complaint is that Maplin lost track of its core demographic: hobbyists and tinkerers.Kind of sad about Maplin going under: it was always the prime destination for the electronics and gadget enthusiast who hasn't yet discovered the internetEarlier incarnations of Maplins were awesome. Speak to any British nerd of a certain age, and they’ll tell you about ultra-knowledgeable sales clerks, and shelves piled high with obscure computer components. But in recent years, it lost focus, and shifted towards a more general audience. It started selling things like drones, tablet computers, and smart home gear.Never really got Maplin. Seems to sell lots of drones and DJ equipment, plus bits and pieces for build-your-own PC types. Not convinced by argument that there is a place for it on High St https://t.co/IL0fMqT4ADAnyone can sell an iPad, but nobody bothers to cater to die-hard techies. The level of selection and technical expertise that Maplin once offered set it apart. I’d argue that it was a major tactical misstep for Maplin to go after the same market as PC World and the Apple Store.A finger of blame has been pointed at Brexit for Maplin’s collapse. According to company CEO Graham Haris, the drop in the value of Sterling is partially to blame for the collapse, although he admitted other other factors were at play. In a statement, he said:“The business has worked hard over recent months to mitigate a combination of impacts from sterling devaluation post Brexit, a weak consumer environment and the withdrawal of credit insurance. These macro factors have been the principal challenge not the Maplin brand or its market differentiation.”Hopefully Maplin can be saved, although its prospects don’t look especially promising. If Maplin ultimately folds, it’ll be a tragedy, particularly for the company’s 2,500 employees.It would also leave a gaping hole on the high street. For all its flaws in its latter days, Maplin was the only retail store that bothered to cater to the lucrative geek demographic. And as someone who has spent countless hours browsing its aisles, I’ll be sad to see it go.","In a sad day for UK geeks, Maplin enters administration"
9908,3897565,2018-02-28 15:00:00,"How to improve battery life on your smartphone or smartwatchIf you've lived with mobile devices for long enough, you've likely had that moment when you received a low-battery warning well before the end of the day. It's more than a little worrying, especially if you're far from home. But how do you wring the most out of your remaining battery life in those situations -- or better yet, avoid those problems in the first place? You don't always have to rely on your phone's low-power mode. We've compiled a slew of tricks that can extend the longevity of your smartphone or smartwatch, and they're not necessarily obvious -- in many cases, it's as much about discipline as it is flicking the right software switch.Start with hardware settingsNo matter which phone or watch you own, it's best to start with the basic settings for your display, GPS, sound and wireless connections. They're easy to change, and the improvements in battery life can sometimes be dramatic enough that you don't need to do much else.Your screen brightness could easily be the most important factor. It's tempting to crank the brightness up to ensure you can see your display in all lighting conditions, but that's bound to sap a lot of energy. Whether or not the brightness is on auto mode, you'll want to dial the setting down to where it's just enough for you to comfortably see what's on-screen. And leave auto brightness turned on as a general rule: Your device is usually a good judge of when to dim the screen or ramp things up. You may also want to lower the screen-timeout interval, since you rarely need to leave an idle phone awake for minutes at a time.Some devices will have additional display options that can lengthen your battery life, although they can vary widely. Phones like the Galaxy S8 and Galaxy Note 8 have options for lower resolutions, for example, while the Razer Phone lets you reduce the refresh rate. And if your hardware has an always-on display feature (such as on numerous Android phones, Android Wear watches and Samsung's Gear line), turn it off unless you really want at-a-glance information.GPS also tends to be a battery killer, and gadget makers know it -- they've implemented many ways to scale back positioning or disable it entirely. On Android, you can typically change the location settings to limit tracking to ""low accuracy"" (WiFi and Bluetooth but not GPS) or ""device only"" (GPS alone). On iOS, you can turn off location sharing or force apps to only use location info when they're active. And if you're wearing a smartwatch, you can sometimes disable it in specific situations. With the Apple Watch, for instance, you can use its companion iPhone app to turn off GPS during workouts.Wireless connections can also waste energy, although you'll want to think carefully before switching them off (they're essential to phones and watches, after all). Bluetooth is usually the easiest to address: If you don't need it for accessories like smartwatches or wireless headphones, keep it off. After that, it's trickier. You can turn off WiFi when away from home, but that will chew up cellular data and hurt location accuracy. There's also airplane mode (which shuts off all wireless connections) if you need it, although it's really more of a last resort than an everyday solution. Smartphones and smartwatches are designed for always-on connections, and you only want to take them offline if you don't intend to use them for long stretches of time.There's only so much you can do with cellular service, however. You can turn off cellular data altogether if you're sure you won't need it away from WiFi, but it's usually better to disable features that consume data in the background, such as iOS' Wi-Fi Assist (which uses your cell data when your WiFi connection is poor) or app-specific cellular-data use. And no, dropping to slower speeds (such as from LTE to 3G) won't usually help. What energy you save may be lost simply because downloads will take more time.Sound and haptics don't usually play a significant role in your battery life, so don't worry too much about keeping your device quiet or vibration-free. Those are more matters of etiquette and personal preference than energy savers, since they're rarely active for long enough to make a difference.Cut background activityIf hardware-related tweaks weren't enough, your next step is to rein in the behavior of your apps. Some programs are particularly chatty by default, and that can be a problem when you have dozens of them on your phone or watch.Before you do anything else, you should ask whether or not you need a given app in the first place. It's tempting to install legions of apps ""just in case,"" but the odds are that you'll only use a fraction of them on a regular basis. Do you really need notifications from an app you rarely ever use? Get rid of the apps you haven't touched in months or those that are just as easily replaced by a website. That can even include social networks like Facebook (whose app tends to be large battery drain) if you're not in a rush to check someone's status update. While individual apps aren't usually battery hogs, they can add up if they're performing background tasks.Whichever apps you decide to run, you'll want to limit notifications. They may only pop up on your screen for a brief moment, but receiving dozens of notifications per day can repeatedly wake up your screen and run down your battery. Whether you're using Android, iOS or a smartwatch, you can go into your settings and either turn off all notifications for a given app or scale back the alerts you get. On Android, you can set notifications to ""show silently,"" and with iOS, you can individually disable lock screen appearances, banners and sounds. Pruning your notifications will keep your device quiet until you genuinely need it, and it could spare you from information overload in the process.Some apps have their own notification settings, too, and it's a good idea to review them to limit alerts to those you care about. You probably don't need to know every time someone likes an Instagram photo or Twitter post -- keep the notifications to essentials like mentions or direct messages. This not only saves battery but also helps you sort through the clutter to find the notifications that matter.In addition to notifications, there are sometimes background processes beyond location tracking that can have a small impact on your device's lifespan. Do you actually use ""Hey Siri"" or ""OK Google"" activation? If not, disable it. While it shouldn't significantly hurt your battery life, there's no point to leaving it on if you're just as happy pressing a button to talk to an AI assistant. Apps that fetch data on a regular basis, such as email clients and weather utilities, also tend to have options for how frequently they retrieve new data. If you don't need always need to see the latest message or forecast, slow the update interval down to whatever you're willing to tolerate.Just don't worry about closing apps when you aren't using them. Modern mobile operating systems like Android and iOS are usually diligent about suspending apps after they've gone unused for several minutes, and they shouldn't use significant battery power unless they're performing background tasks like music playback or uploads. You might even hurt your battery life by closing apps, since you're forcing them to load fresh on subsequent launches instead of quickly loading them from system memory.Limit your usageWe know, we know: It's no fun to curb your device habits, especially if you love chatting with friends or documenting your life with photos. However, it's always good to review your usage patterns if settings changes aren't helping. Thankfully, this doesn't always mean using your device less often.It's a good idea to reduce media streaming when you can. That constant wireless-data use tends to murder your battery no matter which app you're running. If you can, play files offline. Services like Apple Music, Spotify Premium, Netflix and Hulu all have the option to cache media on your devices -- and if you prefer permanent copies, you can always buy downloads from stores like iTunes. You can even save music on smartwatches (Apple Music on an Apple Watch, Spotify on watches like Samsung's Gear series) if you'd rather not play music from your phone while going for a run.You can help your battery life by limiting your uploads, for that matter. Some cloud services, such as Apple's iCloud Photo Library and Google Photos, have options to restrict uploads to those moments when you're on WiFi (or sometimes only when you're plugged in). If you can afford to back up your files when you get home, why not? Likewise, it might be better to save big manual uploads like concert videos until you're near a charger.Yes, there are times where the best option is simply to restrain yourself. Intensive games (particularly 3D titles), continuous GPS navigation, livestreaming and video chats can be extremely taxing on your device's power pack, and there's not much you can do to rein them in. You don't have to avoid these apps entirely, but it's smart to budget your time so that they don't eat more electricity than necessary. You might only want to play a game for 10 minutes, for example, or turn navigation off during the last leg of a trip.If all else fails...Should you try everything on this list and still run into trouble, you have a couple of last-ditch options. Check the health of your battery, whether it's with an app (iOS 11.3 and later have battery health built in) or by contacting technical support. If you've owned a device for long enough and use it often, there's a real possibility that the battery has worn out to the point where it holds a significantly reduced charge. A fresh battery will usually cost you money, of course, but that expense beats having to recharge every few hours.And if that doesn't work? If you're otherwise happy with your device or only occasionally run out of juice, consider getting an external battery pack (Anker, Belkin and Mophie are a few of the more common manufacturer options -- I use and enjoy a Mophie pack myself). While they're seldom cheap and can sometimes be bulky, they can easily prove worthwhile if they give you a few extra hours of use on a long day. Just be sure to balance the capacity you want versus the bulk you're willing to tolerate. As much as you might want to get a gigantic battery that could power your devices through an entire weekend, that behemoth could quickly become a burden when you only need an hour or two at most.Jon has been hooked on tech ever since he tried a Compaq PC clone when he was five. He's big on mobile and is one of those precious few people who wears his smartwatch with pride. He's also an unapologetic Canadian: Don't be surprised if you get an earful about poutine or the headaches with Canadian carriers.",How to improve battery life on your smartphone or smartwatch
9909,3897566,2018-02-28 14:02:00,"Woz was scammed out of Bitcoins now worth over $70,000Apple co-founder Steve Wozniak has demonstrated the issue of bitcoin fraud by falling victim to an old school internet scam. Speaking at The Economic Times of India's global business summit this week, he said that someone bought seven bitcoins from him using a credit card, but canceled the card after the bitcoin was transferred, so the payment failed to process. And of course the card was stolen, so there was no way to retrieve his lost assets. He bought when the cryptocurrency was priced at $700, but the loss would be worth the equivalent of $71,400 today.""The blockchain identifies who has bitcoins ... that doesn't mean there can't be fraud, though,"" he said, highlighting the issue that banks in both the US and UK have been focusing on in recent times. Earlier this month Lloyds Banking Group, like several others, banned its customers from buying bitcoin with credit cards, in a bid to offer some protection in a landscape that remains largely ungoverned.Governments are becoming more mindful of the issues involved with bitcoin fraud, though. South Korea, for example, has limited cryptocurrency trading to real-name bank accounts, while UK Prime Minister Teresa May said the government is taking fraud in cryptocurrencies ""very seriously"". However, many would argue that meaningful action is not being taken quickly enough -- if tech legend Steve Wozniak can fall prey to this kind of fraud, anyone can.","Woz was scammed out of Bitcoins now worth over $70,000"
9910,3897567,2018-02-28 11:01:00,"Watch the world's largest plane hit 46 mph in latest taxi testsWhile Stratolaunch didn't meet its original 2016 launch goal, the company has been putting its massive plane through one test after another over the past year. During its most recent tests, the 500,000-pound aircraft with a 385-foot wingspan has successfully reached a top taxi speed of 46 mph. It still hasn't taken to skies, but that's a huge improvement over the 28 mph it hit during the first taxi tests the space transportation company conducted in December.According to Stratolaunch chief Paul Allen, his team has also verified that they can steer and stop the aircraft -- another success, since the tests' purpose is actually to certify that operators can control the largest plane in the world. The company hasn't revealed what other tests will need to be conducted before its new target launch date in 2020 and whether reaching 46 mph on the ground means we'll see it fly soon. But the hope is that once development is finished and it's been thoroughly tested, Stratolaunch will serve an affordable way to ferry small satellites to Low Earth Orbit.The plane will do so by attaching the payload and its launch vehicle companion to its belly. It will then fly into the stratosphere before letting the vehicle go, so it can make its way to space. Stratolaunch shares a similar two-fuselage combination with Virgin Orbit's LauncherOne, which Richard Branson's Virgin Group is also developing for satellite launches. It's much, much bigger than LauncherOne that can only carry up to 500 pounds of payload, though, and will be able to carry as much as 500,000 pounds.Captured new video of @Stratolaunch plane as it reached a top taxi speed of 40 knots (46 mph) with all flight surfaces in place on Sunday. The team verified control responses, building on the first taxi tests conducted in December. pic.twitter.com/OcH1ZkxZRA",Watch the world's largest plane hit 46 mph in latest taxi tests
9911,3897568,2018-02-28 08:00:00,"Waymo's 360-degree demo ride shows what self-driving cars 'see'One of the big steps necessary to roll out self-driving cars is getting the public to trust them, and Waymo's latest attempt to achieve that is with a new 360-degree experience. Viewable on a computer, your phone or a VR headset, it feeds a simulation of mixed video and sensor data (LiDAR, radar and cameras) from one of the company's Pacifica Hybrid minivans to give you a feeling of what it ""sees"" while driving along.For many people, this is the first time they'll be able to experience a ride in a self-driving car, and perhaps begin to trust that it's at least as attentive as human drivers. The Alphabet (and formerly Google) company has big plans to launch a taxi service this year with thousands of these vans, and 360-degree video provides a preview of what it will be like.After taking five years to accumulate one million miles driven, Waymo says its cars have racked up a million in the last three months (to with 2.7 billion virtual miles last year). That brings their total real-world mileage to five million across 25 test cities, adding as much mileage every day as the average person does in a year. Expect more ""public education and awareness"" advertising to follow, but for now, just take a peek around as Waymo does the driving.",Waymo's 360-degree demo ride shows what self-driving cars 'see'
9912,3897569,2018-02-28 10:15:00,"Fitbit's cheaper, smaller smartwatch leaks outIt's crunch time for Fitbit as it searches for a way to get plenty more folks buying its smartwatches. On Monday, the company said that it was planning a ""family"" of devices for 2018, including a watch that was cheaper than the Ionic and aimed at the mass-market. Now, Wareable believes that it has renders of the device, which is set to launch in the Spring.The render shows a timepiece with the same one-and-two pusher design as on Fitbit's flagship, but a narrower, square-circle case. According to people familiar with the matter, the company was concerned that the Ionic wasn't a hit with women, who found it unstylish. Consequently, the device has been designed with an eye on making it smaller, thinner and with more color options. You can also see this strategy feeding back to the Ionic, which will soon be available in an Adidas-branded variant.It's also thought that the watch is intended to replace the Blaze in Fitbit's product lineup, and could be called the Blaze 2. Alternatively, since it runs on the same platform as the Ionic, it will get a new name that signifies the shift. As for the hardware itself, which comes in black, silver, rose gold and charcoal, it will be water-resistant to 50 meters and will, to save costs, not have a GPS module included.The timepiece will also come with the same blood oxygen sensor that ships in the Ionic, but has yet to be activated by the company. When it is, the system will be able to track sleep apnea, although it's not yet clear when it's rolling out. We've reached out to Fitbit for comment, although it has already issued a non-committal response to Wareable, but all of this certainly smells true enough.After training to be an intellectual property lawyer, Dan abandoned a promising career in financial services to sit at home and play with gadgets. He lives in Norwich, U.K., with his wife, his books and far too many opinions on British TV comedy. One day, if he's very, very lucky, he'll live out his dream to become the executive producer of Doctor Who before retiring to Radio 4.","Fitbit's cheaper, smaller smartwatch leaks out"
9913,3897659,2018-02-28 05:00:00,"Why I Quit Google to Work for MyselfFor the past four years, I’ve worked as a software developer at Google. On February 1st, I quit. It was because they refused to buy me a Christmas present.Well, I guess it’s a little more complicated than that.The first two yearsTwo years in, I loved Google.When the annual employee survey asked me whether I expected to be at Google in five years, it was a no-brainer.Of course I’d still be at Google in five years. I was surrounded by the best engineers in the world, using the most advanced development tools in the world, and eating the free-est food in the world.My most recent performance rating was “Strongly Exceeds Expectations.” If I just kept going, I’d soon be promoted to the next level, Senior Software Engineer. What a great title! Forever after in my career, I’d be able to say, “Yes, I was a Senior Software Engineer. At Google.” People would be so impressed.My manager assured me that my promotion was close. He felt that I was already capable of senior-level work. I just needed the right project to prove it to the promotion committee.Your manager doesn’t promote you?No, managers at Google can’t promote their direct reports. They don’t even get a vote.Instead, promotion decisions come from small committees of upper-level software engineers and managers who have never heard of you until the day they decide on your promotion.You apply for promotion by assembling a “promo packet”: a collection of written recommendations from your teammates, design documents you’ve created, and mini-essays you write to explain why your work merits a promotion.A promotion committee then reviews your packet with a handful of others, and they spend the day deciding who gets promoted and who doesn’t.During my two-year honeymoon phase, this system sounded great to me. Of course my fate should be in the hands of a mysterious committee who’s never met me. They wouldn’t be tainted by any sort of favoritism or politics. They’d see past all that and recognize me for my high-quality code and shrewd engineering decisions.That’s not really how it worksBefore I put together my first promo packet, I never thought about the logistics of how it all worked.In my head, the promotion committee was this omniscient and fair entity. If I spent each day choosing the right problems to solve, making the codebase better, and helping my team execute efficiently, the promotion committee would magically know this and reward me for it.Unsurprisingly, it doesn’t work like that. It took me two years to figure that out.Working naïvelyMy main responsibility until that point was a legacy data pipeline. It had been in maintenance mode for years, but load had increased, and the pipeline was buckling under the pressure. It frequently died silently or produced incorrect output. Its failures took days to diagnose because nobody had written documentation for it since its original design spec.I proudly and lovingly nursed the pipeline back to health. I fixed dozens of bugs and wrote automated tests to make sure they wouldn’t reappear. I deleted thousands of lines of code that were either dead or could be replaced by modern libraries. I documented the pipeline as I learned it so that the institutional knowledge was available to my teammates instead of siloed in my head.The problem, as I discovered at promotion time, was that none of this was quantifiable. I couldn’t prove that anything I did had a positive impact on Google.Metrics or it didn’t happenThe pipeline didn’t record many metrics. The ones it did have made it look like things had gotten worse. My bug discoveries caused the overall bug count to increase. The pipeline’s failures increased because I made it fail fast on anomalies instead of silently passing along bad data. I drastically reduced the time developers spent repairing those failures, but there were no metrics that tracked developer time.My other work didn’t look so good on paper either. On several occasions, I put my projects on hold for weeks or even months at a time to help a teammate whose launch was at risk. It was the right decision for the team, but it looked unimpressive in a promo packet. To the promotion committee, my teammate’s project was the big, important work that demanded coordination from multiple developers. If they hornswoggled me into helping them, it’s evidence of their strong leadership qualities. I was just the mindless peon whose work was so irrelevant that it could be pre-empted at a moment’s notice.I submitted my first promo packet, and the results were what I feared: the promotion committee said that I hadn’t proven I could handle technical complexity, and they couldn’t see the impact I had on Google.Learning from rejectionThe rejection was a difficult blow, but I wasn’t discouraged. I felt I was performing above my level, but the promotion committee couldn’t see it. That was solvable.I decided that I had been too naïve in my first couple years. I didn’t do enough planning up front to make sure the work I was doing left a paper trail. Now that I understood how the process worked, I could keep doing the same good work, just with better record-keeping.For example, my team was receiving tons of distracting email alerts due to false alarms. Old me would have just fixed these alerts. But now I knew that for this work to appear in my promo packet, I should first set up metrics so that we’d have historical records of alert frequency. At promotion time, I’d have an impressive-looking graph of the alerts trending downward.Shortly after, I was assigned a project that seemed destined for promotion. It depended heavily on machine-learning, which was and still is the hot thing at Google. It would automate a task that hundreds of human operators were doing manually, so it had a clear, objective impact on Google. It also required me to lead a junior developer throughout the project, which generally won points with promotion committees.The holiday gift wake up callA few months later, Google made headlines when they ended their long-standing tradition of giving lavish holiday gifts to all of their employees. Instead, they used the gift budget to buy advertising disguised as charity Chromebooks for underprivileged schoolchildren.Shortly after this, I witnessed the following conversation between two employees:Employee A: You effectively are still getting the gift. Cuts like these increase the value of Google’s stock. You can sell your stock grants and buy any present you choose.Employee B: What if I told my wife that I wasn’t buying her a Christmas gift, but she could use the money in our bank account to buy any present she wants?Employee A: You’re in a business relationship with Google. If you’re disappointed that Google isn’t “romancing” you with gifts like you do for your wife, you have a misguided notion of the relationship.Wait a second. I was in a business relationship with Google.It may sound strange that it took me two and a half years to realize it, but Google does a good job of building a sense of community within the organization. To make us feel that we’re not just employees, but that we are Google.That conversation made me realize that I’m not Google. I provide a service to Google in exchange for money.So if Google and I have a business relationship that exists to serve each side’s interests, why was I spending time on all these tasks that served Google’s interests instead of my own? If the promotion committee doesn’t reward bugfixing or team support work, why was I doing that?Optimizing for promotionMy first denied promotion taught me the wrong lesson. I thought I could keep doing the same work but package it to look good for the promotion committee. I should have done the opposite: figure out what the promotion committee wants, and do that work exclusively.I adopted a new strategy. Before starting any task, I asked myself whether it would help my case for promotion. If the answer was no, I didn’t do it.My quality bar for code dropped from, “Will we be able to maintain this for the next 5 years?” to, “Can this last until I’m promoted?” I didn’t file or fix any bugs unless they risked my project’s launch. I wriggled out of all responsibilities for maintenance work. I stopped volunteering for campus recruiting events. I went from conducting one or two interviews per week to zero.Then my project was canceledPriorities shifted. Management traded my project away to our sister team in India. In exchange, that team gave us one of their projects. It was an undocumented system, built on deprecated infrastructure, but it was nevertheless a critical component in production. I was assigned to untangle it from our sister team’s code and migrate it to a new framework, all while keeping it running in production and hitting its performance metrics.As far as my promotion was concerned, this was a setback of several months. Because I hadn’t released anything for my canceled project, the two months I spent on it were worthless. It would take me weeks just to get up to speed on the system I was inheriting, and I was liable to lose several more in the gruntwork of keeping it operational.What am I even doing?It was the third time in six months that my manager had reassigned me midway through a project. Each time, he assured me that it had nothing to do with the quality of my work, but rather some shift in upper management strategy or team headcount.At this point, I took a step back to assess what was happening from a high level. Forget my manager, forget his managers, forget the promotion committee. What if I boiled it down to just me and just Google? What was happening in our “business relationship?”Well, Google kept telling me that it couldn’t judge my work until it saw me complete a project. Meanwhile, I couldn’t complete any projects because Google kept interrupting them midway through and assigning me new ones.The dynamic felt absurd.My career was being dictated by a shifting, anonymous committee who thought about me for an hour of their lives. Management decisions that I had no input into were erasing months of my career progress.Worst of all, I wasn’t proud of my work. Instead of asking myself, “How can I solve this challenging problem?” I was asking, “How can I make this problem look challenging for promotion?” I hated that.Even if I got the promotion, what then? Popular wisdom said that each promotion was exponentially harder than the last. To continue advancing my career, I’d need projects that were even larger in scope and involved collaboration with more partner teams. But that just meant the project could fail due to even more factors outside my control, wasting months or years of my life.What’s the alternative?It’s an online community for founders of small software businesses. Emphasis on small. These weren’t Zuckerberg hopefuls, but rather people who wanted to build modest, profitable businesses that pay their bills.I had always been interested in starting my own software company, but I only knew of the Silicon Valley startup path. I thought being a software founder meant spending most of my time fundraising and the rest of it worrying about how to attract my next million users.Indie Hackers presented an attractive alternative. Most members built their businesses with their own savings or as side projects to their full-time jobs. They didn’t answer to investors, and they certainly didn’t have to prove themselves to anonymous committees.There were downsides, of course. Their income was less steady, and they faced more numerous catastrophic risks. If I ever made a mistake at Google that cost the company $10 million, I would suffer no consequences. I’d be asked to write a post-mortem, and everyone would celebrate the learning opportunity. For most of these founders, a $10 million mistake would mean the end of their business and several lifetimes of debt.Founders on Indie Hackers captivated me because they were in control. Whether their business became a runaway success or stagnated for years, they were calling the shots. At Google, I didn’t feel in control of my own projects, much less my career growth or my team’s direction.I thought about it for months and finally decided. I wanted to be an Indie Hacker.One last thing before I leaveI still had unfinished business at Google. After investing three years into my promotion, I hated the idea of leaving with nothing to show for it. There were only a few months left until I could reapply for promotion, so I decided to give it one last shot.Six weeks before the performance period ended, my project was canceled. Again.Actually, my whole team was canceled. This was a common enough occurrence at Google that there was a euphemism for it: a defrag. Management transferred my team’s projects to our sister team in India. My teammates and I all had to start over in different areas of the company.I applied for the promotion anyway. Weeks later, my manager read me the results. My performance rating was “Superb,” the highest possible score, given to around 5% of employees each cycle. The promotion committee noted that in the past six months, I clearly demonstrated senior-level work. These were, uncoincidentally, the months when I was optimizing for promotion.But they felt that six months wasn’t a long enough track record, so… better luck next time.My manager told me I had a strong chance at promotion if I did the same quality work for another six months. I can’t say I wasn’t tempted, but by that point, I’d been hearing, “great shot at promotion in six months,” for the past two years.It was time to go.What’s next?When I tell people I left Google, they assume I must have some brilliant startup idea. Only an idiot would leave a job as cushy as Google Software Engineer.But I am indeed an idiot with no idea.My plan is to try different projects for a few months each to see if any of them catch on, for example:Google was a great place to work, and I learned valuable skills during my time there. Leaving was difficult because I had more to learn, but there will always be employers like Google. I won’t always have the freedom to start my own company, so I look forward to seeing where this takes me.",Why I Quit Google to Work for Myself
9914,3897664,2018-02-28 14:33:43,"Facebook rolls out job posts to become the blue-collar LinkedIn0LinkedIn wasn’t built for low-skilled job seekers, so Facebook is barging in. Today Facebook is rolling out job posts to 40 more countries to make itself more meaningful to people’s lives while laying the foundation for a lucrative business.Businesses will be able to post job openings to a Jobs tab on their Page, Jobs dashboard, Facebook Marketplace, and the News Feed that they can promote with ads. Meanwhile, job seekers can discover openings, auto-fill applications with their Facebook profile information, edit and submit their application, and communicate via Messenger to schedule interviews.“One in four people in the US have searched for or found a job using Facebook” writes Facebook’s VP of Local Alex Himel. “But 40% of US small businesses report that filling jobs was more difficult than they expected. We think Facebook can play a part in closing this gap.”Now users in the new countries will be able to use the Jobs dashboard found in the Facebook web sidebar or mobile app’s More section to discover jobs using filters like proximity, industry, and whether they want a full-time or part-time gig.The Job posts rollout could help Facebook steal some of the $1.1 billion in revenue LinkedIn earned for Microsoft in Q4 2017. But the bigger opportunity is developing a similar business where companies pay to promote their job openings and land hires, but for lower-skilled local companies in industries like retail and food service.In this space, job applicants often don’t have glowing resumes and education histories that look good on LinkedIn. They might not even be on the site, and if they are, they probably don’t spend much time there. But they may already have their limited professional experience listed and they spend a ton of time casually browsing the site. This lets Facebook connect them with job even if they weren’t actively seeking a position, and quickly apply to lots of different positions by piggybacking off their profile info.“Troy, the owner of Striper Sniper Tackle in North Carolina had trouble finding people with the specific skills he needed until he posted the job on his Facebook Page. He received 27 applications immediately, and hired 10 people” Facebook writes. Those jobs probably wouldn’t appeal to LinkedIn users, and some of those who applied probably didn’t think they were job hunting when they opened Facebook.“Since 2011, Facebook has invested more than $1 billion to help local businesses grow and help people find jobs” Himel writes., referencing the Community Boost program that trains businesses and job seekers to better use the Internet…including Facebook. “In 2018 we plan to invest the same amount in more teams, technology, and new programs. Because when businesses succeed, communities thrive.”The challenge for Facebook may be convincing users that they can still be themselves on the social network. Facebook stresses that potential employers can only see what’s public on an applicant’s profile. But some users still might be paranoid that their party pics or niche hobbies could scare away hirers.The move again proves how powerful being a default daily destination is. Over the past few years, Facebook built a giant business by becoming an alternative to YouTube where people serendipitously discover videos instead of purposefully coming to watch certain ones. That same strategy could make Facebook a massive gateway to local jobs. And it’s coming at a time when Facebook is desperate to prove it can be meaningful to people and make their lives better, rather than just being a time sink.",Facebook rolls out job posts to become the blue-collar LinkedIn
9915,3897665,2018-01-25 08:40:44,"Police patrolling the old town in Kashgar, Xinjiang. The city has been the focus of a major crackdown on the Muslim Uighur people. Photograph: Tom Phillips for the GuardianAt least 120,000 members of China’s Muslim Uighur minority have been confined to political “re-education camps” redolent of the Mao era that are springing up across the country’s western borderlands, a report has claimed.Radio Free Asia (RFA), a US-backed news group whose journalists have produced some of the most detailed reporting on the heavily securitised region of Xinjiang, said it obtained the figure from a security official in Kashgar, a city in China’s far west that has been the focus of a major crackdown.Last year, as Xi Jinping was crowned China’s most powerful leader since Chairman Mao at a politically sensitive congress in Beijing, Xinjiang’s re-education centres were “inundated” by detainees, who were forced to endure cramped and squalid conditions, the report said. Just in the city of Kashgar – which has a population of about half a million inhabitants – tens of thousands of people were allegedly confined. Taking into account the wider region around Kashgar, the number allegedly rose to 120,000.Maya Wang, a Human Rights Watch campaigner who wrote a recent report on the camps, said the figures cited by RFA were credible although growing levels of repression in Xinjiang meant reliable numbers were impossible to ascertain. Estimates of the total number of people who have spent time in such centres in Xinjiang, which has a population of about 22 million, ran as high as 800,000, Wang added. “It’s just like a black hole which people are added to and don’t get out of.”Kashgar, the largest city in southern Xinjiang, has found itself at the eye of a growing security storm since a hardline party leader, Chen Quanguo, took charge of the region in the summer of 2016.Last year BuzzFeed visited one such institution, known as the Kashgar Professional Skills Education and Training Centre, describing a beige building with shades covering its windows. “People disappear inside that place,” one local warned.China defends what authorities call “extremism eradication” schools as an important part of its fight against radicals it blames for a wave of attacks.According to Human Rights Watch’s report, the centres are often housed in converted government buildings such as schools or specially built facilities. Wang said detainees were often held, unlawfully and without charge, as a result of religious “offences” such as excessive praying or non-religious acts such as accessing proscribed websites.Wang said that while authorities claimed the centres were about combating terrorism and separatism, they were in fact designed to brainwash and assimilate Uighurs. “At the political education facilities, they sing patriotic songs. They learn about Xi Jinping thought. These are patriotic measures aimed at making Uighurs love the Chinese government,” she said. “It’s extreme repression and yet completely silent.”","China 'holding at least 120,000 Uighurs in re-education camps'"
9916,3897828,2018-02-27 19:40:39,"Hands on: Asus Zenfone 5 2018 reviewJust like an Android-toting iPhone XOur Early VerdictAsus has made an incredibly beautiful looking device here and if the pricing is right, it could be a flagship competitor to watch out for. That said, it does look like one particular phone already on the marketForGreat lookingBeautiful displayAgainstNot top internalsLooks like a competitorAsus has used MWC 2018 to reveal the new Zenfone 5 (not that one from 2014) to the world with a new design we haven't seen on any of the company's devices before.It features an edge-to-edge display with an iPhone X-like notch at the top, and the elephant in the room is how much this looks like Apple's top-end handset.That doesn't mean it's not a beautiful looking device though, and with generally high-end specs it looks like the Asus Zenfone 5 - along with the Asus Zenfone 5Z - may be one of the stand out devices of this year's top mobile phone show.Watch our video hands on review of the Asus Zenfone 5 belowAsus Zenfone 5 price and release dateYou won't be able to buy this phone any time soon. For some reason, Asus has announced this at MWC 2018 but it won't be on shelves until April for the Zenfone 5 and June for the Zenfone 5Z.We know it's confirmed for the UK, but we've yet to hear any news about Australia, the US or any other markets. Exact pricing details are currently up in the air too, so we can't judge the Zenfone 5 too much yet.Design and displayThe Zenfone 5 is a stunning looking phone, but when we first saw it we thought it was an iPhone X. Whether that's an issue, is up to you.On the front there's a 6.2-inch 19:9 aspect ratio Full HD+ IPS display. This was stunningly bright (over 500 nits) and the phone has a 90% screen-to-body ratio.It's the same size as previous 5.5-inch Asus devices, so it really doesn't feel like a large screen device. Plus it's light at 155g without feeling like it'd blow away in a gust of wind.There's a notch at the top to hide all of the camera tech and facial recognition tech, and the rear is glass.In the center on the rear there's a fingerprint scanner - we found that easy to tap to unlock the phone, but it's unlikely you'll use this much if you set up the facial unlocking technology.The camera placement again is reminiscent of the iPhone X though, so if you own this device expect people to question why the UI on your Apple device looks so different.As for color, you'll have to choose between Midnight Blue and Meteor Silver, but both are bold and look great with a shimmery design on the rear that dazzles when you move it around in sunlight.SpecsAlthough Asus has announced several phones in the '5' range, we've only had time to try the standard Asus Zenfone 5, which comes with Qualcomm's mid-range Snapdragon 636 chipset alongside 4GB of RAM. That's an AI chipset, so expect lots of machine learning features when it comes to optimizing performance and within the camera.The upcoming Asus Zenfone 5Z though will feature the latest Snapdragon 845 chipset inside and up to 8GB of RAM.This is what we have seen powering top-end phones like the Samsung Galaxy S9, so you can expect faster performance from the 5Z if you're looking for a powerhouse of a device.You'll also get more storage here, with the base model of the Zenfone 5Z offering 128GB and the normal Zenfone 5 being restricted to just 64GB of space.Everything else about the devices is much the same though, you'll have the same design and features on both, including Android 8 Oreo right out of the box, so you'll have all the latest features from Google's OS right away.When it comes to camera, there's a 12MP Sony sensor on the rear with an f/1.8 aperture. There's also a second camera working alongside that for when you want to take wide-angle shots.It's up to 120-degrees, and it should come in useful when you want to fit more friends into your photos or you want to get a beautiful photo of a landscape.Within the software there's AI scene detection like we've seen on the Huawei Mate 10, which will notice if you're taking a photo of a dog or flowers for example and fiddle around with the settings to make the optimum photo.We've yet to properly experiment with this feature, but if it works as well as on other handsets you can expect this to be useful when you're just using the phone's automatic mode.There's also an 8MP selfie shooter on the front with an aperture of f/2.0. This camera comes with facial recognition tech so you won't need to unlock your phone with a PIN or fingerprint scan.The Asus Zenfone 5 also has a 3,300mAh battery, which is a fairly typical size, but could stand out thanks to 'AI Charging', which automatically adjusts the charging rate based on your charging habits, to help reduce battery wear - we'll let you know how well that works in our full review.Early verdictThere is no escaping the fact the Asus Zenfone 5 and Zenfone 5Z both look like the iPhone X. The notch, the screen and the camera placement all scream of Apple's handset, but if you want a device that looks like that and runs Android this may suit.Without knowing the price we can't fully determine whether this will be one of the best handsets of the year, but we hope to find out and get one for a full review soon.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.What is a hands on review?'Hands on reviews' are a journalist's first impressions of a piece of kit based on spending some time with it. It may be just a few moments, or a few hours. The important thing is we have been able to play with it ourselves and can give you some sense of what it's like to use, even if it's only an embryonic view. For more information, see TechRadar's Reviews Guarantee.",Asus Zenfone 5 2018 review
9917,3897829,2018-02-27 20:05:47,"Samsung Galaxy S9 might be one of the first phones with Android PThis strikes a nice chordSharesA phone like the Samsung Galaxy S9 is only as good as the software powering it. Thankfully, its recently discovered support for Project Treble means that it should be running Android P and Android Q shortly after their release, SamMobile reports.Project Treble is a relatively new initiative, first detailed at Google IO 2017, that makes the process of adopting the latest and greatest version of Android faster and easier for phone makers. At least, theoretically.What used to take a months, at best, could now be greatly reduced. Treble essentially makes phones future-proof from the point of manufacturing by changing the way these phones work on the software end. The vendor implementation in Android’s architecture has been streamlined, allowing device makers to bring anticipated updates at a faster pace.Why is Treble important for Samsung?Sometimes, even the best Android phones are left behind because of outdated software. While they’re usually covered from a security standpoint, owners want access to new features introduced with each major Android release.This perfectly describes the Samsung Galaxy S8’s update situation, which was in recent news because of issues that plagued the already-late roll-out of Android Oreo. Treble won’t solve the odd issue popping up, delaying the release of software, but it should generally lead to less overall work for software developers.It’s more crucial than ever for Samsung to stay on top of the latest Android releases, at least if it wants to be a true competitor to the Google Pixel 3, which will likely be the first phone to launch pre-loaded with Android P.Seeing Treble on the S9 is a good indicator that Samsung is taking updates seriously.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",Samsung Galaxy S9 might be one of the first phones with Android P
9918,3897832,2018-02-28 04:54:20,"As each new version of Android is traditionally named after a candy or dessert, trying to figure out what the next instalment will be called is almost as exciting as the new features it will bring. Okay, that might be a stretch, but it's definitely a good bit of fun.There are a number of confectionery items starting with P that the upcoming version of Android could adopt as its code name. With that in mind, we've put together this list of our best guesses.PrevPage 1 of 9Next PrevPage 1 of 9NextPop-TartsHaving already teamed up with brand names like KitKat and Oreo for previous versions of its mobile OS, Google could very well choose Pop-Tart as its next Android code name. The only thing working against it is that the actual brand name is the pluralized 'Pop-Tarts', and Android Pop-Tarts just sounds weird.Likeliness: HighPrevPage 2 of 9Next PrevPage 2 of 9NextPavlovaLike its namesake, the Russian ballerina Anna Pavlova, this meringue-based dessert is light and graceful — two words we'd definitely like to use to describe the next version of Android.Likeliness: HighPrevPage 3 of 9Next PrevPage 3 of 9NextPeppermintAlright, so peppermint is a plant, not a confectionery, but we'd argue that most people probably think of hard candy, chewing gum or after-dinner mints when they hear the word. Plus, it just sounds good — Android Peppermint. So fresh.Likeliness: HighPrevPage 4 of 9Next PrevPage 4 of 9NextPEZEveryone loves PEZ, the little brick-shaped candies from Austria that are commonly eaten out of the throats of our favorite cartoon characters. They're sweet, they're colorful, and once you start, you can't stop. Why wouldn't Android want to be associated with that?Likeliness: ModeratePrevPage 5 of 9Next PrevPage 5 of 9NextPopsicleAndroid Popsicle rolls off the tongue like a... well... popsicle. The item itself is sweet, visually appealing, and immediately fills us childhood nostalgia. The one thing holding it back: it's perhaps a little too close to Android Lollipop.Likeliness: LowPrevPage 6 of 9Next PrevPage 6 of 9NextPancakesThe late, great comedian Mitch Hedberg once said that pancakes are ""all exciting at first, but then by the end you're f***in' sick of em."" He may have been onto something there. Android probably won't go with this one, especially since Pancake sounds odd on its own. Android Pancake. Nope.Likeliness: LowPrevPage 7 of 9Next PrevPage 7 of 9NextPeanut BrittleWe can't imagine any smartphone manufacture wanting to be associated with the word 'brittle' under any circumstances, so let's just smash this suggestion with a hammer right now.Likeliness: Very lowPrevPage 8 of 9Next PrevPage 8 of 9NextPiña coladaIf you like piña coladas, and getting caught in the rain, then chances are you'd have no problem with the next version of Android being named after this popular alcoholic beverage. It probably won't encourage young children to start drinking early, right?","Android P is on the way, so which confectionery will it be named after?"
9919,3897904,2018-02-27 12:36:34,"TNW SitesNo matter how talented, an artist will inevitably have to choose a preferred medium. Do you want to work with pen and ink, paints, even clay or granite? It’s no different with a digital artist, who routinely gravitates to apps or environments where they’re comfortable to ply their trade.That’s because there are a ton (and we do mean a ton) of design software options to choose from. No matter where your expertise lies, you can dig deeper into that and every other aspect of digital art creation with this all-encompassing eduCBA Design & Multimedia Lifetime Subscription Bundle. It’s now on sale for just $19 from TNW Deals.If you’re unfamiliar with eduCBA, they’re one of the biggest online educators on the planet. Now they’re throwing open their doors on the entire archive of design coursework — and befitting their industry stature, there are literally dozens upon dozens of courses available.With more than 200 courses including over 700 hours of instruction (that’s nearly 30 days worth of content!), you’ll have expert-led direction to help you master practically any discipline. From 3D modeling, visual effects and animation to graphics, video editing and game design, there are scores of courses covering every program or approach to your field of interest.Photoshop, InDesign, Unity, Mocha, Keyshot, Maya, Nuke…they’re all here. And with comprehensive tests and quizzes to solidify your training, not to mention certificates of completion at the end of every course, you’ll soon have a stockpile of industry knowledge ready to get you hired or promoted across a number of job sectors.Whether you’re looking to expand your prospects or just dive into a cool creative arena on your latest whim, this bundle satisfies nearly any urge at a limited time offer price of only $19.","Learn animation, Photoshop, 3D graphics, all your digital design needs...for only $19"
9920,3897905,2018-02-28 14:40:58,"About TNWTNW SitesBig Dutch bank teases cryptocurrency wallet, people think it’s a scamBitcoin conspiracy theorists have long speculated that banks and financial institutions are out to slay the cryptocurrency market, but it seems the narrative is gradually starting to change: leading Dutch bank Rabobank is apparently toying around with the idea of launching its own cryptocurrency wallet.Crypto-enthusiasts are rushing to Twitter to share links to a newly announced cryptocurrency wallet service called Rabobit, which promises to bridge the gap between bank accounts and cryptocurrency wallets. But what is particularly noteworthy about this wallet is that it has Rabobank behind it.But here is the catch: building a cryptocurrency wallet is just one of many initiatives Rabobank is currently exploring for the future. The company will reportedly announce which projects it will focus on later this year – in the period around late May and early June.The initiative will run as part of the Rabobank Moonshot program aimed at encouraging innovation in technology and the financial services.Unfortunately, there isn’t much information to go by at the time of writing, but from the looks of it – Rabobank is merely using the website to gauge customer interest for this concept, and perhaps a not-so-aggressive marketing stunt.The only button on the website – a generic ‘GET STARTED’ call to action – invites users to leave their email address and some additional info to stay informed about any future updates.Indeed, the frugally-populated website has since raised some eyebrows, with numerous skeptical netizens wondering whether it is legitimate or just another scam.The Rabobank social media team had to eventually step in to clear the air. According to customer service reps, “this website is indeed launched by the Rabobank,” but simply as a measure to “see if there is any interest.”The company also said on Twitter that Rabobank has yet to formally approve the project. “Rabobit has not yet been realised,” a customer rep said, “As soon as we know more, more updates will follow.”","Big Dutch bank teases cryptocurrency wallet, people think it's a scam"
9921,3900590,2018-02-28 16:00:00,"What’s so funny about technology?A tech ethicist and improv comedian meet in “Funny As Tech.”Moments after he stepped onstage, Joe Leonardo sensed the crowd would be difficult to win over.It was a recent Tuesday evening at the Peoples Improv Theater, a New York comedy venue with photos of Stephen Colbert, Will Ferrell and Tracy Morgan on the walls and mantras -- ""Follow the fear"" -- next to the stage entrance.Leonardo and David Ryan Polgar were running behind schedule for their show -- technical difficulties with the projector -- then came out for a cold open.""Tonight, we're talking about the future of work,"" said Polgar. ""What are you worried about with the future of work, Joe?""""We're all gonna be replaced with robots,"" Leonardo shot back, looking half expectantly at the crowd of about 30 people, who looked back.Polgar moved on to the recent Boston Dynamics video, which bears a disturbing resemblance to the Black Mirror episode ""Metalhead."" A four-legged robot precisely opens a door for another robot, then holds it for the companion to enter.""They're already making me look bad, because there's, like chivalry in robots now,"" Leonardo said, to murmurs of chuckles. ""He opened the door for another robot to go through. So not only is he taking my job, he's making me look like a piece of crap on top of that.""""The apocalypse is going to be very polite,"" Polgar replied.Polgar is a speaker and writer on technology ethics. With his law degree and blue jeans/black blazer/brown brogues combination, he is more like a professor in the show. Leonardo is an animated improv actor on the famous Upright Citizens Brigade Theatre's house team. He voices the concerns of the everyman, grounding abstract concepts in personal anecdotes or analogies to lighten up the discussion.In 2010, the two met at Sea Tea Improv, a Connecticut comedy club Leonardo founded, then started the show Funny As Tech last summer. The idea is to bring discussions about technology's impact to a mainstream audience, with monthly shows as well as a podcast tackling subjects like cryptocurrency or tech addiction.While shifts in technology now affect all of us, they're usually discussed in the ivory tower or at industry conferences. Like any subject involving niche expertise, future speculation and no clear-cut answers, it's hard to make entertaining. A show like Funny As Tech presents an alternative: an expert who quotes studies from PricewaterhouseCoopers with a comedian who can make it digestible on the fly. ""I'm the flavor to a bland meal,"" Leonardo told me. ""I'm good spicing.""One theory of what makes things funny is ""benign violation"": a disruption of our expectations or values but in a harmless way. This is basically what John Oliver does for wonky public policy or what Jon Stewart pioneered for media criticism. Talking about something serious in a comedic fashion allows people to engage with touchy issues without feelings getting hurt. It highlights the absurdities and the BS.""Honestly, the most truthful person is the jester next to the king,"" Leonardo said. ""The jester can say things that normal people can't in the court because the jester makes it a joke.""Whether it's fake news or smartphone addiction, tech today is full of potential violations to our quotidian lives that can be made unserious, and comedians have repeatedly mined it. A Saturday Night Liveskit about an Alexa device for elderly people punctures the idea of a clean dialogue with AI through the rambling of actual human communication.In Chris Rock's latest Netflix special, he skewers our always-on smartphone culture even as it relates to the weightiness of his divorce. ""In 16 years, I had more contact with my ex-wife than my parents did in 40 years,"" he said. Missing somebody in this day and age is impossible, he continues, because they're constantly in your pocket. ""I know everything you did today and I know how people felt about it ... I gave you three smiley faces and an eggplant!""The conflict of whether our future is a utopia or dystopia and between the smooth efficiency of technology and our disheveled realities -- all of this creates fertile ground for laughs. In the case of Funny As Tech's future of work show, the recurrent root of the jokes was a fear that automation is destroying jobs, violating our dignity in the process. With a recent Bain & Company study predicting that 20 to 25 percent of jobs may be eliminated in the next 12 years, it may happen sooner than we expect.As the show continued, Polgar raised topics like universal basic income with the three guests from the technology world -- Charlie Oliver from Tech2025, which hosts technology events; Galina Ozgur from accelerator Grand Central Tech; and Lisa Cervenka from careers site The Muse. For the most part, it wasn't dissimilar to an academic panel discussion, except it had more quips and a segment where the guests played an improvisational word-association game.An audience member asked when we will have the leisure time that widespread automation promises, and Polgar cited 1960s cartoon The Jetsons, where work had become nothing more than pressing two buttons. It sparked an idea for Leonardo, and he tapped Polgar on the shoulder.""And how gendered is that show?"" Leonardo said. ""Their [homemaker] is female. There's no reason to generalize that.""""Well, we do have Alexa and Siri,"" Polgar replied.The crowd had loosened up by then and was laughing along. In mock outrage, Leonardo banged his fist on an invisible table: ""Ah, we're living in The Jetsons!""Chris is associate features editor at Engadget. He writes about how technology is shaping culture, society and the everyday human experience. Raised in the UK and Hong Kong, he has worked for the Columbia Journalism Review, Reuters, and the South China Morning Post.",What’s so funny about technology?
9922,3900591,2018-02-28 16:30:00,"The chaos of unlocking your phone in 2018PIN codes and patterns are passe. MWC 2018 kicked off with the usual fanfare of a major flagship launch -- the Galaxy S9. With it, Samsung introduced its own, new, face unlock feature. Google may have added the feature to Android many years ago, but it seems technology has progressed enough to make it worth resurrecting by Samsung -- with some extra biometric backup. The House of Galaxy might have also felt the competitive tug of Apple's surprisingly slick Face ID unlock feature on the iPhone X.Samsung wasn't the only company innovating when it comes to how we get our smartphone working. And it's not just the thousand-dollar flagships, either. Biometrics are here in a big way, although no-one seems to know which method's best. How many of these techniques will last to see 2020?Samsung Galaxy S9's ""Intelligent Scan""Samsung's new flagship devices have both a front-facing 8-megapixel camera and an iris scanner. These work in tandem for ""Intelligent Scan"", which combine the secure identifying nature of your eye's unique makeup with a camera that detects your face. Samsung believes its iris scanner isn't as effective in bright light as it is in the dark, thus it's included the more traditional camera backup this time. With the new system, the S9 tries to sign you in with your eyes by default, but when that fails, it will use facial recognition. Samsung says the technology is learning-based, which means it should improve its ability to latch on to your face as you continue to use it.This is the marquee feature for Samsung when it comes to ID and security: it plans to use this like Apple does for app purchases and feature logins. The company says Samsung Pass will be up first, offering identification for websites through its own Internet app. (Which means you'd have to use that over, say, Chrome on Android.)We're still putting this Intelligent Scan feature to the test ahead of reviewing the device in full, but remember; facial scans don't offer the security necessary for keeping your phone locked. Samsung has also kept its fingerprint sensor, although it thankfully moved it further away from the camera this time.To be honest, the company's packed nearly every security option in here. You can unlock its newest phones with a pattern, PIN, or password; the iris scanner, fingerprint scanner or face recognition; and Intelligent Scan (that aforementioned blend of iris and face scanning).Vivo's in-screen fingerprint scanner offers a glimpse at the future of all-screen phone interfaces: No giant button needed. While we saw that back in January, at MWC this week the company bested, well, itself. Compared to the X20 Plus, which had a single spot for on-screen fingerprint verification, the new concept device has an entire quarter of the screen to do the deed. It's a big enough space that it can also handle dual fingerprint scanning -- if you wanted that extra coating of security.EngadgetVivo's using ultrasound sensors that can apparently read your prints across a bigger area beneath the screen. (In comparison, the last model did the same scanning trick with a tiny camera sensor, but that's something apparently only possible with OLED screens).When Engadget Senior Editor, Chris Velazco, tested it out in person, the tech wasn't exactly flawless, taking multiple attempts to unlock with a fingerprint. It's also folded into a concept phone -- one with a pop-out selfie cam, no less. The chances of an identical phone appearing in the west are pretty slim.Not all phones are born flagships, but that's no longer a good enough excuse not to upgrade from a PIN you made back when Brangelina was a thing. The Alcatel 5 might be a middleweight in spec sheet terms, but it still folds in both a fingerprint sensor and face unlock features. It doesn't sound particularly foolproof: the ""Face Key"" will attempt to detect 100 points on the user's face to verify, so it's possible photos might be enough to trip it up. Still, the unlock feature was swift enough during our brief time with it earlier this week. Somehow, you can even go cheaper: the Alcatel 3 slides in just under 200 Euros.Other optionsAlso announced this week, LG's V30S ThinQ isn't a huge leap beyond last year's V30, and it doesn't pack any new methods to unlock. It does, however, offer voice unlock, something that snuck into the debut V30 last year but hasn't been adopted on any other devices, as far as we can tell. It's not hard to see why: Asking Google Assistant / Siri questions in public is hard enough, let alone with the frequency we all unlock our phones. Still, it's yet another method to add to the pile.We'll throw in a quick hat-tip to Apple and Face ID. Its True Depth camera system -- that's why there's a notch -- is made up of a bunch of sensors (ambient light, infrared and proximity) that detect your face, even in the dark. It offers a more secure version of simple camera-based face recognition and is surprisingly smooth and hassle-free -- so much that Huawei is looking to fold in similar tech into its next phone but better, of course.For now, it's still a mess of techniques out there, and you get what you pay for when it comes to reliability and security. It might, however, be a sign to upgrade your smartphone security.Mat once failed an audition to be the Milkybar Kid, an advert creation that pushed white chocolate on gluttonous British children. Two decades later, having repressed that early rejection, he completed a three-year teaching stint in Japan with help from world-class internet and a raft of bizarre DS titles. After a few years heading up Engadget's coverage from Japan, covering high-tech toilets and robot restaurants, he heads up our UK bureau in London.",The chaos of unlocking your phone in 2018
9923,3900684,2018-02-28 00:00:00,"The Lost Art of the MakefileI work on a lot of Javascript projects. The fashion in Javascript is to use build tools like Gulp or Webpack that are written and configured in Javascript. I want to talk about the merits of Make (specifically GNU Make).Make is a general-purpose build tool that has been improved upon and refined continuously since its introduction over forty years ago. Make is great at expressing build steps concisely and is not specific to Javascript projects. It is very good at incremental builds, which can save a lot of time when you rebuild after changing one or two files in a large project.Make has been around long enough to have solved problems that newer build tools are only now discovering for themselves.Despite the title of this post, Make is still widely used. But I think that it is underrepresented in Javascript development. You are more likely to see a Makefile in a C or C++ project, for example.My guess is that a large portion of the Javascript community did not come from a background of Unix programming, and never had a good opportunity to learn what Make is capable of.I want to provide a quick primer here; I will go over the contents of the Makefile that I use with my own Javascript projects.When to stick with WebpackThe job that Webpack does is quite specialized. If you are writing a frontend app and you need code bundling you should absolutely use Webpack (or a similar tool like Parcel).On the other hand if your needs are more general Make is a good go-to tool. I use Make when I am writing a client- or server-side library, or a Node app. Those are cases where I do not benefit from the specialized features in Webpack.Why does Javascript need a build step?Let’s quickly address the question of why someone would want a build step in a project that does bundle code.I want to be able to write Stage 4 ECMAScript while targeting browsers or recent stable versions of Node. I also like to include Flow type annotations in my code, and I want to distribute type definitions with my code; but I want the code that I distribute to be plain Javascript. So I use Make to transpile code using Babel.Introducing the MakefileMake looks for a file called Makefile in the current directory. A Makefile is a list of tasks that generally look like this:target_file: prerequisite_file1 prerequisite_file2 shell command to build target_file (must be indented with tabs, not spaces) another shell command (these commands are called the ""recipe"")Unless you specify otherwise, Make assumes that the target (target_file in this example) and prerequisites (prerequisite_file1 and prerequisite_file2) are files or directories. You can ask Make to build a target from the command line like this:$ make target_fileIf the target_file does not exist, or if prerequisite_file1 or prerequisite_file2 have been modified since target_file was last built, Make will run the given shell commands. But first Make will check to see if there are recipes in the Makefile for prerequisite_file1 and prerequisite_file2 and build or rebuild those if necessary.A practical example of a Makefile ruleA minimal project might have a file called src/index.js. We want a rule that tells Make to transpile that file and write the result to lib/index.js. But Make looks at things the other way around: Make expects to be told the desired result, and it uses rules to work out how to produce that result. So we write a Makefile with a rule where the target is lib/index.js and src/index.js is a prerequisite:The recipe uses babel to produce lib/index.js using src/index.js as input. The shell commands in a Makefile recipe are almost exactly what you would type in bash - but note that Make substitutes variables and expressions prefixed with $ before commands are executed. You can escape a $ in a recipe command by doubling it (e.g. cd $$HOME). In the recipe above there are two special variables: $< is a shorthand for the list of prerequisites (src/index.js in this case) and $@ is the target (lib/index.js). We will see why those variables are indispensable in a moment.The mkdir -p line creates the lib/ directory in case it does not already exist. The function dir extracts the directory portion from a file path. So $(dir $@) is read as “the path to the directory that contains the file referenced by $@”.Generalized rulesWhen we add more files to the project it would be tedious to write a Makefile target for each Javascript file. A target and it’s prerequisites can include wildcards to create a pattern:lib/%: src/% mkdir -p $(dir $@) babel $< --out-file $@ --source-mapsThis tells Make that any file path that begins with lib/ can be built using the given steps, and that the target depends on a matching path under src/. Whatever string Make substitutes in the position of the % in the target, it substitutes the same string for % on the prerequisite’s side. Now it becomes clear why the variables $< and $@ are necessary: we won’t know what the values of those variables will be until the rule is invoked.Why invoke Babel separately for each source file?Babel can transpile all files in a directory tree with one invocation. But the rule above will run babel separately for every file under src/. There is some startup time overhead every time babel runs; so invoking babel many times is slower when building from a fresh checkout. But thanks to Make’s talent for incremental builds separate invocations make incremental builds much faster. When we ask Make to transpile all files under src/ it will skip files that already have up-to-date results under lib/. I run incremental builds far more often than full builds so I appreciate the speedup!Locating BabelThe babel executable is provided by the babel-cli NPM package. I prefer to install babel-cli as a project dev dependency, which causes babel executable to be installed at the path node_modules/.bin/babel. That way anyone who wants to build my project does not have to take a special step to install babel-cli globally. But then babel will not be in the executable $PATH on most machines. To avoid typing out the path to the executable I assign the location of babel to a variable in the Makefile (babel := node_modules/.bin/babel), and use Make’s variable substitution to splice that path into recipe commands.(Pro tip: you can add node_modules/.bin to your shell $PATH like this: PATH=""node_modules/.bin:$PATH"". That makes it easy to run executables installed by dependencies of the project in your current directory. Executables installed with the project will take precedence over executables installed globally. NPM automatically makes this $PATH adjustment when you run NPM scripts. I type out the path to babel in my Makefile because I do not want to assume that other people have made the same $PATH modification, and I do not always run make from an NPM script.)Transpiling the whole projectWith the above rule in place you can transpile a Javascript source file with this command:$ make lib/index.js # outputs lib/index.js and lib/index.js.mapMake finds the matching target in your Makefile (lib/%), expands the wildcard, finds the source matching file by expanding src/%, and runs babel. But you probably do not want to run make manually for every source file. What you want is to be able to just type make and have it transpile all source files. Remember that Make needs to be told the results that you want. To do that, first compute a list of all source files and assign it to a variable:src_files := $(shell find src/ -name '*.js')The expression on the right side of that assignment uses Make’s built-in shell function to run an external shell command. In this case we use the find command to recursively list all files under src/ that have the extension .js. You could use another command like [fd][] - but find is more likely to be installed on your colleagues’ workstations and on your CI server.That gives us a list of files that we have. But we need to tell Make which files we want. For every file under src/ we want a transpiled file with a matching path under lib/. We can compute that list by applying Make’s patsubst function to the path of every source file:transpiled_files := $(patsubst src/%,lib/%,$(src_files))The substitution expression uses % as a wildcard in the same way as the rule that we wrote earlier.Now we can define a target that lists the files that we want as prerequisites. When we request that target, Make will automatically build a transpiled result for every source file:all: $(transpiled_files)The target name all is special: When you run make with no target specified it will evaluate the all target by default. This is a case where the target is not a file or directory - all is just a label. You should declare non-file targets in your Makefile like this so that Make does not waste time or confuse itself trying to find matching files in your project:.PHONY: all cleanOh yeah, you probably want a way to remove build artifacts so that you can build cleanly. With this target you can run make clean to do that:clean: rm -rf libAutomatically install node modules when package.json changesMake is powerful enough to accomplish pretty much any task that you can imagine. Do you ever pull updates to a project, and find out after some debugging that you forgot to run yarn install to update your dependencies? You can catch that with Make! When you run yarn install the result is that the node_modules directory is created or updated. You can add a rule for the node_modules target to represent that fact to Make. The state of node_modules depends on the content of package.json and yarn.lock, so those files should be listed as prerequisites:node_modules: package.json yarn.lock yarn install # could be replaced with `npm install` if you preferThis change to the all target adds node_modules as a prerequisite:all: node_modules $(transpiled_files)Now Make will run yarn install if and only if package.json or yarn.lock has changed since the last build. I put node_modules before $(transpiled_files) just in case new dependencies include items such as updates to Babel modules that might affect the way that project files should be built.Watch files and rebuild on changesEvery build tool should have a watch-files-for-changes option for rapid development. You can get that effect by pairing Make with a general purpose file-watching tool:$ yarn global add watch $ watch make src/Just make sure that you do not watch lib/ or you will get into an infinite build loop.Using Make to distribute Flow type definitionsI mentioned that I often use Flow to type-check my projects. I want to distribute plain Javascript, but I also want anyone consuming my library who also uses Flow to benefit from my type annotations. Flow supports that by looking for a files with the .js.flow extension. For example when you import a module called User the Javascript runtime looks for a file called User.js; Flow will additionally look for a file called User.js.flow in the same directory, which should be the original source file with type annotations. My Makefile copies every file under src/ to the corresponding path under lib/ and adds a .flow extension according to this rule:lib/%.js.flow: src/%.js mkdir -p $(dir $@) cp $< $@To make sure that Flow runs this step for all source files I compute the list of .flow files that I expect the same way that we computed the list of transpiled files that we expect:flow_files := $(patsubst %.js,%.js.flow,$(transpiled_files))And I include flow_files in the prerequisites of the all task:all: node_modules $(flow_files) $(transpiled_files)Going furtherMake has many capabilities that I have not touched on here. For example Make supports macros that can compute rules on-the-fly for especially complex use cases. And a Makefile can delegate to targets in other Makefiles, which is useful when distributing Make libraries, or for multi-tiered projects where a build process involves combining artifacts from building multiple subprojects. There is much information to be found in the Gnu Make Manual.",The Lost Art of the Makefile
9924,3900849,2018-02-28 14:33:43,"Facebook rolls out job posts to become the blue-collar LinkedIn0LinkedIn wasn’t built for low-skilled job seekers, so Facebook is barging in. Today Facebook is rolling out job posts to 40 more countries to make itself more meaningful to people’s lives while laying the foundation for a lucrative business.Businesses will be able to post job openings to a Jobs tab on their Page, Jobs dashboard, Facebook Marketplace, and the News Feed that they can promote with ads. Meanwhile, job seekers can discover openings, auto-fill applications with their Facebook profile information, edit and submit their application, and communicate via Messenger to schedule interviews.“One in four people in the US have searched for or found a job using Facebook” writes Facebook’s VP of Local Alex Himel. “But 40% of US small businesses report that filling jobs was more difficult than they expected. We think Facebook can play a part in closing this gap.”Now users in the new countries will be able to use the Jobs dashboard found in the Facebook web sidebar or mobile app’s More section to discover jobs using filters like proximity, industry, and whether they want a full-time or part-time gig.The Job posts rollout could help Facebook steal some of the $1.1 billion in revenue LinkedIn earned for Microsoft in Q4 2017. But the bigger opportunity is developing a similar business where companies pay to promote their job openings and land hires, but for lower-skilled local companies in industries like retail and food service.In this space, job applicants often don’t have glowing resumes and education histories that look good on LinkedIn. They might not even be on the site, and if they are, they probably don’t spend much time there. But they may already have their limited professional experience listed and they spend a ton of time casually browsing the site. This lets Facebook connect them with job even if they weren’t actively seeking a position, and quickly apply to lots of different positions by piggybacking off their profile info.“Troy, the owner of Striper Sniper Tackle in North Carolina had trouble finding people with the specific skills he needed until he posted the job on his Facebook Page. He received 27 applications immediately, and hired 10 people” Facebook writes. Those jobs probably wouldn’t appeal to LinkedIn users, and some of those who applied probably didn’t think they were job hunting when they opened Facebook.“Since 2011, Facebook has invested more than $1 billion to help local businesses grow and help people find jobs” Himel writes., referencing the Community Boost program that trains businesses and job seekers to better use the Internet…including Facebook. “In 2018 we plan to invest the same amount in more teams, technology, and new programs. Because when businesses succeed, communities thrive.”The challenge for Facebook may be convincing users that they can still be themselves on the social network. Facebook stresses that potential employers can only see what’s public on an applicant’s profile. But some users still might be paranoid that their party pics or niche hobbies could scare away hirers.The move again proves how powerful being a default daily destination is. Over the past few years, Facebook built a giant business by becoming an alternative to YouTube where people serendipitously discover videos instead of purposefully coming to watch certain ones. That same strategy could make Facebook a massive gateway to local jobs. And it’s coming at a time when Facebook is desperate to prove it can be meaningful to people and make their lives better, rather than just being a time sink.",Facebook rolls out job posts to become the blue-collar LinkedIn
9925,3900852,2018-02-27 19:21:53,"T-Mobile will fire up 5G in New York, Los Angeles, Las Vegas and Dallas first, promising to have it up and running in 30 cities total by the end of the year.So what does this mean for you? Right now… not much. Eventually, 5G will mean waaaaay faster speeds on your various compatible smart devices. How fast, exactly, is still sort of up in the air as telecoms groups nail down and finalize the standards — but it’s fast. Companies have already demonstrated connections upwards of 500 megabytes (not megabits) per second.The catch: Even if you live in any of the aforementioned cities, you’ll need a 5G-compatible phone to get on that network… and, well, those won’t be available until next year.",These will be the first cities getting 5G from Sprint and T-Mobile
9926,3900929,2018-02-28 17:09:29,"AutoSaw uses a pair of youBots, a discontinued model from Kuka, and a modified Roomba to make customized flat-pack furniture – the kind you put together yourself with a screwdriver and a modicum of self-loathing.Users begin by designing their furniture in a basic computer assisted drawing (CAD) interface and the AI does the rest. The youBots measure the wood and place it in the path of the saw while the Roomba follows precise instructions and cuts more intricate designs with a jigsaw attachment.If the idea of having an AI-powered Roomba running around your garage with a blade doesn’t bother you, this might just be the workshop of your dreams.According to MIT CSAIL director Daniela Rus:Robots have already enabled mass production, but with artificial intelligence (AI) they have the potential to enable mass customization and personalization in almost everything we produce. AutoSaw shows this potential for easy access and customization in carpentry.About 10 percent of all time-lost injuries suffered by carpenters are to their hands. And while we’re not quite sure how much the Kuka youBots go for, as they’ve been discontinued, it’s safe to assume they could be had for under $30k. And the newest Roomba from iRobot costs less than $900.To get a finger reatteached you’ll need substantially more. Assuming you don’t need physical therapy or any follow-up surgeries, $80k will get you started.If you’re not the kind of person who risks losing a finger over a weekend project, perhaps you’re one of those people who think furniture shopping is a death-trap for healthy relationships. In that case, save your marriage and buy a robot or three.PhD student Adriana Schulz, one of the authors on the project’s white paper, says “Our aim is to democratize furniture-customization. We’re trying to open up a realm of opportunities so users aren’t bound to what they’ve bought at Ikea. Instead, they can make what best fits their needs.”This all sounds fantastic and the implications are huge. For example, if these robots could be scaled and taught to build houses it could make owning a home affordable for the first time in generations.AutoSaw is still a research platform, the gang at CSAIL has big plans for it going forward, but for now it’s all considered experimental.And before MIT shutters its research labs in favor of cornering the cheap furniture market there’s a lot more work to do. AutoSaw can’t even make Swedish meatballs.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",MIT turned a couple of old robots and a Roomba into an AI-powered carpentry workshop
9927,3900930,2018-02-28 17:04:58,"About TNWTNW SitesCryptocurrency News February 28th – leap newsletterWe return from the depths of the hashrate / with an economic outlook that ain’t great / went to war with Goldman and Telegram / lost $400 million on ICO scamsCoin returnsSo I’m writing this bit the day before because I literally got up at 6AM, didn’t stop working and thus didn’t actually have time to write this newsletter, but Bitcoin is $10,415 as I write this, which is tied to…well, I don’t really know, again. Coindesk says it’s something about a moving average, or whatever. One thing I can tell you is that there doesn’t appear to ever be a reason why these currencies go up. Other than trading volume. Wait, that’s Bitcoin Cash. What’s going on? Seriously, I need to understand this better. Maybe I shouldn’t have invested my 401k in it? And my house? And my wife?CoinboomIn a thing I didn’t see coming, Coinbase made 43% of its 2017 revenue during the December crypto-rush. According some report of some kind, The Verge was able to ascertain that the incredibly large amount of money going through Coinbase. I actually did real journalism and asked the writer Adrianne Jeffries – and it turns out that was entirely from transaction fees, not including the coins themselves as I suspected. This is really interesting because it suggests an absolutely astounding volume on Coinbase – at least 6 people bought Bitcoin, paying the regular transaction fee of $50,000,000!But in all seriousness, if this is true it suggests Coinbase makes a ton of money off of what are not exactlyregular transaction fees. I’d be very curious (and I put in an inquiry with Jonathan Meiri, Superfly Insights’ CEO, to ask) to see what amount of that is credit card or bank based. Either way, Coinbase’s fees are astounding, and making an entire company on fees kind of suggests that, like, anyone with a big enough, easy enough website could absolutely eat their lunch. It also leaves them in a really vulnerable position financially – they’re entirely at the mercy of totally volatile markets.I’m actually surprised business-wise that Coinbase doesn’t move into other altcoins – Ripple, Stellar, and so on – because they’d absolutely make bank on those. Considering their business model seems so landlocked around transaction fees they’d make so, so much. They could say it’s a quality thing, except they have Bitcoin Cash, which I’m not sure why anyone has, but whatever.Another important part of this is that, well, it isn’t 100% of the story – Coinbase has also got GDAX, their professional product, which actually charges less fees but is the big boy Coinbase. What they make there may be kinda hard to find out! And I’m definitely not gonna do it.Yeah, meMoving to VenezuelaSo this is kind of neat, someone crunched the cost of mining one Bitcoin in electricity. The US takes about $4758, the UK about $8402, and Venezuela only $531. Folks this Venezuelan electricity is a freakin’ bargain! Anyone know anyone in Venezuela who would run my rigs for me? Thank you in advance.Bill hatesFeckless walrus defraudedSteve Wozniak, who would like for you to remember he was once at Apple a very long time ago and, indeed, knew Steve Jobs, got scammed out of $70,000 of Bitcoin because someone “bought them from him using a credit card.” Wait, what the fuck? Has anyone done ANY investigation beyond taking that comment at total face value? This is something where you ask him a question like “how?” Seriously, how’d this happen. Did he find someone who wanted to buy his Bitcoin for the same price he could sell it for at any number of exchanges, let them set up a Stripe payment platform, and then just…send them? What exactly is going on here?This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Cryptocurrency News February 28th - leap newsletter
9928,3900931,2018-02-28 16:49:12,"About TNWTNW SitesClap to Find My Phone app lets you clap to find your phoneFew things are more frustrating than not being able to find your phone right as you’re heading out. An app called ‘Clap to Find My Phone‘ wants to save you from that frustration.Take a wild guess how it works.The app is currently Android only, and it seems unlikely to be any use on iOS, where you’d likely have to keep the app open at all times for it to work.But if you are on team Google, you can set it to work even if your ringer is off or do not disturb is turned on. You can change the sensitivity, make it work with vibrations only, blink your screen, or flash your flashlight. You also have the choice between using music or a ringtone.A quick Google search shows there are a variety of other apps that do the same thing, but this seems to be one of the more full-featured ones.Also keep in mind we’ve only given the app a brief try. It worked well enough, but don’t blame us if your battery drains faster or it turns out the app was recording your raunchy conversations and they’re now being used for blackmail.Seriously though, I can see this app being super handy. Helping me find my lost phone at home is easily my favorite thing about the Google Home Mini; this is arguably even more convenient. Just be sure turn it off before you go to a concert.If you’re on an Android device, you can download it from the Google Play Store. It’s free to download, but a premium version unlocks more features.",This clever Android app lets you clap to find your phone
9929,3903620,2018-02-28 17:30:00,"Blue Origin isn't interested in a race with SpaceXWhen we talk about the current era of private spaceflight, the phrase ""space race"" is thrown around quite often. It's meant as a good thing; a space race against the Russians is what put American astronauts on the moon. The idea of rocket billionaires like Elon Musk and Jeff Bezos working day and night to outdo one another in some spectacle of bravado may sound appealing; it would certainly have entertainment value. But a space race isn't necessarily, in and of itself, a good thing. After all, it's why we've been stuck in low Earth orbit for going on five decades.It's difficult to overstate the achievements the US made in going to the moon. From rocket science to pushing the limits of computing tech, the advancements that emerged out of the endeavor are astounding. What's more, it was a stellar opportunity for science and exploration. The argument isn't that it was a bad idea to go to the moon; it was possibly the greatest achievement in the history of humankind, and scientists are still studying the data from those missions. But the way it happened -- specifically, the fact that it was a race against the Russians, rather than a project centered around science and exploration -- has stifled our nation's spaceflight program ever since.The problem with a race is that the end goal is winning. President Kennedy's dream was ""landing a man on the moon and returning him safely to the earth"" before the end of the 1960s in order to ""win the battle that is now going on around the world between freedom and tyranny."" Apollo was a product of the Cold War. Once we beat the Russians to the moon, budget cuts to NASA began to roll out. The planned Apollo 20 flight was cancelled in January 1970, just six months after Neil Armstrong and Buzz Aldrin walked on the moon. Apollos 18 and 19 quickly followed. NASA had the will and the way, but Congress wasn't on board for it. The decades that followed have seen a cycle of ambitious plans downscaled to fit shrinking budgets.Now, SpaceX's founder Elon Musk has made clear that he's interested in igniting another competition. ""We want a new space race,"" he said during the press conference that followed the successful launch of the Falcon Heavy rocket. ""Space races are exciting.""And it's true: They are exciting. There's a thrill in getting there first, in being bigger and better than everyone else. It's part of the allure of SpaceX, and the company has shown it has the substance to back up its bravado with rockets like the Falcon 9 and the Falcon Heavy.But a problem arises when the space race becomes the singular goal. When the emphasis is on building something bigger because you can, to prove that you could do it first, it can make for a difficult environment. Competition can be good and healthy, but it also can diminish values of exploration, collaboration and scientific knowledge. It's a product of a machismo that many believe should have no place in space or in science.To more than a few, SpaceX and Blue Origin are two sides of the same coin. They are both rocket companies founded by white male entrepreneurs. But their approaches have been markedly different. Jeff Bezos' Blue Origin has operated somewhat outside the public eye, solidifying its identity as a rocket manufacturer with a massive new factory at Kennedy Space Center. It's planning multiple launches of its suborbital rocket New Shepard this year in advance of a crewed flight. The company has found success in building on what it already has achieved, rather than continually embarking on massive new projects.SpaceX, on the other hand, is broader in scope. Is it a rocket company? Is it interested in colonizing the solar system? Is it a satellite internet provider? Apparently, the answer to all of these is . . . yes. When it comes to space, ambition is good. However, the question is whether the lack of a clear focus, and an emphasis on visionary goals rather than tangible short-term objectives, will end up hurting the company. After all, a focus on what can be done, rather than what is practical and necessary, could end up expending the company's resources too quickly.But what about Blue Origin's approach to space? After all, in the eyes of many, the company has been thrown in a space race with SpaceX. But the question is, does the company even want to be in this sort of competition?According to Blue Origin, the answer is no.When asked whether Blue Origin felt pressure to accelerate its launch testing and schedules because of competitors' achievements, Caitlin O'Keefe Dietrich, the Head of Public Relations at Blue Origin, told Engadget, ""Space is a big place. It's not a zero-sum market."" The company, it seems, isn't even interested in the space race challenge that Elon Musk presented.Jeff Bezos isn't above poking some fun at rocket size, as he did when he unveiled the company's massive New Glenn rocket, which is currently scheduled for its first launch towards the end of 2020. And it would be foolish to presume that ego doesn't play into his decision making when it comes to his rocket company. But generally speaking, Blue Origin's approach is to think long-term, rather than jumping from rocket to rocket. ""Our philosophy is to use an incremental, step-by-step approach for our long-term space technology development programs. And this approach has yielded us a lot of progress thus far,"" Dietrich said.It's certainly different than SpaceX's approach to rocket building. On a press call for the Falcon Heavy before the rocket's launch, Musk stated that the company has already assigned the bulk of its engineering teams to the even larger BFR because ""I was looking at Falcon Heavy, and thought it's a bit small."" There isn't currently a huge market for the Falcon Heavy; SpaceX has only booked a few launches for it. Yet SpaceX has already put its resources toward building another, even bigger, rocket simply because the Falcon Heavy wasn't quite large enough for Musk (and because of SpaceX's Mars colonization ambitions). It's not exactly the incremental approach of Blue Origin.Blue Origin's comparison of rocket sizesOne philosophy isn't necessarily inherently better than another. But it's important to remember that a space race isn't, in and of itself, a good thing. It certainly can be an avenue of technological achievement, but if it's used to show off prowess at the expense of building solid foundations, it can end up being a detriment. It's important to have somewhere to actually send a rocket, and a plan surrounding its use, before you turn to building the next, bigger one.Space is big, and there is room for many different ways of looking at spaceflight. Elon Musk may want a space race, and he'll likely get it from ULA, Boeing or another spaceflight provider. Perhaps even Blue Origin will join the fray at some point. But for now, the company isn't interested in a rivalry, nor does it feel any pressure to perform based on the recent Falcon Heavy launch. ""Our vision is for millions of people to live and work in space,"" Dietrich said, ""so we are applauding all launch operators that are building new and more capable systems.""",Blue Origin isn't interested in a race with SpaceX
9930,3903713,2018-02-28 16:06:02,"TrendingIn his 2011 book, “The Better Angels of Our Nature: Why Violence Has Declined,” Harvard psychologist Steven Pinker argued that despite common assumptions, violence has dropped dramatically from biblical times to the present. His new book, “Enlightenment Now: The Case for Reason, Science, Humanism, and Progress,” picks up on that theme, exploring how other threats to human well-being have been in similar retreat.Q&ASteven PinkerGAZETTE:In “The Better Angels of Our Nature,” you explored how the trend toward peace has steadily increased. Why expand the premise of that book for “Enlightenment Now”?pinker: To my pleasant surprise, war is not the only scourge that has declined over the course of history. Extreme poverty has been decimated: It’s gone from 90 percent of the world’s population to 10 percent. Literacy has increased from about 15 percent to more than 85 percent. Prosperity has increased; longevity has increased from about 30 to about 71 years worldwide, and 80 in the developed world.Human flourishing has been enhanced in measure after measure, and I wanted to tell the broader story of progress, and also to explain the reasons. The answer, I suggest, is an embrace of the ideals of the Enlightenment: that through knowledge, reason, and science we can enhance human flourishing — if we set that as our goal. The goal, too, is a gift of the Enlightenment, namely the moral commitment to humanism, in which the ultimate good is the well-being of people.GAZETTE:In a recent conversation with Bill Gates you talked about the notion of tribalism — of focusing on yourself and those immediately around you to the exclusion of others. How do you counter that impulse to help only yourself and your tribe, and encourage people to embrace a more altruistic ideal?pinker: At first hearing, the ideal of promoting human well-being might sound unexceptionable, even banal or trite. But it most certainly isn’t! There are distinct alternatives with much greater emotional appeal. These include the idea that the ultimate good is to promote the greatness of one’s tribe or race or faith or nation; to obey the dictates of a divinity and pressure other people to do the same; to achieve feats of heroic greatness; to transcend the teeming masses by doing something that will put you in the history books, such as martial conquest or feats of artistic greatness.The idea that we should get as many people as possible to live long, healthy, happy lives is far from obvious. Thankfully, it is an ideal that the world has increasingly embraced. You see this humanism in statements such as the Universal Declaration of Human Rights, the U.N. Millennium Development Goals, and the U.N. Sustainable Development Goals. It puts the lie to the claim that in the absence of religion there are no shared values. There are. When you get people from different backgrounds together to decide how we should run our affairs, the conversation tends to move toward humanism because it values nothing more — but nothing less — than our common humanity.GAZETTE:You say in the book that journalistic culture is focused on negative headlines, but that seems like something that’s been going on forever. Has social media driven some of this pessimism forward?pinker: It’s not just the internet age. In the book I cite a study using the automated technique called sentiment mapping to chart the use of positive words like “improved,” “better,” and “beneficial” compared to negative words like “catastrophe” and “crisis.” It shows that since the 1950s, the press has gotten steadily more morose. So it is not a sudden shift with the onset of social media: It’s been happening for decades, even as the actual indicators of human well-being have improved. Over the span of increasing media negativity, there have been fewer wars, less crime, and less poverty.Partly it’s that the press has adopted the ethic that pessimism is moral. It’s serious; it earns you gravitas. Optimism, even if it’s just a readout of the data, is considered frivolous — “advertising,” as one editor put it. Of course there is an optimal amount of pessimism; we have to identify suffering and injustice where it exists. But we also need the conviction that we can do something about it. If you think that the world is just getting worse and worse no matter how many efforts people make to make it better, the logical response is: “Why even bother? Enjoy life while you can, because the world is going to hell no matter what you do.” That’s the kind of fatalism I am pushing against.“If you ask people if we are living in an increasingly dangerous or increasingly safe environment, they will think of the latest terrorist attack and conclude that life has been getting more dangerous — rather than going to FBI data on violent crime, which in fact has shown a decline over 25 years.”— Steven PinkerGAZETTE: So much of what we do is actually bad for us. Is our brain our enemy?pinker: Your brain can’t be your enemy, because your brain is you. But the brain has many subsystems. Some of them incline us toward selfish motives, like revenge, greed, dominance, jealousy. But others are more constructive. We have cognitive processes that can reflect on our own predicament. We have positive emotions like sympathy and compassion. And thanks to the tools of culture, the written word, and now the electronic word, we have the means to learn from our mistakes, to figure out what works and what doesn’t.GAZETTE:Is there something in human nature that predisposes us to be negative?pinker: There is a strand of human nature that does that, called the negativity bias. Losses are more keenly felt than gains. Criticism hurts more than praise emboldens. We tend to be mindful of all the things that can go wrong, not so much when we are reflecting on our own lives, but when we are opinionating about the world as a whole. People often say, “My neighborhood is perfectly safe, and my schools are good. But the country is unsafe and the other schools are going to hell” — sometimes called the “I’m OK; they’re not” bias. When people opine they tend to go negative, partly under the influence of gory headlines and violent images.Another quirk of human nature is called “the availability bias”: Our assessment of risk and danger is driven by available episodes from memory, not representative data. If you ask people if we are living in an increasingly dangerous or increasingly safe environment, they will think of the latest terrorist attack and conclude that life has been getting more dangerous — rather than going to FBI data on violent crime, which in fact has shown a decline over 25 years.RelatedGAZETTE:You’ve said that you think that many intellectuals, even those who call themselves progressive, hate progress. Many of those people are in positions of power. Do you think we have a crisis of responsible leadership in this country?pinker: I am reluctant to call every shortcoming “a crisis.” But I do think that an absence of leaders who are willing to commit themselves to what has worked in the past and that, despite imperfections, has led to improvement, is part of the problem. There aren’t enough champions of liberal democracy, of Enlightenment values, of the institutions that have quite obviously improved our lot — such as international organizations, responsible governments, and police and court systems that have maintained the rule of law. These have dramatically improved the human condition, yet they are relentlessly disparaged.And there’s a tendency among intellectuals to point to every unsolved problem as the symptom of a sick society. To me, that’s a cheap grab for gravitas because it assumes that we have a right to expect a perfectly functioning utopia, and that any deviation is a sign that there’s some kind of gangrene at the core. But if we start from the mindset that the universe doesn’t care about our welfare, that things fall apart — the Second Law of Thermodynamics guarantees that — that evolution has saddled us with competitive and selfish instincts, but “the better angels of our nature” can circumvent these tendencies, then we can be grateful for the progress that we have accomplished, try to identify the institutions and norms that have made it possible, and try to enhance them in the future.The most obvious politician who capitalized on this sense of deterioration is our current president, who campaigned on the premise that social problems had spun out of control, and our current institutions could not deal with them — only a radical lurch, and control by a charismatic leader who is not encumbered by the millstone of an administrative state, could do it, guided by his own authenticity and vision. I personally think that this dangerous development was abetted by some of Trump’s worst enemies on the left, who agreed that American society is a hellhole of inequality, racism, and violence. Not only did they abet his narrative of deterioration, but they left large sectors of the electorate indifferent to the choice between Clinton and Trump, encouraging many young voters to stay at home on Election Day and hand the election to Trump.GAZETTE:You have been known for breaking down heady concepts with your accessible prose. But this book is loaded with graphs. What do the visuals bring to a work like this, and how can they better convey information?pinker: For all of my writing on language, I think it can be overrated. Language is not the same as thought; it’s a medium to express thought. There are other media of thought, including visual images. We are primates, with more than a third of our brains devoted to vision. Graphs are a way of exploiting our primate visual system to grasp complex relationships among quantitative data that a string of words is ill-suited to convey. Language is digital, and language is combinatorial. But many aspects of reality are analog, continuous, and multidimensional.We must develop alternative channels of conveying information, because more and more of our lives ought to be informed by data. We have the classic Cartesian axes going back almost half a millennium, and now geographic mapping enables us to see variables as colors and shapes laid over a map of the world. Dynamic graphs depict change in a way that mirrors the change over time in the world. New graphic forms are putting color to use in new ways, and the third dimension. All these are to be welcomed.GAZETTE:Is there a lesson in mindfulness and meditation — in other words, do you think we can train our brains to be less pessimistic?pinker: There is some indication that anxiety increased from the 1950s to the ’90s and leveled off, at least in college students. Now part of this “age of anxiety” is a curse of maturity. When you grow up and take responsibility for the state of the world, you become more aware of all the things that can go wrong, which earlier generations may have not bothered about. Today we are concerned about climate change, and the risks of nuclear war, and poverty, and oppression, more than our ancestors were. We worry about more parts of the world, not just our backyards. Each of us takes on the world’s problems and adds them to our own personal worry list. In part that’s a good thing: It’s better not to be oblivious to the state of the world.Probably the high point for American happiness was the 1950s, when everything seemed great. Belching smokestacks were signs of progress. The atom bomb was proof of Yankee ingenuity. Housewives lived in domestic bliss. Of course, at some point we had to grow up. And as you grow up you start to worry about more things. That leaves us with the problem of how we can be legitimately concerned about the state of the world without worrying ourselves to death. It may be that mindfulness and other techniques of wisdom, of self-control, of proactive arrangements about your own mental life are necessary to maintain a sense of responsibility for the world while keeping your equilibrium.",Harvard’s Pinker makes case for human progress in new book
9931,3903719,2017-10-13 15:33:38,"Operation ElopThe final years of Nokia’s mobile phonesOn October 8, 2017, Joe Belfiore of Microsoft casually announced the death of Windows Phone. In a series of tweets he explained that Microsoft will continue to support the Windows Phone (and Windows 10 Mobile) platform but “building new features/hw aren’t the focus”. That was the end of Microsoft’s smartphone endeavor.Fast rewind to 2010.On September 10, 2010, Nokia of Finland replaced its Chief Executive, Olli-Pekka Kallasvuo, who had been at Nokia for 30 years, with Stephen Elop, a 46-year-old native of Ancaster, Ontario, and the head of Microsoft’s business software unit, in a bid to turn around the company’s struggling smartphone lineup and stop a decline in its market share in the U.S.On February 11, 2011, Nokia and Microsoft announced plans for a broad strategic partnership to build a new global mobile ecosystem with Windows Phone. [1] Under the proposed partnership Nokia would adopt Windows Phone as its principal smartphone strategy, and contribute its expertise on hardware design, language support, and help bring Windows Phone to a larger range of price points, market segments, and geographies.We, the English translation team would like to express our warmest thanks towards journalists and authors Merina Salminen and Pekka Nykänen for their kind support and information dissemination spirit. Please support Merina and Pekka by buying the original book! And when you read the Finnish book or our English translation, please do remember that the story hails from the year 2014, and our mission was to translate the original Finnish manuscript in English, not to rewrite it to reflect the context of year 2018 nor to reflect our personal opinions. So, when the book says “currently”, please read it as “in October 2014”. We have streamlined the text a bit when Americanizing it, and to assist the global reader we decided to show the Euro figures mentioned in the book also in US dollars, using the exchange rate applicable at the time of the reference.When we were working on the English translation, a small piece of news about Stephen Elop and Finland caught our eye, even mentioning the original book. The Finnish daily Iltalehti wrote that Elop had been seen in the Nokia headquarters on the Espoo Karaportti campus on November 13, 2017. The article was speculating that the visit might have been linked to Elop’s current job with the network provider Telstra in Australia, where he started in April 2016, and further mentioned that in the book Operaatio Elop he had been described as “one of the worst, if not the worst CEO in the world”.This book translation is not endorsed by or associated with the publishing house Teos, Nokia corporation, or with any other company or organization. All product and company names and advertising slogans are trademarks™ or registered® trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them.For readers who prefer a Kindle or PDF version instead of this online version, we provide exports in PDF/EPUB/MOBI formats from Medium. Please consider the environment before printing the 300 pages.1. ForewordThe rise and fall of Nokia is a unique story. In just ten years, a small, multi-industry company transformed into one of the brightest stars in industrial history. Equally unique was its demise and collapse, from the pole position of the mobile phone market to its furthest margins. On September 3, 2013, Nokia announced its intention to sell its mobile phone business to Microsoft. That date has been branded on the hearts of the Finns, equal to the loss of Estonia [2] and the September 11 attacks.This book seeks answers to the questions left unanswered in the memoirs of the former Nokia chairman Jorma Ollila: Who was Stephen Elop and why was a Canadian outsider selected as the new CEO of Nokia? What was the logic of adopting a smartphone operating system conceived outside of Nokia? Why Microsoft’s Windows Phone and not Nokia’s own MeeGo or Google’s Android platform, an option once described by a former Nokia executive as “like peeing in your pants in the winter for warmth?” [3] Why did the company lose its top talent and where did they go? Why did the renowned Nokia spirit simply vanish?As Elop assumed his position in October 2010, Nokia’s market position was already under threat, but some believe it was his strategic decisions that led its descent and the sale of Nokia’s mobile device division to Microsoft. Others believe he chose the best option from an increasingly short list of bad options, that Nokia’s decline was inevitable in the face of renewed competition and rooted in its slow acceptance of alternatives to Symbian and its vaunted S60 platform. Still today, some believe that Elop was Microsoft CEO Steve Ballmer’s “inside man,” sent from Redmond on a quest to deliver market success to Microsoft’s foundering mobile platform. In this vast landscape of conflicting narratives, we seek to document the hard choices that led to the end of this small country’s unlikely domination of the mobile equipment market and assess whether there was a way to salvage “the Nokia Way” or if its end was truly inevitable.During the process of researching this book, we have interviewed over a 100 people with first-hand knowledge about why Nokia ended up as it did. Combined, their stories weave a narrative, one which touched — directly or indirectly — the lives of most Finns, as almost everyone in this Nordic country of six million knows someone who has worked at Nokia. Many of the interviewees worked at Nokia between 2010 and 2013 while the company was in turmoil, when the old laws did not apply any more. When key people were replaced. When executive leadership went AWOL. When things which should not have happened happened. This book depicts how the top management decisions cascaded through the organization, what kind of consequences they had, and — most importantly — how they were seen among the company’s middle management and rank and file employees.In addition, we have investigated how Nokia’s actions appeared outside the company. Did the new Windows strategy convince its network provider [4] customers? What was the outlook for Nokia in Silicon Valley? What was the perception of Nokia in the eyes of its shareholders?This book differs from earlier Nokia studies, as it concentrates on recent history and events, seeking to synthesize a narrative of these fateful years. The book is not about the history of Nokia, nor a parting shot by former employees. It is a critical look at Nokia’s exit from the mobile device market, constructed from a neutral point of view using traditional journalistic methods.Many of those interviewed wanted to remain anonymous, as some are still afraid or still revere Nokia’s unique position in Finland. What is strikingly evident is the affection of these former Nokians towards the company. During their days at the company, they believed they were building the future; afterward, they mourn over the wreckage left behind. Ultimately, many just needed to tell us their side of the story.This book follows the events from the day Stephen Elop entered Nokia’s storied history, seeking to unveil the background of the events at all levels and amplify the perceptions of all people involved in this story. To that end, the point of view will shift from chapter to chapter, as the events are observed through the eyes of engineers, middle management, top leaders, and the Nokia Board of Directors, but together seeking to answer one simple question: Could the demise of Nokia have been prevented if there had been a different CEO?We hope this book will provide a unique insight into what exactly happened at Nokia from 2010 to 2013 and, for some Finns, assist them in the necessary grieving process for Finland’s greatest national champions. Nokia mobile phones are now history and unlikely to come back; its customers will move on to something else. However, the end of Finland’s domination of the mobile equipment market has its own silver lining: Hundreds of startups have been founded by ex-Nokians, creating and dominating new markets. The future of the renewed Nokia, one with a renewed concentration on networks and location based-services, looks bright. In the end, one can learn and benefit from the good and bad choices, but what is truly important is to progress.[2] The 1994 maritime disaster on the Baltic sea when the ferry MS Estonia sank causing 852 deaths.[3] This is actually an old Finnish proverb, inspired by the arctic climate of Finland. Interestingly enough, both Nokia’s MeeGo and Google’s Android share the open source Linux operating system kernel.[4] Network providers are also referred to as “operators” or “carriers”. In this translation, we use the term “network provider”. More explanation in the addendum to the glossary in Appendix 2.2. Hope awakensNews editor Mirjami Saarinen confesses she has only vague recollections of the end of a certain workday in 2010. The morning, however she remembers crystal clear.It was September 10, 2010. The majority of the staff of Kauppalehti, a major Finnish business paper, was attending a morning seminar downtown Helsinki. Staff not attending the seminar was producing the morning online news and planning the next day’s paper, when a press release appeared on editor Niko Ranta’s display. The title was so startling that Ranta started to read the release aloud. The atmosphere at the news desk became electric. That was the moment, which divided Mirjami Saarinen’s recollections of that day. She can still remember, how the staff was returning hurriedly back from the seminar, where the news broke amidst the breakfast. After that point, she can hardly remember anything clearly.As it was a Friday, Kauppalehti, as a 5-day paper, had a dilemma. The rest of the Finnish media would consume the news completely during the weekend, but Kauppalehti still had to be able to produce pertinent news for the Monday paper. Based on her experience, Saarinen knew what she had to do. First, she collected all available editors to work on the news. At the same time she worked on fast news flashes for the online front page. After that, the team started to think angles, which would be still topical on Monday. Would the news still be front page material on Monday, or would it end up on page three? How many pages to allocate and so on. At the same time it had to be decided, who would attend the coming press conference. According to Saarinen the rest of the day was like controlling a huge traffic jam.The start of the press release — i.e. sacking of the Nokia’s then CEO Olli-Pekka Kallasvuo (known universally within Nokia as “OPK”) — was not unexpected. That had been speculated all fall by the Finnish media, which traditionally was nearly toothless regarding Nokia; lately, it had become more critical and demanded Kallasvuo’s head on the plate. Until this day, any public criticism regarding Kallasvuo’s position would have been taboo: Nokia was revered — even feared — among the Finnish press, and all critique was typically much muffled and well veiled.What came next in the press release was indeed startling. Everybody familiar with the company had been betting on Anssi Vanjoki — a very strong and controversial Nokia power figure — to succeed OPK. Instead, the press release introduced a practically unknown Canadian as the company’s new CEO. Who on earth was Stephen Elop? A man nobody had heard of! Was he really the best of available bad options? And what a strange name!Saarinen had quickly half a dozen editors working on the news, and more info started to flow in. A software guy. Coming from Microsoft. Had been in charge of Microsoft’s biggest division. More renowned globally than in Finland.A photographer, two editors and news editor-in-chief Arno Ahosniemi headed for the press conference starting at 1 p.m. The auditorium of the Nokia headquarters in Keilaniemi Espoo was filled by members of all relevant established Finnish media as well as international media having presence in Finland, including Reuters and Bloomberg. This was news also at the global level.The stars of the show kept waiting for themselves for a moment, then it all started. Two figures well known by the Finnish media entered the room: Jorma Ollila and Arja Suominen, EVP of Nokia Group Communications. They were accompanied by a smallish, rather sympathetic looking man. He had an unaffected, even modest look. His tie, combining wine-red and red, appeared almost Soviet Union-like. Moreover, his grey suit and white grey shirt oozed of caution. He had an army-style haircut and his dull-looking glasses seemed to fit poorly behind the ears. That man was not the next Steve Jobs, was the quick, collective verdict of the room.Jorma Ollila quickly took the reins. He introduced Elop and emphasized that the whole of the Nokia board had been actively participating the selection of the new CEO. The board had wanted to find the best possible person to accelerate Nokia’s renewal. According to Ollila, Elop had a great combination of software background and proven leadership skills to match the challenge. In addition Ollila — the guy who lifted Nokia to the top — emphasized Elop’s sensitivity to the cultural differences. Understanding the Nokia’s tradition and the essence of “Finnishness” would be the key factors contributing to Elop’s success. Elop was someone, who could understand the very core and the possibilities of Nokia, Ollila estimated.Then it was Elop’s turn. He shook hands with Ollila and swung behind the small round table filled with microphones. It took but a few seconds to realize the man was a master with the words. Smooth appearance and a reliable presence were like a magic wand, erasing any doubts of the media with a single wave. This man could talk… really talk! His English sounded good, unlike his predecessor Olli-Pekka Kallasvuo’s. A glimmer of hope arose among the audience. Maybe it would all end up well after all.Elop started by thanking Ollila and the Nokia board eloquently about his nomination, which was a great honor. Then he continued and outlined his vision about the big changes shaking the mobile industry. Trendy buzz words and phrases like cloud computing, social media, tablets, apps and so on popped up during the speech naturally. According to Elop, Nokia’s problems were big opportunities. Nokia had tremendous strengths, especially its super capable people. “We” slid into the speech subtly and Elop assured he would listen to the employees and the customers very carefully. He maintained he was one of the “Nokians”–almost a Finn, if you please–and said he would cheer for two countries in the upcoming Olympics.After the well-prepared speech was over, Ollila moved next to Elop for the Q&A. Elop sipped water from the glass and was ready.The very first question was addressed to the board chairman Ollila. He was asked when he would retire from his position. Instead of a vague comment, Ollila said “soon”. The second question was also pointed to Ollila. A Swedish reporter asked for a summary of Kallasvuo’s mistakes, but Ollila replied that this was not the time for a retrospective but time to look forward, then added there had been shortcomings in the implementation of the company strategy. Now, it was the time to proceed to the next phase with a new CEO.When the third question was also addressed to Ollila, the situation became a bit comical. Elop, however, reacted quickly and replied instead of Ollila. “What would be the next steps for Nokia?” Elop replied it was too soon to comment on that but he firmly believed the answer would be found within Nokia — and his task was to make sure that would take place. In a similar way Elop elusively replied to the question, whether the mobile device operating system should be changed. However, he stated the operating system would be critical factor in the strategy.Without a prepared speech Elop was, if possible , even more credible. His hands moved naturally and stressed key messages exactly at the right moment. The movement was almost magical. Perhaps, this could turn into something. Perhaps the American shareholders would finally be happy. Perhaps this man could rescue Nokia. Elop’s undisguised ambition would be manna to Nokia.After one hour the conference was over and the media started to return to their offices. At Kauppalehti the task list until Monday had become clear. The press conference would be covered by Ilkka Sinervä. Merina Salminen would analyze Elop’s quotes regarding Nokia’s challenges. Antti Mustonen would make the feature story about Elop. Editor-in-chief, Hannu Leinonen would analyze, in his weekly column, what would happen to Nokia’s special heritage as a Finnish company, which up until then had been steered almost solely by Jorma Ollila.Looking back to that unusual day, it is obvious that at least Kauppalehti editorial staff was quick to point out Elop’s strong and possibly ominous ties to Microsoft. There were instant comments like “Microsoft to buy Nokia?” and many saw Elop’s nomination as the first step towards a merger. For some the vision was frightening, but not for all. One of the latter was an anonymous member of Nokia top management, who had dealt with Elop a lot. He [5] recalls being ecstatic about Elop’s arrival and opened a good — a really good — bottle of red wine that day.We do not know how Jorma Ollila felt after that long day. We do know, that the day was the first step in Ollila’s plan to rescue Nokia, started a few months earlier. Nokia now had a new leader. A kind of leader the market and foreign shareholders had been expecting. The project–called Operation Elop–had started. [6][5] or “she”? — the original Finnish pronoun “hän” conveys no gender information.[6] The writers want to point out that in real life there was no “Operation Elop”. This is simply the name they chose for the original book.3. Mr. Vanjoki, last minute runner-upThe starting point for the replacement of Nokia’s Chief Executive was at a crayfish dinner party for the board members in August 2009. In his memoirs, Against All Odds, Jorma Ollila writes that one of the board members approached him asking if the company management was all right. Something that Ollila himself had been wondering, and so it was that the cat was let out of the bag. For a long time the board had remained loyal to Olli-Pekka Kallasvuo, a long-term fellow worker of Ollila and a well-liked colleague, generally referred to as OPK. The general consensus was that the problems would be resolved without assistance, but now things were going downhill rapidly.After the dinner party things started to evolve. The decision to replace the Nokia Chief Executive Officer was of such magnitude that it would only be made with Ollila’s consent. Towards the end of 2009, Ollila, too, was ready to go ahead with this. In November–December the board had a yearly evaluation round regarding its own performance as well as the role of the chairman of the board. Vice-chairman Marjorie Scardino was heading these discussions, and they were mainly focused on the operative management. In his memoirs Ollila mentions that practically every board member had posed the question of whether the Chief Executive was up to his task.These discussions were the actual initiation of the replacement process. In January 2010, the board had a lengthy discussion regarding the performance of the organization, and Ollila was given the mandate to talk with the Group Executive Board members about the state of the corporate management. This he did, with Kallasvuo’s consent.There were about a dozen of these talks, and, according to Ollila, the opinions about Kallasvuo were evenly split between those unquestionably supporting OPK and those having serious doubts. Ollila writes: “Olli-Pekka is an agreeable person who does not stir up strong antipathy. The doubts, however, were clear and strong. The discussions with the executive board were not the main factor leading to the replacement of the Chief Executive, but did indeed provide additional data in support of the decision-making.”The concerns within the board were said to have grown gradually. With each passing day, the board believed less that the methods used would lead to success. There were product delays. There were decisions made based on a lack of options and in haste. Also the way Kallasvuo and Ollila were working together caused discontent within the board as the two of them sometimes tended to agree things just between themselves. Apparently one example of this was the decision to hire the former prime minister of Finland Esko Aho as the executive vice president for corporate relations and responsibility.In 2007 Nokia reported a record-breaking financial result. After that, the board was able to blame the global financial crisis that began in 2008 for some of Nokia’s problems. The awakening did not take place until 2009, beginning to be noticeable also in the language of the board.During summer 2010, the board visited Silicon Valley, California. The doubting of Kallasvuo’s ability to run the company had turned into a predominant state of mind. In his memoirs, Ollila mentions one board member being against dismissing Kallasvuo. This is contradicted by another source familiar with the case claiming it was only Ollila himself standing in the way of changes. Granted, he is a living legend, but also a challenging character. Many thought he should have stepped down once things started to go downhill in order to enable proper inspection and evaluation of the current situation, leaving room for questioning the existing structures.There were rising concerns among the shareholders, too. “What should we have been satisfied with?” asks a representative of a shareholder referring to the operations by the board chaired by Ollila during the last years of 20th century.The deputy CEO of Ilmarinen Mutual Pension Insurance, Timo Ritakallio considers it surprising that Ollila did not leave the company after his time as the Nokia CEO. He points out that Ollila’s mere presence, although unintentionally, was tying the hands of his successor. “With Ollila having led Nokia’s rise as well as being a very strong-willed chairman of the board, it is obvious that Kallasvuo was not entirely free to operate as he may have wanted to” says Ritakallio.A representative of a big Finnish shareholder holds the board equally responsible for Nokia’s problems. The board left issues unattended, one of which was the situation of Nokia Siemens Networks. NSN spent years in a difficult impasse due to a delay in the integration of Siemens and Nokia. The need for change was significant, but Ollila was not stepping into the role of primus motor in order to change the operation mode. The deadlock was apparently frustrating Scardino the most. She was considering leaving Nokia already in 2009.Many of those interviewed for this book consider it odd that the board appointed Ollila to be the main headhunter for the new Nokia CEO, since he, after all, had been the one to choose Kallasvuo, who now had failed at his task. Other members of the appointment committee were Scardino and a Swedish consultant Per Karlsson, a long-term trustee of Ollila.Dame Marjorie Scardino (born 1947) is the first female executive to have made it to the FTSE Top 100 List of largest British listed companies: She was appointed CEO of the British publishing company Pearson. She joined Nokia Board of Directors in 2001 and was appointed vice-chairman of the board in 2007. Scardino is known as the Iron Lady of the Anglo-American publishing industry. She has also worked as a publisher of The Georgia Gazette as well as The Economist Group CEO. By the time she joined Nokia, she had had a prominent career in the traditional publishing industry but had no experience in internet-based industries nor had any in-depth knowledge of mobile business. According to two major Nokia stakeholders, Scardino’s input as a board member was weak.Karlsson (born 1955) has a background similar to Scardino. Ollila had requested him to join Nokia Board of Directors in 2002. He was a high level company consultant with a notable career. He was working at Boston Consulting Group until he joined Ikano Holdings, a financing company set up by the sons of IKEA’s founder, Ingvar Kamprad. Karlsson and Ollila share a common interest in finance.Out of the three members of the appointment committee, only Ollila had experience in the technology industry, but even he, according to many, was not in touch with the service-driven internet-age mode of operation.Spencer Stuart, the London-based headhunting company specializing in the information and communication technology field, drew up a list of Nokia CEO candidates in June 2010. Ollila and the team selected a short list of names: three from North America and two from Finland. Among the American candidates was the Canadian Stephen Elop. The Finnish candidates were Anssi Vanjoki, head of Nokia Mobile Solutions and Niklas Savander, head of Services and Devices.Vanjoki (born 1956) joined Nokia in 1991. He was a member of Ollila’s core team, the so-called dream team that led Nokia to its success. Other members of the team were Kallasvuo, Matti Alahuhta, Pekka Ala-Pietilä and Sari Baldauf. Before Nokia, Vanjoki had worked at the 3M conglomerate. At Nokia, Vanjoki had been heading Nokia Mobile Phones, Multimedia division as well as Markets division. During summer 2010, he was carrying out a task the board had assigned him, examining the research and development operations. At the same time he was in the process of generating a new strategy for Nokia, again assigned to him by the board, with Kallasvuo’s consent.Savander had been with Nokia since 1997, but his choice as the next Nokia CEO seemed unlikely from the start, despite his appropriate background in sales, marketing and services. He was deemed somewhat reserved as a leader both within and outside the Nokia organization.Having completed the candidate short list, the appointment team started to travel. The board was appreciative of Ollila’s efforts: “He did a huge job traveling and carrying the main responsibility.” Ollila flew on a private plane from Helsinki to San Francisco to interview the foreign candidates. He had set up the schedule in such a way that he could meet five candidates in three days. Three of the interviews were in East Palo Alto Hotel in Silicon Valley — the first at breakfast, the second over lunch and the third in a meeting room. He then continued to Microsoft in Redmond to meet with Elop over breakfast in the privacy of his suite. In the afternoon, Ollila flew east to Southampton to meet with the fifth candidate over dinner.It was no easy task to carry out. In Finland he was a king, but in Silicon Valley he was the chairman of the board of an outdated technology company.To fully appreciate Ollila’s difficulties in carrying out the task, one has to be aware of the perception of Nokia in California. An American analyst believes that the board overestimated its chances to attract a top American corporate executive for the next Nokia CEO. California is at the core of America’s own cell phone service technology CDMA (Code Division Multiple Access). “Everyone there is in the system-on-chip manufacturer Qualcomm’s camp. All Google and Apple executives, are close to Qualcomm and live in the CDMA-world. These people have always seen Nokia as a dinosaur representing the European GSM (Global System for Mobile communication), the past. From California, the view is somewhat different to the one from Espoo.”In his memoirs, Ollila mentions being unsure, as he was returning home, whether the new CEO would be found amongst those interviewed. Fairly quickly the US list of candidates was reduced down to two names. The number one candidate was the number two man in a well-known technology company. According to Ollila, he was an executive in his fifties and who had been with the company for a number of years, having risen to his position through various roles in the company. Ollila says to have met with this candidate twice. He sees this candidate as the right choice — he was well acquainted with the technology companies in the world, and both his leadership style as well as his values were suitable, but after long consideration, the candidate withdrew from the process for personal reasons, not Nokia, claims Ollila.Various number one candidates have been speculated on, both in the media as well as in the interviews for this book. One is Apple’s current CEO Tim Cook, who in 2010 was Apple’s number two man. Based on our interviews, Cook very likely was part of the process and a candidate for this job. He fits Ollila’s description quite well.Cook joined Apple in 1998 and was 49 at the time of Ollila’s travels, which could be interpreted as “in his fifties”. The fact that he was also a member of Nike Board of Directors only added to his suitability.The other name proposed as Ollila’s number one candidate in the interviews was Sun’s Jonathan Schwartz. He does sound like a good candidate, but doesn’t quite fit Ollila’s description. He was only 44 years old at the time and had already resigned as Sun’s CEO in February 2010, which doesn’t fit Ollila’s description of a number two man, despite the fact that Oracle had purchased Sun around that time.An American reporter, David J. Cord, living in Finland, wrote in his book The Decline and Fall of Nokia that Ollila’s number one candidate would actually have been former Sun CEO Scott McNealy. The media was widely spreading this idea only to discover that McNealy had already a few weeks earlier denied this in a tweet that leaves no doubts: Ollila had never asked him to run Nokia.The wildest guess was Yahoo’s then CEO Marissa Mayer, a former Google executive with Finnish origins. Mayer had a small child, so her family situation would have prevented her move to Finland. But she surely does not fit the description of “a man in his fifties”.We believe Ollila’s description of the number one candidate is true. What possible reasons would he have had to share false information? He could have just as well left that part out altogether.In order to understand the selection process, it is important to know who were the ones making the decision.In 2010, the Nokia Board of Directors consisted of six other members in addition to chairman Ollila, vice-chairman Scardino and Karlsson.Lalita Gupte (born 1948) was the chairman of the board of ICICI, an Indian financing company. She had solid experience in both operational and official posts in finance.Bengt Holmström (born 1949) was professor of economics in MIT. For a Finnish researcher, he was an exceptional academic superstar.Henning Kagermann (born 1947) was the former Chief Executive of the German software company SAP. He was a top name in European technology industry in his time.Keijo Suila (born 1945) was the former Chief Executive of Finnair. He was one of the most valued corporate executives, one who in the past had also worked in several senior positions in Huhtamäki and in Leira.Isabel Marey-Semper (born 1967) was an executive in the French cosmetics company L’Oreal. She was experienced in matters of high level strategy, corporate planning as well as intellectual property rights in European companies.Risto Siilasmaa (born 1966) was the founder and chairman of the board of F-Secure, a Finnish information security company. Siilasmaa was one of the highly respected corporate executives within the technology industry in Finland.The lack of technological competencies stood out in this crowd. Only Siilasmaa was representing the current information and communication technology. The other expert in the field, SAP’s Kagermann, represented the older generation. The honorable German gentleman is not likely to have spent his time in the prevailing techno scenes. Holmström was moving in respected technology circles in the US, but as a researcher. Ollila had valued finance and consumer business experience in IT over technology when forming the board. Karlsson and Gupte had finance backgrounds, Suila, Scardino and Marey-Semper were experienced in consumer business. Having Scardino and Marey-Semper as board members for a high-end technology corporate like Nokia, struck many of those interviewed for this book as rather odd because their mobile competence was scarce. It is also worth noticing that there is hardly any public data on Marey-Semper.One could ask, why weren’t there any other type of competencies present in the Nokia board? A technology start-up entrepreneur? Venture capitalist? Someone with up-to-date connections to network providers, subcontractors, and, above all, mobile device consumers?Horace Dediu, an analyst who is well acquainted with both Nokia and Microsoft, points out that with Nokia competitors, the boards mainly had advisory roles. For example, the biggest shareholders, Sergei Brin, Larry Page and Eric Schmidt had the strongest voice in Google. According to Dedieu the American technology companies are not willing to render power to financiers or other outsiders, because that would weaken the disruptive thinking that defies and questions existing structures. The most distinct example of disruptive thinking and the role it plays is the legendary founder and Apple CEO Steve Jobs. Dedieu believes the Nokia board was professionally managed, but instead of focusing on vision, it focused on optimizing.Let’s take a look at the Apple Board of Directors in 2010 for comparison. It was chaired by Arthur D. Levinson, chairman of Genentech Board of Directors. Pharmaceutical and technology businesses have similarities, such as R&D at the core of operation. Innovation as well as protection of intellectual property rights are both of utmost importance. One member of the board was Bill Campbell, chairman of the board of software company Inuit, with a long standing career in the software business. The technology industry was represented also by Ronald D. Sugar, chairman of the board of Northrop Grumman, an aviation and aerospace technology company. Al Gore, the former Vice President of the United States, was there to manage high level public relations. At Nokia, the former prime minister of Finland, Esko Aho, had the equivalent role as a member of the Nokia Group Executive Board. The consumer point of view in the Apple board was represented by Millard Drexler, the chairman of the board of the clothing company J.Crew.The Google Board of Directors in 2010 had six other members in addition to Page, Brin and Schmidt. L. John Doerr, a venture capitalist specialising in technology industry and a former executive of the Amazon online store, John L. Hennessy, a professor of computer science at Stanford University and the founder of Atheros, a semiconductor company, Ann Mather, a board specialist focusing on gaming and internet business and a former executive at Pixar Animation Studios, Paul S. Otellini, a former CEO of the semiconductor company Intel, K. Ram Shriram, Sherpalo Ventures CEO and a professor of biochemistry at Princeton University, Shirley M. Tilghman.Two of Nokia’s fiercest competitors, Apple and Google, obviously had boards more competent in global technology and internet knowhow than Nokia. To aggravate the situation, the Nokia Board of Directors was manned more with fine titles than substance. Scardino was the only American on the board despite the fact that the highest level of software competence was found in the US.Would the Google and Apple boards have chosen Elop as the Chief Executive Officer? Hardly. To them, Elop represented the bygone world. He had no knowledge of consumer business and came from Microsoft, a dinosaur that had failed to progress from the PC to the mobile environment.The board members were aware of the great responsibility on their shoulders. What they most wanted was to get rid of the deep feeling of frustration. Moreover, all progressive work had come to a halt because of the ongoing replacement of the CEO. Therefore, the recruitment was swiftly processed. When Ollila’s number one candidate declined, the only ones left were Elop and Vanjoki.According to the magazine Bloomberg Businessweek, Elop’s experience and his CV had impressed Ollila. Elop had been leading Microsoft’s Office business worth $19 billion, one of the world’s biggest and most profitable business enterprises. Elop also had a reputation of not being afraid to take the bull by the horns and of being able to solve internal conflicts.As a matter of fact, Elop had already made an impression on Nokia leaders in 2009 when Nokia and Microsoft were in negotiations over provisioning of Microsoft Office applications in Nokia Smartphones. The negotiations had proven difficult. Nokia was at its peak, and Microsoft was known for their inflexibility. Problems arose right at the very beginning, says one of the Nokia leaders. At 9 a.m., an army of Microsoft lawyers marched into the meeting venue Nokia had chosen. A porter at the reception requested them to sign a traditional piece of paper to enable him/her to present them with visitor passes. Something in the wording of that piece of paper was not to the Microsoft lawyers’ liking, and in the end, they were allowed in without visitor passes. That day of negotiations had an unpleasant start.The negotiations carried on as they started, with difficulty. However, in April, a Nokia executive Kai Öistämö and his team had met with the Microsoft negotiating team who were now led by Elop. He had made a good impression on Nokia managers with his frankness and eagerness to solve things, and he wasn’t being political about anything. He had given an impression of himself as being a strong leader and a master of words. His Finnish counterparts perceived Elop’s demeanor as familiar and pleasant. On the eve of May Day, much to the surprise of both parties, there was a breakthrough in the negotiations and the agreement was signed later on in the summer.The news of Elop’s performance in these negotiations must have reached the ears of those who were now in the process of electing him as the new Nokia CEO.Vanjoki had many supporters both within and outside the Nokia organization. He knew Nokia and its reference groups like the back of his hand. In August 2010, it looked like the scales were about to tip in his favour. The board had not yet made the final decision, but the outcome seemed almost certain. The new CEO would be Finnish. The strategy work assigned to Vanjoki would not go to waste. A new era was on the horizon for both Vanjoki and Nokia.By September 10, 2010 the tables had turned. Elop had after all been appointed as the new Nokia CEO. What happened during these few weeks?The main driver in the events was Scardino. She was the spokeswoman on the board for the foreign shareholders, in particular for the American pension fund investors. As a member of the appointment committee, she was the natural point of contact for the American pension funds that were dissatisfied with the progress Nokia was making. For the foreign shareholders, Vanjoki was not a sufficient guarantee for renewal to take place. A bigger shake-up was needed, and the shaker needed to come from outside the Nokia organization. Scardino told her colleagues that only after talking to Elop did she realize the gaps Nokia had in understanding the new era. Scardino was the one to tip the scales in Elop’s favour at the last minute, the appointment committee presented Elop as their preferred candidate.The Nokia Board of Directors were between a rock and a hard place, says an analyst who has studied Nokia for a number of years. They were forced to prove to the American investors that Nokia was no longer just a Finnish company. Although Nokia shareholders were spread across the globe, from the American point of view too many of Nokia employees were still based in Finland. The investors could only be assured by a big move: Either transfer Nokia headquarters to United States, or, appoint a non-Finn, preferably a North American, as the new CEO. By choosing Elop, the board could keep the headquarters in Finland.Another analyst, who is very familiar with Nokia, believes that also Elop’s excellent command of words as well as his seemingly impressive background with Microsoft worked in his favour in addition to him being a North American. Those appointing him were hoping to get a charismatic frontman like Steve Jobs. Elop’s connection to the Windows operating system was not likely to have weighed in the negotiations. Had this been a factor in the recruitment process, the Nokia operating system strategy would not have been so drastically changed as it eventually was, says the analyst. But he/she does think it is possible, that the American shareholders pressured the board to choose someone from a software company like Microsoft.According to him/her, no one in United States considered Elop for the job because his CV was not suitable: He had no in-depth mobile competence nor consumer business understanding. By appointing Elop, Nokia showed just how far to the margin it had drifted. There simply weren’t any A-list candidates available. If there were no suitable candidates with software backgrounds available, the next best choice would have been to appoint someone with a telecommunications background either from a chipset company, a network provider or a competitor, suggests the analyst.The board was concerned about Elop’s commitment. Would his family join him? Finland was far away and a different kind of environment. Elop’s response was that this has been discussed with the family, and that it would be a good solution for everyone concerned. Elop was considered sincere about it, but what about after he has been travelling 200 days yearly for a few years? Other concerns were raised. What about him not having experience in consumer business? Some members of the board were bothered about his tendency to speak quickly. Would he be able to listen, would he get people onboard or would he be a solo artist raising himself above others?The board was aware of Elop’s history of job hopping. They considered it to be a normal feature of American business culture, deeming the Finnish business culture to be closer to the Japanese one. The new era of steep and fast changes required agility and new ways of thinking. The board believed Elop had these capabilities.In the end, the decision was unanimous. A person involved in the discussions says that Vanjoki was considered an enthusiastic, bubbly and innovative personality, but that he was also considered a somewhat contradictory character, even within the organization. Vanjoki has historical baggage, unlike Elop, and the board thought it best to emphasize renewal. In retrospect, whether the choice was right or wrong, at the time of decision there was a clear logic to it, points out a source who was following the process closely. For years there had been questions about Nokia’s strategy for entering the US market, with nothing but uneasy glances as a reply.In August 2010, the Nokia Board of Directors made the final decision. As a result, Vanjoki resigned two days later. Apple’s Jobs called Vanjoki asking him to work for Apple, but Vanjoki declined. He was not going to be just another hired executive.The commentary following Elop’s appointment was cautiously optimistic. His merits were considered good, particularly his communication skills, experience in software business as well as the fact that he was North American. When interviewed by the largest Finnish newspaper, Helsingin Sanomat, the new CEO said he was fully aware of Nokia’s dominant role in Finnish society. Elop went on listing characteristics he considered typically Finnish: Openness, integrity, transparent communication, ethics and respect for other people. Naturally, ice hockey as well as the salty liquorice, “salmiakki”, were mentioned, too.The news editor-in-chief, Mikael Pentikäinen, wrote in his article the next day that, based on his background and characteristics, Elop had every chance in succeeding in his task: “Everyone in Finland is wondering, if Elop will move Nokia out of Finland, but there is no indication of that. Elop will move to Finland, and Ollila, who will continue as chairman of the board of Nokia, will continue to maintain Nokia as the flagship of Finland’s economy. There is every reason for us Finns to believe that Nokia will get a strong, new beginning with Elop now in the lead. The better Nokia succeeds, the stronger Finland and its economy will be.”The commentary of Nokia personnel in the media was moderate, nobody wanted to dismiss the new boss straight away.Enthusiasm for ice hockey as well as his software competence worked in his favour. Concerns were raised with regard to Finland’s districts, if the Finnish ties were to weaken now that the CEO was a non-Finn. Local newspapers were even more concerned about the various Nokia sites across Finland. For instance, the Kaleva newspaper in Oulu wondered what will happen to Nokia’s functions in Finland with a Canadian heading the company. “What would happen to the Nokia sites in Salo and Oulu?”, Kaleva asked.The news of the replacement of the Nokia CEO reached international media. According to the German Frankfurter Allgemeine Zeitung newspaper, Elop was Nokia’s last chance. The British Financial Times did an interview with Elop and Ollila, in which they rejected the idea that Nokia would abandon its own operating system. Ollila stated that Elop had not been hired to renew the Nokia strategy.There were more doubts expressed in the American media. In Seattle, Elop’s home town, the Seattle Times pointed out that Elop was the third high ranking officer in Microsoft to have left the company within a year. The newspaper did an interview with Rob Enderle, an analyst, who thought Microsoft lost a great talent. According to Enderle, Elop had high hopes for the position of CEO, but that at Microsoft, there was only a slight chance at this. Jim Cramer, a host of the Mad Money program at the financial news channel CNBC joked: “It doesn’t matter who Nokia hires, short of Steve jobs, it still won’t save the company from obscurity. There is no way to make a comeback to the mobile phone market. Nokia’s biggest problem is that the company isn’t on the radar of the key US demographic that decides which cell phones will sell and which won’t — teenagers. Our teenagers don’t know jack about Nokia and this guy from Microsoft ain’t going to change that.”The Wall Street Journal believed Elop’s primary task would be to ensure a convincing competitor to Apple’s iPhone, since that is where Kallasvuo had failed. According to WSJ, the software executive would need to navigate through “a group of cultural and institutional underwater rocks” and that Elop was known as a pragmatic and decisive leader, who could turn large entities into smaller, more manageable parts. However, the American newspaper did have doubts about whether Nokia’s actions had been bold enough: The head of Microsoft’s business division was not the most obvious choice to speed up Nokia’s business integration or image renewal nor was he used to being the underdog. New York Times thought the appointment of a Microsoft executive was telling a tale of Nokia and Microsoft working more closely together than before. The newspaper reminded of Microsoft having similar innovation issues to Nokia, and mentioned that Nokia had failed in building profitable business relations with four of America’s biggest telecom network providers, which put together were selling over 90% of mobile devices in the US.The mobile nation was eagerly waiting to see if the new CEO would make an appearance at Nokia World in London, one of the most important events for Nokia stakeholders, on September 14, a week after the announcement. The event was considered particularly exciting for investors. The audience was curious to hear Vanjoki’s announcement to leave Nokia. He was an executive valued by investors, customers and reporters, who were accustomed to hearing bold statements from him. At his farewell appearance, Vanjoki presented Nokia’s new Communicator. Seemingly cheerful, he thanked the Nokia World audience for the 20 year journey and made his exit from stage, as they applauded.In addition to the new Communicator, Nokia launched four new smartphones. The executives were doing their best for Nokia’s credibility.Due to the replacement of the CEO, the main speaker in London was Niklas Savander. He pointed out to the audience that Nokia was selling 260,000 new smartphones daily, which was more than Apple and Android put together. Savander promised a sale of 50 million devices for the models presented in London. He also thanked Olli-Pekka Kallasvuo for a fine 30-year career in Nokia.Kai Öistämö, Nokia’s chief development officer, countered concerns arising from the appointment of a Microsoft man. Since Öistämö knew Elop well from before, he was sure Elop would adapt well to Nokia.A large customer also spoke at Nokia World. Vittorio Colao, CEO of Vodafone, the British network provider, was of the opinion that the best markets for device manufacturers as well as network providers to be in were in developing countries. Colao complimented Nokia on its ability to survive the smartphone battle and said he was well pleased with the ambition Nokia was showing.The day after the event, Elop did make an appearance after all. He met with customers but not the media. Officially his duties would not begin until the following week.4. The lame legacy of Mr. KallasvuoOlli-Pekka Kallasvuo left behind him an organization in which three corporate divisions — smartphones, feature phones and services — competed for the resources, power and attention. The smartphone unit would have needed support from the services unit, but came only second in the pecking order after external paying customers.The product portfolio of the company was exceptionally large. This strategy had worked well while business was still blooming, even if only a small part of the company’s vast product range was successful, those best-sellers brought in enough money for the business to be successful. By 2010 the vast product range had become a burden. There had not been a best-selling product in several years and the situation had started to gnaw at the sales staff, especially. The company had in its hands a huge number of products that did not sell well. The still high sales volumes were blinding. Attention was focused on the positive fact that the company was selling 400 million phones annually even if the majority of the sales volume came from 30 euro basic phones which had next to no impact on the bottom line.The constant delays in the phones-to-market schedules increased the burden. The prototypes of feature-rich lead products were developed fast, but the completion and testing for the mass market entry took too long. Management time was wasted in the meetings that focused on minor details such as a minor software adjustment. Sometimes more than ten vice presidents were present in such meetings. The product schedules were perpetually delayed until it became evident that demand for such products would no longer exist at market entry.The situation was worst for the company’s biggest money maker, its smartphone operating system Symbian. With over 6 million lines of code, the software platform had become unmanageable. Hardware design and Symbian software development were almost in a state war and were at each other’s neck daily. Time, money and mental resources were wasted to tweak the outdated Symbian for each product. There were so many product lines that the product managers could not manage to keep up-to-date what was going on.Although considerable strategic weight was given to the software development and services, Nokia, in essence, was a pure hardware manufacturer in regard to its profitability, money-making mechanisms and operating principles. After all, software and services accounted for less than 1% of its revenue.Up to then, the company had managed to cover its costly software in the phone pricing, but now this strategy no longer worked as competitors had started to launch phones of superior quality.During the Kallasvuo era, the confidence of investors and shareholders in Nokia’s management had waned from initial euphoria to next to nothing. A Nokia analyst at an American venture capital investment company remembers having a critical view on the capability of Nokia to switch over from basic phone business to smartphones. The analyst considered Nokia to be very vulnerable with its “institutional baggage” in the form of 130,000 employees together with Nokia Networks (NSN) and with the majority of the employees being located in Finland. The analyst also states that Nokia was focusing on the wrong technology platform and using billions of euros to its software development.Nokia was more vulnerable compared to its competitors. Korean Samsung, as a conglomerate, manufactured computers and other electronic devices in addition to mobile phones, and was therefore not so susceptible to suffer from a slowdown in one of its product segments. Samsung was able to sell its mobile phones for retail businesses at a lower wholesale pricing, as their transactions also included other products than just mobile phones.Apple secured their profitability with expensive Mac PCs and iPods at the time when iPhones were not yet bringing in much revenue.According to many interviewees, Nokia as an organization had drifted into a state of inertia. Elop would soon find himself in the middle of a battlefield of middle-aged men. Instead of external competition, the competition was internal. Common interest had been replaced by the optimization of the vested interest. The famous Nokia-spirit was had begun to ebb away. Constant organizational changes confused the working environment as employees had to reapply for their positions. People were somewhat arbitrarily transferred to new positions. There were employees, whose projects had been ed, but they got to keep their jobs.The matrix organization structure played a key role in the management problem: People were part of a project under different teams, but nobody had an overall responsibility of the end product. The team spirit killed any individual creative spirit. Ideological and innovative individuals were labeled as lone wolves. Yes-men with no opinions of their own would flourish. For example, the normal trial-and-error software development technique was no longer used in Symbian software development. A person who was in charge of software development says that the problem was in the management which adjusted and fine-tuned projects ad nauseam. Even according to Nokia’s internal evaluation, the projects with the least management level involvement were the ones best on schedule. When the engineers were left alone to do their work, the results came forth.Mikko Kosonen, the former Senior Vice President of Strategy and Business Infrastructure in Nokia and currently the President of the Finnish Innovation Fund Sitra, wrote a book on strategic agility together with Professor Yves Doz of INSEAD, a top-rated European business school. [7] In principle, strategic agility existed, but in practice it was only a dream. The lack of strategic agility and rigidity resulted in playing safe. In the technology driven business, that marks the beginning of the end.When talking about the working environment atmosphere, many mentioned the word ‘fear’. Fear of losing one’s job or position kept their mouths shut even when something should have been voiced. A sugar-coated picture was given to the management. An employee working in the strategy department resorted to check the true status of upcoming phone projects from a friend working in development, because the official status given could not be trusted. Nokia was the emperor with new clothes, but nobody dared to say it out loud.The layoffs had started in 2008. When money was becoming an issue. The organization had been streamlined many times over, but the scope of the operations remained unchanged. At every decline of the financial outlook, streamlining continued. There were divisions which had been fully reorganized 3–4 times within a year.The Group Executive Board was equally stagnated. Niklas Savander, Kai Öistämö and Tero Ojanperä had shuffled their roles several times, but nevertheless stayed in the company. According to an outdated Nokia principle, it was considered beneficial for the executives to hold several different positions to increase their competence. During the growth era the principle had worked.But when the phone sales started to decline, new people and fresh ideas would have come in useful.The company had gone to the dogs, at least partially. But what would the customers think of the situation? Elop knew that the feedback was not going to be good.The customer base was divided into two. Network providers traditionally had long-term commitments with phone manufacturers and they continued selling Nokia phones like business as usual. For several years, European network providers had enjoyed economic growth in the wake of Nokia’s success and were therefore more inclined to overlook the problems their trusted business partner had started to experience. The feedback from the large electronics companies and other retail businesses with shorter order cycle was more hard-edged.For example, the French retail chains Carrefour and Océan started to question Nokia’s famous customer orientation. The French retail chains were wondering why Nokia force-fed its own music applications and other applications to its phones even if the customers wanted iTunes or Spotify. Nokia had not entered into strategic alliances with service providers, because it believed that it can produce such services by itself. According to a former Nokia sales director, Nokia should have integrated popular services such as Spotify into its phones and advertised to its consumers how the services worked best in Nokia phones. Instead of doing this, a lot of money was spent to fight against such services. As a device manufacturer, Nokia was not as agile in the service segment as the service providers.Network providers were also slowly awakening to reality. They were worried about the inflexibility of Symbian which meant that it was not a popular platform among application developers. That could not be overlooked as Apple’s iPhones and phones based on Google’s Android were now used as a new baseline for phones. Network providers compared the data usage of smartphone users. Users of Samsung Galaxy running on Google Android used ten times more data compared to the highest data users with Nokia phones. So the users of Galaxy, which offered a seamless user experience, stayed in the network using data applications for much longer periods of time. And the network providers started to be more insistent in demanding to know what Nokia was going to do to increase the data usage in their phones.After the initial hiccups, the popularity of iPhones’ skyrocketed 2009. When Elop was looking at his Nokia playing field in the end of 2010, iPhone had already become Nokia’s biggest competitor. It caught Nokia off guard and happened unnoticed while Nokia had closely watched its traditional competitors, the phone manufacturers.Nokia had lost a big chunk of its smartphone market share. According to Strategy Analytics, Nokia’s market share had shrunk to 34.4% by the summer of 2010 whereas in the beginning of that year its market share had still been at about 38%. Nokia had put a record number of 26.5 million smartphones on the market, a whooping 61% more than one year before, but it still was not enough to retain its market share in the skyrocketing smartphone market. 77 million smartphones were sold worldwide during the summer 2010. That was a record 78% more than year before.Apple had started with low production volumes, but was increasing its volume quarter by quarter. In 2009, the production volume of Nokia was triple the volume of Apple, but in 2010 only double. It was exceptionally peculiar since Apple had only one smartphone in the market while Nokia had tens. During the summer of 2010, Apple reached the second market position with its 18.5% market share for the first time. Third market position was held by RIM whose Blackberry phone had 16% market share.Nokia’s strongest market area was Europe where Nokia dominated the smartphone markets. Nokia’s strength in Asia and Latin America was its ability to launch durable and affordable feature phones. In these market areas, the Nokia brand was strong and Nokia’s distribution network seamless. In those markets, the status quo would be good enough. However, in the United States Nokia as a phone brand was practically non-existent. Elop realized that starting with a clean slate was the only option in the US. There was also a lot of baggage as Nokia had alienated the American network providers with its arrogance. American network providers were not dependent on Nokia to the same extent as their European counterparts, who had huge numbers of Symbian smartphone users as their customers. In an interview with Helsingin Sanomat in October 2012, Jorma Ollila admitted that Nokia’s strategy in Silicon Valley had proved to be a complete flop. “The Mobile Phones unit had 1,000 employees in Silicon Valley and their main task was to follow the latest trends in the software development. Google and Apple did a better job at it. It was Nokia’s biggest failure.”Expenses were watched over carefully after Kallasvuo’s rein. The Financing and Purchasing departments had much leverage as it was the rigorous cost control policy that was behind Nokia’s initial success. During the low-yielding years, expenses were controlled even more rigorously. R&D costs had been cut heavy-handedly. The dominating role of Financing department had been established during the Ollila era and was further reinforced during Kallasvuo leadership.This ideology of extreme efficacy was causing difficulties. According to the platform based R&D, devices and software utilized as many of the same mechanical and software components as possible. Ideally, hundreds of different smartphones were produced using only two to three different platforms. Software was also built based on software platforms and different features were added on top of the base platforms. This operating principle was both efficient and cost-effective. According to a manager working in the middle-management of the Symbian and MeeGo platforms, what was gained in cost-efficiency was lost in inflexibility. The overall budget was not to be exceeded even if using a slightly more expensive component would have been advantageous for a better end result.According to a manager, too much attention was paid to small segment earnings instead of looking at the big picture. Costs were controlled by projects and units and some projects were terminated even if it had made sense to keep them up and running to be further developed in other units. Plenty of babies were thrown out with the bathwater.Cost control was further intensified when the company started using more consultants. A manager formulated it like this: “A US-based consultant looking into saving a mere dime, was more occupied in optimizing his own business rather than that of Nokia”.In all this gloom and doom mentality, the new CEO was about to find some positive surprises in Nokia. The challenger attitude was still alive and well within the company. It had been dormant and buried deep, but was brought back to life by the crisis. The Finnish work ethic can be characterized by the solicitous and pedantic work attitude. One manager in the company’s top-management described it as “manic fear of failure”: Every little detail was checked and rechecked over and over, and even after doing so there was still the shadow of doubt if everything possible had been done. According to this manager, this attitude was prevalent, irrelevant of the fact whether the company was doing well or not. This philosophy, allegedly dating back to Ollila, was deep-rooted.Positive in the situation was also that the low-end phones were still yielding profits at a steady pace in the developing countries. The Nokia brand was strong in India. Nokia was still challenging the local cut-rate phone companies in China. The low-end low-cost phones seemed to be the lifesaver when the times were hard: The steady cash flow from their sales was to keep the profitability at a tolerable level.Many were of the opinion that Nokia’s Ruoholahti Campus in Helsinki was the place to watch for. The MeeGo unit developing smartphones based on open-source software had 2,000 top software engineers developing something that could be the next big thing in software engineering. Great expectations were laid upon the first MeeGo phone and before the end of his leadership, Kallasvuo had removed many obstacles from MeeGo’s path.In production and logistics Nokia was world-class. Nokia’s industrial engineering techniques had been synchronized in the beginning of the 21st century to the extent that Nokia could easily move production batches from one continent to another wherever production capacity was readily available. This operating method was based on the innovative dfm (design for manufacturing) process developed by Nokia. Engineers had designed the details of the manufacturing process with extreme care to avoid any unnecessary activities (/functions). This was of vital importance, especially during the peak years, when Nokia sold half a billion mobile phones requiring 120 billion components. In addition to the optimization of the logistics, Nokia’s mobile phone assembly was also tuned to perfection. Ideally, only 3–4 base units aka engines were used for all phone models in the manufacturing pipeline. During high demand, base units were always in stock, so the production could be started on the double. Some 150 types of covers, keyboards and other small components, and 300 types of sales packages were in use at a time. The components needed for the final stages of the phone production process were ordered with 24-hour lead time at its best. Suppliers were often located in the immediate vicinity of Nokia phone factories. There were no inventories as production runs were done to order.Seamless cooperation with the companies supplying production equipment and machinery further increased the efficiency. In more critical areas of production line, e.g. as regards the component mounting equipment, there was strategic cooperation with 2–3 suppliers at a time. Cooperation with fewer suppliers would have made Nokia more vulnerable and with more suppliers, less efficient.Citius, altius, fortius — faster, higher, stronger. This motto well described the everyday life at Nokia at the time. The efficiency of the engineering processes of the company was simply mind-blowing. Nokia also had world’s best know-how in the fields of radio technology, modem technology and hardware design.Elop would soon come to realize that Nokia’s sales organization was lacking. The sales technique adopted from Asia was applied globally. In a mass market area like India, large sales staff was required as there were tens of thousands of points of sale. In India alone, Nokia had 5,000 salespersons at its peak, whose job was to present the new phone models to independent retail dealers. These retailers did not have inventories, so Nokia sales staff was continuously restocking the points of sale.In Europe the wholesale market for mobile phones operated differently. Purchasing was done in a more centralized fashion. It was good enough, if the manufacturer had good relations with the purchasing directors of the largest network providers and consumer electronic retail chains. The sales staff in retail stores did not have influence on the retail selection. Nevertheless, Nokia still had a huge number of salespersons also in Europe. A member of sales staff visited 15 points of sale a day on average, mainly to do some chit chatting and to dust some retail phones. Bizarre performance evaluation metrics were applied to such sales staff: Visiting 15 points of sales a day constituted a job well done. A sales director earned the bonus by introducing the Asian sales model to the set number of countries even if the model was not viable in Europe.A former Nokia sales director now working for a competitor says that it was precisely this close relationship with network providers that got the Nokia sales system into a rut. For 15 years Nokia sales more than doubled. The same happened with key customers i.e. with the network providers. Both parties only focused on the positive outcome ignoring the weak signals of brewing troubles. The network providers did not know how to tell Nokia that their phone models were no longer appealing to customers. In 2010–2012, many members of Nokia sales staff still believed that everything was just fine, and that the next phone model launch would come and save the day.This sales director remembers proposing a sales technique change for two consecutive years. In his view, a more quality-based sales model would have been more viable in Europe. Instead of the army of phone-dusting sales staff, a smaller number of committed “sales reps” would have been used to visit retail dealers to organize well-planned and targeted sales campaigns and activities.Sales, just like many other functions, were plagued by too much complexity. Salespersons with direct customer contact were good at their job and knew their customers well, but they were given too many additional tasks that took time and energy from the actual sales work. Their immediate managers understood and supported them, but the decision makers were located far away, sometimes even on another continent. Even if there had been wisdom and goodwill in the workforce, the organizational system had made everything insurmountably difficult. The situation was aggravated by the plummeting market.The sales director mentioned that things are done differently by his/her current employer. When the head office gives marching orders, everybody follows suit. If the key product or product line sales are not up to par, feedback is given promptly. Additional funding for marketing is also allocated fast if needed. In a similar situation at Nokia, there was a lot of talk, but hardly any action. One’s money was not put where one’s mouth was.Nokia controllers considered Sales merely as a necessary evil and salespeople as an unruly flock that needed constant watching. From the point of view of salespeople, there seemed to be no common sense in doing things and progress has become extremely sluggish. The finance department just wanted to wait out the problems. “Before Elop, there was nobody in the company who would have had the guts to say that enough is enough”, says the director.Nokia had led the way in certain sales strategies, such as in online marketing. Nokia.com had grown exponentially during 2007–2009 when measured by the number of visitors. Online sales had doubled in six consecutive quarters. Right things had been done at the right time. This is substantiated by the fact that at the same time Apple reached the landmark of one billion online customers in its online store. E-commerce was a rapidly growing market. However, Nokia’s online sales had dried up as a result of the profitability issues that started in 2008. There were no resources to further develop the online sales and online marketing, even if the customer base had just started to move from brick-and-mortar stores to using online shopping and services.Jyri Engeström, a long-time Silicon Valley resident, is one of the few Finns who have been involved both in the development of the Nokia operating system and the Google operating system. In 2007, Google bought the social networking and microblogging service Jaiku owned by Jyri Engeström and Petteri Koponen.According to Engeström, Nokia and Google were as different as chalk and cheese. Nokia was then developing Maemo software by an outdated organization consisting of hundreds of people coordinated by low-level managers between various office sites. Google was developing Android by a small compact unit lead by the charismatic Andy Rubin, who had joined Google following a company acquisition, just as Engeström did. In 2007, there was a narrow window of opportunity for Nokia to enter into collaboration with Google and according to Engeström Nokia should have seized the opportunity then. Engeström says that Nokia’s belief in the superiority of its own software development manifested in arrogance and diminished Google’s interest in partnering with Nokia regardless the fact that Google valued Nokia as a similar trendsetter in hardware design then as Apple is considered now. The decision by the then Chief Technology Officer, Tero Ojanperä, to set up Nokia’s US headquarters in the White Plains suburb of New York was of big symbolic significance and raised eyebrows in American software development circles. The question was: Why did Nokia ignore Silicon Valley?A manager in the Nokia smartphone product development recalls that when Android was just emerging in 2007–2008, Nokia had been sneering at such a small-scale American project. Android was not taken seriously as its developing teams in Silicon Valley were small. It was believed that there was no way such small teams could compete with Nokia’s large developer base of thousands.Elop had now the task of prioritizing the actions to be taken in the wake of the lame legacy of Kallasvuo. Cutting the expenses with a heavy hand was to take place. A sensible operating system was to be chosen for the smartphones. US operations needed a makeover.5. The wonderboy from AncasterThe small, picturesque town of Ancaster is one of the first European settlements in Ontario, Canada. The area is known for its historical downtown and good hiking paths. There are around 30,000 residents. The weather is like in Helsinki. Because the Great Lakes keep the winter relatively warm, the average temperature in January is -5 ºC (23 F). In July, the average temperature is 22 ºC (72 F). Over the years, Ancaster has grown to become part of the ninth largest city in Canada, Hamilton. The nearest metropolitan city is Toronto, 70 km (44 miles) away to the northeast. Equally far away to the west is Waterloo, where the mobile phone manufacturer RIM (later Blackberry) started its activities in 1984. If you head southeast, after 100 km (62 miles) you end up in Buffalo in the United States, and you pass Niagara Falls along the way.Stephen Andrew Elop was born in this environment on December 31, 1963. His father designed transformers at the electrical company Westinghouse. His mother was a chemist. As the middle child of three boys in his family, Stephen had a normal, middle class childhood. His first job was as a caddy at Hamilton Golf & Country Club, and according to legend, he learned how not to laugh at people who are trying their best. At the same time he developed a hatred toward tobacco. Nothing was more disgusting than when someone asked him to hold a cigarette stained with lipstick when they went to swing at the ball, Elop has stated.His free time was dominated by his interest in technology. Even his grandfather had worked as a radio operator in World War 2. He chose the Hamilton Faculty of Engineering at McMaster University in Hamilton as his place of study. The university, which received its name from the founder, William McMaster, is one of Canada’s upper middle-tier universities. In the worldwide QS university ranking, in 2013 it was ranked at 140, the fifth best in Canada. The University of Helsinki was 69th in the same ranking, and Aalto University at 196th.The year was 1981, when the eager budding engineer started his studies. Besides studying, he wrestled with 30-hour work weeks. Professor of computer engineering, David Capson, remembers Elop as the character who walked into his office and past him carrying a ladder. Elop climbed up the ladder and peeked up in between the ceiling tiles. In his dirty hands was a spool of cable. He was building a new and exciting thing, an ethernet network that covered the whole campus. 22 kilometers of cable was used, according to press reports. Capson had had hundreds of students, but he says that Elop had left an impression on him. Elop was exceptional and well-focused, one of the two best students that year, Capson remembered.Elop met his wife while doing computer work at the university. The two of them had different opinions on how computers work. The disagreement led to a bet.“It was just flirting: If I am right, I will take you to dinner. Nancy won, so we went to a rib restaurant. Not very romantic, but a good start. It led to marriage”, Elop told the Forbes magazine in 2005.Elop graduated in 1986 as the second best in his course. He has less education compared with many other top leaders. Five years of hard work brought him a bachelor’s degree in computer engineering and management. It was now time to transition into work life.After that, things started to happen. Elop developed into a successful, sales-oriented leader who understood customers. When Elop stepped into the business world, information technology was revolutionizing the workplace. Apple and Microsoft were hot topics. Secretaries exchanged their typewriters for desktop PCs and companies started appointing IT managers. Elop joined a small software company called Soma. Their first success came quickly, when Lotus, who were known for their 1-2-3 spreadsheet program, bought Soma for a good price in the early 1990’s. Elop continued working in the consulting unit of Lotus, until 1994 when he moved to the fast food chain, Boston Chicken, as the Chief Information Officer.His first steps as a leader in a publicly listed company were colorful. The American company, Boston Chicken, was a fashionable company in the US during the mid 1990s. A year before Elop arrived, shares were sold out immediately after a share issue, and the share price had tripled. Things had already overheated too much, and finally debt and the new chicken counters which had appeared in grocery stores forced Boston Chicken to apply for protection from creditors under the US bankruptcy laws. Before the bankruptcy, there were 18,500 employees and 1,100 restaurants. The euphoria had taken the company to the wrong side of the law. The lower level managers jumped from the sinking ship. One of them who had made the right conclusion was Elop, “We didn’t like what was happening in the company. There were good and bad times, but the food was good, at least”, Elop had recalled about the experience.At that time, Nokia had unveiled its first Communicator, and was bringing the second version to market.Boston Chicken is known nowadays by the name Boston Market, and was, from the time of the bankruptcy till 2007, under the ownership of McDonalds. The chain had profiled itself with the Boston marathon. There one can eat three whole chickens, two potato pies, eight pieces of cornbread, six side dishes and two desserts in under an hour without help.The train continued onward. It was the year 1998. The persistent, smart, and fast-moving engineer went to work in California and moved upward in the San Francisco software company, Macromedia, via the IT and sales departments to become CEO. Macromedia had given the world the web page design program Dreamweaver, as well as Flash, a multimedia technology which brought graphics and animations to web pages. There, Elop really started collecting the experience, which he used to charm during the Nokia times. The internet bubble had just burst. The bottom dropped out of the markets, and new competitors were threatening to take away their livelihood. Newspapers rattled on about how Macromedia was headed for disaster.Elop took focusing as his dictate. He reviewed the company’s strengths, weaknesses, even topics that seemed irrelevant, and decided to throw all of Macromedia’s chips into Flash. The change was huge, but afterward, Macromedia made a bigger profit than at any point before the bubble burst.During that time, Nokia was wrestling with cameras and Symbian. Both were announced in 2002. The first 3G phone was announced the same year, and the clamshell model in 2004.At Macromedia, Elop was instilled with a belief in big changes and a stubborn focus on a chosen strategy. The seed had been planted. Change was possible even in difficult conditions, when the correct products are chosen, and when one can see which direction the world is heading, was his reasoning. The association is easy to make: Windows Phone became his new Flash.In 2005, there was a merger ahead. Macromedia was bought by another American software company, Adobe, which we know from, among other things, the PDF document editing program Acrobat, and the professional photo editing software, PhotoShop. The merger was difficult. Seeing through the deal would take seven months because of antitrust officials. Employees were confused: What would happen to them? What about the products? Elop built from this a second leadership philosophy: Everyone supports everyone else, no one is more important than anyone else. Arms linked, everyone together.What happened? Macromedia had the best financial results during that seven months than it previously had during its entire history.After pushing through to the end with the merger, Elop continued with Adobe with the title WWFO, world wide field officer. He was responsible for sales, country-specific marketing, partnering and customer service. His last work day at Adobe fell exactly one year from the time he had started at the company. He received his yearly salary, $500,000, with bonuses of $315,000, and a severance pay of $1,880,000. His stock options were, at his moment of departure, worth $22,500,000. If his term had lasted less than a year, he would have not gotten the extras.During the same time period, Nokia announced its final successful flagship model, the N95, and Olli-Pekka Kallasvuo was the CEO.At his next employer, Juniper, Elop’s salary arrangement looked quite similar. His work ended on the same day that nearly $800,000 worth of options became free to cash in. Elop was clearly swimming in money. Next stop would be Microsoft.But first to Juniper. They manufacture network equipment, meaning hardware. Aside from software, Elop gained experience with hardware in this way. His title was COO, Chief Operations Officer. Even if Elop was not the CEO, on the headhunter lists he was already one of the absolute elite at this stage, and could definitely get a job in the senior management of any major IT company. The jump over to hardware manufacturing added to his value, even if there were no great achievements made during his year at Juniper.During this period, Nokia was at the top of its game. Its market share had surpassed 40 percent for the first time.Elop won the jackpot on the eve of Thanksgiving in 2007. The CEO of the software giant Microsoft, Steve Ballmer, called him and wanted to meet. They met. They talked about information technology, the change brought in by mobile phones, cloud services and Google. Elop sensed that Ballmer was interested in him, if there was a suitable position open. There might just be one opening up, so they met again after a few months. There were also other Microsoft leaders along, including Bill Gates.It didn’t take long after this, when Elop met Ballmer at the Kitchener-Waterloo airport near Hamilton. The men drove to Elop’s house. The family still lived in Hamilton. They all sat around the fireplace in the room downstairs. They talked the whole afternoon into evening. Nancy Elop was especially pleased with the visit. She was able to ask Ballmer what kinds of schools they have there in Redmond, Washington.A few days later, Ballmer called and asked Elop to come to work. Elop was stuck in a difficult quandary. He was supposed to start as the CEO of Juniper. There was already a briefing prepared about the nomination. Elop described the decision as the most difficult one he had made in his life up till that point. Ballmer won. Elop took over the leadership Microsoft’s Business Division at the beginning of 2008.Even if Elop was totally unknown in Finland when he was appointed at Nokia, in North America he had become a star. He led the largest division of the world’s largest software company. Contrary to popular belief, the Windows operating system is not Microsoft’s largest source of income. It is the Office software, which belonged to the Business Division. Windows brought in 27 percent of the sales, the Business Division 31 percent. The profit in the Business Division was almost half, in other words about 7 billion euros ($10.2 billion) a year. Turnover was 19 billion euros ($27.7 billion). It was estimated that there were half a billion users at the time.However, Microsoft faced a challenge with the Office suite of software that includes Word, Excel, PowerPoint, and a few other programs. Google had started to offer the same services over the internet for free. So users could make texts, spreadsheets, and slide presentations without paying for software. The work happened over a network connection to software on Google’s servers. The documents were also saved there. Elop started to work on a solution for this problem.He had a move to Redmond ahead of him, this time a real move. The Silicon Valley Elop had commuted from Hamilton by plane. Adobe had paid $145,000 during 2006 for Elop’s travels between home and work. Juniper had reserved $200,000 a year for his travels between home and work.The family had grown to 7 over the years. Having children was an adventure, and required a lot of persistence.Elop told the Kauppalehti Optio financial magazine, in an interview in the fall of 2010, how the family was able to have a second child after a long struggle in the 1990’s. It required calls to the US Senate, the Canadian parliament, and the Chinese government. The countries were in agreement on one thing: It would not work; don’t even try. The Elops pushed, persuaded, and negotiated. They did some hard work, until eight months later, the heavens opened up. The papers were in their hands and the Elops had gotten what they wanted: An adopted daughter. The wish came true a few weeks later, when the Chinese officials let them and their nine-month old bundle out of the country. The difficulties continued, however: The child needed a citizenship. Canada had a policy of not giving citizenship if the child lived outside the country. Elop lived in the United States because of Boston Chicken, and their adopted daughter was Chinese. The combination was too much for the bureaucrats. Finally, the Canadian prime minister decided otherwise. The issue was put to rest during his visit in San Francisco with the Canadian Governor General — in other words an official representative of Queen Elizabeth ceremoniously granted citizenship to Courtney Elop.Courtney, who at the time of writing this in 2014 was 18, and her big brother, 22, got triplet sisters for company, who are now 14. The father has said about the triplets’ birth, that the couple has maximized its capital invested into fertility treatment.Elop said how, in the middle of the 2000’s, he occupied his weekends with his children’s hobbies and playing ball. He recounted how he encouraged the children’s individuality. Each of the triplets got to have her own birthday party. Once they had a birthday party on the morning of the closest Saturday, another in the evening, and the third was on Sunday morning. Everyone invited their own friends, even if all the friends were the same.Despite all the commuting, Elop had bought a fabulous house in Silicon Valley. Its subsequent sale became a scandal when he started at Microsoft. In Microsoft’s 2009 financial statement, reporters found a footnote: “Mr. Elop received help with moving expenses, travels, shipping his belongings, in getting a temporary apartment, and in what he had to pay himself.” The statements showed an expense of $4.1 million. House prices in California had plummeted, but Elop was allowed to recover his losses. The sum should be compared with Ballmer’s salary from the same period: He received $1.3 million. Microsoft shareholders were furious, and the company finally had to change its policies in supporting its leaders.The Elops bought a house in Redmond in 2008, which had 8 bedrooms and 1,100 square meters (11, 480 square feet) of living space for just under $4 million. The house had, among other things, a tennis court and a wine cellar.During that period, Nokia built its first touchscreen phone aimed at the mass market, the Nokia 5800 Xpress Music, which went into sales at the end of 2008.Google Docs. The challenge was formidable. It was believed that Microsoft was doomed. How could a dinosaur from the past compete with an agile player in internet technologies?Only a few years passed by, and the arrangement had been turned on its head. Elop had built up free versions of Office, which were funded by advertisements. They had more limited functionality and could be used over the internet, but together with the commercial version, the experience was better than with Google Docs. This direct response to Google had demanded massive changes in Microsoft. Elop’s halo grew. Elop’s accomplishment, Office 2010, pushed ahead like a train and even surpassed expectations. He overtook Google without dropping the commercial version, and as a result made Microsoft a leader in cloud services.“Google can be beaten, Google can be beaten”, Elop repeated like a mantra during press interviews during that period. It’s not a wonder that Nokia’s headhunters contacted him.What kind of man did Nokia choose then? Everyone knew that a huge visionary like Steve Jobs would not come. Elop was a doer and change manager. He got the trains to run on time, but didn’t necessarily inspire anyone, were the appraisals. The office guy, pencil pusher, representer of his product. A general, and even his hair was short. A guy, who among nerds, starched his shirts. But full of energy and eager to work, using the well-known term 24/7. And loud.“Stephen is certainly not shy. He definitely says what he needs to say.”, according to one assessment. “It never felt like he was afraid to ask dumb questions”, said one colleague from Microsoft.Elop described himself during those times as passionate, vigorous, rational, decisive, and detail oriented. He recounts that technology is a part of himself, and he admitted that he feels at home in rooms full of engineers talking about programming strategies. At the same time, he was saying how he has passionately given himself to conversations about the feelings of consumers.What about charisma? Did he have it? One of the authors of this book met Elop for the first time one month before his appointment at Nokia. Microsoft had invited journalists to Amsterdam to show off their ways of working in their Holland office. The event was centered around a small seminar, where the main speaker had to be replaced at the last moment with a person from the US. Before it began, the speakers congregated in the front of the auditorium. One’s eyes naturally found themselves drawn toward a certain individual who had a charismatic aura to him, someone who was clearly a leader of some sorts. He was the Microsoft Holland director, Theo Rinsema. Elop was next to Rinsema. He fit into the category of “the others”, even if he seemed to be conversing a lot.When Elop got on stage, he spoke in a technical manner about cloud services. He spoke fluently, but not in a way that would blow your mind. When he offered an interview with the reporter after his speech, the answer was “no”. It was, of course, partially due to the fact that the purpose of the seminar was to get familiar with the office solutions, and Elop had come to the program by surprise. The main reason, though, was that his speech gave very little that could be used as material for a press article.Fortunately, a picture was taken. It would be of use in a few month’s time.For a hobby, Elop had mentioned flying. In the online publication of the Wharton School in Pennsylvania, it was noted that there were two types of pilots: One type wanted experience gliding in the air, the feeling of freedom when flying through clouds. For the other, the attraction was in the technology. They loved navigation, meters, and the software behind them. Elop said that he was in the latter group.The question was unavoidable: Would Elop be, after all, the correct choice for Nokia, troubled by its engineering-centric culture? Wouldn’t a more visionary and charismatic figurehead be needed? The virtual world is a long way away from leading people and managing the media game.The considerations were unnecessary. Elop was Nokia’s chosen one. It was time for him to get to work.6. Platforms and ecosystemsIn Finland, we’ve heard over and over how the very heart of Nokia smartphones, the Symbian operating system (OS) was mediocre, old-fashioned, and slow. Its intended successor, MeeGo was being delayed and delayed. Why couldn’t they get Symbian to work? What was the hold-up with MeeGo?The answers lie partly in technology — this chapter takes a brief look into that.First, the basics. A smartphone is a small computer. In order for it to work, it needs a piece of software called the operating system, just like a computer does. The OS in Apple iPhone is iOS. In Samsung’s most popular models it is Android. On Nokia’s smartphones, it was Symbian when Elop joined Nokia.The user interface (UI) is what the users see on their screens, and how they interact with their phone. Menus, icons, and keys are all part of the UI.Let’s take a brief look into history. Symbian was developed by telecommunication companies Ericsson, Panasonic, Motorola, Nokia, and Psion in 1998. In those days Microsoft Windows had a near-monopoly dominance in personal computer (PC) operating systems. PC industry players like Dell, Compaq, and others saw their profits being squeezed because, in practice, every computer in the market had to run Windows OS, and Microsoft was naming its price. This is why Nokia and its partners wanted to keep the mobile operating systems in their own hands. As a result, Microsoft and Nokia became nearly arch enemies. Negotiations between Nokia’s Jorma Ollila and Microsoft’s Bill Gates are known to have ended in harsh disputes.At the same time, a root cause for Nokia’s problems started to emerge. Those device manufacturers who had selected Symbian OS for their smartphones were able to build their own UIs. The idea was to use a common baseline, but to allow each phone brand to launch their own look and feel. Software technologies in those days were still immature. For example, Ericsson needed to adjust Symbian deep inside its core systems so they could get the Ericsson smartphones to look different from Nokia smartphones. The resulting derivatives were called UI platforms. Nokia built two of them: S90 for Communicators and S60 for smartphones. [8] This way it was easy to launch devices in the business phone category, or multimedia phone category. The trick worked and Symbian became a market leader.From there, application developers became part of the equation. App developers code additional applications which are sold, for example, in Apple’s App Store and Google Play. In other words, apps are those Instagrams, WhatsApps, and Angry Birds. Creating applications for Symbian was challenging, as the app developer needed to operate deep under the surface in order to get their apps to work. Learning the development tools took forever, and each Nokia model needed their own versions of the app.Apple had a different idea. It created just one phone model in which the UI and OS were combined. This made things easy for the app developer. In addition, an exclusive marketplace, Apple’s App Store where it was easy for people to buy their Angry Birds and other apps, was a revolutionary idea.Google joined the action in 2005 by purchasing a company called Android. As the legend goes, its founder Andy Rubin had started to develop its software for the very reason that Symbian was so complex and its development environment poor. Everyone expected Google to announce their own smartphone and the press was speculating and speculating. On November 5, 2007 Google launched a free operating system for smartphones but no device. This was a big surprise. Google did not want a Google smartphone, it wanted the entire mobile phone industry. The revenue would come from ads and services.Android took its most important step in early 2007. After the launch of iPhone, Google had scrapped its original, keyboard-centric user interface. The work was started from scratch and based on the touch screen. For additional apps, Google followed Apple’s example: Apps had to run in each Android phone without any modifications.The thinking was different at Nokia’s Keilaniemi headquarters. Nokia was trusting its old war horse with their touch screen development. Symbian’s old menu structures, which were accused of being complex and for a good reason, lie deep inside the core system and Nokia was stuck with them. Nokia’s touchscreen felt superimposed and the app developers’ pain continued. In the end, Nokia acquired Symbian to itself but that was too little too late. Over the years, so many layers and additions had been integrated to the system that it had become an unmanageable lump. If a coder changed something in the right arm she did not know what would happen to the left toe. Phones got jammed, died suddenly, and rebooted themselves without a warning.Samsung introduced their first mobile phone in 1989. The beginning was slow since in those days Motorola was dominating the Korean market as they pleased. In the mid-1990’s Samsung had considered giving up the entire mobile industry because of the low quality of their products. Their primary products were semiconductors, motherboards, memory circuits and integrated circuits. After the first Android smartphone arrived on the market in 2008 by Taiwanese HTC, however, Samsung was back in. It launched the first Samsung Android smartphone in April 2009, that’s 18 months before Elop arrived at Nokia. And so the Android ecosystem quickly became dominated by Samsung. When Elop joined Nokia, Android’s market share had risen to 25 per cent.Nokia’s alternative to Symbian was born behind the scenes and partly in secrecy. It started in early 2000 when a small group of Nokians started to figure out whether Linux, an open source operating system created by a Finn, Linus Torvalds, could work in smartphones. Even though using open source software added legal barriers, the project was finalized and the first device bearing its fruit, an internet-enabled touch screen tablet, was launched in 2005. The tablet lacked phone capabilities though, and based on our interviews, Symbian directors blocked it.The operating system was named Maemo. As soon as it was permitted to be fitted on a phone, that first Maemo smartphone was a reasonable success. It attracted a community of open source developers who created Maemo apps. With 12,000 members, this was the largest mobile developer community in the world.Then Nokia did something remarkable. It partnered with the chip manufacturer Intel, and the two companies renamed Maemo to MeeGo. [9] In order for us to understand why this union was disastrous, let’s take a brief look into technology.One of the hardest parts of creating mobile phones is embedding the software operating system onto hardware electronics. Nokia built their smartphones using processors from the American chip manufacturer Texas Instruments. In addition, Nokia now needed to align MeeGo with Intel’s processors. The work was cumbersome, and especially power consumption proved difficult. Intel’s chipsets were designed for laptop computers, so in Intel’s world, power consumption was not a problem. Nokia’s competitors, however, were using processors from Qualcomm which were specifically designed for small devices with a low power consumption.Software developers spent months streamlining Nokia’s Maemo and Intel’s corresponding software, Moblin. This enormous amount of work did not carry over to an end product, and ultimately did not benefit the customer experience. Instead of one giant corporation the work was now carried out by two, and the process was stalled even more. MeeGo was delayed and delayed.This proved costly. The original Texas Instruments chipset was becoming obsolete and the replacement from Intel did not work either. At the same time, competition was moving onto Qualcomm’s second generation processors. Nokia was running propeller airplanes while others had moved onto jets.We’ll revisit the cooperation between Nokia and Intel in chapter 14.Apple and Android brought a magic term into the mobile phone industry: Ecosystem.An ecosystem is a set of e device manufacturers, app developers, service providers, and network providers where all benefit one another. The more apps one platform has, the more eagerly consumers will buy those smartphones. The more smartphones that are sold the more services are used, etc. This creates a positive domino effect.On a strategy level, Nokia had understood the importance of ecosystems and mobile internet, and in 2008 decided to turn itself into an internet company. The Services and Software unit was led by Niklas Savander. The beginning was slow, services were previously developers in units independently, and partly overlapping. Billing mechanisms, technical platforms and frameworks, business models — each unit had their own way of addressing these issues. Music was created in Great Britain, maps in Germany and email in Oulu. That brought decisions to the director level and turned them into politics.In August 2007, Olli-Pekka Kallasvuo had announced Nokia would collect all of its services under the brand name Ovi, which in principle sounded good. The launch, however, was tragic. At the time of the launch ovi.com did not link to any services. All it was a static page with a collection of icons.With time, the services began to harmonize somewhat, but around 2009–2010 ovi.com did not even work with one set of username and password. Colors, fonts, and the general look and feel were a mess. The smartphones and their services had no graphical or functional similarities. Most issues would have been technically an easy fix, but Nokia’s complex organization made it too hard. The ecosystem was scattered.As if that was not enough, there was a leadership catastrophe. In 2009, the company decided to split the responsibility of services between Savander and Tero Ojanperä. The model was dubbed two-in-a-box. Responsibilities were not clear, and the very introduction of this management model indicates just that.In early 2010, Nokia faced the music and announced its navigation service would become free. This was an important milestone. Services themselves would not become a major source of revenue, but a means to sell devices and engage users.The situation was dire. App developers thought Nokia was a difficult partner who did not understand how important an app store is for consumers. Nokia’s own services did not fly and its software platforms were becoming obsolete. Around that time, Nokia’s own application store ovi.com had 16,000 apps, and the Apple App Store had 300,000. The means to maintain a functional ecosystem were poor.But: Newly finished Symbian 3 was a promise of something better. And MeeGo started to be ready for commercial use.[8] The S90 reference in the original Finnish book may have been a misunderstanding. The Series 90 UI platform (S90) was developed by Nokia for media phones running the Symbian OS, such as the Nokia 7700 and 7710. It was different from the Symbian Series 80 UI platform (S80) developed for Nokia’s Communicator devices like the Nokia 9210 and 9300. Devices running the S90 UI platform did not reach major commercial success and eventually the platform development was cancelled, while elements of the S90 user interface continued to live in the Linux-based Maemo platform by Nokia.[9]MeeGo was using source code from Maemo by Nokia and Moblin by Intel.7. The euphoria of the initial weeksStephen Elop was abroad on his first workday at Nokia. Maybe it was a sign of things to come. It was Tuesday, September 21, 2010. He participated in meetings that had been booked for Olli-Pekka Kallasvuo.After he came to Finland, he immediately started to get to know the personnel. His first appearances were met with approval and people were charmed. Interviewees tell that they felt euphoric. Elop gave such a convincing first impact that people finally felt that there would be some decisive actions. From the first moments, he seemed to beam energy. Jorma Ollila’s thought that “a Canadian would be a better match with Nokia than an American” seemed to work. The company had a CEO that felt like he belonged, but who would obviously also get things done. He was like a half brother, whom the Finns soon started to call by a more Finnish name: Seppo Elo.The good news started to leak outside Nokia. According to these leaked insights, Elop talked openly in front of the thousands of Nokia employees about both the good and the bad things, which was unheard of. One could ask anything from the new CEO. His doors were open both in practice and metaphorically. In a speech to the Oulu personnel, Elop told them that he daily heard that some people had never had a chance to voice their opinions. In North America he claimed he’d only seen that happen once or twice. It was time to stop holding back bad news. It was time to increase transparency in Nokia.On his first workday, Elop sent an email to every Nokia employee. In that email he asked people to answer three questions: What do you want me to change? What do you not want me to change? What do you think I might miss? He promised to take the comments he would receive and process those to form the basis of his leadership at Nokia. One of the interviewees said that she answered the questions completely openly and without holding back. She told us that she was very impressed when Elop — or one of his assistants — replied. According to the reply she had made excellent observations, which were well fitted with the basic principles according to which Elop would base his strategy. There was also a promise to look into and fix any and all shortcomings mentioned. The main message was that the Nokians could only win as one company. Those who would put themselves in the game would be rewarded. Fairness would be high on the agenda.This message fit the company perfectly, one interviewee remembers. “I felt like now, finally we could roll up our sleeves and start winning big.”According to Elop’s later comments, the majority of the messages were about the responsibilities and how very split they were. Things fell between the cracks. Elop got inspiration for one of the favorite sayings he used in his first weeks: At Nokia everyone and no-one is responsible.From very early on, Elop got close with the employees. He would interview managers on lower levels of hierarchy and even blue collars to get to the bottom of what Nokia was really like. He wanted to minimize the filtering of middle managers. He was seen in the canteen. Nokia employees working in the headquarters could find themselves in the elevator with the CEO, dressed in jeans and carrying a backpack. In one of the first internal information sessions, Elop complained how difficult it was to mark an email read on Nokia phones. According to his experience it was done differently on different Nokia models. One of the attendees claimed that Elop was wrong. He was invited on the stage, and after some clicking it was agreed that the CEO had been right. The message was clear. The new CEO would voice his opinions on the product level, unlike his predecessor.People were quick to notice that Elop was an email-person. Many of the interviewed people mentioned that it was difficult to get hold of him on a phone. You had to either meet him, or send an email. According to one interviewee, Elop had almost a magical way to respond to emails immediately. Email replies would be sent regardless of the hour. Some started to wonder, whether the man slept at all. The most important mission seemed to be to create trust and hope amongst the Nokians. At the time, there were still 65,000 of them, excluding the employees of NSN, the network infrastructure provider. Elop approached Nokians with stories from his own professional history: “At Microsoft we beat Google. We can beat Apple just as well. RIM can be beat. We can be better than anyone. We are in the first minutes of the first round with Nokia”, was a message repeated over and over again in meetings with the personnel.Another Elop-ism was pulled from Macromedia. Elop loved to share the story of how Macromedia focused on Flash and succeeded (described in chapter 5). Nokia could do the same.After the initial euphoria of the first weeks, the board started to have a nagging feeling: When was Elop listening? He was either travelling or talking all the time. In English. Learning or even attempting to learn Finnish was limited to single words and pronunciation. However, it seemed like the personnel thought the worry was without grounds. Elop was very likable and socially skilled, the workers were easily swayed to his side. The people felt like this was the first time in ages they were heard, which noticeably improved the morale. The message was clear: If there is a problem, do something! If you cannot, tell me why not.“Stephen was a spectacular motivator”, said one interviewee.“He had an unbelievable poker face and was very convincing in assuring people about things that would later turn to something quite different. He seemed like a leader who is very goal oriented and committed to his job”, said another.Many people also brought up the positive feelings stirred up by the humane side of Elop. Petra Söderling, who used to work in Symbian, remembers seeing Elop for the first time in a Town Hall personnel session organised in the canteen of the Nokia House. Everyone working in the headquarters in Espoo were invited and were anxious to see what type of man would appear on stage. Elop made a lasting impression: He was warm and emphatic, and appeared to have a humble attitude towards the task given to him. He also talked about his family and children and made jokes of himself, which was something new to the Finnish audience.A few months later Söderling had the chance to spend a few days in the company of Elop, during the Mobile World Congress in Barcelona. There again he was warm, and very open towards everyone. He shook hands with the people working at the Nokia stand both in the morning and when closing for the evening. He thanked people for their contribution, looked them in the eye and seemed like a member of the team. Compared to his predecessors Jorma Ollila and Olli-Pekka Kallasvuo, the warmth of the new CEO seemed fresh and nice, according to Söderling.A director who visited network providers together with Elop had also very good experiences of how Elop handled Nokia’s stakeholders. He was active, appeared smart and knowledgeable in front of the customers, and spent time together with them.At least a part of Finland was almost in the state of Elop-hype. During the first days in Finland, Elop went to Stockmann’s, the largest department store in Helsinki, to buy underwear. While scanning the barcodes, the young sales assistant asked, whether she could give him some advice on Nokia.Of course there were those who were not charmed by the Canadian. One Nokian remembers being confused and surprised when he realised that most of his colleagues found Elop inspiring and thought that he would boost Nokia. To him Elop had only been unremarkable and colorless, not much else.In the first leadership team meeting, Elop referred to himself as a hockey coach that has arrived to lead a new team. The team was good, but now it would have a better coach than before. The goal was to calm the working environment in the leadership team and reduce the fear the members had for their positions.The ways of working in the leadership team were changed quickly. My colleagues have talked more during the fall of 2010 than they have for the past 10 years together, said Mary McDowell, General Manager for Nokia Mobile Phones (the feature phones unit), in an interview with Bloomberg Businessweek. According to Juha Äkräs, Executive Vice President, Human Resources, Elop forced the leadership team to look at themselves in the mirror and to review their own actions. For the first time in history, the goals, key performance indicators and reward plans of everyone in the leadership team were shared amongst the members. “We no longer work with objectives that are contradicting. We all look in the same direction”, said Äkräs to Financial Times.Behind all the talk, there was something deeper going on. Elop kept his distance, says one former member of the leadership team. The new CEO seemed family oriented, and visited Finland only shortly. It felt like he would rather have spent his time with his family in Seattle. The same leadership team member says that he had wondered whether Elop was actually hoping to work in Seattle, as the competition for the successor to Steve Ballmer was about to start. A comment repeated by several interviewees was that as every member of the leadership team wanted to make a good impression with the CEO, Elop accidentally ended up with more power than he should have. The leadership team became a poodle rather than a terrier. There were tensions within the team, caused by the nomination of Elop. No one was willing to say more about the topic. It is, however noticeable, that Elop was quite cautious in changing the leadership team. Very often a new CEO brings in some of his trusted people from earlier companies, so as to speed up the renewal of the new company. There was really just one person following Elop to Nokia: Susan Sheehan. Sheehan travelled with Elop, wrote or got his speeches written and took care of Elop’s personal messaging. Officially Sheehan was reporting to Arja Suominen, Senior Vice President, Nokia Communications. In reality, she worked directly for Elop without reporting about her work to anyone else. According to an interviewee who worked with the communications department, it was clear that Elop understood the importance of communications and followed closely what the Finnish press wrote about him and Nokia. This kept the department busy, as someone had to translate and summarise the articles in English. The negative headlines of the Finnish press got more and more stressful for Elop with time, the person says. Elop was also uncomfortable with the interest that was directed to his person. The communications team had to explain to Elop, why he was sometimes ambushed by a journalist of 7 Päivää, a Finnish yellow press publication, in the parking lot. Most of the time the communications people were glad to finally have a CEO who was a good speaker. However, there were obvious cultural differences that were challenging. For example, Elop rarely listened to the journalists during interviews. Finnish journalists expect there to be a dialogue with their interviewees, rather than a presentation.The middle management was puzzled by Elop. In meetings he would just sit and listen, says one director. The only feedback he would give, was “good job”.Elop has said himself that during the first weeks of the fall of 2010 he met and interacted with thousands of people who were working for Nokia, customers of the company, and partners. There were tens, if not hundreds, of flights. He met with the network service providers, large mobile phone distributors, application developers, other mobile device manufacturers as well as with subcontractors. There was no time for hobby flying. His own Cessna Turbo 182 plane was stored in the US.During the first fall, Elop even visited Cupertino in Silicon Valley. During this visit, Elop met with Steve Jobs. There is no information about what the two talked about. It might sound strange that he visited a competitor, but it’s common practice with CEOs of large corporations: During these visits one introduces oneself, listen to the thoughts of the other party and tries to feel whether there would be possibilities for collaboration. Thus, this meeting with Jobs was not related with Nokia platform decisions.Elop’s first tweet after being nominated was sent in late November 2010. It was four months since his previous tweet.Thanksgiving … a day to be thankful, for both our personal good fortunes and for the promise of what lies ahead.Elop said he liked it in Finland. He said that it was easy for him to understand his new home country. Finland shares the same cold and dark winters as Canada. A supporter of the Vancouver Canucks ice hockey team had also become a supporter of the Espoo Blues hockey team. Ice hockey is one of the best ways to meet people in Finland, Elop said. He also said that he had heard so many stories about saunas and the related rituals that he had tried it. It had been pleasant. Elop had more challenges with food. Especially the combination of spaghetti and sauce with fish was strange. Timo Ritakallio, deputy CEO of Ilmarinen, a large Finnish pension fund, remembers he tried to get Elop to join dinners and talk in events, but had no luck. The excuse was always the same: Elop was too busy.“I knew he played tennis. So I asked him to come to the court at 7am since, while in Finland he would send emails even at 5am in the morning. But he said he didn’t have time”, says Ritakallio.Ritakallio says that he got to know Elop at an event organised by Harry Harkimo, a serial entrepreneur, at the Winter Classics (Talviklassikko) ice hockey game on January of 2011. The game was played between the two Helsinki teams IFK and Jokerit. A number of Finnish corporate leaders and decision makers, including the CEO of Kone Corporation Matti Alahuhta and the Parliament Spokesperson Sauli Niinistö tried to get to know the man who had been hired to save Nokia. [10] Ritakallio says that it was evident that the size of that task was visible in Elop: “He had understood the overwhelming public pressure and interest towards his role.”Elop’s family had stayed in Canada. Elop said that he wanted to better understand the demands of the job and how much he would have to travel before making the final decision on moving his family to Finland. He said that his family liked snow and that he felt that they would find it easier to settle in a country where ice hockey was such a large part of the culture.Elop stayed in touch with his family mostly by phone. He said that he had given several Nokia phones to his children. On his second tweet as Nokia CEO, Elop talked about the phones his son was using:The story got continuation after a few days. On that day the son was carrying N8, E7 and X3. These tweets however didn’t have a word about the most important part. The dad had an important job: He would have to change the world so that his son wouldn’t have to feel embarrassed for these phones on the school yard.[10] After the original book was published, Matti Alahuhta has left Kone Corporation in 2014 and Sauli Niinistö has been elected the 12th President of Finland, in office since 2012.8. The rumble beginsBusinesswise, Elop’s Nokia career had an unpleasant start. Bad news about the sales of the smartphone flagship model N8 arrived the same day he started work. People who had pre-ordered their phones were told that they would not receive their devices until in October. Investors became nervous because the sales of the phone were originally promised to start in June and now these sales would be missing from the July-September earnings. The share price fell nearly five percent.The Board of Directors had given Elop a fairly basic task: Increase the top line (turnover), stop market share erosion, and fix the bottom line (earnings). However, only after a couple of days, the Californian website, Venture Beat, knew that Elop had been given the green light to dramatically change the existing strategy. This authorization also covered the operating systems used by Nokia phones. Reliable sources within the Nokia board confirmed that the news story was correct. As a consequence, as early as three weeks after his start at Nokia, Elop launched project Sea Eagle to analyze the various options for the existing smartphone strategy. At the same time, Venture Beat released another predictive Nokia-related news story. According to the website, Nokia would be adopting Microsoft’s Windows Phone platform alongside the other Nokia smartphone platforms. However, this was unfounded. An alliance with Microsoft had not even been properly discussed internally at this point.The board was most concerned about Elop’s relationship with Ollila. They were afraid that Ollila will continue his domineering role, even though the new CEO needed his own space to do his job. One member of the board reports that he noticed how Elop censored his own words every time Ollila was present and smoothed over his statements about Nokia and the bad shape it was in then.After the Board of Directors conducted an annual internal evaluation of its own activities without the chairman, vice-chairman Scardino delivered feedback to Ollila and told him about the board’s concerns regarding the degree of freedom Elop had.Exactly one month after Elop became the CEO of Nokia, the rumble began.It was the time for the publication of the July-September 2010 earnings report. The numbers were good and pleasant to report. Profits were more than expected and net sales had increased by five percent from the corresponding period in the previous year. Smartphone sales rose 61 percent year-on-year and ten percent from April to June. N8 was ready and in stores, which held out the promise of a positive outlook by the end of the year. Elop said he was surprised at how the shortage of components, rather than weak demand, had limited the sales. Analysts praised the company’s profitability, which Nokia had started to defend at the expense of market share. Hints about the acute crisis were impossible to find even between the lines. The only negative message was the drop in market share. “Amazing Nokia” was the first reaction of the financial magazine Arvopaperi. The stock price jumped more than seven percent. American MKM Partners’ Nokia analyst Tero Kuittinen opinioned: Staggering earnings report. Good sales figures in Europe were the key factor. The earnings were better than expected, net sales were better than expected, the profitability margin of the Mobile Phones unit was better than expected, as were the average phone prices.Staggering, perhaps, but Elop was unmoved by the results. The new CEO had to show who was the boss. He announced that he would lay off 1,800 people, including 850 from Finland.“In the five weeks since joining Nokia, I have found a company with many great strengths and a history of achievement that are second to none in the industry. And yet our company faces a remarkably disruptive time in the industry, with recent results demonstrating that we must reassess our role in and our approach to this industry” Elop stated in the press release.Credit rating agencies agreed. Moody’s maintained their A2 rating, relying on the strong financial position and net worth but said it would consider lowering the rating if the market position in expensive smartphones did not go back to earlier levels and the operating profit margin did not rise above 10 percent. During the July–September quarter, the percentage had been 6.2.The worst layoffs were targeted at the Symbian product development and the service-oriented Services unit, which Elop instructed to focus on a consistent user experience instead of separate products. These actions were drastic: The size of the workforce would decrease by at least 10 percent, perhaps closer to 20 percent among product developers. The mood was one of both shock and confusion. Nokia had about 20,000 employees in Finland at that time including the employees of Nokia’s Networks unit. Symbian product development employed a total of four thousand people in Salo, Oulu, Tampere and the Helsinki metropolitan area. The statutory negotiations [11] concerned all of them. Nokia offered severance packages to those volunteering to leave the company.That same afternoon, Elop faced investors for the first time in a conference call. He had started to embed far-reaching expressions in his speeches.One of them was the US. Elop promised to devote a large part of his time to recapture the American market. According to him, there was no “systemic reason” why Nokia could not succeed in the US. It was all about execution and focus on the right things. Elop reminded everyone that there are only three or four strong network providers in the US. They knew exactly what kind of products they wanted to bring to the market.At a later date, according to Elop, Nokia will have a “crisply articulated strategy” that would liberate “innovative capacity.” Elop agreed with concerns about Nokia’s inability to get things done and vowed to change it. He said he saw Nokia as an unpolished gem with tremendous strengths. Nokia was the market leader. It had an army of tens of millions of smartphones all around the world. Relations with the network providers were good. Thus, Nokia would differentiate itself from its competitors and stand on a sustainable footing again.Elop spoke about MeeGo, the operating system that was designed to be the future platform for expensive smartphones. He said his first impressions led to confidence and high expectations. However, he said it was clear that the first MeeGo device would not be released until next year.The investor call was a great success. The new CEO responded to questions smoothly and quickly. Reporters and analysts praised the plans and the visions. Forbes went furthest and wrote a very positive article in the second week of November. It construed that Elop’s wishes for MeeGo were high. As Elop had pledged to cut the workforce by three percent and was planning to increase the speed of product launches, Forbes estimated that Nokia stock value should rise by 20 percent. The stock value would then be 10.20 euros ($14.60). However, the magazine pointed out that the forecast would only be realized if the profitability of basic feature phones would be maintained.In addition to the layoffs, Elop made another important decision. In 2008, Nokia bought a promising Norwegian company named Trolltech. Whereas an application developer would have to spend half a day to implement a simple application for Nokia smartphones, the tools from the Norwegian company would let them accomplish the same job in minutes. Trolltech’s miracle product was named Qt. The acronym is pronounced like the English word “cute”. Qt would be the Nokia ecosystem. It would challenge Apple and Google’s Android. Nokia’s own internal software development would also be done with Qt in the future.Stock Analyst Sami Sarkamies says he thought that the strategy was promising. Qt could have been used to create a unified interface for all of Nokia’s smartphones. The consumer would not have needed to know if the phone was based on Symbian, MeeGo or Windows. Usage of all devices would have been more or less the same. The developer would have had to implement the application only once.“One could say that, with minimal changes, all applications work on all of our devices, and the work that remains is our problem,” Sarkamies says.An experienced application developer echoed similar sentiments: The new strategy was credible and a welcome step forward. Interest in Nokia grew as the speed of software development reached the level of its competitors. The tools were good. The strategy was clearly a defensive move to support the MeeGo platform projected as the future platform for smartphones. There would be a natural transition between Symbian and MeeGo. Together they would form an evolving ecosystem. A relevant question was: Why didn’t Nokia redraw the lines earlier? The main reason is obvious: As an outsider, Elop was able to do what the internal power struggle had hitherto prevented. Reason finally prevailed and the most obvious absurdities that Nokia had experienced could be eliminated.At the same time, Elop brought application developers to the center of mobile phone development. He had been wooing them earlier when he made a surprise visit to the Nokia World event to give away the $1 million prize to Nokia’s developers. In his speech, he borrowed from his former boss, Steve Ballmer, who had many years previously declared: Developers, developers, developers! Elop repeated the same thought with style, discretion, and without affectation. Developers would have a big effect on Nokia. The prize was awarded to Kenyan John Waibochi’s virtualcity.co.ke mobile service, which solved the logistics problems of small businesses.At the end of November 2010, Elop started making changes to the executive leadership team. According to the announcement, the leadership team would have more sales power at the beginning of January 2011 when Jerri DeVard would start as the head of marketing and communications. DeVard had over 25 years of experience with large consumer brands. For example, she had worked for Revlon and had been part of Barack Obama’s election campaign.The newcomer was expected to stir things up and get things going. The leadership team of grey suits appeared to be highly “technocratic” even though one of the members was female. One reason for welcoming the American brand expert was due to the fact that, to American ears, native English sounds better than a perfect English-speaking Finn. In addition, the Nokia brand was trending downwards in international comparison. The brand consultancy firm Interbrand estimated that Nokia was the 8th most admired brand at that time. This meant a drop of three positions from the previous year. Only the motorcycle manufacturer Harley-Davidson had lost more brand value than Nokia.[11] Finnish labor law requires employers to conduct statutory negotiations before any job reduction. More explanation in the addendum to the glossary in Appendix 2.9. The consultant with a Microsoft connectionOne of the best-kept secrets regarding Nokia’s strategy choices was the role of McKinsey & Company, the management consultancy.McKinsey & Company, a US-based management consultancy is one of the best known in its field. The New York Times magazine listed it as the most prestigious in the field in 2011. McKinsey, and the two other US-based management consultancies Boston Consulting Group (BCG) and Bain, hire the most talented students from business and engineering schools. They send the fresh hires abroad for training. McKinsey has nearly a hundred offices in 60 countries. Its non-disclosure agreements and requirements for secrecy are tight. McKinsey consultants are not permitted to publicly discuss matters of individual customers because the McKinsey business is built on customer trust. This professional confidentiality also binds former employees.McKinsey is a privately held company, and does not share its financial figures publicly. However, in 2011 Forbes estimated McKinsey’s turnover to be roughly 5 billion euros ($7 billion) with approximately 9,000 consultants. According to an estimate found on the internet, a team of one experienced and four junior McKinsey consultants charge roughly half a million euros ($700,000) a month. This is well over $100,000 per consultant. A book published in 2014 compares McKinsey to a luxury product; when they are summoned it sends a message to others: We can afford McKinsey!Over the years, the best brains of McKinsey have made some strange recommendations. In 1980 McKinsey told AT&T that mobile phones would remain a marginal product. The airline Swissair went bankrupt 12 years after they started to follow a strategy created by McKinsey. Enron, the energy company that went through a scandalous collapse, was one of the largest customers of McKinsey. More than that, Jeff Skilling, the Enron CEO who was convicted of federal felony charges relating to Enron’s collapse, was an active McKinsey alumnus (former employee). In 2000, McKinsey recommended the media giant Time Warner a merger with internet service provider AOL. Jeff Bewkes who was the CEO of Time Warner some years later, has called this merger the largest mistake in corporate history. Into this gallery of horrors, we can also add the case of Nokia from 2009. An organizational change was done during Olli-Pekka Kallasvuo’s tenure. This change fully paralyzed the company at a critical moment. The change was made based on the recommendations of McKinsey.These errors are counterbalanced by customer satisfaction. 85% of customers return to use McKinsey again. When you choose McKinsey, you know that the consultants are not stupid and that they have an efficient global organization to support them.Stephen Elop had good relations with McKinsey already prior to his joining Nokia. He had used the consultancy already back when he was at Juniper Networks, and continued to work with them at Microsoft. Endre Holen, a Norwegian based in McKinsey’s Seattle office, had become Stephen Elop’s trusted man. Holen most likely was one of the most influential people behind the Nokia strategy and renewal. Elop contacted him soon after being appointed as the CEO of Nokia and asked him to participate in analyzing what should be done with the company.According to McKinsey’s Seattle office website, Holen has worked for the company for over 20 years. He has a master of science in structural engineering from the Norwegian Royal Institute of Technology in Oslo and an MBA from Berkeley. His clientele consists mainly of high technology and telecommunications customers. He is experienced in projects ranging from strategy and product development to sales and marketing. Industries mentioned include software, product manufacturing, services and wireless technologies. Like for Elop, flying is a hobby for Holen.The website lists an article published in the McKinsey Quarterly magazine written by Holen and a colleague. The article is about Kevin Johnson, who became the CEO of Juniper Networks after Elop jumped to Microsoft at the last minute. The authors — and thus also Johnson — believe that after entering a new company, a fresh CEO has only a very short time window to announce the changes (s)he wishes to make. If (s)he misses that window of opportunity, the changes cannot be made or are considerably more difficult to execute. If the changes have not been put into action within 12–18 months of the entry, it is too late. According to Johnson, and most probably also the author Holen, a new CEO also has to have a basic understanding of the state of the company starting from the very first day. This could be something like “This is a good industry, but our company is in trouble”. Within a few months, the CEO needs to have figured out the long term goals, the strategy to achieve them, and the leadership changes necessary.The article about Johnson was written in June 2010, only three months before Elop was nominated to lead Nokia. Johnson says that upon his arrival at Juniper, he posed four questions to the leadership team. It shouldn’t be a surprise that these four questions included the three questions that also Elop put to Nokia’s leadership. In an email sent to his direct reports, Elop had only omitted one question: What are you most proud about Juniper (Nokia)? There are also other interesting coincidences. Johnson came to Juniper from Microsoft, of all places.Holen actually has surprisingly many ties with Microsoft. Elop’s ideas on leadership, on the other hand, seem to be inspired by Holen.Following Holen’s arrival at Nokia headquarters in Keilalahti, Espoo in the fall of 2010, a steady stream of junior consultants from across the globe also began to arrive. This was usual practice at McKinsey: The best candidates for a project are invited to join, regardless of where they were based. The young employees of McKinsey’s Finnish office also started to commute between Keilaniemi and Helsinki city center. These consultants took over the strategy work, regardless of the fact that Nokia itself had an unusually large strategy department. McKinsey was tasked with building a “Winning Strategy” together with Nokians to turn Nokia around. Another Norwegian consultant, Trond Riiber Knudsen from the Oslo office of McKinsey took the position of Endre Holen’s right hand man. He specialized in sales and marketing.The people who worked at Nokia strategy department around this time say that the situation was very confusing even prior to the McKinsey invasion. After the spring 2010 layoffs, the size of the department had gone down from 250 to under 200. In this environment, people acted as though they always had something important and urgent under way. It was very difficult to know what others were working on, as projects were classified confidential or secret. This was combined with a limited amount of teamwork and a culture of not questioning the common beliefs. Symbian was considered the sacred cow that all career-conscious employees supported at all costs. Of course it also made sense to speak favorably of MeeGo. According to the official documentation, the strategy department has invariably supported MeeGo. Windows Phone, on the other hand, was practically unknown to all analysis prior to Elop.The Nokia strategy department had become a sort of a stepping stone for all young and aspiring wannabe-people. Outside the department there was also talk about the number of people whose last names were the same as those of several well known Finnish corporate leaders. There was a Halmesmäki, a Juusela, a Suila, and a Sundbäck to name a few.These people were more or less closely related to their more famous namesakes, but according to one Nokian, to an outsider, it seemed like these people were eased into the department to get a line on their CV from world-class strategy work.It seems understandable that Elop chose to use super-expensive external consultants despite the size of the internal strategy team at his disposal. Nokia needed to get a fresh outside-in view. Company profitability or market share didn’t give much reason to trust the work of the strategy department. The mission given to McKinsey was: “Make sure you challenge us”. Elop wanted to understand Nokia properly, and to make sure that nothing was overlooked. What were the strengths of the company? What sort of partnerships were needed? Could there be some hidden gems somewhere? That Elop allowed Holen to participate in Nokia leadership team meetings aptly reflects the nature of the assignment. Holen almost became an additional member of the leadership team.The arrival of McKinsey marked the start of a countdown for Nokia strategy department. Elop continuously cut down its personnel. By spring 2011, the headcount in the department was down to a hundred. The explanation was that after Windows Phone was chosen as the new strategy, the main focus was on implementation, not planning. By 2012, the Nokia strategy department had shrunk down to 50 people. Replacing internal strategists with consultants caused conflicts. As one can guess, cultures collided. The external consultants were seen as invaders, especially as their personalities, most notably Riiber Knudsen’s, caused conflicts. Most junior consultants were still inexperienced in international business and their attitudes were, in the words of several interviewees, arrogant. Nokia engineers were honest and said things bluntly and openly, as is the Finnish way. Consultants experienced in the American culture however often assumed that they were embellishing the facts. PowerPoint presentations were made at an astonishing speed. McKinsey would make a 100-page presentation set out of thin air, said one interviewee. If you wanted to make sure that your initiative would get implemented, you should always engage McKinsey, said another. Many were also wondering what fresh insights and approaches the consultants could come up with.There are varying estimates on the number of McKinsey consultants engaged with Nokia at this time. A total of 50 is an educated guess. Usually there were 5–10 of them on average working on site, at the busiest times there could be tens of consultants. The McKinsey consultants had their own assistant as well as their own premises with a “war room” at the Nokia House headquarters.The massive size of this endeavor can be deduced from the sales numbers of the Finnish McKinsey. The common practice at McKinsey is that the local office does the invoicing on a case. The turnover of McKinsey Finland was a little under 13.9 million euros ($20 million) in 2009 and a bit above in the following year (14.2 million euros). However, in 2011 their turnover more than doubled surpassing 36.4 million euros ($50 million).In the financial statements submitted to the Finnish trade register, there is a line called “other costs” after employee costs. These “other costs” were 8.3 million euros ($12 million) in 2010, and 28.7 million euros ($40 million) a year later. These figures most likely give a very reliable estimate on the pass-through billing. Considering that these “other costs” were 5.8 million euros ($8 million) in 2009, we can make the rough estimate that Nokia paid about 20 million euros ($30 million) for McKinsey consultants. Reading the financial statements of McKinsey Finland is almost amusing. The text on the action report stays the same word to word, year after year, only the numbers change. Sometimes not even that: Every one of the reports for the years 2010, 2011 and 2012 states that the ending fiscal year is the twentieth year of McKinsey’s Finnish subsidiary (actual numbers are 21st, 22nd, 23rd respectively). Even though the turnover more than doubled in 2011, there is hardly a mention of this increase in the report.McKinsey was not the only consulting company that was interested in doing business with Nokia. All consultancies recognized that this was the perfect time to act. In large corporations, a new CEO very often starts to drive changes and needs help. At least one of these other consultancies raised Android as a clearly winning bet for Nokia. The capability to effectively distribute phones to the developing markets was considered as the strength of Nokia. Nokia, however stuck with only McKinsey.Now we come to an important point of interest. What McKinsey and other consultancies sell, is not just recommendations, but also glory. The end result of their effort is owned by the client, never McKinsey. McKinsey helps large enterprises in making a great number of important choices. However an outsider still thinks that these are choices that the company and its leadership made.The relationship between clients and consultants is symbiotic: Who pays tens of millions for recommendations they don’t implement? No one. That is why people believe the consultants. And consultants tailor their recommendations to please their customers. McKinsey has such a high reputation that it is known to have been used also as a rubber stamp. On occasion, it has been invited merely to give rationales and supporting arguments for decisions that were already made before it entered.We are not claiming that this is what happened with Nokia. Many of the people we interviewed thought that both Holen and McKinsey were objective and followed the appropriate hygiene rules. According to one estimate, a consultancy such as McKinsey cannot afford to enter a company as public as Nokia with a ready-made strategy. One leadership team level interviewee said that the analysis Holen presented them with was very convincing. McKinsey was dedicated to their work. Also, most of the corridor talk on Holen appears to have been rather positive than negative.Regardless, the role of Holen raises questions. Why did Elop choose as his right hand man a consultant who had such strong ties with Microsoft? A man who was in an ongoing customer relationship with them? Why didn’t McKinsey consider him unfit for the project? Shouldn’t McKinsey make sure that there is a firewall between consultants that work with competing clients. There is no doubt that there has been a conflict of interest with Holen. For example, we do not know what his role has been when Microsoft decided to use Windows Phone 7 to increase its efforts on entering mobile phone markets.According to one reliable source, Holen was or had at least been the account manager for Microsoft at McKinsey.It looks like neither Elop nor Holen had fully thought through the implications of Holen’s role. How does it look like if a consultant who actively works with Microsoft, or at least is close to the company participates in Nokia leadership team meetings? The dual role of Holen is like adding fuel to the fire of conspiracy theories.Holen’s own LinkedIn profile has minimal information. There are not too many recommendations from others, either. It is likely that Holen has limited the number of recommendations he wishes to show, but there are two themes on this list that catch one’s attention. The person who has been most active in giving recommendations to Holen is Niklas Savander. He gives a thumbs up for Holen on almost all aspects relating to strategy, change management, and mergers & acquisitions. Teemu Suila, who earlier worked in Nokia strategy and is now the Chief Operating Officer at Rovio praises Holen’s strategy skills. The third most-active recommender is Zig Searfin, who happens to be a vice president -level leader at Microsoft, according to LinkedIn.It appears that the board of Nokia was unaware of the linkages of Holen and of the double role of McKinsey. We do not claim that these linkages would have directly affected the choices that Nokia made. However, we all know that it is easier to lean towards the more familiar option, the one that you can easily find supporting data on. Rather than towards something more unknown. In terms of appearances, it is indefensible that the strategy choices of Nokia and operative decisions relating to them were made by two people with such close linkages to Microsoft.McKinsey continued to work closely together with Nokia also after Windows Phone was chosen. After the size of the strategy department stabilized to around 50 people, the consultants became more and more involved with regular, less strategic projects and participated in developing device sales strategies for Microsoft.10. The platform choiceMicrosoft’s CEO Steve Ballmer was in a tight place in January 2011. He was flying with his staff on a private plane to Helsinki, when snow and fog prevented them landing in Helsinki. A decision was made in the Swedish airspace: The plane will land in Stockholm instead of Helsinki. Ballmer would continue from there on a scheduled flight, which were still able to land in Helsinki-Vantaa. Ballmer’s tall and stooping image would have been a topic of rumors on any flight, so he hid from the situation by quickly heading for the lounge in Arlanda, states Wall Street Journal. Suddenly, he hears his name on the intercom. There was something unclear with his ticket for the scheduled flight. They wanted him to check in at the desk.Luckily for Ballmer, no one noticed him being paged. He took care of his ticket discreetly and snuck over to Helsinki to meet Stephen Elop, while avoiding the public eye.The events leading up to Ballmer’s flight to Helsinki started in the end of October, when Elop had started mapping out the strategic options for Nokia. The Qt strategy, announced in October, was built up with great seriousness and the management, as well as the board, had accepted it. But when Elop had, over time, gotten familiar with the company better, he began to change his mind. According to one member of management, Elop started to view Nokia as too mixed up. With the smartphones, a clear choice had to be made: Either continue our own way, in other words put our effort into MeeGo, or go unequivocally in either the direction of Google or Microsoft. Apple was out of the picture, because it had shut out other manufacturers from its ecosystem.Microsoft under Ballmer’s leadership had shown up in the smartphone world with new energy, when it announced its new operating system, Windows Phone 7, in February 2010. The first phones using it appeared in sales in November. The reception had been favorable. The graphic design had been considered fresh and original. The tile based start screen differentiated from competitors and pleased many. The way social media was integrated into the phone’s functionality was praised. In the middle, instead of separate services, were people and their messages. Windows Phone 7 was easier to use than Android and more modern than Apple’s iOS. The differences could be compared to a house. On the iPhone, one room led to another, for example from the kitchen to the dining room, always via the entrance way. Android was like a doll house. The user could jump into any room from the outside. Windows Phone, on the other hand, got rid of some of the walls between rooms. The usage of the phone was no longer based on silos formed by the different apps. The product was actually quite good, unlike Microsoft’s earlier concoctions.The newcomer’s solution had its beginning in the latter half of 2000, when the company was thinking about a successor to its successful PC operating system, Windows 7. Microsoft’s design department was accused of copying, but without cause. Now they wanted something new. After dozens and dozens of brainstorming sessions, it was decided to go with tiles. The brilliant idea was to put tiles beside each other and on top of each other instead of icons. This is how the exciting layout got started; the content was alive and targets were easy to touch.Eureka! Now they only needed boldness at the management level. And boldness was found. The first incarnation of the system was in mobile phones. Windows Phone 7 was born as a test bed for Windows 8 for PCs, where there was also the intention of bringing in the tiles.Thanks and praise for the freshness of Windows Phone 7 was received with joy at the end of fall in 2010, but the financial success was meager. During the first six weeks, Microsoft announced that they had delivered 1.5 million phones to retailers. The manufacturers at the time were Samsung, HTC, Dell, and LG. What was not shared was whether or not the phones were sold to consumers or if they were lying around in the stores. Microsoft wrestled with the same problem as Nokia. If the ecosystem is to succeed, it had to be large enough.In the Nokia management in the fall of 2010, Windows Phone was shot down straight away. Choosing it as the only platform would be approaching madness was a common opinion heard. Guarantees of success against Google or Apple were not present. Elop was, from confirmed sources, thinking along the same lines. He considered the Microsoft choice unsure, but from sources in management he still continued to ensure it remained on the agenda.In November 2010, when Nokia started actively researching external alternatives, Microsoft’s share prices had gone up. It was approached with the same seriousness as with Google. Elop had collected a close-knit group around himself at this point, who would back his decisions. Three leaders who had been with the company a long time were in this group. They were Kai Öistämö, Niklas Savander, and Timo Ihamuotila.During those times, 46 year old Öistämö was one of Nokia’s most controversial characters. He started at Nokia almost directly from the school desk in 1991, after he did his dissertation in his hometown, in the Tampere University of Technology. The tall and slender doctor proceeded with big strides in his career in the mobile phones division, and in 2006 was in charge of the whole group. He was appointed to the board in 2005, and in 2008 he rose to leadership of the Devices division. In July of 2010 his title became Chief Development Officer.As mentioned previously, Öistämö had done business with Elop already in the summer of 2009, when they negotiated bringing Microsoft Office to Nokia phones. The two of them got along well, and Elop started quickly confiding in Öistämö when he started at Nokia. His job description was to be responsible for strategy, business cooperation, business development, and joint ventures in the field.Öistämö was a pleasant, modest, and easy-to-approach person. Interviews portray him as friendly and tell that he doesn’t carry an air of importance. One person described him as “terribly nice”. Öistämö is married with three children. His hobbies include tennis, skiing, and golf. His professional values, however, change the overall picture. According to many who have been interviewed, Öistämö is one of the top culprits of Nokia’s difficulties. The claim is based on the years 2008–2010, when Öistämö was in charge of the Devices division. He bloated up Symbian, defended it at every opportunity, and created tens of device versions. Öistämö had bloated MeeGo, together with Alberto Torres in the summer of 2010, to an organization of over 2,000 people.As a leader, Öistämö is described as a yes-man. He is claimed to sniff out his own bosses’ opinions before sharing his own opinion. What ends up transpiring is always pleasant to his boss. He has another incomplete skill as a leader of people — he doesn’t give feedback, nor does he have a grasp on how to develop people. But he is fair and analytical, according to appraisals.As a person, Niklas Savander gets different appraisals. He is described as distant and arrogant. He is, according to some, a typical salaried manager, who would rather protect his own position than come up with new ideas. One stock analyst tells us how colleagues actually shun Savander, because he seemed so full of himself.One partial explanation for this might be found in the man’s family history. His father, Magnus Savander worked, among other things, as a CEO in the conglomerate Rosenlew, known for home appliances and harvesting combines. His mother Christina was born into the von Frenckell family, who owned Rosenlew. After school, Niklas left Pori to go study in Helsinki and graduated from Helsinki University of Technology’s mechanical engineering department in 1987. A year later, he had also gotten papers from the Helsinki Swedish-language Hanken School of Economics, with a degree in international marketing. Savander came to the marketing unit of Nokia Networks after working 9 years at Hewlett-Packard. The year was 1997. The road quickly led to assignments in enterprise devices, marketing, and technology platforms.In 2009, the American magazine Business Week listed him as one of the hopes of the business world. According to the magazine, Savander was leading the Services division at that time, when Nokia had expanded from phones into mobile internet. He had gotten mobile phone users to buy music, games and navigation services. Under his leadership, Nokia had also developed services for feature phones used in developing countries.Savander is married and he has two children. According to Wikipedia he plays and referees in ice hockey. His other hobbies are telemark skiing and golf.Savander’s achievements at Nokia bring up conflicting comments. People who worked with him a lot say that the first impressions you get of Savander are wrong. He is described as a born leader, who concentrates on the big picture. On the other hand, his achievements with Nokia’s internet services, especially with ovi.com, remained only as promises. Colin Giles, who had been fired from his position as head of sales, was believed to have been used as a scapegoat, so that Savander would not need to leave due to Nokia’s loss of status in China.Like Savander, Timo Ihamuotila belongs to a well-known business family. He is the son of the former CEO of the Neste oil company and the cousin of the CEO of the Marimekko fashion company, Mika Ihamuotila. Ihamuotila completed a licentiate degree in the Helsinki School of Economics in 1997. Only the PhD dissertation is missing from his doctoral degree.During Elop’s time at Nokia, the 44-year-old Ihamuotila is described as precise and quick-witted. As a counterbalance to being somewhat colorless, he is considered a leader who knows his business. He is described as an upright, transparent, pleasant, and a very professional financial leader.Ihamuotila started his career in assignments in finance, first in the Kansallis-Osake-Pankki bank and in 1993 as a risk analyst for Nokia. In 1996, he left to Citibank, but returned to Nokia to a leadership role in finance in 1999. Afterward, he was responsible for, among other things, Nokia’s CDMA business in the US, the mobile phone product portfolio, and from 2007, global sales. On the same year, he was appointed to the board. He started as the Chief Financial Officer in 2009.Ihamuotila is married and has three children. In his free time, he plays tennis, skis, reads, and spends time with his family.It was Monday, November 15, 2010, when Elop and his three soldiers headed to Microsoft’s control center in Redmond.Elop had told about his first impression of the negotiations to Bloomberg Businessweek. When he, Öistämö, Savander and Ihamuotila were waiting together for a taxi in front of the Bellevue Hotel near Microsoft’s headquarters, Ballmer had sent a huge limousine to greet them. Elop said how he felt so awkward that he would have wanted to walk. A group of four received them in a small conference room. This group included, together with Ballmer, Qi Lu, who was responsible for internet services, the leader of the mobile unit Andy Lees, and the person responsible for the Windows Phone technology, Terry Myerson. After small talking about being back to the site of his former employer, Elop went straight to the point. Nokia had decided to either stay with its own software, or team up with Google or Microsoft. The decision would be made soon. He said that he would publicize the decision in the analyst meeting on February 11, 2011.Myerson recalled that Elop has given a familiar impression at the meeting. This strength is again not from intuitive decision-making, but is a question of creating clear and quick processes, where a rational person feels comfortable. Microsoft was certainly interested in such things.The next time they met was on December 6 in the W New York — Times Square hotel in New York. Jo Harlow, who was in charge of smartphones at Nokia, was also present at the meeting. The task of the conclave was to decide whether or not Windows Phone would be able to run on Nokia’s chipsets. The concrete technical analysis continued after a few weeks in Reykjavik, Iceland.A source who was present in the Microsoft negotiations says that Elop was very neutral in the meetings. He often discussed with his team and made the members express their opinions. According to sources, decisions were made together, as opposed to Elop being a dictator and driving Nokia into the lap of Windows Phone. He really coaxed out the expertise of his team, the source says.The first contact with Google came when Elop and Google’s CEO Eric Schmidt talked on the telephone, and Elop told Schmidt that Nokia was making some big decisions. Besides Schmidt, Andy Rubin also took part in the call.It is good to clarify a couple of things at this point. Microsoft’s Windows Phone 7 was proprietary software. Only Microsoft could modify it. It charges license fees from phone manufacturers. Android by Google is open source software. Anyone can use it for free and modify it however they want.Why did Nokia even need to negotiate with Google? If a manufacturer wants their devices to access the Google app store, they must use Google’s standardized version of Android. The manufacturer agrees to preload, among other things, Google’s maps and the Gmail email service. After fulfilling these requirements and many others, the manufacturer can fully enter the Google ecosystem. Google allows modification of the user interfaces, but only to a limited degree, so that using the device remains similar independent of what device is used.Another option is to put Google’s standardized version to the side and download Android for free and create one’s own version. This is what the Amazon online store did with its Kindle tablet, and built services and an online store by itself. In 2010, however, this option was off the table for Nokia. Nokia was looking for a ready-made ecosystem.Elop was as direct as he was in Redmond in his first contact with Google. Symbian was dying and MeeGo was late for its schedule. Nokia wanted to understand if Android would be a good replacement for these. At the same time, Nokia said they would also study Windows Phone and compare these two. The follow-up work was taken up by Öistämö and John Lagerling, a Swede who was in charge of Google’s Android partnerships. On November 11, 2010, in other words four days before Elop and his four-man-team made the trip to Redmond, Lagerling arrived in Finland. First there was dinner with Nokia’s American leaders, and on the next day there was a meeting with a delegation of about 10 people from Nokia in Keilalahti.According to a source present, Google seemed to really want Nokia to join the Android world. The company assured that Android can be customized more than Nokia understood, especially compared with Windows Phone. Even if Google was criticized continuously for having Samsung, HTC and Sony Android phones differ from each other too much, Nokia would be given leeway to create its own user experience. Google saw that Nokia differentiated from these competitors in that it had a global area of operation. Nokia would be able to create better local services and user experiences for network providers and customers, one person present remembers being discussed. The Nokians also noticed that they had been living partially with misinformation. Nokia could continue with Android with its own maps side-by-side with Google’s maps. The same applied with the app store. Nokia’s music service as well as ovi.com could continue, as long as the phone had Google Play.The discussions continued at a fast pace after the first visit. Google seemed to really want Nokia.Some constraints were set by the Open Handset Alliance behind Android, OHA. Unlike Windows Phone, Android is not controlled by one company, rather by an alliance of 84 companies which is led by Google, where the members are able to use Android in an equal manner. Google was in a difficult position. By giving Nokia special privileges, it risked its relationship with other manufacturers. The reactions would be difficult to predict. Creativity was needed.As the negotiations proceeded, a solution was found. Google offered Nokia, among other things, plenty of say in choosing the direction of Android development. By directing Android development to align with its own competitive goals, Nokia would gain some advantage, even if the changes would be available for everyone at the same time. Now Nokia was interested. Android and Nokia had an area where their interests converged in a brilliant way: Developing countries. If Android could be made to work on cheap hardware, Nokia would be best at getting in through in developing markets. The arrangement was enticing. Google would secure the position it was dreaming of in smartphones, and Nokia would become part of virgin Android markets. The precise details remained hidden, but Nokia was able to learn that Google worked Android into clearly cheaper models than Windows Phone.Another flexible point of Android was in its predictability. Nokia wanted to publicize the new software features earlier than when the phones go into sales. The reason was brutal: Nokia was more solid than its Korean competitor and needed more time to build a phone. If the information about the new Android features was available earlier, Nokia would have enough time to get them in the first wave, like the others. Google was willing. It promised to make the publicizing of its plans earlier and to release the source code to its partners. The solution would have been useless for other Android manufacturers in relation to Nokia, but would not have broken the OHA rules.Google made a substantial offer regarding distribution of income. Nokia would have gotten a portion of the income from Google’s search engine, app store, and other services which originate from Nokia phones, and the terms would be in relation to Nokia’s influence in the ecosystem. We don’t have information about precise percentages, but at any rate, Google’s promise was quite exceptional, considering that Nokia would still have been able to keep its own services in its phones.Contrary to what Nokia has claimed, Google was ready for concessions. It was ready to flex as far as it could in the framework of OHA, and even then some more.Then some big money stepped into the game, as well as the mysterious Nokia employee with the name Rahul Mewawalla.According to his profile in the social media LinkedIn, Mewawa transferred to Nokia in 2010 from the television company NBC. Previously he had worked at Yahoo, among other places. During his time at Nokia, Mewawalla’s title was, according to LinkedIn, Global Head of the Business Division, Global P&L Leader, Global Vice President and General Manager.Mewawalla had a simple task. He created pressure outside the official negotiations, and above all he tried to milk money from all the possible contract partners. Mewawalla’s thinking was based on the assumption, that MeeGo would succeed and produce Nokia a revenue stream of sum X. If Nokia were to choose some other platform, it would lose that sum. Sum X was therefore an alternative cost to choosing an external platform, and the sum was several billions of dollars. Many interviewed describe that reasoning as strange. Mewawalla’s role and methods of negotiating were considered strange in a Finnish company. The opponents had difficulties understanding which direction Nokia was going.Nokia found understanding from Microsoft with relation to money, but Mewawalla ran into a wall with Google. Google notified Nokia that it would not be able to pay one cent for someone to use its free operating system. It definitely does not fit into Google’s way of doing things, was the answer.Creativity was again needed.The board of Nokia followed the progress of the Microsoft and Google negotiations calmly. In 2010, there was the normal number of meetings, 13, out of which a few were held by phone. In the fall there was no reason seen to speed up the pace. Three options, continue with our own software, Google and Microsoft, were all under consideration at the same level, and all were being researched with open hands. The information close to the board was under review. Also Elop seemed to be proceeding with his eyes open, and avoided sales pitches on behalf of Microsoft. Even if Elop became a board member only in spring 2011, he was in the meetings presenting the information expected of the CEO.After some time, doubts began to gather around Google. Mostly the board grew wary of losing their own software work. As part of the Google camp, Nokia was expected to end up with huge numbers of layoffs, because the platform would come from outside, and the possibility of other software work was limited. The potential of the maps company, Navteq, would be weakened. Nokia had bought it four years prior for the huge price of 5.7 billion euros — the deal was one of the largest in Finnish business history. The board started asking if we are ready to make such big sacrifices. Another conclusion that had come up was, through Android, Nokia would become a slave of Google. In Elop’s speech, one would hear his familiar words of “our ability to differentiate from the competition” on the scale. Samsung had taken over the Android ecosystem, and they would be difficult to compete against, because, through its displays and semiconductors, it had a competitive edge in price, R&D, and logistics.The choice of Android might increase sales, but what would happen to profitability? Nokia’s services team had doubts about Google’s trustworthiness: Would they dare to give them strategically important user data for their services without fear of misuse?The arguments for Microsoft became better, day after day. Microsoft needed Nokia more than Google. They could not risk letting Nokia jump over to Google’s ship, because it would be the beginning of the end for Windows Phone. Microsoft promised Nokia its own app development as well as innovation. Doing things themselves was part of Nokia’s culture, so Microsoft assured that they would suit them better than Google. Nokia would be able to influence the end result. Both companies were challengers, and they both had a common enemy.There were also certainly suspicions toward Microsoft. It was hard to imagine it as a cradle of innovative culture, and the Windows Phone ecosystem was only just getting started. The board was given working Windows Phone 7 phones, so that the members could get to know the possibilities of the platform. The relevant question remained in the air, nonetheless: What would a Windows Phone device be like? How would we make it competitive? How would it differentiate?In the internal dynamics of the board, there were some strong characters besides Ollila — Henning Kagermann and Risto Siilasmaa. Only they had a background with software.Siilasmaa is described as being active in the board and bringing in valuable input. He knew Microsoft — he is known to have admired the company during its greatness in the 1990’s, and on top of that Siilasmaa’s company, F-Secure, had made its business by patching the security holes in Microsoft Windows.The third big hot potato was Nokia’s own hope, MeeGo. It was late. But it looked better. Anssi Vanjoki had, before he was let go, come to the conclusion that MeeGo had been turned in the right direction. Alberto Torres, who was often in disagreement with Vanjoki, was also in agreement. MeeGo would reach its goal, as soon as the leadership problems could be cleared out of the way. There were estimates in the management that the broad category of products would be ready by the end of the beginning half of 2013. At the end of the fall, Nokia’s most organic path, MeeGo, was the choice most supported by management.The strategic lineup started looking like this:MeeGo Nokia would continue with its own software, differentiate from its competitors, and its money would stay in its own pocket.Google Success was clear. Nokia would become a mass producer. The risk is low and the profit expectation is low.Microsoft Would it work? Nokia would have to get its software from outside, and they would need to pay licensing fees. The risk is high, and the profit expectations are low.At around the end of 2010 and the beginning of 2011, the situation was getting tense. Elop had gone to Seattle before Christmas. When he came back to Finland, he continued the decisive conversations.According to an estimate from the board, the fate of MeeGo would culminate during these weeks. Elop had expected more support from the network providers than what they had. A CEO from one of the world’s 10 largest network providers confirms the claim. He recalls Elop visiting for lunch and showing off MeeGo devices. The CEO remembers that he had been unimpressed. The picture he got of the phones was that they had appeared too late. He told Elop directly that Nokia would find it difficult to create a real ecosystem with them. It would have taken more money than what Nokia had, the CEO remembers.According to a member of the Nokia leadership team, the lack of credibility that Torres had as leader of the MeeGo project also affected Elop’s considerations. An operating system is chosen based on its technology, but in an equal playing field, the option which shows the most credible plan gets picked, the CEO reminds us. It would be very difficult to push MeeGo to the side, if it had been led by someone more convincing than Torres.On January 3, according to Wall Street Journal, Öistämö walked into his boss’s office and notified Elop that he was worried about the possibilities MeeGo had. The two of them calmly decided to talk with twenty MeeGo people, from programmers to managers. Before the first interview, Elop collected everything that was known about MeeGo on a whiteboard, according to the version described in Wall Street Journal. The products under development, their launch times, and the error level of the software. The news was bad. At that pace, Nokia would have three MeeGo devices in sales before the end of 2014.Elop tried to call Öistämö, but Öistämö’s battery was empty. “He was probably trying out an Android phone”, Elop joked later. When they finally talked on the January 4, the truth was, according to Öistämö, a bitter pill to swallow.Mewawalla kept at his tough push throughout the end of that year. The sources of money, nonetheless, remained dry. Google’s thinking was that it would not pay money for using Android, but it notified that it understood Nokia’s need to get cash to help with the cash problem caused by the platform change. The creativity appeared in the form of patents. Google offered to buy Nokia’s patents to be used for Android. In this way, Nokia would have gotten cash and Android, Nokia’s new platform, would get more power against its competitors. Nokia still kept its cool. It wanted cash and considered Google’s offer for the value of the patents too low. One possibility to take care of the cash flow would be to overcharge for the patents, but Google didn’t seem to be ready to do this. What was probably the most likely scenario, was that the parties’ price expectations were too far from each other.Elop kept a physical distance from the negotiations. He and Schmidt had not met in a real negotiation even once. There were two or three phone negotiations, but there were only meetings at events, at the most. Elop was also not known to have met any of the other Google negotiators. This makes one wonder, when it is known that Elop met Microsoft’s Ballmer at least twice in direct negotiations.In the beginning of January, something happened. The negotiations with Google stopped.According to one version of the story, it started at the world’s largest consumer electronics fair in Las Vegas on January 6–9, 2011. The newest Android phones and tablets got a very good reception. So good, that Nokia’s value in the eyes of Google dropped decisively. According to this version of the story, Google understood that it could take over the world without Nokia, nor would it need to risk its relationships with other manufacturers by taking in Nokia with special conditions. Now Nokia only had the standard conditions, which are jokingly said to be: Welcome to Android, the source code can be found for free in the internet.This version of events are not likely the real story. The deciding point in January was, according to a dependable source, that Nokia finally understood, that it would not get money directly from Google under any conditions. Nokia wanted a quick solutions and billions of cash. Google offered a position based on Nokia’s strengths in Android, but it would only have produced money in the long run.When this was understood, the negotiations with Microsoft got a fire lit under them. They moved to questions about business activities and marketing. The meeting in London was with Öistämö’s Microsoft counterpart, Andy Lees. Someone who was closely following the negotiations told Wall Street Journal a few months later that at this point the contact almost broke. Nokia realized that Microsoft was, after all, offering the world’s largest phone manufacturer the standard agreement, even if Nokia was casting all its chips into the game. The Nokians showed their eye for the game. The Microsoft crew knew that Nokia was negotiating with Google. Nokia jumping ship to Android would ruin Microsoft’s chance of a century. Just because of this, Ballmer flew to Helsinki, to assure the Nokians that Redmond was serious.On January 10, 2011, Öistämö, together with his colleagues spent the morning in a windowless room in the cellar of a London hotel. Nokia told Microsoft that it wanted freer rights than the competitors to innovate with Windows Phone. The Navteq maps had to be the basis for all Microsoft services. On top of that, Öistämö decided to ask a large sum of money. Öistämö had calculated correctly. Microsoft had come to the conclusion that it could not afford to let Nokia slip from its hands. According to Bloomberg Businessweek, the negotiations proceeded quickly after this. When the parties met for dinner in an Indian restaurant, the agreement was sorted out in time for dessert. The description in the magazine was partially a legend fed to the public. All the items, including the money, were negotiated throughout the fall, but they created a package out of all of them.The contract was completely different from what Microsoft had made with other manufacturers. It included marketing money, reduced license fees, and special rights with regards to technology. The road appeared to be opening up. Nokia would get its ecosystem and the driver’s seat for Windows Phone. The special status was only achieved, according to Elop, because Nokia promised to do everything it can for the benefit of Windows Phone. The arrangement included a commitment not to use other smartphone platforms.The board got the results put before them on the second week of January.The reception was favorable. According to the information the board had, only three people presented questions that were even somewhat critical. Otherwise, the reaction was a poodle-like “sounds good”. The decision was, in the end, unanimous. No one voted against. Microsoft’s terms with its billions of euros were viewed as the best.In our interviews some recounted how they actually thought differently.One interviewee describes how he found the agreement directly unreasonable. Nokia took a risk, but Microsoft would take most of the profits. Another said how he suspected that Windows Phone was not ready as a product. Also, it was understood that Microsoft had a bad brand reputation. Elop’s activities caused bad feelings. One described how Elop borrowed from his ex-workmates, when talking about the details of the system. Elop seemed to be bypassing the normal communication channels with Microsoft, and gave an impression that he knew more about the product than what he told the board. He was thought to have insider information, which made the board think that he was making his decisions based on better information than the rest. One board member uses the word “blindsided”. Elop gave the understanding that Windows Phone was going strongly upward. In reality, looking back, Google benefited, he estimates.One thing that was considered very brazen was Öistämö’s strong support for Windows Phone. MeeGo’s difficulties were, in many people’s opinion, the achievement of Öistämö. He had led the team for 18 months out of the last two years. Now he had become a turncoat and stood behind Elop.The board had put forward a proposal, where Windows Phone would be used for the expensive smartphones, and Android for the cheap ones. According to a reliable source, Elop had said that Ballmer would not agree to any special terms, but that Microsoft demanded exclusive rights. The standard agreement for Windows Phone would always be available, but in this case there were billions of euros invested. Microsoft offered so much money in the short term, that the offer was difficult to refuse. When on the other side was Google’s zero-offer, the game started to be clear. Google started getting the message that Nokia was going with the competition.The negotiations should have been confidential, so the Nokians were quite shocked when one of Google’s leaders, Vic Gundotra tweeted on February 9:The date of the hashtag pointed at Nokia’s capital markets day, where it was promised that the new strategy would be announced. The hint was easy to understand. “I guess they did not like the decision”, Elop commented on the tweet later.For senior managers in Nokia, the first four months of Elop’s leadership were a tough time. One picture of the period was a combination of confusion and self-defense. When survival through organizational changes was uncertain, everyone concentrated on making an impression on their new boss.Before announcing the decision, Elop invited 200 people underneath the top management level for two days at Windsor, near London. Windows Phone 7 devices were handed out to the group, so that the managers could demonstrate the features to their subordinates. Windows Phone phones were sold out right away in Finland, when lower management started to familiarize themselves with their new area of work.The board convened for their decisive meeting on February 10, 2011. There was nothing left to decide. The briefings for the morning were already ready or in progress. The board accepted Microsoft Windows Phone 7 smartphone as its only platform unanimously, out of formality. The most serious issue was the promise of Nokia’s own platform and development work. The board wanted to avoid the mass layoffs, which would have resulted from choosing Android. The board certainly affirmed that they would be sailing in unknown waters. The brand would get watered down, when they started selling Windows Phone 7 devices instead of Nokia. The atmosphere could be described like this: They would have wanted to make a decision, where they could say, “Wow, now this will really take off.” Microsoft gave so much money, but it still felt like they were between a rock and a hard place. Their thoughts went in the direction of hoping that the ecosystem would gradually take off.The details of the contract are still mostly secret. It is known that Microsoft promised to pay Nokia $250 million a quarter to support the platform. Microsoft would buy licenses for Nokia’s patents and would put money into the marketing of Windows Phone. The sums to be used for marketing or the wishes regarding them were not detailed in the contract. They would be decided case-by-case. Nokia would pay royalties of about 10 euros ($15) per device to Microsoft for using Windows Phone. There was a minimum sum per year for royalties, which Nokia would be obliged to pay regardless of the sales volumes, and the sum would increase over time. The base fee would be, for a long time, bigger than the minimum royalties. The royalties would start running only when phones were in sales, so in the beginning, Nokia’s cash flow was strongly on the receiving end.Microsoft would start to use Nokia’s maps and navigation platform in all its services, and would benefit from Nokia’s relations with network providers and could charge users for its own services together with the phone bills. Nokia would install the Bing search engine on its devices. The revenues from the services would be shared. Nokia would get its own share from the advertisement revenue from Bing searches and from the ads in the maps.The contract was most likely for at least five years, if not longer. Nokia was not allowed to use competing platforms in its smartphones. The first opportunity to withdraw with a reasonable penalty was probably at the end of 2013.Even if time was spent mostly on smartphones in Nokia’s strategy planning, the board decided in the same meeting regarding the other parts of the new strategy. There would be two support columns besides Windows Phone.The name of the first one was “the next billion”. Nokia calculated that there were 3.2 billion people still without a mobile phone. The company started approaching these potential customers in Brazil, Russia, India, Indonesia, China, and Africa with its old feature phone platform, which is called S40. Despite the name, it is a separate product from Symbian and S60. They promised internet, full QWERTY keyboards, and partial touch displays in their simple feature phones. Because of the huge sales volumes, S40 was believed to be an attractive platform for Java app developers. For people with no access to the internet, there would be services based on text messages.MeeGo was put on the back shelf, just in case. The developers would get to launch one product to market. Afterward, they would start to search for and sniff out the next big disruptions. The service portal ovi.com was, in practice, given its farewell. It would join the Windows Phone app store.Nokia announced that it would restructure its management and organization. Estimates made beforehand were off in that all the Finnish members of the Group Executive Board would keep their positions. New blood was found in-house. The new members were Colin Giles (sales), Rich Green (technology), Jo Harlow (smartphones) and Louise Pentland, who as the person responsible for legal matters, brought patent issues to the Group Executive Board. Because Torres, who was responsible for MeeGo, left the Group Executive Board immediately, the number of members grew to 13.In the beginning of April, 2011, there were two business units: Smart Devices, which was specialized in smartphones, with Jo Harlow at the lead, and Mobile Phones, with Mary McDowell at the lead. The Markets unit, responsible for product sales, would be led by Niklas Savander and Services would be led in the meanwhile by Tero Ojanperä. Kai Öistämö, who had an important role in the Windows Phone negotiations would be in charge of development.When the February 11 finally arrived, Elop made a tweet directed back at Google’s Gundotra:@selop@cheureux Or this: Two Bicycle makers, from Dayton Ohio, decided to fly.#NokMsft #feb11The sentence refers to the Wright brothers, the designers of the first working airplane. When Orville Wright lifted off into the air on December 17, 1903, the flight lasted 12 seconds and was 36 meters long. Wilbur Wright flew 259 meters on the same day, and the flight lasted 59 seconds. What was reassuring in Elop’s short allegory was that unlike with many pioneers of flight, both brothers survived their test. Wilbur died of typhoid fever at the age of 45, and Orville at the age of 76 of a heart attack while he was fixing the doorbell of his home.11. ReactionsElop’s busy fall and early winter peaked at London’s Hotel Intercontinental on February 11, 2011. News about Nokia’s selection of the Windows Phone platform had already been released. After introductions, Elop took to the stage wearing a dark grey suit and tie. Watching his speech, one wouldn’t have guessed that investors had already voiced their opinion of the strategy — the price of Nokia shares had fallen 14 percent.Elop started with a short introduction to how mobile phone markets had changed, how the competition between devices had become a war of ecosystems, and how Nokia would be winning that war. Then Elop quoted Winston Churchill:The pessimist sees difficulty in every opportunity. The optimist sees the opportunity in every difficulty.According to Elop the whole technology industry was based on this optimism. He said that he was very thrilled about the partnership between Nokia and Microsoft. Together the two companies would have a chance to change the direction of the war on ecosystems.Elop then invited Microsoft CEO Steve Ballmer on stage. Ballmer boasted of how the users of Windows Phone were extremely satisfied with the platform and how the number of applications was rapidly increasing. The collaboration with Nokia would boost development of the ecosystem even further. For example, Nokia’s map services and superior camera technology would become a key part of the ecosystem.According to Ballmer, plans for the first Nokia Windows Phone devices were already moving forward, including talks with network providers and chipset vendors.The duo, former subordinate and boss, went to the middle of the stage for handshake photos. The pair perched on two bar stools to answer questions. One of the first questions was about the schedule; when would the phones come on the market? Elop said that they wouldn’t give any estimates of the schedule at that time. However, he assured that Windows Phone would allow Nokia to release phones even faster. The next question asked about changes to the relationships with other phone manufacturers on the Windows Phone ecosystem. Ballmer answered that Microsoft would continue to collaborate with them. Nokia would get a special status, but no exclusive rights. Elop added that the goal was to ensure the success of the Windows Phone ecosystem. Within that ecosystem Nokia could differentiate as it would enjoy a special status.Elop said that in collaboration with Microsoft-based phones Nokia would be better positioned to compete in the sub-hundred euro price category than with Google.The Nokia press conference was the technology news of the day around the globe. Internet publications started immediately to comment on the announcement.For most, the first reaction was a shock. Nokia was expected to announce some collaboration with Microsoft, but it was thought to bring Windows Phones on the markets only in a few countries. No one had foreseen that Nokia would go all in with Windows Phone, with no other options.In the interviews for this book even some major Nokia shareholders expressed their surprise in the choice. They had expected a strategy based on a mix of platforms.“I expected a multi-platform strategy. MeeGo was a bold move and I expected the company to hold on to it. I saw business potential in Windows, mainly because Nokia had such a large number of enterprise customers”, said one owner.“Choosing only Windows was an extraordinary strategy choice. None of the financial details, goals, terms and conditions were revealed. This left the investors totally in the dark. That was a strong signal that the terms were still open. It was obvious that this was done in haste”, says another owner.Even three years later it is still confusing to read the reactions that have now been shared in the public. Journalists and industry outsiders seem to have been most accurate in their forecasts of the things to come.One of the commenters, C. Enrique Ortiz, editor of a blog called About Mobility, stated on a post dated February 11 that Nokia had not correctly identified what the real threats were. The real competitors were Apple with iPhone and the manufacturers of Android phones. The manufacturers — not Google or Android.“Relationship with Microsoft will help fence off HTC and others just on the Windows Phone front, but that is a tiny front. And if you ask me, this looks like the beginnings of a relationship that may end up in Microsoft absorbing Nokia”, Ortiz estimated.According to Tero Kuittinen from MKM Partners, Elop had decided to risk the Symbian sales for the rest of the year by moving forward with Windows Phone. “Nokia jumps into the freezing waters with a platform that only has 3% market share”, Kuittinen commented.Many thought that the combination of Microsoft and Nokia was a no-win situation. “Nokia sold itself for free. Google and Apple laugh on their road to duopoly”, analysed Neil Campling from Aviate Global LLP brokerage.A developer shared his frustration on Forum Nokia: “Wow, what can I say! Nokia just killed all my interest in developing anything on its platforms ever again.”“I’m shocked! One of the biggest wins in corporate history for Microsoft. For the first time ever a leading technology brand rejects its own platform for its smallest competitor”, commented Tomi Ahonen, a technology consultant.Motorola’s reaction was also telling. Nokia’s competitor stated that the partnership was uninteresting, almost a non-event. Alain Mutricy, head of Motorola Mobility, stated that the partnership was a strong indication that Nokia would not be competitive in short term.But the reaction was not all negative. After the initial shock, some positive comments started to come up. Many thought that the collaboration could still be a win for all parties.“I’m very excited. Nokia makes excellent devices and Windows Phone 7 is really a great operating system. Toast for the beautiful partnership! This union is made in heaven”, Gary Marshall from Techradar congratulated.“This deal will lift Nokia back up in the forefront of smartphone manufacturers”, beamed Andrew Harrison for Carphone Warehouse.The Finnish newspapers made some hilarious interpretations. On one of the tabloids there was a headline that claimed that Elop had said that it was time for Nokia to “shoot a duck in the head”. The tabloid further stated that Elop had called this — quite rightly so — a Finnish proverb. The meaning was lost in translation. Elop used the proverb “to shoot ahead of the duck”, which was meant as a way to make clear that it was time for Nokia to become more tuned towards sensing and anticipating the changes in mobile industry. The tabloid journalists had never heard the proverb before, and it was thus wrongly translated.Stock markets gave the heaviest verdict on the announcement of February 11th. After two days of market activity Nokia had lost 5.7 billion euros ($8 billion) of its market capitalisation. To give some perspective: Microsoft later bought Nokia’s phone operations for 5.4 billion euros ($7.5 billion).The parties most affected by the choice of Windows platform voiced their opinions loud and clear. On the day of the announcement one thousand people marched out of the offices at 2 pm in Tampere, one of the centers of Symbian development (half of the 3,000 people who worked on the site worked on Symbian development). Kalle Kiili, the union spokesperson, told Finnish News Agency (STT, Suomen Tietotoimisto) that many people actually used their flexible working hours when leaving the office early. The employees were still very loyal towards Nokia. Kiili hoped for more information and clarity on cost savings and plans to increase efficiency. Even if people demonstrated, the personnel considered Windows to be a better companion that Google, according to Kiili.The reactions in the MeeGo camp were stronger. In the main development site at Ruoholahti, Helsinki people watched the London event via a webcast. One witness describes the scene:“Almost everyone went straight to the bars, and didn’t come back to work for days. The best Linux developers started to leave Nokia the following week. Intel was especially quick to make offers.”Social media was boiling. Joe Wilcox from Betanews summarized the feelings: First Tunis. Then Egypt. Now Nokia. There was a storm of protests against the decision across Twitter.February 11, 2011 was a Friday. Two days later the yearly mega event Mobile World Congress kicked off in Barcelona. On the press event that Nokia organised on Sunday evening the biggest question was finally voiced publicly. Someone from the audience shouted: “Are you a Trojan horse?” Elop was calm in his reply. He said that he obviously was not a Trojan horse. Nokia had been careful to include the whole leadership team in the strategy development process. Only the Nokia board could make a decision with this big of an impact on the company, he assured.Elop reminded the audience that even though Nokia would have to pay royalties for using Windows Phone, they would reduce costs on product development. Elop didn’t answer to questions about the size of those savings.On the third day of the Barcelona event there was a surprise as Elop joined Ballmer on stage during a press conference. Together, they assured the crowd that the ecosystem of Nokia and Microsoft would be the best for all network providers. Elop promised to listen to the service providers who were worried about the power of Apple and Google. Network providers were promised new ways to increase revenues from their own applications and services. The priority of Nokia-Microsoft is to create the best platform for network providers to create value, Ballmer promised.Ballmer answered the last question in the London press event. He recounted how he had first talked about the Nokia strategy choices together with Elop in November. Elop had then told him that the decision would be made in only a few months.“To me that sounded, should I say, fast. And here we are. I think it’s incredible.”Ballmer is one of the most experienced corporate leaders globally. He has worked in the IT sector, in the fastest moving business his whole career. Still he thought that the Nokia decision making process was exceptionally fast. So fast, in fact, that he commented on it publicly.In just three months Nokia had made the decision that would seal its destiny. This decision were prepared by a man who had only worked for the company for five months — a CEO who had come from outside the industry.The question is unavoidable: Had Elop and his team had enough time to see the full picture?How could they know whether Microsoft had been open and honest? It’s not stupid to leave something unsaid. It’s stupid not to ask. Fixating on making the announcement on the Capital Markets Day in London raises some eyebrows — as if it was more important to make the decision than make it a good one.An entertaining detail in these Nokia strategy development initiatives is related to numerology. The strategy work that Kallasvuo had ordered from Anssi Vanjoki was known as 10–10–10, as it was to be announced on October 10, 2010. The London capital markets day was on February 11, 2011 as stated before. Europeans commonly write that date as 11.02.2011. This is a palindrome, the same digits regardless from which end you read it.12. The great bluffOne wonders how, in connection to the selection of Nokia’s strategy, there was barely any criticism regarding the reasons. Columns, editorials and some analysts explained how Nokia could not differentiate with Android, how it could innovate with Microsoft, how the Nokia-Microsoft ecosystem was the best for network providers and how phones with MeeGo OS would have gotten to the market too late. All like straight from Elop’s mouth.Many people interviewed for this book attack the reasonings of Elop surprisingly vigorously. Impressions vary from “complete bullshit” to “distorted”.In this chapter we will examine the main arguments point by point. The review is partly unfair, since hindsight is always 20/20. Still, it’s valid: Argument proven wrong by time is still wrong. The target is to consider the situations at the time of the decision making, rather than from the viewpoint of what has happened since.Elop’s reasonings can be described with three words: A great bluff.According to the people interviewed, he had immersed himself with everything related to mobile business during the first few months. He knew the platforms, their ecosystems and was excited about the devices all the way to the smallest of details. A good example of this is an interview with a programmer in Financial Times who, as a protest, resigned after February 11, 2011. He described a face-to-face meeting with Elop. Exhibiting deep knowledge of the subject, Elop had made many excellent and detailed questions related to both technology and functionality. To have based his decision on wrong information was an impossibility, according to this programmer. Ergo, he was distorting the facts. Conclusions from interviews within and without Nokia are in line with this thinking.In early 2011 Android, MeeGo and Windows Phone were like three sprinters shoulder to shoulder at the finish line. Anyone could become the winner. All had their pros and cons. Once one had been chosen to be a bit better than the others, it was natural to bend the arguments to support the winner. Some of Elop’s arguments were valid. Some were forced to fit the mold. Some were just empty words.Nokia Could Not Differentiate With AndroidThis is the argument most used by Elop. According to him, Samsung had taken over Android. Nokia would have become a simple hardware manufacturer, fighting for its existence without proper weapons. The fate of the likes of HTC, LG and SonyEricsson has since validated this argument, according to Elop and Siilasmaa.The CEO of one European network provider sees it a bit differently. He thinks Nokia itself proved the argument wrong. After the launch of the first and only MeeGo phone, N9 — less than half a year after publicizing the Microsoft strategy — Nokia clearly differentiated itself from the competition by industrial design, states this CEO. Anyone who held an N9 or a Lumia phone understood that this was a Nokia, not a Samsung. Nokia would have brought a modern, prize-winning design language to the Google ecosystem, differentiating from the competition before the customer would have even turned on the device.What gives this argument some additional weight is the fact that, other than Motorola, all the Android manufacturers came from the Far East. Due to the Asian culture emphasizing authority, design there often takes a backseat according to an industry insider. Designers would rather please their superiors than have strong opinions or wild visions and because of this, Nokia would have had an edge. Scandinavian design principles emphasize clear lines and practicality, he says.Nokia could have also taken full advantage of its brand within the Android world. Taking away the negative aura of Windows Phone and lack of applications affecting the image, Nokia’s brand would have helped to overtake Samsung. It would have been able to differentiate.Nokia had a house full of very capable open-source coders released from MeeGo, ready to create their own UI on top of Android. Nokia maps were better than Google’s; the camera functions better than what the competitors had. What more would you have needed?However, there was a justifiable concern that, with Android, Nokia would be just another hardware manufacturer. That ended up happening regardless.Former Apple and HP executive Jean-Louis Gassée put it brilliantly. In his mind, Nokia would have brought gorgeous phone designs to Android and by combining ovi.com applications and services with the existing Android offering, people would have said Nokia has joined a winner. Quite a different image, considering the alternative of binding yourself to a widely hated software giant.One Nokia board-level executive says Android was never properly studied. It could have been customized way more than was stated by Elop. This executive also notes, rightly so, that there was practically no way to do any customization on top of the Windows Phone user interface. Poignantly, in June 2014 — after the phones business was already sold to Microsoft — Nokia published a software called Z Launcher. It was basically Nokia’s own UI and user experience on top of Android. The answer to the question “Why this was done?” was revealing — because it was possible.MeeGo Phones Would Have Gotten To Market Too SlowlyOne interviewee with first-hand knowledge of the matter says this argument is as stupid as it gets. Many people agree. You get as many products out as you have product programs initiated and executed.Elop adopted this thinking from Kai Öistämö and Jo Harlow. In their communication to Elop at the beginning of January 2011 it stated that “with the current speed”, Nokia would have three MeeGo phones out by the year 2014. No doubt, this was true. But you have to consider the phrase “with the current speed”. In reality, Nokia could have brought MeeGo phones to the market faster and with a broader selection than the Lumia offering.Why?We go back to technology. As said before, MeeGo had been implemented on top of two chipsets. Additionally, in the summer of 2010, contact was made with the chipset vendor ST-Ericsson. The target was to develop an inexpensive but powerful foundation for MeeGo. An employee involved in the project is certain that the resulting features and usability would have been on par with Android. That would have meant three chipset vendors for MeeGo. Windows Phone was confined to one chipset, which limited product variety.The strongest counter-argument to Elop is the N9 phone running the MeeGo OS. It would have been easy to create variants. After all, the first two Lumia phones were basically the same device with two different housings, based on the N9 chipset.According to the financial analyst Sami Sarkamies, it’s not at all clear whether Windows was chosen objectively. Justifications are hard to verify and arguments changed within six months. Among the wrong assumptions was, for example, the thinking that the MeeGo upgrade release cycle of six months would be too slow compared to Lumia. Also wrong was the assumption that MeeGo would have fewer languages available. According to a MeeGo team member, there would have been 42 languages available from day one. Windows had less.When talking about MeeGo, what was often forgotten was that it also worked in laptops, vehicle consoles, smartphones and alarm clocks, hence making the ecosystem broader than just phones. MeeGo was compatible with multiple gaming platforms. Also compatible with Android.Lumia phones, due to the shortcomings of Windows, had a lot more limitations compared to MeeGo. With Windows Phone you could not have as big displays as with Android. It also limited the maximum amount of pixels for the camera. And as you’ll find out in the following chapters, only in connection to the new strategy did Nokia find out how half-baked Windows Phone was. According to one estimate, it was a year behind MeeGo. An example: MeeGo was working on a device with a full QWERTY keyboard. Windows Phone still didn’t support that.With MeeGo you would have had a broader selection of phones compared to Lumia. Absolutely. Delivered to the customer at least as fast as Lumia phones. For sure, MeeGo was late. But it’s wrong to assume that it would have never gotten ready.Nokia and Microsoft Have a Special RelationshipIf on February 11, 2011, you would have asked Hugh Brogan, a Brit, whether Nokia made a good or a bad deal the answer would have been swift. Bad.Brogan was an Elop of his time. He had gotten a great proposal from Microsoft in 2001. Windows phones were on their way to the market and Microsoft wanted Brogan’s company Sendo to make them. Brogan was excited: Windows for mobile phones, supported by the Microsoft marketing budget. Phones would be on sale August 2001, exclusively with Sendo. They had a special relationship with Microsoft. Brilliant!Pretty soon Microsoft announced that the software was not ready yet. To speed things up, Sendo stepped in with implementation and took a loan from Microsoft.Two weeks before the public launch, Brogan’s and Sendo’s worlds collapsed. Orange, a mobile network provider, announced a Windows phone made by HTC, a Taiwanese manufacturer. Brogan understood that Microsoft had given HTC all the fixes and additional implementations made by Sendo. According to Brogan, Microsoft had only wanted to bankrupt Sendo and highjack the knowhow that would become their property according to the contracts. Sendo turned to Nokia and started manufacturing Symbian phones, but filed for bankruptcy in 2005.Not many companies have as bad a reputation as Nokia’s new partner had. Microsoft was founded by Bill Gates and Paul Allen in 1975. It claimed its fame by taking over the operating system market for personal computers in the 1980s. MS-DOS, which later evolved into Windows with different versions, became the de facto standard of personal computers. Microsoft went public in 1986, creating three new billionaires and 12,000 millionaires out of its employees. The 1990s was a time of expansion for Microsoft and it has made multiple big acquisitions along the way.Gates is still the biggest individual shareholder with about five percent ownership. Steve Ballmer, who in January 2000 started as the CEO, is a close second with about four percent of the shares. Ballmer started at Microsoft in 1980 and was the 30th employee to join the company.When Microsoft’s position in the market was close to a monopoly, they took advantage of the status. Forcing Windows OS users to use Microsoft’s own internet browser and media player are among the issues that were under government investigation, and there have been accusations, lawsuits, and judgements. Microsoft’s disregard of the laws even forced the EU commission to issue a fine for not complying with the EU ruling in a case related to fair competition. Wikipedia has its own page dedicated to critique against Microsoft. They have been scolded regarding the treatment of sexual minorities, unfair license practices and shady acquisitions.Following the purchase of Nokia, a list with a headline “In memoriam: A list of Microsoft’s former strategic telecom partners” appeared on the internet. In July, 2006, Steve Ballmer was sitting in front of the press with the CEO of Nortel, Mike Zavirovski, laughing at the iPhone. The two companies were to create business solutions for mobile networks together. Two years later Nortel went bankrupt. LG of South Korea signed a multiyear agreement in 2009 promising to use Windows Phone OS as their primary platform. There was to be 50 different phone models brought to market. They quickly gave up and moved to Android. In the year 2000, Microsoft had made a similar alliance with Ericsson, with similar results.Microsoft has of course also acted in positive ways at times. When Steve Jobs returned to lead Apple in 1997, the company was in shambles. They were bleeding money, the products were a mishmash and there was no focus. Gates stepped in to help. He invested $150 million into Apple and the two companies announced a broad partnership program. Microsoft Explorer became the default internet browser for Macintosh; Microsoft Office was made available for Macs and Microsoft promised the development of other programs too. The injection of cash brought peace of mind for Jobs, and the rest is history: In May, 2010, Apple overtook Microsoft when measured by market capitalization. There were of course some alternative motives related to the investment. Microsoft settled some patent disagreements and expanded the market for Microsoft Office. Microsoft has since sold its share of Apple.At the time of the decision related to Nokia, Microsoft seemed like an untrustworthy and selfish partner. The special relationship promoted by Elop was more a subject of fear than joy. Nokia had no guarantees of this special relationship working out. A bit of number crunching didn’t help. The two companies had equal amount of revenue in 2010, but Microsoft’s profit was 15.2 billion euros ($20.4 billion) compared to that of 2.3 billion euros ($3.1 billion) for Nokia. Market capitalization of Nokia was 26 billion euros ($35), which was 15% of that of Microsoft. Microsoft had 30 billion euros ($40 billion) of cash in the bank and in the context of Windows, Windows Phone was just a sideshow. There was only one guarantee for the special relationship: Stephen Elop, the person.What about the earlier cooperations with Microsoft? What could we tell from history?First, Symbian mobile phones with Microsoft Office support were supposed to come to the market during 2010. Ominous silence surrounded the project. Elop and Öistämö had not been able to make the cooperation work on schedule. Nobody knew this at the time, but Office appeared in Symbian phones for the first time only in April 2012.With Windows Phone, Nokia Could InnovateAt this point we evoke the help of hindsight. What could Nokia have innovated during the Microsoft cooperation?Better cameras than the competition? Nope. The monster camera with 42 megapixels was done with Symbian.Advanced camera and photo applications? Yes. Why not create those within Android?Wireless charging? Yes. Nokia was the first one to bring to market a properly working version of a phone with wireless charging in Lumia 920. It was based on an industry standard, so competition was quick to catch up. As a sales argument, wireless charging turned out to be a bad one, so Nokia has since moved to providing it as an accessory rather than part of the phone itself.Location services? Yes. With this, Elop was absolutely on the money. But you could have had your own maps also with Android.What about other Nokia-specific applications and system updates? These were things already done once with Symbian. Not exactly innovation.Anything else?Not really. Innovation was limited to navigation, and those results were provided to end customers free of charge. And yes, it was a differentiating factor.Former Nokia director Christian Lindholm summed it up well in Digitoday, an online publication, in the summer of 2013. According to him, the mobile phones industry as a whole was in a state of standstill due to the disappearance of an innovation factory called Nokia. The Android camp was silent, products were selling without much need for new inventions. People we interviewed put it like this: All the innovation done with Microsoft could have been done with Android too.Nokia-Microsoft is the Best Ecosystem for Network ProvidersConsidering this statement, Nokia came in at a good time. There was a market for the third ecosystem. Network providers liked Nokia-Microsoft and when Nokia adjusted its operations all the more to their liking, this argument holds water even in hindsight.Elop had been making his rounds with network providers before choosing Windows Phone and as stated earlier, MeeGo was left without needed support. According to the information coming from sources at the Nokia board level, the attitude towards Windows Phone was similar. A third ecosystem was welcomed, but the network providers were not willing to help it succeed. It was up to Nokia and Microsoft.Later in this book, we’ll find out that many of the international network providers were against Windows Phone from day one. It was seen as starting from too far behind the competition. Scepticism was taking over. The operator billing function touted by Elop had lost its value since Apple and Google had already gotten to the credit card information of the customer. In developing countries where credit cards were not so prevalent, this argument was more valid.And when it comes to the possibility of network providers having their own sections in the Microsoft application store — it didn’t matter. Chances of making money with that were next to nothing.In hindsight, most network providers agree they were delusional from the start. The third ecosystem was not really needed. Why? We’ll get to that later.Don’t get this wrong. Some of the network providers did support, also financially, the birth of a third ecosystem. However, the majority of international network providers, while hoping for Nokia to succeed, didn’t consider that believable. Multiple manufacturers had already tried Windows Phone, it was known but no longer seen as even a decent differentiating factor.13. The catastrophe called SymbianFebruary 3, 2011 was a wet and rainy day in southern Finland. It was nearly freezing. The steady stream of icy rain was fading away to a drizzle towards the evening. It had been almost four and a half months since Stephen Elop started.Human resources manager Salla Jämsä remembers well how the employees were invited to the AB cafeteria [12] at Nokia House, and how the email stated that the event was mandatory for all invitees. The cafeteria was closed the whole day, and a stage was built especially for the event on the west side of the lobby. It was nearly one o’clock in the afternoon. Not all of the 5,000 employees working in Nokia House were there, but even so, there didn’t seem to be enough oxygen in the air for everyone. The doors were covered by security, and everybody was repeatedly reminded about turning off their cameras — no recording of the event was allowed.Jämsä remembers the speech clearly. “He started by building on how he’s covered all these continents. And spoken with hundreds of people. He seemed to be speaking honestly, but I felt that he was being provocative. That how could things possibly be so bad, all of a sudden. I didn’t get the sense that he was putting us down, disparaging our work. He was constantly referring to the market situation.”Elop gave a serious and straight-faced presentation, according to Jämsä. No tears, no laughter. Very neutral. “When he introduced the parable of the burning platform, I thought he might jump off the stage. I thought, maybe that’s why they built the stage. Well, he didn’t jump.”About half an hour later, the speech was over, and so was the event. Jämsä recalls somebody with a good hunch of Elop’s plans retorting, I told you so. Otherwise, nobody had a sense of how vast the consequences of such a short event would be. When the internally published memo on the speech was leaked, an avalanche was set in motion.There is a pertinent story about a man who was working on an oil platform in the North Sea. He woke up one night from a loud explosion, which suddenly set his entire oil platform on fire. In mere moments, he was surrounded by flames. Through the smoke and heat, he barely made his way out of the chaos to the platform’s edge. When he looked down over the edge, all he could see were the dark, cold, foreboding Atlantic waters.As the fire approached him, the man had mere seconds to react. He could stand on the platform, and inevitably be consumed by the burning flames. Or, he could plunge 30 meters in to the freezing waters. The man was standing upon a “burning platform,” and he needed to make a choice.He decided to jump. It was unexpected. In ordinary circumstances, the man would never consider plunging into icy waters. But these were not ordinary times — his platform was on fire. The man survived the fall and the waters. After he was rescued, he noted that a “burning platform” caused a radical change in his behaviour.We too, are standing on a “burning platform,” and we must decide how we are going to change our behaviour.Over the past few months, I’ve shared with you what I’ve heard from our shareholders, operators, developers, suppliers and from you. Today, I’m going to share what I’ve learned and what I have come to believe.I have learned that we are standing on a burning platform.[…] For example, there is intense heat coming from our competitors, more rapidly than we ever expected. Apple disrupted the market by redefining the smartphone and attracting developers to a closed, but very powerful ecosystem.In 2008, Apple’s market share in the $300+ price range was 25 percent; by 2010 it escalated to 61 percent. They are enjoying a tremendous growth trajectory with a 78 percent earnings growth year over year in Q4 2010. Apple demonstrated that if designed well, consumers would buy a high-priced phone with a great experience and developers would build applications. They changed the game, and today, Apple owns the high-end range.And then, there is Android. In about two years, Android created a platform that attracts application developers, service providers and hardware manufacturers. Android came in at the high-end, they are now winning the mid-range, and quickly they are going downstream to phones under €100. Google has become a gravitational force, drawing much of the industry’s innovation to its core.Let’s not forget about the low-end price range. In 2008, MediaTek supplied complete reference designs for phone chipsets, which enabled manufacturers in the Shenzhen region of China to produce phones at an unbelievable pace. By some accounts, this ecosystem now produces more than one third of the phones sold globally — taking share from us in emerging markets.While competitors poured flames on our market share, what happened at Nokia? We fell behind, we missed big trends, and we lost time. […] The first iPhone shipped in 2007, and we still don’t have a product that is close to their experience. Android came on the scene just over 2 years ago, and this week they took our leadership position in smartphone volumes. Unbelievable.We have some brilliant sources of innovation inside Nokia, but we are not bringing it to market fast enough. We thought MeeGo would be a platform for winning high-end smartphones. However, at this rate, by the end of 2011, we might have only one MeeGo product in the market.At the midrange, we have Symbian. It has proven to be non-competitive in leading markets like North America. Additionally, Symbian is proving to be an increasingly difficult environment in which to develop to meet the continuously expanding consumer requirements, leading to slowness in product development and also creating a disadvantage when we seek to take advantage of new hardware platforms. […] Chinese OEMs are cranking out a device much faster than, as one Nokia employee said only partially in jest, “the time that it takes us to polish a PowerPoint presentation.”[…]The battle of devices has now become a war of ecosystems, where ecosystems include not only the hardware and software of the device, but developers, applications, ecommerce, advertising, search, social applications, location-based services, unified communications and many other things.[…]How did we get to this point? Why did we fall behind when the world around us evolved?This is what I have been trying to understand. I believe at least some of it has been due to our attitude inside Nokia. We poured gasoline on our own burning platform. I believe we have lacked accountability and leadership to align and direct the company through these disruptive times. We had a series of misses. We haven’t been delivering innovation fast enough. We’re not collaborating internally.Nokia, our platform is burning.We are working on a path forward — a path to rebuild our market leadership. When we share the new strategy on February 11, it will be a huge effort to transform our company. But, I believe that together, we can face the challenges ahead of us. Together, we can choose to define our future.The burning platform, upon which the man found himself, caused the man to shift his behaviour, and take a bold and brave step into an uncertain future. He was able to tell his story. Now, we have a great opportunity to do the same.Stephen.The parable is old and much used. It was created by the change management guru Daryl Conner in his book Managing at the Speed of Change in 1992. In 1988, he had been looking for a metaphor for the kind of commitment you need to manage change. The news came on the TV. There had been an accident. The oil-drilling platform Piper Alfa, off the coast of Scotland, had exploded and 167 people had died.One of the survivors — there were 61 — was interviewed at the hospital. They asked Superintendent Andrew Mochan why he took that potentially fatal 100 foot (30 meters) leap into the sea. Without hesitating he answered, “It was either jump or fry.” Conner understood that Mochan had chosen between certain death and probable death. Change managers need to commit to their decisions with similar determination. There is no looking back. Major change needs to be driven through with the force of certain death behind them. No matter how scary the change feels.This story made its way to Harvard Business School and the McKinsey canon, and took on a life of its own. Consultants use it to describe the severity of the starting situation. The best-known use is from Harvard Professor John P. Kotter’s 8-step process for leading change. The first step is to know there is a need for change, and create a sense of urgency — your platform is on fire. If people believe it is possible to go back to how things were, change will never happen. Actually, creating a sense of emergency on an Elopian scale was not a part of Conner’s original thinking. Conner says that he has learned to live with this — as long as the situation isn’t made out to be worse than it is. Conner has also stated that the situation does not need to be catastrophic to ensure commitment. Today, they say that the burning platform metaphor should be used with caution. Your jump into the sea is motivated by fear and anxiety, that are negative traits in a company culture. The basic rule is: Don’t ignite the flames, just notice if they are there.The pivotal question about Elop’s speech is, was the picture he painted about the situation at Nokia accurate, or was it exaggeratedly bad? After the speech, the flames were roaring, in any case — in that sense it doesn’t matter whether they were there before the speech or not.The timing of the speech was extraordinarily dramatic. Elop held his speech a few days before the board meeting where the decision about the future smartphone platform was to be made. Because the CEO had just vilified Nokia’s main products, there was no plausible going back. And because the speech fanned the flames aboard the platform to new heights, there was no time to reopen the negotiations with Google. Even though the board had in effect already chosen to go with Windows Phone, in hindsight the CEO played the board for fools and tied their hands. After the Burning Platform speech, switching to Windows Phone was the only choice the board could make.The board saw the memo as a gross miscalculation. This wasn’t the only source of friction between Elop and the board during his first months as CEO. Many board members felt that Elop was making decisions too independently and not keeping the board informed. A CEO does not need to clear internal messages with the board, but in this case the message was too volatile to have been delivered without the input of the board. The chairman Jorma Ollila conveyed a load of bitter feedback to Elop on this score.What about giving him the boot? Should they have fired Elop? There was no serious conversation on that point. The timing would have been impossible. The new CEO was just turning the course of the ship Nokia, and was the guarantor for the deal with Microsoft. They had started, and were in the middle of, a massive change. Everybody is replaceable, but firing Elop would have led to an unpredictable uproar and great uncertainty about the direction of the company. Switching CEOs would also not un-say the speech. The products had been trashed, and that was that. What’s done is done.There was some support for the speech in the boardroom, too. The company should be aware of the crisis it is in. Also, Ollila defended Elop to outsiders. “The memo was an excellent wakeup call to our personnel. I, too, have used that metaphor”, Ollila said on the financial TV show A-Plus, airing on the Finnish TV channel Yle.A noted expert on corporate governance feels that Ollila was in a key role for whether matters moved forward according to Finnish best practices or not. A controversial speech such as the Burning Platform should have been first approved by the board. As this had apparently not happened, our expert pinpoints the key question to be, whether Elop had consulted Ollila about the speech, or not.“The chairman of the board is in a crucial role, as he is in charge of implementing the decisions in practice. If Elop had Ollila’s backing for his speech, it can be said to have followed the guidelines of corporate governance, even if the rest of the board was left in the dark.”, our expert evaluates the situation.There is no certainty on the matter, but it seems as if the speech came as a surprise to Ollila. If that is the case, Elop certainly broke the Finnish guidelines for good governance. That the board made its final decision about the platform as a rubber stamp formality one day before the announcement, was normal according to our expert, but only as long as the process had been conducted with open discussions and good guidance. This had apparently been the case at Nokia.The board was surprised again a few days later. When the choice of Windows Phone as the platform for smart phones was announced, Elop also let out that Symbian would be dropped after a transition period of a few years. There had been a lot of discussion in the board about how to publish this news. Elop had made clear his opinion, that they should publish the fact that Nokia will be using only one smartphone platform, Windows Phone, in the future. Many others raised the objection that communicating the choice of one platform over others has risks. The network providers might draw some conclusions about the fate of Symbian.Even people outside the board tried to change Elop’s mind. A person who had seen the draft of the press release told us that they tried to get some changes done. They felt that the role of Symbian living side-by-side with Windows Phone should have been played up. There should have been a strong message of how Symbian will be developed further and that it will continue to be competitive while the other platform is being driven in. The crucial message, according to our source, was that the platforms were to co-exist, that a continuum was being built, with complementing parts.When the shutdown of Symbian was finally reported, part of the board was surprised at how it was done. A person involved tells us that there had been an intense exchange about the contents of the press conference. The drafts that he saw had no mention of the shutdown of Symbian, or the goal of selling 150 million more Symbian devices. “The final call about how the message would be communicated, was probably made by Stephen alone”, was his judgement as an insider.The situation was infernal. Elop had vilified the current Nokia offering. Almost in the same breath, he had reported that they would be driven down, but that no replacement models were yet forthcoming. According to Salla Jämsä, the personnel realized within a few weeks, that a horrible error had been made in their corporate communications. Symbian was done for. Panic did not set in, according to her, but disappointment and depression became the reigning emotions. A quiet grief, a very Finnish reaction. “In some other country, there might have been riots. On the other hand, everyone had believed that we had to take on an outsider. That no Finn, say Vanjoki, could’ve done that, when they’d have so much heart in it. We did think that this solution was the only possible one.”The sales team realized they’d run up against the challenge of their lives. They had to sell 150 million outdated phones. The number caused some raised eyebrows, as the forecasts of devices to be sold had been reasonably accurate before. The responsible department had perhaps been blinded. The numbers had perhaps been crunched based on the old, growth period methods, even though the market share had been dwindling rapidly. A new theory started to take over: Perhaps it wasn’t a forecast, so much as a goal. When you tell a salesman to sell a hundred phones, he’ll do his best to sell a hundred phones. If you tell him, sell 150 phones, he’ll try even harder. This is what was guessed to be Elop’s logic.The company tried to soothe those who doubted them. Jo Harlow, in charge of the Smartphones unit, opined that consumers don’t really follow the news. Change would be slow. Decisions would be made based on what’s available in the store.Harlow hit the nail on its head, but it was the wrong nail. Retailers make the call on who’s king in the stores. And they stopped buying Symbian. The CEO of a European network provider describes how Nokia salespeople came around a few weeks after the speech, and tried to convince them that business would continue as usual.Another director of a European telecoms network provider described meeting a Nokia sales team a few days after Elop’s speech. “They looked completely lost. I’ve never been in such a horrible meeting. Some were late, some may be hung over. They showed the new models unveiled at the congress, but joked about whether they were any good. They didn’t even try to hide their disappointment. I said, we’d better just go, that’s how bad it was”, the director told us. “All of the network providers had large purchase commitments and the plans were laid based on them. The commitments were immediately opened for re-negotiation. That we’re not going to buy these, even though we did commit to buying. Everybody backed out, which of course destroyed the revenues.”A well-known stock analyst told us of several discussions he had during that spring with various sales managers from Nokia. They described their emotions as deep shock. Symbian died within a week due to the top global network providers getting scared. Any normal CEO would have said that the company will continue to back Symbian heavily, that it will be a part of the low-end range of devices, that we have a host of wonderful plans for Symbian. The normal sales pitch, that nobody necessarily believes, but that is a mandatory part of business as usual.The leadership did what they could. Elop gave his assurance that investments would continue, and that the phone upgrades would be available up until 2016. In April, the new version of the platform, called Symbian Anna, was released, in addition to two new phone models. In August, the struggle continued. The platform’s new Belle version was released, without the word Symbian in its name. Finally, the platform started to look like it should have looked years ago. There were three new phone models, too. After the network providers withdrew their orders, the sales team started focusing on business clients. The story was that businesses would prefer to stay with their old systems, as long as Nokia kept up the support for updates.Time passed Nokia by, in any case. In January–March 2011, the Nokia market share in smartphones dropped by 5 percentage points to 27.7%. In the final quarter of that year, when the first Lumia device came on the market, the Symbian share in smartphones was 11.7%. It took only a year for the market leader to drop to third place in market share. The Android market share had jumped to 50.9 percent.Nokia’s own actions played a part in the dramatic drop. A well-known stock analyst’s evaluation of the situation was, “Symbian wasn’t given a fighting chance to keep up sales, because they made only a few different devices on it. In a way, they were trying to force Nokia enthusiasts to wait for the Windows phones”.In April 2011, Nokia announced that they were letting 4,000 people go and at the same outsourcing the development and maintenance of the Symbian platform to Accenture. 2,800 people would move to Accenture. Harlow used flowery language in describing their reasoning. According to her, the collaboration served to demonstrate “our ongoing investment to serve our Symbian customers” and “also shows our commitment to provide our Symbian employees with potential new career opportunities”.Accenture and Nokia described how they would chart the possibilities for training and new career opportunities for the employees. In June 2011 the transfer size was still estimated at 2,800 people. In the end, the transfer included 2,300 employees, 1,200 of which worked in Finland. The rest had found other positions within Nokia or had resigned. Four months after starting the work, Accenture confirmed that they would wish to let go a large amount of the former Nokians. They were offered voluntary severance packages. A package could amount to 15 months of pay.The Union of Professional Engineers in Finland was quick to find fault with the outsourcing deal. It was thought that Nokia had transferred their social responsibilities to Accenture. In June 2012 the union estimated that about half of the employees transferred to Accenture had left. The consulting firm, according to the union, treated its employees unfairly and was pressuring them to accept worse terms for their work contracts. At the same time Accenture was looking for new employees and bragging about how much work they had available. The union described the situation as oddly strange.In October 2012, Accenture announced that they were firing over 300 employees and that they were re-evaluating the future of their Oulu office. Pertti Porokari from the Union of Professional Engineers in Finland wondered whether Nokia hadn’t just outsourced the firing of these employees to Accenture. A few months later, the Oulu office was doomed. 275 people were fired, and 46 were sent on unpaid leave. The remnants of Nokians were slowly weeded out, and Accenture has announced layoffs afterward, too.A source outside of Accenture believed that the company had tarnished their reputation against their will. They were in earnest, and truly believed they’d have work for these engineers. Nokia had promised them contracts on Symbian and also Windows Phone, and the IT industry was constantly calling for more employees. Accenture was banking on getting high-performing resources for their own massive IT projects, too. Symbian engineers would be used to the appropriate tools, and would be reasonably suited to the upcoming projects. So as the need for Symbian resources would dwindle, people would move to other tasks, and also naturally some employees would leave of their own accord or retire.The cost of the Nokia-Accenture deal was never published. The Finnish branch of Accenture doubled their turnover in 2012 to half a billion euros ($670 million), and more than doubled their profit to 15.5 million euros ($20 million). In the following year, their turnover shrank by several dozen percent. The deal with Nokia had taken several scenarios into consideration, so it was never renegotiated.The impact on Accenture’s income remains unclear, because as a multinational corporation they transfer gains between countries.After the Lumias hit the stores, Nokia continued to struggle ahead with the Symbian line-up. The results were depressing. In February 2012, the market share of Symbian was 8.7 percent. By the end of the year, it was 1.2 percent. Nokia’s blindness to what people were looking for at the time can be seen for instance in the release of the Nokia 700 in August 2011. They marketed it as the smallest device ever released with a full touch screen. Only a few months later, Samsung came out with the huge screened Samsung Note and was pushing Galaxy models with ever increasing screen sizes.Nokia’s old work horse was able to leave the stage with style. The last Symbian model was the Nokia 808 PureView, that had a 41-megapixel camera.Immediately, the device was dubbed ‘the monster camera phone’. Engineers Eero Salmelin and Juha Alakarhu had been working on it for five years. The device revolutionized imaging technology. With a massive number of pixels, you could digitally zoom the image without loss in image quality. The resolution of the images was shocking. To keep the file size of the pictures reasonable, the camera combined pixels, further improving picture quality.The device was a sensation when it was launched at the Barcelona Mobile World Congress in February 2011, and was rewarded as the best new release of the congress.Why put the monster camera on a Symbian device?The PureView had a chipset selected by Nokia, a Nokia-built sensor for taking photos, and Nokia’s own software, that couldn’t be just embedded in a Windows Phone. Microsoft did start thinking of the required changes immediately after the cooperation was announced, but the amount of work needed turned out to be greater than anticipated. Nokia decided to release the PureView first on a Symbian phone to create positive hype for future devices. They succeeded: Nokia was making headlines across the world, and in a positive light for a change.The last Symbian phones left Nokia factories in the summer of 2013.The goal of 150 million devices sold was not met. The real number ended up as just under 100 million. Now, Symbian is dead and buried. Stores have no Symbian devices on their shelves, and Nokia has not accepted new Symbian apps in their app store since the beginning of 2014. Any existing Symbian apps may not be updated.For Salla Jämsä, the effects of Elop’s speech became concrete quickly. She moved to a new position supporting the people being laid off. In the fall of 2011 Jämsä knew that her time at Nokia was coming to an end. There would have been work to do in HR, but she was rapidly losing interest. In September 2012, she made her decision: She would leave Nokia to start her own business by making use of the support Nokia was offering to departing employees proposing to start new companies, and is now a partner at the executive search company Transearch.Our interviewees told us how they had often wondered why Elop made this mistake. Some think that he was naïve enough to believe the speech would remain a secret. Others think he leaked it on purpose.The former is hard to believe. From a company the size of Nokia, soured by bitterness due to layoffs, the memo was inevitably leaked. Elop could not be stupid enough to imagine that it would stay internal, and it didn’t seem as if there was any serious attempt to keep it internal, either. Even though employees were forbidden to record the speech, one Nokian described seeing a video of the speech where someone in the front row was obviously videoing the speech on their mobile. Obviously, the ban was not enforced.The most likely explanations center around inexperience. Elop’s work history was from sales of enterprise devices and software. Business-to-business sales have long cycles. Clients can be told that we are working on a new product that will be ready in a year or two. Network business works like that. In consumer businesses, the buyer needs to be convinced of the superiority of the product every day. Elop underestimated or was completely unaware of the strength of consumer rejection of a product.“I ran into the speech first in an online discussion, and assumed it was a hoax. I thought it couldn’t be true”, recounts a representative of a major shareholder of Nokia.An often-recounted explanation of the Burning Platform is Microsoft. According to that theory, Microsoft forced Nokia to reveal the full extent of the cooperation immediately, so that Windows Phone would fix its reputation. The wildest speculations suggest that Nokia got a promise of payment for this. We’ve not received any confirmation to these rumors, but neither do we have information that would discount them. Some offer the explanation that Nokia needed to get their Symbian expenses down quickly. Elop might also have wanted to disperse a Symbian clique within Nokia, that would have been resisting renewal.The best guess is likely the simplest, though. Elop wanted to push for speed. He was forcing decisions forward with his characteristic need for progress toward a goal rather than stopping to consider barriers or consequences. Elop himself defended his speech in the spring of 2012. The transformation caused by the new strategy was huge, and would touch thousands of employees. It had to be published with a bang, as the story would have leaked in any case, when job descriptions started changing. He said that he had understood that there was no other option than a rapid change of course. The personnel needed to understand the depth of the crisis.The speech also reflected Elop’s goal to open the culture of Nokia. He wanted to tell it like it is. He has wondered whether the breadth of the publicity caused by his speech wasn’t due to this. For once, a leader of a corporation was brave enough to speak without the gloss of marketing speak. For many, the speech was proof that Elop was able to look at things as they are, and was ready to make drastic changes. Some feel that the memo was proof that Elop was the right man to save Nokia from the sea.“The message really hit home for me. I thought, now we finally have a clear direction, and finally a leader, that takes responsibility for the whole mess.”, recalls a former salesman.One developer working in R&D describes having been thrilled after reading the memo on the intranet. “The text was absolutely spot on. Only after my buddy, at the coffee table, pointed out the possibility of the memo leaking to the press, I realized the risk. My guess is that Elop didn’t realize that the memo might spread outside the company. He told us, in this other event, how he always believed the best of everyone. I think he just trusted people too much.”The same goes, according to this developer, for an internal video where Elop was showing off the first Lumia phone. Only after that, too, was leaked, did Elop stop telling the employees about new things.A member of the board doesn’t agree. He believes that Elop leaked the speech on purpose. The board had gone over the basic elements of the speech, and agreed on the analysis of the situation. He didn’t think it was naïve to suppose the speech would stay internal. Any leaks from Nokia were usually about new phone models. The culture was to have discussions with the personnel on even the most sensitive issues. Based on this, he finds it to be unlikely that the Burning Platform memo had been leaked against Elop’s will.The majority of the speech was undeniably true, and the memo was only one factor in the Symbian catastrophe. This point of view is supported by looking at the relative changes in market share. The memo’s effects can be seen only in the numbers of April–June 2011. The market share dropped by 33% (so not percentage points) during that period. Before that, the share had been dropping by about 15 percent per quarter. By the last two quarters of 2012, the percentage drop had gone back to usual, at first 13% and then 7%.Most of the analysts and Nokia’s highest leadership still agree. The Burning Platform was a huge mistake. It would have been possible to reveal the shutdown of Symbian once there was an option on the market, when the Lumia phones were already available in stores. The speech and information released on the Capital Markets Day removed all possibility for a reasonable transition period. Nearing on the grotesque, in July 2013, the European Association of Communication Directors (EACD) gave Elop the European Communication Award for “outstanding communications achievements on a European level”. The argument was that Stephen Elop is recognized for his direct and transparent communication style, and the proof was the Burning Platform memo.In 2011, that award was in the unforeseeable future, and cut no ice with anyone, but everyone in Finland knew what the Burning Platform had achieved: The Lumias were needed fast. And they needed to be good.[12] The Nokia headquarters campus in the Keilaniemi district of Espoo, known as Nokia House at the time, comprises three buildings, called AB, CD and FG.[13] The full Burning Platform memo that was initially released on the Nokia intranet, was soon published by multiple sources, including Engagdget.14. The MeeGo swansongSuperlatives were abound even though many knew that the device was going to be the one and only MeeGo phone from Nokia. The Nokia N9, introduced in June 2011, did raise expectations on a completely new level.“I have not been as impressed with any new Nokia product over the last five years”, says a well-known analyst.Relief was the general feeling after the introduction. Despite all its problems, Nokia was still capable of creating competitive devices. So elegant that one could imagine the new phone competing against the iPhone. The design was a triumph of Marko Ahtisaari who was the design chief at Nokia. Ahtisaari had started as head of product design in 2009 and employees felt that he had introduced a new spirit in the design work. Being a Finn, Ahtisaari knew the Nokia organization and could communicate efficiently across the organization and inside his own team, so design aspirations by the design team were now more often implemented than in the past. The design team had also sensed Elop’s arrival, although the N9 design had been born before he arrived. Nokia of the Kallasvuo times had been focusing on internet and navigation services design, and now with Elop in charge the focus of the design work and brand building was back on phones.Many feel that the N9 is the most beautiful Nokia phone ever made, and the general impression was that it completely renewed the what and how a Nokia device looked and felt.The design work for the N9 started for the first time from the user experience instead of the technical specifications. Earlier Nokia had been developing new phones based on engineering and technical capabilities. In the end the design had been added on top of the technology. The goal with MeeGo was to revise this way of working since Nokia knew it absolutely had to differentiate from its competitors. There was almost an infinite number of black touchscreen smartphones on the market.A senior design leader in Finland judges this as a success. The beautiful design language was emphasized by the sharp edges and the harmonious aspect ratio of the device. The N9 felt good in one’s hand because of the design elegance, device proportions, and finishing details. It felt like the first-time holder’s own phone. The design leader especially appreciates the color design choices. The device cover was colored-through polycarbonate, available in black but also in the more radical cyan and magenta.The color design was influenced also by non-aesthetic reasons. The design language and color selection of the N9 was difficult to copy in large volumes. The phone body was milled from a single piece of polycarbonate instead of being assembled in the traditional way from separate front and back covers. This allowed Nokia to make the N9 into such a solid and finished-looking device, and thinner too.The wow effect at the introduction was emphasized by the lack of any leaks before the introduction. A member of the project team describes how all internal test users had to report their test phones’ visible identification codes into an internal system that would have allowed Nokia to match any possible leaked images with the test phone users. This very rigorous discipline was dictated top-down after Elop had outlined the urge to shorten the time between the new phones’ announcement and general availability.A director who had worked at Nokia praised Elop for changing the company’s engineering-driven mindset and raising design questions to the executive board level. Previously, the user experience had been compromised by technology issues. All possible technical features had been added to new phone models no matter if they were needed or not. In comparison, a design-led company such as Apple had always understood the importance of user experience and the slim and stylish design.Later, the N9 design language was copied to Lumia phones across the board. Nokia wanted both the inexpensive and premium Lumia models to carry a unified design language to raise consumers’ interest to go and try a new and unfamiliar mobile operating system.But what about the software? At the end it was also about the software. Was MeeGo better than Android? Would it have had a chance to beat the Windows Phone?At least the N9 software was promising. N9 was a pure touchscreen device since there were no physical keys on the front face at all. It was all about swiping with your finger. When you swiped your finger across the edge of the screen you moved from an application to the desktop. Applications running in the device were visible on one desktop as open windows and this made it easier to switch from one application to another. One of the three desktops was reserved for social media and messaging with contacts. Good ideas and innovations were plentiful in MeeGo.The software was also robust. It did not crash or hang like Symbian did at the end.Reception towards the N9 and MeeGo was still contradictory. It took time to learn how to use the phone. One could pick up a new iPhone and start to use it immediately. MeeGo was not this intuitive, normally people do not just start swiping with their fingers across the screen. Was the learning curve short enough or too long; opinions were floating around. Some felt that MeeGo was more complicated for novice users than what the Windows Phone was. One analyst’s verdict summarized the N9 by stating that it will only interest a narrow niche segment.Enthusiasts were committed, however: This would have been the right baseline for the future smartphones of Nokia.The history of N9 illustrates the decision-making vacuum at Nokia before Elop came in. The device was created relatively fast after Elop had announced that the one MeeGo device project will be completed. Developers belonging to the project said that the back-and-forth stopped immediately now that there was a clear goal with the work.MeeGo had been suffering from an unclear direction setting inside Nokia for several years. This was because of the internal power play with the Symbian team and delays caused by personnel changes. One member of the Nokia Group Executive Board said that MeeGo was a “terribly great project but it was contaminated with Symbian, the old way of working was injected into MeeGo.” The developers had been sidetracked and more people had been added to the project. A better option would have been to continue with a smaller development team because it was known to be the best way to create something new. The Nokia executive thinks that the worst mistake was made in 2008 when Symbian and MeeGo were put in the same organization and a gigantic Devices and Services unit was formed. From the Kallasvuo times there were multiple overlapping user interface development projects at Nokia. Similar solutions had been under development both in the Enterprise unit working on business phones and in the Multimedia unit working on other expensive phones.The MeeGo user interface development began from high-flying theories that tried to model human behavior, different personalities and society, and connections between these. The goal was to support natural human behavioral patterns instead of forcing people to comply with the technology. The original plan was to develop just one top-notch MeeGo smartphone that would then be renewed annually just like Apple was doing with the iPhone. The result was disappointing; the user interface was very complicated and it was seen to resemble Symbian.In late 2009 when Ahtisaari had started as the new head of design the developers were told that the new leaders did not understand the high-flying concept so it was scrapped. Simplicity became the new mantra. The new MeeGo home view was a simple launchpad and the application user interfaces were simplified forcefully. The end result was yet another disappointment. It was seen to be too much like the main competitors Android and iOS. Faith in MeeGo’s competitiveness was eroding and people felt that Linux and open source would not be relevant sales arguments towards consumers.A breakthrough was made in August 2010. The new concept idea was based on swiping gestures and a working prototype was built in a couple of days. Starting from the first conceptual images people started to believe they have a winning formula in their hands. The only thing missing was to build the product and get it in the hands of consumers as fast as possible. At the same time Anssi Vanjoki was building his strategy and MeeGo was in a key role. In the Vanjoki vision MeeGo would become the flagship of the new Nokia. There was strong faith in MeeGo among the Nokia leadership, and this was amplified by a media event in August in Oulu, Finland. Rich Green, the Nokia CTO, and an executive from Intel were prominently demonstrating the fruits of the MeeGo collaboration to media representatives.However, a major mistake had been made at Nokia. It still remains unclear who did it and why. It had to do with the previously mentioned collaboration with Intel. The new operating system needed strong ecosystem partners across the industry. In addition, Nokia wanted to steer the microprocessor hardware technology development because the archrival Samsung had that expertise in-house, and Nokia felt this to be a key competitive advantage. Despite Intel’s power management problems inherited from the PC world, Nokia’s technologists felt that those can be resolved. One person later commented that Intel had been really anxious to start the collaboration. Of their competitors Texas Instruments was in a worse condition and negotiations with Qualcomm were not progressing. Therefore Intel.There was a dramatic handicap in the Intel chipset, however. The new LTE (long-term evolution) cellular technology was growing rapidly in the US and the Intel chips did not yet support the technology. LTE was one of the 4th-generation cellular technologies, commonly known as 4G. 4G and LTE allow faster communication speeds and thus offer a better internet experience when mobile.This delay and the lack of 4G were derailing MeeGo. Nokia had initially chosen to use the Texas Instruments’ chipset in MeeGo and that was getting outdated while competitors were starting to use the next generation chipsets from Qualcomm. Intel was also lacking an inexpensive technical platform to compete against the cheap Android devices.When Elop came to Nokia he was told that Intel was a difficult partner to deal with. He did not believe in MeeGo despite the pressure from the Intel side so he made the well-known decision, backed by the concerns from the network providers and from the unclarities looming around the MeeGo ecosystem. The N9 would become the MeeGo swansong, no matter how great the product or the user interface would be.Who was then behind the Intel collaboration decision? There’s two views into this. Some say it was Alberto Torres, the person responsible for MeeGo in the Nokia Group Executive Board. Some say the most active person was Kai Öistämö, backed by Anssi Vanjoki. The ones who were opposing the collaboration were argumenting that Nokia would have to pay an immense price in the form of slower progress in MeeGo. Olli-Pekka Kallasvuo landed in the pro-Intel camp because he wanted to tell the world good news of MeeGo’s progress. It was felt that MeeGo got more credibility when Intel was backing it.When the N9 was introduced in the Nokia headquarters on June 21, 2011, many felt that the phone had already been aborted. The abortion had been done in the sales channel. Nokia was in a financial crisis and there was no marketing budget for the phone. The phone was practically denied any chance of success.The first N9 phones arrived in the stores in October. Because the plan was to start selling the first Lumia phones in a month or two, the role of the N9 was to fill this gap. The first Lumias were reserved for the most important customers and biggest network providers so the N9 was then the offering for the remaining customers. The N9 and the first Lumia phones were sold in different countries because the first Lumias supported only 30 languages. As one example, there was no Russian language support in the Lumias, so Nokia was selling the N9 in Russia. Also Finland — the most loyal Nokia market — was selling the N9 first. The strategy to sell the phones in different countries was a planned one so that consumers would not be able to compare the phones next to each other. Also the marketing and communication were different. The Lumias were promoted in all possible means and the N9 was practically muted.It had been only in the beginning of February 2011 when Ramon T. Llamas, the analyst from IDC had suggested that Nokia should start selling the MeeGo smartphones on the most important smartphone market in the world, in the US.The N9 became an awkward pain point to Elop. Critics liked the phone but Nokia could not promote it because there was a fear that it would dilute the success of the Lumia phones. It looked like the success of the N9 came as a surprise to Elop. It would have been difficult to imagine how consumers would be interested in a device that was a dead end with a limited supply of applications. When Elop had been asked in London why anyone would buy the first and the last MeeGo phone, the man with a flu had responded: “I guess you just answered your own question.”Why did Elop then launch the N9? There are probably two reasons. First there was a need to fill the gap caused by the Lumias becoming available only later, and possibly also the agreements with Intel may have required Nokia to complete one product. There could have been also some more human rationale. He wanted to reward the MeeGo team who had been working on the platform and product for years. Some say that the plan was to show how bad MeeGo was compared to Lumia but this speculation can easily be dismissed. One director who worked in MeeGo believes that Elop wanted to keep MeeGo alive also to retain the best talent working for Nokia instead of them fleeing right away to competitors to work on similar products.In the light of MeeGo’s pre-defined destiny it was actually selling decently. With very low marketing effort it was sold in the 2nd tier countries with a couple of million units. People who purchased the N9 were more like open source enthusiasts or technology lovers than average consumers. The first Lumias were obviously to sell much better.Certain markets like the Baltic countries used the N9 to replace missing Lumias in the marketing campaigns on the fly. When the local Nokia teams realized that they won’t get the Lumia phones in 2011 as promised but only in the beginning of 2012, marketing messages towards the local network providers were changed from the promise of the Windows dreamworld to MeeGo and the “future disruption” that was the N9 slogan.In Finland the N9 managed to create quite a frenzy. This was about technological and business patriotism and joy of Nokia launching a modern and competitive smartphone after a long time. Also outside Finland many said that the N9 was the best phone Nokia had ever created. Sami Aavikko, an executive of the Finnish network provider DNA, said that there was a “noticeable fuss” around the N9. He further continued that in case Nokia had decided to continue with MeeGo and invest in its marketing and further development, the smartphone world could be different now.Robin Lindahl was responsible of Nokia’s global network provider sales. He said that network providers were interested in taking the MeeGo phone in their smartphone range without real evidence of the potential of the new platform. The N9 got rave reviews from partners all over the world, including India and China. Network providers still trusted Nokia and believed that a third smartphone ecosystem could be built around MeeGo. Linux inside was intriguing and the design and features of the phone were competitive. The interest towards MeeGo on the network providers’ side did fall flat after the Windows Phone strategy was announced, however. Some network providers did take the N9 in their stores but only because the product was of high quality and there was demand for it. Even in Finland the fuss around the phone faded away when people realized it would be the only MeeGo phone. For the average consumer the transition from one operating system to another is a big change that requires a lot of learning.Retrospectively it is always easy to state what the chosen path should have been. Based on what it possessed, MeeGo would have had the capabilities to re-initiate growth at Nokia — this is what we heard from tens of ex-Nokians who worked with MeeGo and also partners such as network providers. The most obvious proof point is the enthusiasm the N9 was received with; it was greeted with more positivism in the industry and by opinion leaders than the Lumia phones. The design was new and the open operating system was new, and the operating system seemed to be more approachable to developers than what Symbian had been or what Windows would become.“Nokia should have played the MeeGo card to the end”, says an earlier MeeGo director. “It was typical at Nokia to keep on executing one strategy at a time, first Symbian, then Windows Phone. Therefore MeeGo was never considered a truly viable alternative.”The MeeGo director says that the overall investment in MeeGo was so small in the Nokia scale that Elop did not shut down the operation because of money. The ramp-down was strategic by nature. In the eyes of Microsoft, MeeGo was impossible also because of the Intel partnership. It was not acceptable for Nokia to engage in a strategic collaboration with another large technology player. The director believes that MeeGo was portrayed to Elop in the maximum negative manner.According to a manager who worked in MeeGo, Elop was given false information of the scalability capabilities of MeeGo. One example of the false information was that MeeGo would not have been able to deliver as many different phone variants as the network providers were requesting. The manager pointed out that the schedule estimates for new model introductions were skewed because of the Symbian history. It was far slower to further develop Symbian than what the situation was with MeeGo, he continues. Also the MeeGo director feels certain that MeeGo would have been scalable enough to power as many phone models as Windows. This was not about the development team, he believes; given the standard Nokia impossible mission with triple marketing budget and one year to go, they would have tried at least.According to another MeeGo Director, Jussi Hurmola, no roadblocks had been identified to stop MeeGo, if Nokia had decided to invest in MeeGo and reform itself around the new strategy.The MeeGo partner ecosystem came to a halt after the Nokia announcement to switch to Windows. Developers, other device manufacturers, network providers and media content producers fled away. The ex-MeeGo team from Nokia established a new smartphone and software company called Jolla.A director who worked for MeeGo feels that there would have been markets for MeeGo phones. As an example, he believes that some Chinese network provider would have been eager to distribute MeeGo phones when given exclusivity. Network providers were very favorable towards Nokia due to the long and mutually beneficial business history.The Nokia plan was to make MeeGo the primary smartphone platform of the company by 2013 and consequently use Symbian in some lower price point devices, says a former MeeGo manager. However, Nokia’s own Asha device range and the sub-100 euro Android phones did fill that market segment. The manager points out that Symbian was sequentially renewed over the course of several years to build enough time for MeeGo to get mature enough. Time ran out, however. Elop as the new CEO did not see enough business potential in MeeGo especially when no single network provider was willing to market it as a flagship product. Launching the N9 was an expensive exercise for those network providers who decided to range it. When a network provider decides to invest in the marketing a phone with a new operating system, with the relevant productization and technical adaptations in its network, it is expecting continuity in the product range.Carolina Milanesi is an analyst who has been following Nokia for several years. She believes the crucial mistake at Nokia was to cling to Symbian for too long. The end result could have been different if the Symbian ramp-down had begun in already early 2010 and all development and marketing investment shifted to MeeGo. She believes that through this development the MeeGo ecosystem could have had a critical mass of applications in 2011. Many people who were interviewed for the book reminded that MeeGo could have come also with Android application support — even the later Jolla incarnation of MeeGo has been able to run Android applications. MeeGo with Android compatibility would have tackled the big problem Nokia was facing: Too few applications.A person in the MeeGo middle management believes that the momentum of the N9 was so strong that Nokia should have doubled the MeeGo marketing budget and should have forgotten Windows. Jukka Taskinen, an ex-Nokia person said that the N9 was deprived of high-volume sales success because the product was not marketed on big markets like Germany and the U.K. Taskinen believes that the N9 was deliberately kept out from the most important countries because it would have had outshined the Lumia 800. A member of the Nokia Group Executive Board feels that bringing the N9 to the markets in the winter of 2010 would have been a good choice for Nokia. An analyst who had been following Nokia for a long time can see no rational reasons behind the MeeGo killing. Nokia’s research and development budget was so big when Elop started that the MeeGo investment could have had a chance to continue. The analyst is further pointing out that MeeGo was a “cult success”: It had its enthusiastic group of followers. A manager who worked for MeeGo believes that the common technology platform would have eased component availability that had been a problem at Nokia: “With another device manufacturer, MeeGo would have had a completely different success. A big reason behind the growth of Android is that it is being built on top of a reference hardware platform. Practically all hardware vendors supplying cameras, displays, and other components are part of the Android ecosystem. They work by default for Android. And when you have a well-tested Android operating system, it becomes very easy to build a phone. The same could have happened to MeeGo.”Not everyone was a believer, though. Ross Rubin, the analyst, said that it would have been difficult and immensely expensive to build the MeeGo ecosystem. As a proof he points out Hewlett-Packard and Blackberry who went through very difficult times when building their ecosystems. Most of the people interviewed for this book said that the reason to kill MeeGo was that it would not have been possible to build a compelling ecosystem that could have competed against Android and iOS, in a reasonable amount of time. Enthusiastic developers and consumers did exist, but they were not enough.One person reported Elop being very nervous when he met with the MeeGo team to articulate the reasoning why he had decided to stop the project. Elop was a fluent speaker as usual but his voice was trembling — maybe he knew that the technical argumentation would not work with this audience. The MeeGo team did not embarrass the CEO, however. With stoic calmness they listened to the information sharing.To Intel it was a bitter loss to realize that Nokia would bail out of MeeGo. In November 2011 Director Patrick Bliemer spoke about the disappointment in an interview and said that Intel would continue to further evolve MeeGo into Tizen with Samsung.The MeeGo developers at Nokia were relatively optimistic after the N9 swansong. Some were transferring to work on Meltemi (see chapter 15) and others were expecting to find new jobs outside Nokia. Competitors had hired the best experts right away after the Windows strategy had been communicated. In May Intel announced they will establish R&D centers in Espoo and Tampere in Finland. Intel was recruiting MeeGo experts with full speed. The Ministry of Economic Affairs and Employment in Finland was anticipating that Intel would be hiring hundreds of developers. That was obviously not going to compensate for the 1400 soon to be unemployed Symbian and MeeGo developers Nokia was planning to expel.In addition to the mass layoffs planned at Nokia there were about 2,000 software engineers to be expelled from subcontractor companies who had worked for Nokia. One of these companies was Ixonos.In May 2011 there was a nervous man sitting in an office in Herttoniemi, a suburb in Eastern Helsinki. Kari Happonen, the CEO of Ixonos, looked like the last couple of months had been a rocky ride. Nokia had been the largest single customer of Ixonos. Statutory negotiations were ongoing and the estimated layoffs would affect about 100 people. Ixonos had 1200 employees in total and 800 of these were in Finland. Ixonos had already been ramping down some work and the company now had empty office facilities. Happonen did not expect Microsoft to invest in developing the ex-Nokia ecosystem in the same way Nokia had done. In Eastern Finland a software engineering company called Weego had been optimized to serve the needs of Nokia MeeGo. 80 percent of the company employees were working in Nokia projects. Some of the Weego people were even working in Nokia premises. The CEO of Weego, Pasi Ollikainen, said in an interview that the Windows strategy decision was a massive surprise from Nokia and not many could have been able to anticipate it. The savior of Weego was their Android and iPhone expertise. The small Finnish software engineering house found a new source for growth from developing smart TV applications for Samsung.The bloodline of MeeGo continued in Jolla that was established in 2011. Jolla is a company formed by the ex-Nokia chief software engineer Marc Dillon and some other ex-Nokians from the N9 development team. Thanks to the open source nature of the MeeGo software, Jolla was able to start utilizing the main components of MeeGo for free. Things were proceeding fast. Jolla started to develop their own smartphones with the Jolla brand. In November 2011 Jolla announced the Sailfish operating system.The Jolla phone shows what MeeGo could have become. The user interface has been further developed and it is substantially more logical than what the N9 was. The Jolla phone has been developed by some tens of software developers and one can imagine what MeeGo could have become without the never-ending hassle, indecisiveness and Intel slowing things down.In a way Jolla is also answering the ecosystem fears that were plaguing MeeGo. The free Android ecosystem that has grown next to the Google-controlled Android could have been the lifeline of MeeGo. Android applications could have been made portable with reasonable effort to the N9 and this would have guaranteed enough applications. Therefore the likelihood of the pure independent MeeGo ecosystem will remain a mystery. One cannot say it was completely impossible because of for example the Chinese ZTE and Korean LG were interested in MeeGo.Amidst all the speculative praise one must acknowledge some facts. For the first, Intel has continued to struggle with power management issues with its 4G chipsets. The time to wait for a working solution would have been long.Also the most prominent heir of MeeGo, Tizen, remains close to square one. It has been used in accessories like Google Gear but very few smartphones have been announced based on Tizen. Last but not least, if the markets were too crowded for a new platform driven together by Microsoft and Nokia, how could Nokia have established a new platform on its own?15. Secrets of MeltemiIt was June 2011 in the Northern Finnish city of Oulu. An awaiting atmosphere prevailed at the Elektroniikkatie 10 research and development center. An executive of the Mobile Phones unit, Antti Vasara, had come from headquarters to talk about upcoming changes in Oulu.Vasara started with the good news, that Oulu’s R&D center would not be closed down. Those present started to applaud, with cheerful looks on their faces. Then came the bad news. Symbian and MeeGo development in Oulu would stop completely. Joy was replaced by shock. Suddenly, 1,100 employees in Oulu were under threat of being fired. Then Vasara continued with some surprising news. A new unit would be established in Oulu, which would save most of those under threat.That unit became one of the biggest secrets during Elop’s era at Nokia. Nokia has never confirmed that it existed and still does not. It has been very difficult to get any information about the project, as many of those involved declined to comment. We got the impression that Nokia still wants to keep their employees’ lips sealed.Why would it be that way? First, let us see what was under the hood of the project called Meltemi.Meltemi started as a research project back in 2010, during Olli-Pekka Kallasvuo’s management. The idea was to build a new basic phone platform, as the old workhorse S40 was becoming outdated. The project, or at least its predecessor, was already ongoing when Stephen Elop stepped in, and approximately 70 employees were wrestling with it. The project rose like a rocket on Elop’s agenda at the end of spring 2011, as management realized that Windows phones could not be built at a low enough cost to compete against the surge of cheap Android models.Mary McDowell, who was responsible for the Mobile Phones unit, got the lead for the assignment. The goal was to generate a new category between the Asha basic phones and the Lumia smartphones. The price in this category would be approximately 100 euros ($140), which would be cheaper than Android devices at the time, and would eventually replace the S40. The target was to reach the goal in one year.First, 250 employees in Oulu were transferred to the Meltemi project. Some of them had earlier worked with Symbian, and some with MeeGo. The group eagerly started to work with their new assignment. New engineers joined the work little by little, and eventually 500 people were working on the project in Oulu. In addition to Oulu, the R&D centre in Ulm, Germany was enlisted to the project. The idea was simple. The goal was to create a device that could be classified as a smartphone, but which had as its key selling point an attractive user experience, instead of relying on an ecosystem. The focus was on social media and a few preinstalled applications. Of course, one should also be able to install additional applications. The N9 applications should work on Meltemi as such, and thus would be part of the Qt ecosystem.But what was Elop’s logic? Why did he kill MeeGo — the pillar holding up Qt strategy — in 2011, but continue developing Meltemi?Meltemi was the foundation of the “Next Billion” strategy, to get the next billion people to use the mobile internet, by conquering people still without a mobile phone in emerging markets. According to an extreme interpretation N9 was just a prologue to Meltemi, created to generate the applications that Meltemi required. However, this time Elop advanced in radio silence. He had learned his lesson after the Burning Platform speech. The sales of Asha devices would not be cannibalized by saying that something better was on the way. The project was kept secret even within the company.As the work proceeded, a few technical decisions were made quickly. The phones would be built upon new hardware, including graphics acceleration, and that 128 MB of memory would need to be sufficient. As opposed to the N9 there would not be multitasking, with only one application running at a time. Applications would need to start up in less than one second.Meltemi needed to succeed at almost any cost. An employee from the project said that even traveling was less tightly regulated, compared to the rest of the company. It was calculated that a day trip to Oslo or a couple of days in Ulm would save a week’s worth of e-mail negotiations.The schedule still started to break down in the fall of 2011. One cause to that was the design team being located in London. One developer described how the design department was able to generate user interface ideas that were graphically attractive, but at the same time the direction kept continuously changing. At the end of 2011, the first version of user interface resembled the graphical looks of Windows Phone. The font was white and thin on a black background, and the elements on the display were text, rather than buttons to be pressed. The lower left hand corner had a “back” arrow, similar to Windows Phone. N9-like icons replaced the Windows tiles. In the spring, N9-like swipes were added to the user interface. This version didn’t live for long either. The third draft was very similar to the user interface of N9. The interface was touch based, with no hard keys. One could open notifications (such as incoming messages) from the upper edge, similar to Android. Nokia’s maps, music service and video calls would be included. At that stage product development told the design team that this had be the final plan, if they wanted anything to be finished, said one person involved with the assignment.Another aspect delaying the project was the new hardware. There were issues adapting the software to it. Calls, browsing and WiFi started to work on the device only in the beginning of 2012. Not at the same time in multitasking mode, but overall on the same device.At the end of 2011, management understood that development needed to gain speed. The most crucial people working on Meltemi in different cities flew to Nokia’s R&D center in Ulm, Germany. The goal was to make a giant leap, to pull people together, to have communications flowing without breaks. Nokia had booked an entire hotel in Ulm to accommodate the people involved. Approximately 50 people were continuously present, with participants changing all the time. The intermediate goals of the three-month camp at the end of January and February were just about reached, and Mary McDowell was seen onsite encouraging employees. At the end of March, when the camp ended, a tough fact had to be faced. Even though many aspects had advanced, such as the delay when launching applications, the readiness target had not been reached.The first phone model, codenamed Clipper, had been cancelled during the project. It was designed as a sibling to the cheapest Lumia, the 610. The devices resembled each other, but Clipper and its display were smaller. Clipper was replaced by codenames Goa and Zhora. Their role models were Lumia 820 and 920, and they also were smaller than their role models.In addition to Meltemi being a secret, there was even a bigger secret inside the project, one that has been kept unrevealed to the public until these days.The idea was splendid. As the iPad conquered developed markets, Nokia would take the corresponding market in developing countries. A small but high quality tablet would be built, and it would endure dust and moisture. The main markets would be Russia and China. Instead of small phone stores, the tablet would be sold in the giant electronic stores in China.The Meltemi tablet advanced far. The display was 7 inches, and the price was planned to be 250 euros ($330). The consumer reaction was tested with a half-ready demo device. The device was given to test consumers, and it was revealed little by little that the device would come out with the Nokia brand name. How much would they pay for the device? Consumers were so tired with cheap Androids that they replied they would pay 350 euros ($465) for the device. The reception compared to the price planned for the device was so good that the tablet would have been a shock to the Chinese low cost manufacturers, estimated one person involved. The tablet would have run all Android applications. Even some iPad applications would have worked. In addition, country specific applications would have been built for Russia and China.Within the Nokia organization the Meltemi tablet project took a different path than the phones. Chief Technology Officer Henry Tirri led the project, to ensure a time-to-market as fast as possible. The project was due to be transferred to Mary McDowell in the fall of 2012, when it was time for discussions with the sales channel.However, the days of Meltemi came to an end already before that. The unfortunate project became squeezed, when the pricing of Android phones dropped ultra low in the hands of Chinese low-cost manufacturers, and a Lumia close to the price range of Meltemi was created in cooperation with Microsoft. Teams were asked, in the beginning of May 2012, whether they could promise Meltemi to be ready by the end of the year. One developer tells us that their team’s response was that it is possible, but do not expect anything before that.Meltemi means a dry wind blowing from the North across the Aegean Sea during the summer. The group was hit by it in June 2012. One morning, when people came to work, they noticed the Meltemi wikis and source code were closed. Some realized that executives had been absent from project meetings for a couple of weeks.“We concluded that only bad interpretations can be made in this situation”, a software developer recalls.During the day, head office informed that the project was going to be killed. They gave three reasons.The product did not come out fast enough.Lumia phones could soon be built cheap enough.There was no marketing budget. All the investments will be focused to Lumia.The final stroke was Nokia’s financial collapse. The Meltemi teams were told that Nokia did not have the financial means to do marketing for S40, Lumia, and Meltemi devices at the same time. According to one employee, waking up to the need of marketing funds excessively late illustrates the panic mode and shortsightedness of the Meltemi assignment. The reason is likely simple: The costs of bringing Meltemi to market, and especially the required marketing investments, would have hit Nokia’s cash assets too hard.As Meltemi did not officially exist, it could not be officially cancelled. Nokia formulated in their release, that going forward, they would focus on S30 and S40 basic phones, and Lumia and Asha smartphones. You had to read between the lines that a third option had been dropped. In a press conference, when asked about Meltemi, Elop replied that he has never confirmed a project with such a name, but that Nokia has had to cease some development initiatives. It was said that the R&D center in Ulm, Germany, would be closed down, which gave the final confirmation that Meltemi was dead. “This was a slap in the face. Nobody assumed that the entire center with 730 people would be closed down. At the end of last year, new employees were still hired to the R&D center”, said Ulrike Kleinebrahm from the workers’ union IG Metall.The Meltemi people were devastated. The project was in the final stretch, the completion was in sight. The products were far in development, and had been presented to the distribution channels. Elop said one of the reasons for stopping development was that they had received bad feedback from the target customers. One employee involved assures that it was in fact the opposite. Employees would have understood if the closure had taken place three months earlier, when rearrangements were done in Ulm, and the phones had technical issues. Three months later those problems had been overcome. It was difficult to understand the slaughtering of a nearly ready product. The frustration burst out in a LinkedIn group. The group carried the name MPD Alumni (Meltemi Product Development).“I thought we mattered. What a naïve thought! Trust — melted. Joy — melted. Passion towards Nokia — melted. I have no clue how to get it back”, one commented.However, the opinions on the Meltemi readiness stage varied. Many used the expression: “Almost ready”. According to them, the products could have been brought to the market very quickly. These views appear colored by bitterness. According to one reliable view, Meltemi was two to three weeks away from a phase where it would have been eligible for “use on a daily basis”. The term means that Nokia employees could have tested using Meltemi phones as their main devices. The stage is reached when the phone stays on in most cases for the entire day, and the most important functionalities are available. It is still a long way to sales readiness. Meltemi devices used to cut calls after approximately one minute when the project was halted. The application startup time was on average three seconds, a long way from the one second target.The internal ending event of Meltemi was on June 14, 2012. The event was streamed to Nokia sites across the globe. Elop revealed what the development of Meltemi cost per month.“I wish I had taken some notes”, recalls one who was there. “The sum was most likely many millions of euros per month. The project had grown into psychotic dimensions. For example, when a team was released from MeeGo we may have been asked that here’s a team, would you have some use for it?”Some think that the goal of the Meltemi project was impossible to attain from the beginning. Even though Meltemi was built upon some assets from MeeGo, at the end the Meltemi team had to build a new operating system from scratch. Thus, it was not a slimmer version of MeeGo, but rather its own platform, built on the Android core. There was so much work involved that the original estimate turned out to be too small.Still, the timing for Nokia to obtain low-cost smartphones to compete with Android phones was only half a year away. It is a completely different story how consumers would have reacted to them. Carolina Milanesi, an analyst that has followed Nokia for a long time, thinks that the prices of Meltemi devices, 70–140 euros ($94–188) for smartphones and 250 euros ($335) for tablets, would have been too high for developing markets. Android had got Asians accustomed to ultra-low-cost devices.According to Daniel Chung, who was responsible for Nokia’s network provider relationships in China, the Chinese network providers took the stance that Meltemi pricing could not have competed with local Android devices, which gave consumers more functionality and applications against a smaller financial investment.Meltemi development swallowed 50–100 million euros ($63–126 million) according to a rough estimate. At the same time, employees were taken through a rough ride. Only a few months before Meltemi, Elop had announced that MeeGo people would be transferred to reinvent the future disruption of the mobile device world. The MeeGo people that were expecting shiny new assignments were driven into a dead end for a second time. Maybe that is why the project is still officially a secret. Meltemi is difficult for its former employees as well. If you search the keywords Meltemi and Nokia on LinkedIn you can find approximately twenty hits. The rest use euphemisms for that stage of their career.Had the project succeeded, the name Meltemi would have remained unknown to the wider public. It had a new Asian name under work, like Asha, as the markets were assumed to be in Asia. Meltemi belongs to the group of working names such as MeeGo’s two versions Harmattan and Fremantle. Harmattan is a trade wind present in Western Africa. Fremantle — or more specifically Fremantle Doctor — occurs in the western coastal regions of Australia during the summer, and cools down the heat in afternoons.16. Towards the first LumiaThe cooperation between Nokia and the new partner began with traveling, partying, and presents. Microsoft’s Windows Phone employees gathered at Daman’s Tavern in Redmond to celebrate the most important deal of their history. The AllThingsD website, focusing on digital technology news, reported that toasts were made with a drink called “The Noble Finn”. Ingredients: Finlandia vodka, Chartreuse liqueur, soda water, sugar, and lemon juice drained with reindeer antlers.In March 2011, Windows Phone engineers arrived in Finland, led by Terry Myerson. The teams got to know each other by snowshoeing and in the sauna, after which the Yanks were made to roll in the snow naked. The next day they transitioned to business at the Salo factory.Within weeks and months, the cooperation settled to its established ways. One of these involved Iceland. The direct flight to Reykjavik is about the same from Redmond as it is from Helsinki. The leaders met often at the state-owned Culture House, a stone’s throw from Höfði, where Ronald Reagan and Mikhail Gorbachev had met for their famous meeting of 1986.At the Nokia end, Jo Harlow, who had been nominated to lead the smartphone business, bore the greatest responsibility for the cooperation. She had been given ten months — the first Nokia Windows phone was to be on the market before the year’s end. The schedule was twice as fast as what was customary at Nokia.Harlow had arrived at Nokia in 2003. Before taking the responsibility for smartphones, she had led the marketing of mobile phones in North America, marketing worldwide, and she was the responsible for the Symbian phone business. She had made it through two organizational changes under Olli-Pekka Kallasvuo. This trusted player, also under Elop, had captained the Duke University basketball team in North Carolina in the 1980s, and had graduated with a bachelor’s degree majoring in psychology. Before Nokia, she had held leading positions in sales and marketing at Reebok, known for their sports products, and at Procter & Gamble, famous for their consumer goods.Some twenty work pairs were created as the basis of cooperation. The idea was to make people working in similar roles at either company responsible for progress.These work pairs were set up in sales, product development, and marketing, among others.Harlow was paired with Myerson. The common denominator was found quickly. Myerson had graduated from Duke seven years after Harlow. Nokia employees presented each with an E7 phone decorated with the Duke logo.Kai Öistämö’s counterpart was Andy Lees. Öistämö has recounted how he virtually lived with Lees as the deal was taking shape. At that time, the contacts might come any time of the day, because Lees was vacationing in Hawaii and Öistämö in Italy. As the deal progressed, the contacts became only weekly.Marko Ahtisaari’s partner in Redmond was Albert Shum. Shum, having started at Microsoft after Nike, created the basic features of the Windows Phone user interface, among others the appearance based on tiles.The pair exchanged ideas, and started to focus on future product launches. The target was to make the device and the software work together as if they were one and the same.Teamwork was made easier by video conferences that were held 5 to 10 times a week. In addition, Nokia transferred a director, Waldemar Sakalus, to Seattle to handle the Microsoft relationship and to divide the product development tasks among five Nokia locations: San Diego, Beijing, Salo, Tampere, and Taiwan. A Microsoft alum Kevin Shields was hired to investigate what Nokia could, on its own, build on top of Windows Phone.Based on information leaked to public domain, the alliance started to resemble a match made in heaven. The world’s best phone hardware manufacturer and the world’s best software house were working together. Network providers, developers, and technology buffs were keen to see what this common effort would produce. Belief in success started to arise.The reality behind celebratory speeches and common acquiescence presented another face. A Nokia employee belonging to the Markets unit remembers the shock he experienced two weeks after the public launch of the cooperation, as he saw the list of the features of Windows Phone 7. The list looked much different than he had anticipated. The most drastic surprise was in the language support. The engineer remembers thinking that pages were missing in the document when he saw the languages supported. But they weren’t missing. Microsoft had concentrated on North America and Europe with Windows Phone, where the expectation was to elevate the status to that of an expensive enterprise phone. Nokia, on the other hand, was working globally. It wanted the phones everywhere. The engineer also understood that in addition to the languages, leading network providers such as Vodafone, Orange, Telefonica, and T-Mobile required more than what the list of features had on offer. Windows Phone was very closed. Application interfaces, with which network providers could integrate their music services into the system, for example, were missing. There was a new problem facing Nokia that already had been called condescending: They would need to respond to network providers saying “thank you for your wishlist, but Microsoft does what it does and the feature will come when it will”.They had gone from the frying pan to the fire, to the curse of a closed software platform.Microsoft’s interest in adding languages was minuscule at first, and Redmond balked at offers to help. According to Microsoft, language support was so deep within the code that letting Nokians to work on it would have revealed too much of the code.New inadequacies emerged continually. There was no support for the front camera that was necessary for video calls. Multimedia (MMS) messages did not work according to standard, and when a Nokian called about this to the US, it was felt as if the other end did not even understand what an MMS message is and why it should conform to standard. It was difficult to create custom ringtones. There was work to do in country-specific requirements. Many countries set very detailed requirements on phones. If these weren’t met, it was futile to even try.“We began to wonder whether anyone had researched the Windows Phone on a practical level before the agreement was signed — and realized that nobody had,” one Nokian recounts his team’s thoughts.The board woke up quickly to notice the same problems. Windows Phone turned out to be less complete than what had been understood. A person present in the first workshops between Microsoft and Nokia top leadership teams tells us how only at this stage it was realized that it was simply difficult to integrate a camera with Windows Phone. The pixel count allowed by Windows Phone was limited.Only at this stage it was also revealed that the Windows Phone could not be adapted to an entry phone category in the manner that Nokia had envisioned. When making the operating system choice, there had been estimates to be able to reach about a hundred euro ($130) price range.It began to strongly look as if the contract had been reached hastily — and the homework had been done skimpily. In addition to languages, the Nokia leadership team was surprised by the deficiencies in multitasking. And, how would it be with Microsoft’s widespread business applications, such as the text processing program Word, and the presentation software PowerPoint: Would they be integrable into Nokia’s upcoming phones?There were at least some improvements on the way. There was a new version of Windows Phone in the works that would be known as Mango, or Windows Phone 7.5.Even with the deficiencies in the platform, Microsoft reassured Nokia quickly about their software skills. An employee from the Markets unit recalls how quickly in the corridor chatter people started to comment that now they were in fact dealing with a software house. The software was of high quality, it was ready, made with care, and there were fewer bugs as compared to Nokia’s own products. The code was even revered. Versions appeared on time, and their content corresponded to the promises made.Windows Phone only accepted one chipset as its basis, but luckily it happened to be the same on which Nokia was building the American version of its N9 model. Within a couple of months, Windows Phone had been made to run on Nokia hardware, and under three months of making the cooperation public, Elop was bragging about walking around with a phone in his pocket. The development was running, according to Elop, faster than ever before in Nokia.The worrying was, however, continuing among the operatives. A Nokian remembers how fast he had realized that it was difficult to get requests through with Microsoft, because the Windows Phone team was so small. He estimated that when Symbian had six times the number of people as compared to S40, the Windows Phone team had fewer people than the S40 team.A member of the leadership team reiterates this: “The cooperation was sold to us with the argument that Nokia has a strong position and that it can influence the development. This proved to be hard. The requests never got through. Microsoft had their own ways of working. Flexibility wasn’t one of their strong points.”There was shared understanding at least on a more general level. Microsoft alleged to have changed their priorities to serve Nokia over other Windows Phone manufacturers. Myerson wasn’t shy to say that the workload was weighted in Nokia’s favor in relation to Nokia’s effort on the Windows Phone.If Nokia had faced surprises in the beginning of the cooperation, these also came Microsoft’s way. Nokia had been silent in the negotiations on their camera innovations. When the Microsoft team heard about the 41 megapixel PureView technology, its importance was understood immediately: “Wow, what a cool thing!”About a month after making the cooperation public, Elop told the news agency Reuters that the phones were progressing at a good pace. At the same time, he responded to the speculations that Microsoft might purchase Nokia.“To the extent that a partnership has been formed around what they’re really interested in, then what would an acquisition bring other than a good year of antitrust investigation, huge turmoil, delays?? We didn’t even broach the possibility of an acquisition with Steve (Ballmer),” he said.On April 21, 2011, Nokia and Microsoft finally announced that the cooperation agreement had been signed. The contract, hundreds of pages in length, spelled out, in addition to financial matters, which individual technical items belonged to which party. There were very few changes made to the guidelines drawn in February. According to Öistämö, the signing was a great milestone, but even more than this, he was glad of the concrete progress made by the cooperation. The Windows Phone Mango version was already being tested on Nokia devices. Mango was too far along for Nokia to have any influence on it before the agreement. The new version seemed set to bring about many improvements Nokia had requested. Lees concurred and stated that the companies now knew very accurately which part of the code belonged to Microsoft and which to Nokia. It was said that the cooperation was more about having agreed on common ways of working than about what Nokia can or cannot require from Windows Phone.The sales of the upcoming phones to the network providers was in a good shape during this time. Typically, a network provider needs to know the future plans of the manufacturers about 12 months ahead of time, which means that in the case of the Windows phones, the timescale had to be scrunched. Thanks to N9, Nokia had a likeness mockup to show already at a very early phase, and the Windows Phone screen grid appearance was also widely known. A leader involved in the sales says that the reception was entirely different than if Microsoft had been selling their phones by themselves. “Network providers knew, in fact, that when Nokia is involved, the possibilities are completely different. They believed in Nokia’s capabilities in hardware. They thought that if Nokia gets free hands on the hardware side, and the platform is made to work as it had been made to believe, it’s now or never that Windows will make a breakthrough in phones.”On May 10, 2011, Nokia received a blow under the belt from its new partner. Microsoft announced the purchase of the internet phone company Skype for almost six billion euros ($8.5 billion).The deal was poison to Nokia’s dreams about a network-provider-friendly ecosystem and showed where they stood with respect to their relationship with Microsoft. Nokia’s interests did not weigh when bigger wheels started to turn. Skype was a thorn in the flesh for network providers, because internet phone calls ate into their voice call revenue. Providers weren’t making a profit selling Windows Phones if it was too easy to make internet-based phone calls on them.Network providers did understand that internet-based phone calls were the future. That is why they were developing their own services to compete with Skype. There was even a new kind of a phone call in the works, multimedia phone call (IMS) that differed from internet phone calls at least in one respect; the standard had a built-in possibility for billing calls.Nokia had to hold back in dealing with network providers after the Skype deal. “We cannot tailor your call solutions as part of our operating system. This role is reserved for and only for Skype.” According to someone who had worked for the Markets unit, the world’s second largest network provider, Vodafone, in particular sent a clear message along the lines of “if our solution cannot be configured on equal terms with others, we will not sell these devices.”Nokia was careful not to criticize its partner in public. Elop admitted the problem only a year later when he revealed that network providers shunned Lumias because of Skype.However, the journey of these two companies on two sides of a fence with different company cultures gradually started to progress. During the spring and summer they communicated that they were ahead of schedules. At least the launch of one common phone model within the year was still within the timeline. The communiqués given in conjunction of the signing of the agreement led one to believe that the first priority would be in the speed of market entry. The device would be very similar to other Windows Phones.This emphasis felt more correct, day by day. While Nokia and Microsoft were dancing their mutual minuets, the smartphone market was forging fast ahead. The Symbian catastrophe had wrecked the value chain of many distributors. The biggest network provider customers were Vodafone, Telefonica, T-Mobile, and China Mobile. Especially the Europeans reacted quickly. A director who liaised with network providers admits directly that Nokia had to stomp the prices and network providers were required to subsidize the sales of phones. A phone languishing in the warehouse needed to sell within two months, and half a year was a long time in this fast-paced market.A dirty wake was forming also in Asia. One Nokia salesperson says that for instance in India Nokia had been selling about 10 million phones quarterly. The average stock was for about 45 days, i.e. five million phones. As one phone was priced around $50, the remaining supply in the hands of the distributors was substantial. Many distributors suffered great losses in forced sale events, and many of these were wholly dependent on Nokia. Many felt betrayed.Independent retailers in China quickly abandoned Symbian. Network providers reacted more slowly. According to a director from the sales unit, China declined slower than the rest of Asia because the local network technology deviated from the standard. Foreign competitors had a harder time entering this market, especially since the two largest network providers were Nokia-friendly. But when the train started rolling, it was difficult to stop. “China Mobile is the world’s largest network provider and it was the most important seller of Symbian. It started to support Chinese manufacturers. Device makers such as Huawei, ZTE, and Lenovo got a head start to an immense growth. The situation was changing incredibly fast after this,” says a respected stock market analyst.The sinking Symbian started to become a real problem. Nokia issued a startling market warning on the last day in May. The revenue in April–June would be substantially lower than expected. The formulation of the reasons for this was interesting. According to the release “the situation has been made weaker by the competitive dynamics and market trends across multiple price categories, particularly in China and Europe, as well as a product mix shift towards devices with lower average selling prices and lower gross margins. In addition, pricing tactics by Nokia and certain competitors have made the situation more difficult.”The word ‘Symbian’ was not even mentioned. [14] On top of everything, Nokia announced that because the forecast of the second quarter had changed substantially, it will no longer publish targets for the whole year.At least there were savings in the making. Layoffs would result in savings of about a billion euros ($1.35 billion) annually from 2013 and onwards. Elop estimated that these savings had materialized faster and more than expected. If it qualifies as a merit, Elop had in fact gotten rid of personnel from the company effectively.Analysts made quick calculations: The entire Nokia group had become loss-making, also the cash flow had turned negative. The market panicked. The share price went down 18 percent. At the same time, Nokia lost its top spot in the Helsinki stock exchange to Nordea. Investors described Nokia’s situation as incomprehensible, because the beginning of the year had developed reasonably well, and the annual meeting in the beginning of May had had a positive vibe to it. The release of the day got the nickname “the horrendous Nokia upset”.The reason for this upset was very visible on the retail shelves around the world. Android phones filled the shelves vacated by Symbian phones that retailers had moved aside. Network providers were fond of Android, because it was available for a variety of price categories. For example in India and in China, a sizable chunk of buyers look for phones costing below a hundred euros ($135). The range of Android phones just made this mark, and they offered so many features for such a price that their demand skyrocketed. The Chinese budget device manufacturers and Samsung captured the game.Operators still wanted to remain in touch with Nokia. Nokia still wanted to stay in the game because the position of Apple was evoking fear.A leader with a Finnish network provider says that Apple was much more arrogant toward network providers than what Nokia had been. It was not uncommon for heated calls at odd hours to come from Apple’s London office. The topics were such as the missing helicopter at an iPhone launch event. The network providers had no say in the pricing of iPhones or in the sales and marketing actions. Apple only offered “take it or leave it” deals.The difference with regard to other vendors was huge. Nokia was, compared to Apple, a domesticated business partner. Even Google was not as irritating as Apple, although it was the sovereign leader in the Android world.But: The network providers feared Google. It had begun to tighten the contract terms. Google had become the unknown card in the phone game, so there was goodwill toward Nokia despite the Symbian catastrophe. Many network providers had a long track record of making good business with Nokia, which still dominated the feature phone market.The choice of Windows Phone as the platform resulted in a mass exodus among developers, at first. This choice upset many, as Nokia had been an eager advocate of open source software. Nokia had recently marketed Qt, and many had invested in Qt training and certifications. A strong community had formed around MeeGo, and Symbian had been changed into an open source platform.The credibility vanished. Developers were faced with a dilemma: Why build Symbian applications when the market fell from under the platform? Why build Windows Phone applications when there was no market? Microsoft was also burdened by old sins. Developers had been required to change their tools during the last ten years many times over. “The experience was much more bitter than that with Symbian developers,” estimated a renowned stock analyst. “Among Symbian developers, the work was a continuous uphill battle, but a developer working with Microsoft’s mobile platform often fell flat hard. In addition to having to learn the new tools, they had to rewrite their programs.”In the light of the past with Microsoft, and due to the prevailing uncertainty, the solution was obvious: They moved elsewhere. The number one choice so far had been Apple. While the second choice had previously been either Symbian or Android, it was now Android. A developer is usually able to port their application to two or three platforms, so Windows Phone was sidelined.An experienced Finnish developer recalls that this hangover had, however, passed quickly. “The cold hard truth was that the Windows Phone tools were even better than Qt. And the code was brilliant. What you could do worked like a charm, and the set had been chosen so that all essential functions were there,” he told us.The sandbox was, however, crowded, the developer recounts. There were a lot of things missing that you could have realized with Apple or Android. For example game engines could not be ported from the outside, they had to be coded anew. The investment expected from developers was remarkably high compared to expected revenue. “The platform was not incomplete, as much as insufficient. What was there was excellent, but half of what was supposed to be there, was missing,” the developer describes.The grand picture was as follows. Passionate Symbian and MeeGo developers switched en masse over to Android. But Windows Phone received increasing interest. New entrants from among PC and enterprise developers embraced it. Only those who didn’t understand the need to jump ship continued with Symbian.Elop performed a cunning trick in August 2011. He appeared in Singapore Communasia Communications Symposium and instructed the audience to put down their cameras and not to take pictures with their phones, as he was about to show something confidential. From his pocket emerged a device which, despite everything, was the prototype of the first Nokia Windows Phone. The British newspaper Guardian remarked aptly: Elop could not have asked the audience any more clearly to photograph at that moment. Guardian also wondered how it was possible that a photo from the event that began to circulate was apparently professionally shot, on a tripod.The prototype had in fact been named: Sea Ray. The appearance was observed to be a direct copy of the N9. A camera button, mandated by Windows Phone, had appeared at the side, and the flash on the rear cover had been placed differently. The camera was identical to that in the N9, and it would have eight megapixels. The operating system was the new Microsoft Phone Mango version. There were no Nokia-specific apps visible in the prototype devices.In August, Elop met with the board. Elop drove his train with continually more steam, and announced that the phone family should get a name. The groundwork had been done, two hundred suggestions had been sifted, and a shortlist of the best options was presented to the board to review. Elop told Reuters how the board had been about to fall into a familiar trap once again, asking for more time, as there seemed to be no common favorite. Elop had wondered why they should wait until the following week, or the next month. The decision could just as well be made on the spot.And so the name was born. In one day.Lumia. A Latin-sounding play on the Finnish word for snow. Had been in use as a Finnish surname since the end of the 1600s. Evokes impressions of light in English. As to whence and by whom the name was brought to the board, the etymology is silent.Before the choice, it emerged that in Spanish — the language in the important South American market for Nokia — the word “lumia” had an esoteric slang meaning of “prostitute”, but only in archaic forms of Spanish influenced by Roman languages. This was not a hindrance. According to consumer studies, 60 percent of Spanish speakers took the name positively. The first impressions were more related to light and style. Of course, the media had a field day when the reference to the side meaning was found in Spanish dictionaries. They neglected to mention that this meaning was archaic, rare and only used in slang to begin with.Besides, it was in good company. In South China, “Peugeot” translates to the same meaning as “Lumia” in Spanish.Many Finns in those days wanted to believe in Nokia. The always positive foreign minister Alexander Stubb tweeted on August 12, 2011:The great news in the beginning of the fall was the Google-Motorola deal. In mid August, the Android powerhouse announced that it would purchase Motorola’s phone business for 8.8 billion euros ($12.5 billion). The rationale, according to Google, was the patent portfolio. This did not prevent disquiet: It was feared that Google would start to favor Motorola within Android.According to Elop, his first reaction to this was relief. “The very first reaction I had was very clearly the importance of the third ecosystem and the importance of the partnership that we announced on February 11, it is more clear than ever before” Elop said referring to the Microsoft-Nokia alliance in competition against Android.“My second thought was that If I happened to be someone who was an Android manufacturer or an operator, or anyone with a stake in that environment, I would be picking up my phone and calling certain executives at Google and say ‘I see signs of danger ahead,’” Elop said anticipating the disbanding of the Android camp.In the beginning of September, the Windows Phone started to be a reality. Joe Marini, working at Microsoft, tweeted that he had received Nokia’s Mango phone to try out. He described it as handy, having a solid feel, good camera, and responsive UI. He said he would have liked a larger screen. He gave an overall rating of 8/10.This was all promising. But: Nokia’s market share in smartphones had dropped to 15 percent.[14] Looking at the May 31 stock exchange release by Nokia, this statement is wrong, as the release states: “Nokia is continuing to invest to bring new innovative capabilities to its Symbian line up.”17. The Lumia journeyOctober 26, 2011. This was the day the mobile phone business had been waiting for already a long while. Nokia was to launch their new Lumia smartphones. The London congress center was packed with hundreds of technology reporters, bloggers, and analysts. The screens of laptops and iPads glowed in the dark, the sense of anticipation was palpable. Soon they would see what Nokia’s Windows strategy meant in practice.Elop stepped on the stage. When the picture of the Lumia 800 smartphone was projected on the screen, the three thousand strong audience burst into applause. The reaction illustrated the feelings: Nokia finally brought forward something that might bring the top position back. Elop was like a fresh father emerging from the birth ward, saying: “I am so excited to introduce you to the new Nokia Lumia 800.”The applause quieted down, the listeners waited for the lowdown. Elop said that the Lumia 800 was “a simply elegant phone that brings a gentler structure to mobility.” According to him, every detail of the design was paid attention to. Every detail left out received just as much consideration as those included.After the speeches, the audience was allowed to try out the new devices. The event hall was full of tables with new Lumias. Nokia employees clad in blue shirts presented the features of the devices with smiling faces. Lumia 800 drew the biggest buzz. Bloggers and reporters stood in line to be able to try out and photograph the novelty and to publish their verdicts as fast as possible. Apart from the design, the price of the new Nokias were of interest. Lumia 800 cost 420 euros ($580), whereas the newest iPhone was double that. The cheaper Lumia 720 was only 270 euros ($375): It was meant to compete with Android. The most striking feature of Lumias was their color. They were available in blue, red, and black, when the competition was in black, white, or grey. Lumias did not have really new features. Nokia Drive car navigation was an old Nokia application, but the music service had new features.A patriotic wave of pride filled the chest of the Finnish reporter. At last, Nokia had a phone with a working operating system for the mass market. With a device like this, it was possible to start reaching the customer abreast with iPhones and Android phones.Initial comments in the media were cautiously enthusiastic. The design and features of the Lumias, such as the camera and maps, were praised. According to analyst Carolina Milanesi, this was a top achievement within 8 months. She was not convinced with the name, but as a product, the Lumia was positive. Especially interesting, to Milanesi, was the price. Another British analyst, Ben Wood, described the situation as follows: “From a complete catastrophe to a real change in strategy, resulting in two fine products.”At the press conference, it was announced that in October the Lumias would go on sale in six big European countries. After this, the sales would be extended to India, Russia, and Indonesia. The actual gauntlet would be faced the next year, when the Lumia would be launched on the American market. The analysts reminded that to make a breakthrough with Lumias, Nokia had to make it in the United States. There resided the most important financiers, innovators, and opinion makers in the mobile business. The fare presented in London was still pretty light. Milanesi remarked that before going to the United States, Nokia needed to improve, because the full support of American network providers was a necessity.Sales of the phones started in the most important countries in Europe in November 2011. From January to March, the sales exceeded two million.This number was not too bad considering the numbers available from similar competitors. Apple launched the first iPhone in the summer of 2007. Within the first three months, it sold about three million units. The robust growth in sales only began a year later, in the fall of 2008. Samsung also had about a year’s delay before the demand of their Android phone started to climb.Nokia thus had hope. They only needed to get more Lumia models to sell and to evangelize Windows to customers. For Lumias, the difficulty factor was in the explosive growth in the sales of smartphones. The market research company Strategy Analytics estimated that in 2012 the sales of smartphones would grow 33 percent, to 650 million units. Nokia had to hurry up if it were to retain its market share.On January 10, 2012, Nokians prepared to show their best effort at CES (Consumer Electronics Show) in Las Vegas. This giant consumer electronics fair was an annual event where vendors presented their wares aimed at the American market. The day was important for Nokia. Winning at CES was a must to open the American market. On Monday night Finnish time, Nokia organized a press conference where the new top model Lumia 900 was shown. The phone was for sale exclusively by AT&T. On the outside, the phone resembled the Lumia 800 sold in Europe, but it had a larger display, a better battery, and a camera on both sides of the phone for video calls. Lumia 900 was the first 4G phone from Nokia in the US.After a couple of weeks, Microsoft published their annual figures. CEO Steve Ballmer bragged about the “company’s own phones”. Ballmer meant the Lumia 900 that was chosen as the best phone in CES. The CEO never even mentioned Nokia by name.The march of the Lumias moved on. On February 27, Nokia presented another smartphone. This was the cheapest Lumia so far, 610, with the price tag of 189 euros ($250). Jo Harlow, in charge of the smartphones, believed that the company to reach a wider market with a more affordable device. To make this possible, Microsoft had relinquished the Windows Phone hardware requirements. This model had half the memory of former Lumias and a new version of Windows Phone called Tango that was aimed at cheaper hardware. Because of the reduced memory, only a part of Microsoft Marketplace applications worked on the phone.It was still a fact that a smartphone costing almost 200 euros ($270) was an impossibility for the greater part of people on the globe. Nokia was facing a big problem: It would need to launch phones at below 100 euros ($130), fast. This was not possible because of Microsoft’s hardware requirements. Nokia was permitted, by Microsoft, to equip only the most expensive Lumias with Windows Phone. The software company wanted to ensure that the consumers would see Windows phones as equals to iPhone and Samsung Galaxy top models. Microsoft believed that this image would not have formed if Nokia were to sell hundred-euro Lumias. Windows would not have, in the beginning, worked technically in the cheapest models. For Nokia, this limitation was bad. With only the most expensive smart phones, it was not possible to generate enough sales to replace the Symbian business.On Nokia’s biggest market, in China, Elop put all his personal charm at stake. According to a person having worked in a top position in marketing in China, Elop’s relationships with Chinese network providers were good. These were also grounded in former successful Symbian business in China. In March 2012, Elop shook hands with China Telecom CEO Wang in a flashy ceremony. With this handshake, the sales push began to get the first Nokia Windows Phone on the world’s largest growth phone market. The Chinese government supported the transformation to use the local standards TD-SCDMA and TD-LTE. Symbian phones did not use these technologies, but Nokia was able to compensate the dwindling Symbian market with TD-SCDMA-based Windows Phones. Elop admitted that it would take time to launch Lumias with Chinese technology. The Chinese government had another goal: Network providers were encouraged to develop their pricing models as well as their profitability targets in the direction to make the Chinese start using low-cost smartphones. This goal did not align with Nokia’s interests.In March, Nokia also signed a Lumia deal with China Mobile. The state-owned China Mobile is the world’s largest network provider while China Telecom China’s third-largest. Lumia had already a presence on China Telecom’s website, even though the sales had not yet begun. At the end of March there were more news on cooperation: China Unicom would also start selling Lumias. Elop believed Lumias will be able to differentiate, because the groundwork with Chinese partners had been long in the works. The spearheads were maps and Microsoft’s software which would differentiate Lumias from iPhones and Androids. The situation looked good from Nokia’s standpoint: It was still ahead of Samsung in China. Even though Nokia’s turnover in the world’s most populous country had dropped 18 percent due to Androids, its market share was still 12.7 percent, whereas Samsung’s was 12.2 percent.In April 2012, the news threw cold water on the enthusiasm. Nokia issued a profit warning and published shocking figures from the first quarter: The loss was 260 million euros ($347 million). According to the media, Chinese network providers’ interest in Lumias was slim. The reason was Android.At the end of the month, Nokia published the quarterly report and concurrently announced that the sales in China had collapsed. At the beginning of the year, only 9.2 million phones had been sold, compared to the 23.9 million in the previous year. Elop defended by pointing out that the Chinese government had a strong home preference. The Chinese network providers bundled local manufacturers’ phones with low-cost call plans. According to Elop, during the last few weeks, every feature phone sold in China had been domestic. He also mentioned pressure from another direction: The Chinese bulk manufacturers such as ZTE had started to sell their brand worldwide. They would bring competition outside of China as well.There were setbacks in the United States as well. Nokia had to disclose a software bug in Lumia 900 that can cut off data transfer. The company offered a $100 rebate to the affected customers via their phone bill. This bug was a blow to Nokia’s campaign in such a vulnerable stage.By the summer of 2012 Elop had had enough. The leaders responsible for the Lumia launch, Niklas Savander and Jerri DeVard had to go. Elop was, however, happy with the actions taken in the United States. This was manifested in the region lead Chris Weber’s promotion to be the executive responsible for sales and marketing, and a member of the group executive board. Elop considered Weber to have done well, despite the difficult starting position.Nokia had been highly popular in North America during 1999–2000. Nokia mobiles had been forerunners in technology and design. Owners of the Nokia candy bar phones with their embedded antennas had received looks of admiration from Americans with their old fashioned whip antenna Motorolas. Nokia’s phones had sold like hotcakes at the turn of the century and the its market share had been over 50 percent. In 2001, the market share had started to decline. The reason was that Nokia could not offer CDMA phones to network providers because Nokia had become fallen out with Qualcomm.Qualcomm was a thorn on Nokia’s side. Almost a four-letter word, if you asked the Nokians.The home base of Qualcomm, founded in 1985, was in San Diego, California. Its main products were components for mobile phones, data transfer standards, and satellite positioning systems. Qualcomm had sold its own mobile phone manufacturing base to the Japanese Kyocera and focused on making money with the technologies it owned. The mode of operation of this American company had been unscrupulous. It had exclusive rights to the CDMA technology it had developed. That technology had become the prevailing mobile data communication technology in the US. In Europe, the chosen standard was GSM.Nokia did use Qualcomm’s chipsets at one time, but the contract had terminated in 2005. [15] Negotiations for the renewal of the licence contract pitted two giants against each other: Nokia was at the pinnacle of its success and Qualcomm had acquired an unambiguously solid position in the American mobile ecosystem. The contract had not been renewed, and the companies had ended up in a three-year patent war. When the dispute was resolved in October 2008, Nokia paid a one-time fee of $2.29 billion to Qualcomm for the patent contract.An even larger payment was to come. Quarreling with Qualcomm, Nokia had fundamentally slowed its entry to the 4G market in the United States. Verizon, one of the country’s largest network providers, had invested heavily in CDMA networks and cooperated closely with Qualcomm. Nokia was left out of this game, and the CDMA device manufacturer spot had been taken by Samsung. The foundations for the future success of the Korean company in the United States had now been laid. Nokia had also succeeded in ruining its relationship with another large American provider. Nokia had been AT&T’s main supplier, but messed up its relationship. AT&T had wanted Nokia to tailor their offering by implementing AT&T-exclusive features. Nokia declined because it wished the phones to be unambiguously Nokian and because of the added cost of tailoring. Other manufacturers succumbed to the demands of the American network providers.In 2011, Nokia’s market share in the United States was zero. Network providers retained the memory of its antics from years back. It entered the 4G business with a remarkable headwind, and American consumers hadn’t even heard of Nokia. The situation was made worse by the choice of Windows Phone, out of all the world’s operating systems. Network providers had been fed up with Microsoft’s forced feeding of Windows during the last ten years.The reality is that in the United States, a new phone model only enters the market at behest of the network provider.In the spring of 2012, Nokia began a gigantic advertising campaign with AT&T for Lumia 900. AT&T had invested a record $160 million in the campaign. American television viewers were inundated with 30-second Lumia spots at primetime. The advertising spots had been purchased from all large networks: NBC, CBS, and ABC. There is no more expensive way to advertise a phone, worldwide. And the pounding of the impression in American minds was unrelentless: The rapper Nicki Minaj rose up in New York’s Times Square to promote Lumia 900. Later, another problematic mega brand used Minaj in their rescue attempt, the market share loser in American soft drink market, Pepsi. The campaign did have an effect. The sales figures in 2013 looked much prettier. The three-month sales volume almost doubled compared to the year before, from 1.1 million to 2.1 million.This was an important milestone to Elop. He had to be able to show the Nokia board as well as the shareholders that it was possible to succeed in America.This fortune was, however, short-lived. The American providers are not known for their patience. A new phone is allowed at most six months by the network providers. If the device isn’t selling, it will be dropped from the selection or its sales price will be lowered. The Lumia American sales might have looked great from the Finnish vantage point, as the starting point had been zero. From the network provider’s viewpoint, the sales of two million pieces were modest. The same numbers were attained by marginal players such as Sony and Kyocera. In July, the Wall Street Journal reported that AT&T had started to sell Lumia 900 at half the price. You could now buy the phone at $44.90 on a two-year call plan whereas its price earlier had been $99.90. The call plan in question is a normal American sales tactic, where the network provider entices the consumer to commit to a data-intensive smartphone.This action tarnished the Lumia price image: In the future, they would be even more difficult to sell at a high price.Though Nokia was struggling with its sales on the featherweight range, it had a couple of major trump cards. It had its reputation as the former king of the mobile phone business, and it still had its abilities as a device manufacturer. Together with the king of software business, Microsoft, it could be able to grow as a counterweight to Apple and Android that had become too strong from the perspective of network providers. This was the line of reasoning with the American operators in 2012.One of the contract items in the Windows cooperation was the investments in marketing. According to estimates, Nokia and Microsoft were planning a ca. 500 million euro ($667 million) push in the marketing of Lumia phones in the United States. Microsoft was offering Nokia a spot as a so called prime device manufacturer. The software company, however, insisted that the upcoming Lumia 920 phone would be marketed in the US as “Windows” and not as “Lumia”. Elop did not budge. The Lumia name would not be sacrificed. Nokia had worked long to build the brand and many good properties were associated with the name. Nokia declined to campaign together, and Microsoft took HTC on its side. In the fall of 2012 the American market witnessed a Windows Phone campaign, but its poster boy was a HTC phone.On September 5, Nokia held a press conference, this time in New York. The newest Lumias 920 and 820 were on display, and they worked on the Windows 8 operating system. Elop presented the camera technology of the 920 in news stories and TV spots enthusiastically. According to Elop, photos taken with the phone were of much better quality than those taken with e. g. Samsung Galaxy S3. Nokia was highlighting its biggest asset, to which it had invested tens of million dollars to develop.Nokia was also vocal about their maps. The consumer was not, however, buying phones on the basis of a camera or the maps. The decision to buy was usually brand-based. In 2012, the iPhone fervor was at its hottest.The biggest gap in the Nokia Lumia 920 was still in apps. You couldn’t get Spotify, Hipstamatic, or the newest Rovio games to run on it. Lumia 920 got a mild press reception. Even though the phone was on par with competitors in terms of its technical specifications and its usability, it was not revolutionary enough to rise above the crowd, according to comments. The expectations of the buyers had become unreasonable. Anything less than a revolution was too little.On the same day as Lumia 920 was launched in New York, Nokia released a video on Youtube, featuring the optical image stabilizer of the new phone. The video showed a young woman riding a bicycle. In one scene on the video, Lumia’s optical image stabilization technology was in use, while another scene had been filmed without the stabilizer. The video gave an impression that it had been shot with the new Lumia device. Technology bloggers got interested and started to dig in. It turned out that the woman on the bicycle had been filmed by a professional camera crew riding in a van. The next day, Nokia issued an apology: “This was not filmed with Lumia 920. At least not yet. We apologize for the confusion we created.” According to Nokia’s communications department, the idea with the video was to simulate how image stabilizing can improve image quality.The situation was extremely embarrassing for Nokia.Nokia’s own marketing blunder destroyed the most important sales argument for the new flagship smartphone. A scapegoat had to be found. Elop launched an internal investigation of what had happened. Ilari Nurmi, responsible for smartphones strategy and marketing, was chosen to be the guilty one. Nurmi left the company without making any noise, like the norm is in situations like this. He confirmed over email to the news agency Reuters that he had left Nokia, but did not mention if this was because of his own initiative. Nokia did not comment on this.Worrisome news arrived from China in September. China Mobile had chosen Lumia phones in its range and now they indicated they would also start selling the iPhone. China Telecom and China Unicom had been selling the iPhone already for one year and now they were about to start selling the Samsung flagship phone, Galaxy S3. Smartphone competition in China was now in full speed.Nokia’s phone business decline in China continued in October. The revenue fell nearly 80 percent year-on-year, by about one billion euros ($1.3 billion). The fall was due to the collapse of Symbian smartphone sales and worse than anticipated demand for Lumia phones. A year earlier, China had been the best market for Nokia. Now it had fallen into second-to-last place after North America. Also in October, new price reductions were announced in the United States. The prices were reduced before the products were on the shelves. Best Buy had taken pre-orders for Lumia 920 for 115 euros ($149). The phone was bundled with a wireless plan by AT&T. Sales of Lumia 920 began on the major European markets in October: France, UK, and Germany. In November the sales started in Australia, Asia, the Middle East, and the United States.It looked like the Lumia sales were a continuous roller coaster. When hundreds of millions of dollars were spent in marketing campaigns, sales numbers were good for a couple of months. After six months of launching, the momentum had usually vanished and Lumias disappeared from the minds of consumers. Especially in the United States, there was some major fluctuation in Lumia sales due to the network providers’ advertising campaigns affecting consumers’ purchasing decisions. And this fluctuation was showing no signs of cooling down. Towards the end of 2012, a fleet of new Windows Phone 8 phones were coming to the market. The Windows Phone 8X by HTC was already in stores and the Ativ S by Samsung was soon to become available. Lumia 920 did not have too many advantages against these: It had a good camera and a low price. The new Lumia was 200 euros ($250) cheaper than the latest iPhone 5.Then, finally: Solid ground under the feet. Or at least it looked like that.In November, Lumia 920 pre-sales began in the Amazon online store in China — and the phone was sold out in half an hour! This was highly encouraging for the actual shipments that were planned to start after Christmas. The price for Lumia 920 was 450 euros ($580) in China, 200 euros ($258) less than in Finland. Although the price was still high for the Chinese, there was interest towards the product. Lumia 920’s different color variants had also taken the four top spots on the pre-order list of the large online store Expansys China. The Samsung Galaxy Note II took the fifth position on the list that indicated how many consumers wanted to buy the products. More good news arrived in December: China Mobile started to sell the TD-SCDMA version of the Lumia 920.Good news was coming also from the US. The black and white variants of the Lumia 920 were the most popular phones with AT&T. The AT&T top ten list of phones actually had four Lumia 920 color variants. The Nokia share price rose by nine percent in Helsinki and six percent in the United States.Also in Germany, Lumia 920 was selling well. It looked like the Germans had forgotten the shutdown of the Nokia factory in Bochum in 2008.Elop was full of hope in the interview with the Finnish newspaper Helsingin Sanomat. He said that the mobile phone industry is going through a major transformation that will help to improve Nokia’s position. More and more consumers were beginning to look for an alternative to Android. The CEO said: “We are at this very moment in a very important phase in renewing our strategy. We are launching important feature phones and smartphones to the market. I can assure you that Nokia is doing the best work right now in a long time.”Was this just a temporary frenzy or was the smartphone business finally making some sustainable progress?The company share price indicated the latter — the price had doubled over the last six months.Steve Ballmer of Microsoft was sharing more good news when he announced at the Microsoft annual general meeting that Windows phones were selling four times more than a year ago. While Windows accounted still only for a few percent of the global smartphone market, the direction was right. Stumbling competitors were not a bad thing either. Samsung had difficulties with ramping up their own Windows phone Ativ S due to a component shortage. The iPhone 5 was suffering in Europe because the phone worked only in two LTE networks. The Lumia phones operated in more than 20 European LTE networks. For those customers who appreciated the fastest possible network speed, 4G compatibility was a decisive factor.Also some network provider representatives joined the crowds of Lumia supporters. Nokia was said to have returned as a pioneer in the mobile industry and changed from a follower to an innovator again. In addition to the camera innovations people were pointing out the augmented reality features in Lumia phones that combine mapping and virtual imaging.January 2013.It was cold and dark. Nokia announced its annual financial results. Despite all the good estimates and mentions on the lists of the most popular phones, the Lumia sales had eventually turned into a disappointment: A mere 4.4 million phones had been sold from October to December. During the same period Apple had sold 47.8 million iPhones. The Nokia smartphone unit had been making a loss for the full previous year. It was also worrying that distributors had exceptionally high volumes of Nokia phones in their warehouses. In light of the market share figures, Nokia’s situation was catastrophic. The research firm Strategy Analytics stated that Nokia’s share of the smartphones market was only six percent. Apple and Android had captured 92 percent of the market.Billboards by the streets of Beijing urged consumers to celebrate the New Year with the Lumia 920T. However, many Chinese retailers had nothing to sell. China Mobile, the biggest network provider in China, accused Nokia for the lack of devices: They had received only a third of the volumes they had ordered. The news agency Bloomberg quoted China Mobile saying that Nokia’s production was slow and did not meet the demand. Missing the Chinese New Year — the best shopping season of the year — was a pivotal mistake by Nokia in a situation where their market share on the Chinese smartphones market was already less than one percent.On February 25, 2013, mobile people across the world convened in Barcelona for their annual trade show.Nokia introduced four new phone models at the Mobile World Congress. Head of marketing Chris Weber was raving about Nokia for the first time having a complete portfolio of phones running Windows Phone 8. The global market share of Windows Phone smartphones was three percent but Weber was bravely defending the chosen path. What was more important than big advertising campaigns was to make phones that would not let down the consumer. He had to say this because the marketing budget was already gone. There was no money in the bank any longer. Despair started to be visible. Weber said: “Many have asked if this year will become a turning point for us. Our answer is that the most important thing now is to maintain focus and grasp the opportunities ahead.”The comment was interesting, considering that Elop had been saying for two years that Nokia is going through a year of transformation. Now it sounded that the belief in the turnaround was shaking in the company. No wonder, as the market share continued to plummet and there was still no cheap enough smartphone on the market. Nokia introduced the Lumia 520 in Barcelona and while it was the cheapest model of the range at 139 euros ($180), it was no match for the cheapest Android phones.The media circus continued. On July 11, 2013, journalists and bloggers were invited by Nokia to New York. The familiar figure walked to the stage. Information had already leaked on the internet that Nokia would be introducing a new camera phone. Microsoft engineers had finally been able to integrate the Symbian PureView monster camera in Windows Phone. The Lumia 1020 had not only the 41 megapixel camera, but also a optical image stabilization, and wide-angle optics by Zeiss. Elop boasted of Nokia reinventing the camera zoom. The CEO with his assistants demoed a picture of a needle in a haystack, the SLR-level [16] long exposure time and showed a sailing video that had been shot at sea. AT&T would start selling the phone in the United States.Yet another camera whose features had been honed into perfection.Would the buyers of the phone appreciate such perfectionism? Nokia was the only phone manufacturer making a big number of high-end cameras. Samsung and Apple did not invest into super cameras.The analyst Arthur C. Clarke of the research company IDC was singing praise for the Lumia 1020’s image and sound quality: “This device is breaking the boundaries of magic.” Clarke, however, did also state that the smartphone race will not be won with high-quality images and sound: “Nokia’s lead will not be enough to overcome their competitors in the eyes of consumers.” Trends did not support Nokia either — the new instant photography craze was not about image quality at all. Popular imaging applications, such as Instagram, were deliberately decreasing the image quality. Especially young people liked the foggy and sepia-tone images they were posting on Twitter and Facebook. And there was no Instagram for Lumia.The media wondered why a super camera was needed. The online magazine Business Insider wrote that the Lumia 1020 will “Almost Certainly Be A Dud”. Business Insider pointed out that the new phone is to a great extent same as the Lumia 920 that was already on the market. The site wrote that the phone “is only useful for people who need to work with giant, poster-sized images.”The sales of Lumia 1020 were also hampered by the economic downturn. Consumers postponed the purchase of a new phone or preferred a cheaper smartphone. Many felt that the Lumia 1020 was priced too high. With a two-year service contract the price was $300, or about 230 euros. At the same time the iPhone and Galaxy S were sold for $199. The lowest price for the iPhone 5 was $128. The journalist Marguerite Reardon of CNET summarized: “If the camera quality is truly superior to that of other devices out there, then I’d say consider the Lumia 1020. Keep in mind it’s about $100 more expensive, even with a two-year AT&T service contract, than the other top-selling smartphones.”One hundred dollars was a huge price difference in the economic downturn.On July 18, 2013, Nokia published their interim report. Elop said that the low-end Lumia 520 had started strongly in China, France, India, Thailand, United Kingdom, United States, and Vietnam. During the period from April to June the Lumia sales were 7.4 million, which was the highest quarterly Lumia sales ever. Elop said that the sales volume indicated the growing positive development of the Windows Phone ecosystem. Another piece of good news was that the big Spanish network provider, Telefónica, had chosen the Lumia 1020 in their device range.Despite the CEO’s nice words, the reality was harsh. The smartphone unit had just made a loss of 32 million euros ($42 million). However, the situation had improved in one year — 12 months earlier the phones unit had made a loss of 364 million euros ($444 million) in the same period. The phone business had now cemented itself as the element driving down the Nokia bottom line. Elop admitted that the Lumia pricing had been a really tight call. Competitors like Samsung were selling their flagship models with aggressive campaigns. The average selling price of Nokia’s smartphones had already dropped from the beginning of the year from 191 euros ($252) to 157 euros ($206). This indicated that the bulk of the Lumia sales were lower price point models.The website GSMArena released their most interesting phones list at the end of July. Lumia 1020 was second after Samsung Galaxy S4. The list had been made based on how many online search hits the phones had received. An interesting detail was that the inexpensive Nokia Asha 501 was on the third place.At the end of July, Nokia released another Lumia device. The Lumia 625 equipped with a large 4.7-inch screen cost 220 euros ($292). The device seemed to answer the two pain points in the Lumia range: Consumers wanted big displays for a cheaper price. The device was specifically targeted at the emerging markets. Lumia 625 did not raise as much interest in the global media as Lumia 1020, but the research firm Strategy Analytics said that the 625 would be selling well, since the price was right.More good news were announced in the late summer. Windows Phone was making progress on the smartphone market.In some countries the market share was already in double digits. The growth had been the fastest in France and the UK, with a market share of nine per cent. However, Southern Europe was still struggling with recession, and the Lumia market share was dropping. The share of Windows Phone had fallen in Italy and Spain. In those countries Android phones had taken a firm grasp of the markets due to their price. Android phones had a 70 percent market share in Europe and Apple had 18.5 percent. In the United States, Windows Phone had not made any progress despite all the marketing efforts — it had only four percent of the US smartphone market. Android accounted for 51.5 and Apple 42.5 percent. But the speed of Windows was still accelerating in the United States. Products were launched faster than before. Lumia phones began to be available in multiple price categories. The new Windows Phone 8 software upgrade enabled making cheaper yet more powerful phones. Nokia people also felt that marketing was better than before: Both the Microsoft and AT&T advertising campaigns were in line with Nokia’s own marketing messages.In August the technology news site TechCrunch wrote that Nokia’s market share in Windows Phone device had increased to 87 percent. Samsung and HTC were left in the dust. TechCrunch predicted that the other Windows phone makers will soon be leaving the market.The monopoly situation had never been the goal of Nokia. Since announcing the Windows Phone strategy, Elop had emphasized that Nokia wants to promote the entire ecosystem. The demise of the other manufacturers was at odds with this plan. At worst, the Lumia strategy now seemed to be progressing like Symbian had done: It was launched as the industry standard but the dominance of Nokia was driving the other competitors away and eventually the whole Symbian fell in Nokia’s arms. TechCrunch also pointed out that Nokia’s position towards Microsoft was becoming dangerous. The only device manufacturer may easily be acquired by the platform vendor. By buying Nokia, Microsoft would gain control over the entire ecosystem.Nokia has not released any Lumia total sales numbers nor separate sales figures that could be summed together.By combining multiple sources, we have come to the conclusion that between November 2011 and April 23, 2014, approximately 52 million Lumia smartphones running the Windows Phone operating system were sold.It takes two months for Samsung to sell this number of smartphones.[15] According to a former Nokia CDMA team member in San Diego, California, all CDMA phones (IS95 and CDMA2000) developed by Nokia before the 2005 Qualcomm contractual dispute were using Nokia chipsets, developed by Nokia and manufactured by Texas Instruments. Some CDMA phones with Qualcomm chipsets were developed by Original Device Manufacturers (ODM) for Nokia and sold on certain markets, but these were completely designed and developed by the ODM partners with Nokia logo added on top.18. The long wait for the tabletAll along, the idea of the cooperation between Nokia and Microsoft was to extend beyond phones. The iPad tablet revolution had left Microsoft out in the coldand both sides saw advantages in the situation. If Microsoft’s forthcoming counterattack were to succeed, Nokia would be able to ride along. And Stephen Elop started to hint about a tablet soon after choosing the Windows Phone operating system. In April 2011 he informed the public of his plans. “There are now over 200 different tablets on the marketplace, and only one of them is doing really well. I don’t want to be the 201st tablet on the market that you can’t tell from all of the others. We have to take a uniquely Nokia perspective. We could take advantage of Microsoft technology and software, and build a Windows-oriented tablet, or we could do things with some of the other software assets that we have. Our team right now is assessing what’s the right tablet strategy for Nokia.”The press was quick to reach conclusions. Windows 7? Unlikely. MeeGo? Unlikely. The next Windows version, rumored to be suitable for touch. Probable.Microsoft and Nokia had cooperated on computers before. In 2008, the computer market was shaken by a new phenomenon when people wanted to buy smaller and cheaper portable devices. Netbooks became the fastest growing market segment. Their product philosophy was a small screen and a stripped-down bare bones structure. The focus was on using the internet — so, often there was no hard disc, CD or DVD drives.The most common operating system was Linux. In the first half of 2008, only 10 percent of netbooks used Windows.At the same time the market exploded: During 2008 11.4 million netbooks were sold, this was 30 times more than the year before. The PC operating systems giant was between a rock and a hard place. Netbooks were eating into the sales of Windows laptops. Should they protect old revenue sources or run after new ones?True to its history, Microsoft went for protectionism. It refused to sell Windows XP to all netbooks or kept the license too expensive. However, netbook screens quickly got larger and more features were requested, and Microsoft got its own quickly.Nokia stepped into the picture in 2009. It announced the plan to bring a Windows based netbook called Nokia Booklet 3G to market. The device caused confusion from the start. Why did Nokia suddenly pursue the computer market? Why was it based on Windows, its competitor in mobile phones? And after all: What was Nokia going to achieve with the device?Nokia Booklet was beautiful and handy, but expensive and its performance mediocre. Sales were poor, as expected. The timing was poor, too. Market share of netbooks was at its peak in early 2010 and started to dwindle fast with the tablet revolution. In short: The device didn’t differentiate itself from others and it came out at the wrong time.According to Elop’s statements in 2011, the mistakes of Nokia 3G Booklet would not be repeated in the new Microsoft cooperation. But how?A director who worked with Nokia Design says that there were several tablet projects in circulation. Among these was the previously mentioned Meltemi tablet, which Elop probably referred to when mentioning Nokia’s own software assets. He hinted that there was also ongoing work around Windows itself, that is, Windows 7.However, the most natural choice from the start was Windows 8, Microsoft’s response to iPad. With it, Nokia could reinvent the tablet, create a completely new kind of device around it. Head of product design Marko Ahtisaari said in the spring of 2012 that he spent about a third of his work hours on a tablet. The unique device that Elop called for, Ahtisaari said, was work in progress. The mantra was heads up. Usage in both smartphones and tablets should be easier than tapping icons, eyes glued to the screen. According to Ahtisaari, voice would play a role in it.A director who worked with Nokia’s design unit says that the heads up slogan was misunderstood for something groundbreaking, a user interface such as Google Glass. It was, in his view, about a general principle of larger screens, the large and easy-to-hit Windows 8 tiles, controlling the music player with buttons on the device without having to take the device out of the pocket, and similar small improvements that lessened the need to squint at the screen.Elop thus hinted at a forthcoming tablet. In March 2012, it was announced that a tablet based on Windows 8 would be launched in the last quarter of the year and the Taiwanese firm Compal Electronics was selected as the manufacturer. Even the size of the first batch was known: 200,000 units.In June 2012, the roof caved in on Nokia. Microsoft announced its plans to launch two tablets of its own, called Surface RT and Surface Pro.According to a source in Nokia’s Board of Directors, the announcement was as big a surprise to Nokia as it was to all other Microsoft partners. In Nokia’s plans for the future, it would bring out the Windows 8 tablet at around the same time as the first Windows Phone 8 and then it would concentrate on the Meltemi tablet.To understand the harshness of this blow, one needs to remember Microsoft’s strategy at that time. It was almost completely based on software. The most significant Microsoft-branded hardware was the game console Xbox. The rest were accessories like computer keyboards. The change in strategy was a shock to PC manufacturers, because Microsoft suddenly became their competitor. What about Nokia? Even worse. If Microsoft started to manufacture tablets based on its own software, how soon before the same would happen in phones?Furthermore in summer 2012, Nokia’s special status as Microsoft’s partner was just writing on paper. The tablet strategy would have to be built from scratch.Nokia decided to wait. Since they had the opportunity to do so, they would first wait to see how Surface tablets and other manufacturers’ Windows tablets would sell. In the fall of 2012, the decision to wait seemed to have been a wise move. Surface tablets quickly turned out to be a disaster for Microsoft. Their sales started at the end of October 2012 and were poor from the beginning. Surface RT, based on their own Windows 8 applications, was a particular disappointment. The basic reason was familiar: There were few Windows 8 applications, even fewer than Windows Phone applications.In June 2013, Microsoft had to make a $900 million write-off for its Surface stock, which was one of the reasons for Steve Ballmer’s resignation/dismissal. According to market rumors, 3–5 million Surface tablets were stocked, and during the eight month period, only 1.7 million had been sold. For comparison: In November, Apple had sold 3 million iPads in 3 days, and 57 million during the whole time that Surface was on the market. However, the runaway winner was Android tablets. During July–September 2012, iPad’s market share dropped for the first time below 50%.Nokia continued to wait.In October 2013, the waiting came to an end. The phones division had been sold to Microsoft a couple of months earlier, but now the time was seen to be right. One could think the timing strange, because Surface and the Lumia tablet would compete fiercely against each other, despite the fact that soon, they would both be under the same roof.According to Elop, Microsoft had nevertheless approved the launch and knew about it before the purchase of the phone business. Microsoft had seen — and still saw — that the device would differentiate itself enough from Surface tablets.The Lumia 2520 tablet was based on the second generation of Windows 8. Elop’s hyperbole turned out to be just empty rhetoric. In reality, only three things distinguished Lumia 2520 from Surface tablets: LTE connectivity, a better processor and an additional battery in one keyboard version.Analysis is easy in hindsight. Microsoft was the worst option for Nokia’s tablet strategy. As it stepped into the world of the Windows Phone, Nokia was the last of the large mobile phone manufacturers without a tablet. And above all: Application development for Windows phones and Windows tablets were two different worlds. Microsoft had, in fact, chosen a different strategy than its competitors. For Apple and Google, smartphones and tablets were cut from the same tree. They had the same operating system, the only difference was screen size. An application created for the phone was used in the tablet as such. And if one knew how to use the phone, using the tablet was child’s play. Microsoft drew the line in a different place. The crowning idea of Windows 8 was unifying the user experience on PCs and tablets. As explained in the beginning of this chapter with netbooks, the most important thing for the company was to protect its old bread and butter. PC users had to find it easy to switch to tablets — or even better, PCs and tablets could be morphed into the same device.Because it chose Microsoft, Nokia was dropped off from this game. Tablets became a lost opportunity for Nokia, and a big one at that. During 2012, 116 million tablets were sold. 46 percent of those ran on Android. In 2013, 195 million devices were sold, meaning a 68 percent growth. Android’s market share had increased to 62 percent. The share of Windows tablets was a meager 2.1 percent.Similarly, the response to Samsung’s large screen Note smartphones came late. The first two large screen Lumias were launched in Abu Dhabi at the same time as the first tablet — a couple of months after the decision to sell off the phones business.Why so late? Because at first, Windows Phone 8 didn’t support large enough screens. And even if it did, tile sizes were uncomfortable. The third row of tiles and support for large screens became available for Windows Phone 8 with its third update in October 2013.To top this all off, Nokia had to recall 30,000 Lumia tablet chargers for repair or replacement. The charger, manufactured by a subcontractor and sold with the device in eight countries, could give its user an electric shock. Using of the charger was to be stopped immediately.19. The next billionMary McDowell had a difficult task ahead of her at the ExCel exhibition center in London. It was September 14, 2010. During the next thirty minutes she had to convince journalists, analysts and bloggers on the awesome future of Nokia feature phones, while they were distracted by other topics: The recently appointed Chief Executive Officer Stephen Elop, who had not shown up yet; Anssi Vanjoki, who was set to leave the company, but had once again captivated his audience like a rock star; MeeGo, which had not been mentioned at all; and Symbian and its potential improvements.McDowell had the responsibility for leading Nokia’s cash cow: Feature phones. They were supporting Nokia, even as smartphones were underperforming. She talked about Nokia’s tough new goal: Mobile internet would be brought within the reach of the next billion consumers. McDowell said: “Nokia is proud of bringing the internet and mobile devices to people in every corner of the world. If the internet is the great equalizer, mobile is the great enabler.”In 2010, the foundation of Nokia’s business consisted of devices priced at a few tens of euros ($30–50), with which one could make calls, send text messages, and use simple web services. Thanks to efficient production, feature phones yielded larger profit margins to Nokia than smartphones. The amazing efficiency was based on the S40 operating system, which had been introduced in 1999. Nokia conquered the world with S40. It was made possible because the system could be tailored at a low cost to mobile network providers operating in different regions. By 2012, Nokia sold 1.5 billion S40 devices across the world.McDowell, a 46 year old American, had studied computer science at the University of Illinois. She had worked at Compaq and Hewlett-Packard before joining Nokia in 2004. She was appointed directly to the Group Executive Board, where she was the only woman. First, she led the Enterprise Solutions unit, responsible for phones targeted at business customers. In 2008, McDowell was appointed Chief Development Officer, and in summer 2010, Executive Vice President of Mobile Phones. McDowell retained her position as the leader of the Mobile Phones unit also when Elop started as CEO. Even though the market share of Nokia’s feature phones had slightly declined, McDowell’s unit was making a reasonable profit, and had promising growth opportunities. When Elop announced his new strategy in February 2011, bringing the mobile internet to a billion new users was an important part of it.The race among the giants was, however, tightening. In January 2011, Eric Schmidt, the CEO of Google, wrote in Harvard Business Review: “As I think about Google’s strategic initiatives in 2011, I realize they’re all about mobile… But to realize that vision, Google needs to do some serious spade­work on three fronts. First, we must focus on developing the underlying fast networks (generally called LTE)… Second, we must attend to the development of mobile money… Third, we want to increase the availability of inexpensive smartphones in the poorest parts of the world”Speculations on the restructuring in the mobile phone business landscape heated up. Would Google buy Nokia’s mobile phone business, or even the entire company? When Nokia gave a profit warning in May 2011, due to the Symbian catastrophe, feature phones were still making a profit. In August, Nokia’s mobile phone market share had collapsed from 30.3% to 22.8%. Nokia was still the largest manufacturer in the world, in terms of volume. It had shipped 97.9 million units in the second half, whereas Samsung had shipped 69.8 million phones.In September, Nokia announced that its feature phone production in Europe would come to its end. The factory in Cluj, Romania, would be closed down by the end of the year. Henceforth, inexpensive phones would be manufactured in Asia, as their markets were there. In addition to the factories in China and India, a new factory would be built in Vietnam. A person who worked in Nokia Communications remembers having seen Elop unusually nervous in Cluj. Elop spoke to the factory staff via an interpreter. The audience, who had heard about the termination, were naturally hostile. The mobile phones business still looked promising: In the same month Niklas Savander, the executive responsible for the sales and marketing of devices, estimated that the demand for feature phones was on the rise.The most significant markets for Nokia’s mobile phones were in India. Nokia made a critical mistake in bringing dual-SIM phones late to the market. According to Ramashish Ray, who was responsible for retail sales in India, Nokia was two years late: “Slow reaction to market reality, leadership bureaucracy and the diffusion of the decision making to too many forums”, Ray lists the reasons for the delay of the dual-SIM phones.Dual-SIM devices became wildly popular in India, as they allowed several people to share a single device. In addition, patchy network coverage could be improved by using SIM cards of two different network providers. Nokia announced its first dual-SIM phone in August 2010, and shipped 18 million of them to sales points during the third quarter of 2011. This was larger than Apple’s global iPhone sales during the same period. Typical to Nokia, volumes were great, but competitors moved even faster. Samsung had time to fill the Indian market with its own dual-SIM phones right under Nokia’s nose.Nokia Money was also popular in India. The project began when Olli-Pekka Kallasvuo was at the helm. The goal was to develop a simple payment solution to inexpensive mobile phones, and thus enable the disadvantaged in developing countries to become users of financial services. Out of the 6.6 billion people in the world, only one billion had bank accounts back then, whereas four billion were mobile phone users. In the Indian countryside, for example, cash transfers were made through couriers carrying piles of cash. Payments using mobile phones would bring money transfers to the present day. Mary McDowell had acted as the godmother for the project, which raised great expectations. More than a hundred people were developing the service. Obopay, an American company developing mobile payment applications, was involved in the project. Nokia Money was one of the fastest growing mobile services at Nokia. Tens of thousands of financial service agents were selling it to consumers.Elop spoke about Nokia Money in excited tones still in the beginning of 2011. A person who worked with Nokia Money in Oulu in Northern Finland remembers how feelings were conflicted when other Oulu Nokians were brought to the slaughterhouse at the former premises of the butcher house Atria to be notified about their layoffs, while at the same time, Nokia Money was presented as exemplary.The hype was short-lived. The interviewee was negotiating the launch of the service in South-East Asia in summer 2011, when he got a call from the headquarters: Come home. Nokia Money would be ramped down. According to the interviewee, Nokia Money became a victim of a strategic choice, i.e. focusing on smartphones and shortsighted cost cuts. The added value from Nokia Money could not be proven as fast as Elop’s penny-watching watchdogs would have liked. He reminds that it is easy to calculate that WiFi adds 25 cents of value to the mobile phone, but determining the value-add for a service is much more complicated.Later, Nokia Money continued its life in a company called Mobile Mistral Oyj. Sports Tracker, an application that later became a success, had been carved out in the same way earlier.London, October 26, 2011. Asha phones were presented to the audience at Nokia World. Asha is a Sanskrit word for hope. Asha was indeed the planned means to get to the next billion mobile internet users. Asha was supposed to have all the goodies that a consumer in developing markets could want: A five-megapixel camera, touch screen and full QWERTY keyboard. Dual-SIM. Music player and a battery lasting 52 hours.The price of an Asha varied between 60 and 115 euros ($75–150). They were more expensive than the cheapest Nokia feature phones, but clearly more affordable than smartphones. In terms of features, they were similar to smartphones. Nokia again tried its recipe for success: Slightly more reliable and more stylish phones compared to competitors, and features that should appeal to an Indian or Chinese consumer.At the same event, Nokia also announced their cooperation with Rovio. It was an attempt to inject some strength into the flagging S40 platform with the Angry Birds game. It was also included on Asha devices. Rovio’s marketing director Peter Vesterbacka said he believed that the growth of Angry Birds will accelerate along with S40. At the London press conference, Elop emphasized how hard it was to implement a sophisticated design at massive volumes. It certainly was, but could the consumer appreciate the effort?Nokia had had a strong foothold in India for a long time. Hopping on to a bus at Delhi airport in the early 2000s, the most common ringtone one heard was the Nokia tune. Nokia leaders got an audience with the minister of telecommunications with one phone call. Nokia had been one of the best-known foreign brands in India. The situation started to change in 2010. Nokia tune became increasingly rare in Delhi each year. Nokia offered Asha feature phones at the price of a cheap Android smartphone, which was a lost cause from the beginning. Samsung’s Rex basic phones were disrupting from the other end of the price range, as their price-quality ratio was perceived to be better than Asha’s. Until then, Nokia’s low-end devices had brought retailers large volumes and commissions. But now the rising Android manufacturers one-upped them. Samsung spent significantly more on marketing and lubricating retailers. Indian phone manufacturers saved on components and software, and the quality varied a lot, whereas Nokia emphasized quality to the bitter end. When the going got tough, the nice kid did not make the grade anymore.February 27, 2012. At Mobile World Congress in Barcelona, Nokia presented three new Asha models and two new Lumia smartphones. Nokia’s share price dropped almost six percent on the Helsinki stock exchange. Nokia’s new models did not convince investors.In April of the same year, Nokia announced the results of the first quarter. It was a sad read. The most crushing news came from China: The revenue of mobile phones had dropped by 70%. The S40 models did not please the Chinese too much.Magnus Rehle, a former analyst with the Nordic network provider Telia-Sonera, said that the problem in China was the inability to attract small application developers. The phone must have global services such as Facebook and Twitter, but also needs local apps. The game is lost without them. Another mistake in China, according to Rehle, was spreading the efforts over too many fronts. There was demand for affordable smartphones, but Nokia pushed feature phones to the market. Nokia brand as such still had a lot of value in China. Rehle believes that had Nokia sold affordable smartphones equipped with relevant applications under its brand, and not Ashas disguised as smartphones, there would have been a guaranteed demand.On an investor call related to the quarterly review, Elop bravely reiterated that experiences with Asha were encouraging. He promised that Nokia would invest significantly in the research and development of feature phones. Chief Financial Officer Timo Ihamuotila assured that Nokia’s profitability in feature phones would remain competitive. The management did what they could to reassure investors, who were losing their last hope with Nokia.June 2012. Layoff of 10,000 Nokia employees. Mary McDowell had to leave, too. Her role as head of Mobile Phones was filled by Timo Toikkanen, 46. Toikkanen, a lawyer by education, had led Nokia’s business in Hong Kong, China, Middle East and Africa. Previously, he had been responsible for strategic operations and business development. In Hong Kong, he had served as the chairman of the Finnish Chamber of Commerce in Hong Kong, as the vice-chairman of the European Chamber of Commerce, and as a member of the Executive Committee of the Hong Kong Wireless Technology Industry Association. He was networked deep into the Asian business elite.Operational leaders had now been changed. However, more radical measures were needed, as the situation had become unbearable.Nokia was still missing a smartphone priced at under 100 euros ($130). Something had to be done. In September 2012, Nokia presented a new model: The Asha 309. At the same time, Nokia announced that Ashas equipped with a touch screen were smartphones from then onwards. Nokia’s official Conversations blog said: “The new devices offer a fluid ‘swipe’ user interface and an open environment for third-party app development — characteristics that have earned the complete Asha Touch range full smartphone classification from global market research companies and analysts such as GfK and IDC.”The view was well justified from a technical point-of-view: The new Asha models had maps, a touch screen, WiFi, internet radio, an improved browser and Facebook and Twitter applications. If the popular Ashas would really be considered as smartphones, it would revolutionize the market shares. The smart Ashas could get Nokia back into the major league, at least on paper.The announcement still smelled fishy. Would consumers buy it?In October 2012, Nokia announced its third quarter results. Asha sales were strong. Sales of feature phones had increased by three million units in three months, even though Wall Street had expected a decline. The position in developing markets looked good, for a change. Many old competitors, such as Motorola and Sony Ericsson, had entirely abandoned the production of cheap mobile phones. Nokia faced local competitors, such as Spice and Micromax in India, which had products of lower quality than Nokia had. Nokia was bringing its maps services to cheaper models, which could squeeze competitors even further. Even Europeans suffering from a downturn were buying Nokia’s cheap models, which was positive as well. The ten percent increase in Nokia’s European sales volumes was a testament to this.“Nokia is back in the game in feature phones”, estimated the British analyst Neil Mawston in the Finnish newspaper Helsingin Sanomat in November 2012. In the same article, Elop said that Nokia had sold 6.5 million Asha devices during the previous summer. According to Gartner, Nokia was still the second largest mobile phone manufacturer after Samsung. Samsung’s market share was 22.9% and Nokia’s 19.2%, taking smartphones and feature phones together. The difference was not that great. However, profitability separated Samsung from Nokia. The Korean giant sold both smartphones and cheaper devices evenly, whereas Nokia’s sales were mostly cheap phones. Nokia had dropped to a marginal seventh position in smartphones. From July to September, out of the nearly 200 million smartphones sold globally, less than 3 million were Lumia devices. The industry at large did not go along with Nokia’s self-imposed decision to classify the more expensive Ashas as smartphones,. As Ashas were built on top of the S40 feature phone platform, most industry analysts had decided to classify them as feature phones.Worse still, the growth of the mobile phone market had stopped. In the fall, 428 million units were sold, compared to 441 million a year earlier. On top of that, a growing share of the phones sold were cheap smartphones. Those, which Nokia did not have. The price level of phones was declining across the board. If a smartphone would cost 70–100 euros ($90–130) going forward, Nokia would need to lower the price of its feature phones to 30–60 euros ($40–80). It would be the final blow to the profit margins.The Nokia wagon was hurtling down the slope, but Nokians tried to find joy in the smallest achievements. Christmas sales had gone well — in Finland. Asha had become a hit. In addition to the flagship devices, such as iPhone and Lumia, Santa carried cheap phones designed for the Indian market, in his bag. Asha appealed to the youth, because it was preloaded with Angry Birds.The tenth of January 2013 was a happy day for the Finnish economy. Nokia issued a positive profit warning. Big headlines made a reappearance in the reports of business journalists. They were truly enjoying being finally able to write positive news on Nokia. The Mobile Phones unit and the Lumia range had beaten expectations. More than 14 million Ashas and Lumias had been sold. Nokia’s share price price rose by a stunning 16 percent. In a BBC interview, Ian Fogg, an analyst with IHS, estimated that Asha would be one of the winners in the future. Fogg reminded that as much as a third of all phones sold in the world in 2016 would be affordable smartphones. Access to e-mail and internet would be sufficient for a growing segment of the world’s population. Only those wanting great gaming capabilities and fast internet connectivity would opt for an expensive smartphone.The good fortune lasted for two weeks. On January 25, 2013, Nokia reported a barely profitable 2012. The income was entirely due to Nokia Siemens Networks. The device business reported a loss that was nearly as large — 700 million euros ($920 million). Phone sales volumes were still large, 336 million units in the previous year, but profitability had evaporated. The former ruler of the mobile device business still pushed out large volumes, as in the old days, but was no longer bringing money in. The business had turned into a fool’s game, which undeniably showed up in the bottom line.In the days following, the market research firm Strategy Analytics published information on mobile phone producers’ market shares. Nokia’s share had decreased significantly in both smartphones and feature phones. Samsung had extended its lead as the largest phone maker in the world. The Korean company put on a fantastic performance with their phones: In three months, almost four billion euros ($5.3 billion). Nokia’s phones were loss-making. The leadership of the phone business had moved from Espoo to Seoul, South Korea. There was also a worrying rumor about Apple’s affordable iPhone. Bloomberg and Wall Street Journal wrote that Apple was seriously planning a sub-$200 iPhone. There was a common belief in the industry, that if this were to happen, there would be severe consequences especially to Nokia, which had a tight cost control, as well as to RIM and HTC. Gartner estimated that the mobile phone game during the ongoing year would be tighter than it had been earlier. Gartner still believed that Nokia would be able to raise the number of applications and the prices of devices, which would help the company back on the path of growth. Nokia was after all, still the second largest device manufacturer.In February 2013, at the Mobile World Congress in Barcelona, Nokia announced Asha 105, priced at 15 euros ($20). Its predecessor, Nokia 1280, had sold 120 million units worldwide. The low price was a surprise. In terms of quality, Asha would easily beat its similarly priced Chinese competitors. Elop reminded that there were still 2.7 billion people in the world, who did not have a mobile phone. Asha 305, which had been launched the previous summer, was chosen as the best mobile phone of the show. Two other Nokia phones were also candidates to win the prize. Nokia’s expertise in feature phones was still valued.In February, Nokia launched a new dual-SIM device. Asha 310, priced at 100 euros ($137), would start to sell during the first quarter in Asia, India, Middle East, Africa and Brazil. However, the dual-SIM market was lost. In March, the newspaper Hindu Business Line reported that D. Shivakumar was let go. He had been responsible for Nokia’s operations in India from 2006 to 2011. In April, market research firm GfK-Nielsen revealed that Samsung had surpassed Nokia in India. Psychologically, it was a heavy piece of news. At its highest, Nokia’s market share had been 80 percent. The newspaper Economic Times of India estimated that Samsung’s overtaking was due to its strong reinforcement of the product portfolio. The Samsung Rex phone had become tremendously popular in India. In the news article, Nokia’s former sales director Sunil Dutt wondered how Nokia’s fall by the wayside was possible in just six years.India caused other problems as well. During early 2013, the Indian tax authorities had taken the bookkeeping of the Chennai factory under their magnifying glass. They suspected that Nokia had been avoiding taxes. The newspaper Hindu Business Line reported that the unpaid taxes in question amounted to a hundred million euros ($140 million). The Sriperumbudur factory was located close to the city of Chennai, and for instance, Asha devices were manufactured there. Telecom companies had come under close scrutiny of Indian tax inspectors. For example, the giant network provider Vodafone faced tax payment demands in the range of a billion dollars, based on an acquisition completed years earlier.April 2013. The first quarterly review threw cold water on shareholders. The profitability in feature phones had finally collapsed. Nokia had sold 55 million mobile phones in the three-month period from January to March, compared to 70 million a year earlier. The forward-looking statements by the management indicated that the problems in feature phones were expected to continue. The sales in China had collapsed already at the beginning of the year, and it seemed that Middle East and Africa would follow. One reason was that the feature phone stock of Asian network providers had grown too large. Consumers increasingly shied away from buying cheap feature phones, instead opting to buy affordable smartphones. Network providers emptied their stock more slowly than before, and did not purchase new devices from Nokia.Despite everything, Elop was smiling his famous smile in May 2013 in Delhi. Nothing in his appearance revealed the crisis. India was going under. Samsung had invested $1 billion for marketing Rex in India. Nokia did not have the weapons to respond to such a strike. According to Ramashish Ray, Asha was a decent product, but its fate was to lack the kind of partners in India that Symbian had had. With Symbian volumes, Nokia had been able to get the best players in the business as partners. Along with the collapse in volumes, Nokia had now lost these partners.Nokia still had an ace up their sleeve. In Delhi, Nokia’s gospel was that Asha was not just a phone, but also an operating system that would replace S40. Improving Asha was possible thanks to an acquisition Nokia had made. In November 2011, Nokia had half-secretly acquired the Norwegian software company Smarterphone, which had a product of the same name. According to their marketing, it made all phones smart. The acquisition had become public in the first half of 2012. Now that Ashas had touch screens, they would start to resemble smartphones even more, with the help of Smarterphone. The most important innovation was the swiping technique, which meant that one could use Ashas with the convenient swipe movement familiar from smartphones. The new Asha operating system, based on Smarterphone, became the replacement for both S40 and Meltemi at the same time. Peter Skillman, who had worked on MeeGo and N9, had designed the user interface.Asha, Nokia’s last hope.In July, Nokia reported their quarterly results as usual. The Nokia group had made a profit of 243 million euros ($316 million) during the second quarter. The result was clearly better than what the analysts expected. A year earlier, the company had floundered in a loss of 377 million euros ($479 million). The phone business still looked sad. It had made a loss of 32 million euros ($42 million), even though the situation had improved from the previous year, which was in the red by 364 million euros ($462 million). Smartphones were losing money, feature phones barely breaking even. Even though the volumes had not significantly declined, the revenue had collapsed. Phones were dumped at rock-bottom prices.Elop announced that the Mobile Phones unit would start the statutory negotiations for reducing jobs. Layoffs threatened 440 people working for the unit of which 160 in Finland. The remaining 500 people in Oulu were most afraid. Nokia had tried to move feature phone software development from Oulu to China already for years. Everything else had been transferred already: Mechanics, production and component manufacturing. S40 development had stayed in Oulu for the reason that no programmer in Nokia’s R&D site in China wanted to work with an antiquated operating system.In the investor call, Elop repeated the familiar refrain like a parrot: Significant measures had been taken, stock levels had been lowered, further statutory negotiations for personnel reductions are on the way. The next Ashas would offer a completely new customer experience. But at that stage, it was all too late.The fall of feature phones was the final nail in the coffin to Nokia’s phone business. After that, the only option left was selling the business. Feature phones had supported Nokia through the difficult years, and prevented a complete crisis. As late as in 2011, feature phones brought profits of 1.5 billion euros ($2 billion). In 2012, it had dropped to half a billion ($0.7 billion). In 2013, the feature phones barely made a profit, and in 2014, according to estimates, the business will result in heavy losses for Microsoft.Many interviewees said that Nokia left S40 adrift, despite many efforts. When it still had money to spend, the management focused on smartphones, and under-invested in S40. When the bad years started, the cost cuts were applied first to the feature phone platform. The interviewees thought that the opposite should have been done: Put all the effort on feature phones once it was noticed that the smartphone game was lost.Another problem was the price level of affordable devices, which had reached unprecedentedly low levels. According to the Swedish analyst Helena Nordman-Knutson, Nokia was unable to get involved in the fiercest price war, due to its heavy cost structure. The Mobile Phone organization was too expensive to sell phones at a bargain.Looking at the offering from the point of view of an Indian or Chinese buyer, it was easy to see why Nokia could not compete. In 2012, it was possible to get an Android device by a local manufacturer for even under $50. With $50–150 one could purchase a Samsung-like branded Android device with a five-inch high-resolution display, eight megapixel camera, dual-core processor and the versatile Android ecosystem. Nokia’s response was the Asha construction built on top of S40, at almost the same price, but with a smaller display, no dual-core processor, and fewer megapixels and applications.20. Tough times for Nokia sitesThe news was devastating. Two out of three would lose their jobs. The personnel at Salo factory were invited to an internal info session on February 8, 2012. Production personnel numbering 1,600 were requested to join the session. Everyone had been anxiously waiting for news since the previous fall, and now it was happening. A chapter of Finnish industrial history was about to close: The factory at Salo would cease to manufacture phones and the production would be transferred to Asia. Only a limited crew involved with research and development, as well as smartphone customization would remain in Salo. Ready-made Windows phones would be brought in, onto which Finnish workers would install the software and package the phones.The announcement was downright humiliating to Salo personnel. Salo was the place where Fjalar Nordell and Lauri Koskinen had launched radio receiver production in 1928. This was where Salora had started the production of black and white television sets. This was where Mobira had developed and manufactured the first car phones, and Salo had given birth to Cityman, Nokia’s first handheld mobile device in 1987. Not to mention the millions and millions of NMT and GSM phones manufactured there.The info session was understood to be exceptionally grave. After the session, which started at 10 o’clock, the rest of the day was announced to be paid leave, which was totally unheard of.What put an end to the Salo factory? Why was Salo no longer profitable?Salo was the most modern of Nokia’s production facilities. The equipment and the production process were top notch. The true strength, however, were the people who had manufactured mobile phones for 25 years. The knowhow and the integrity of the personnel were unparalleled on a global scale. The efficiency of the production was world class. For example, when Lumia 800 and 900 phones were manufactured, there were never any factory-related problems.According to a director who knew the Salo factory intimately, the problem was the process for designing the production of phones, which was outdated. Salo simply got going with the production slower than the competition. Apple could get started with a production batch of one million phones in one day. In Salo, it took 8 to 12 weeks before similar production figures could be reached. This was not because of the factory, though, according to the director, but because of Nokia’s production process was initially planned in 2003–2005 period when it was sufficient to get phones out in smaller batches. The first batch was sold in Europe and then onto Asia.When Elop joined in 2010, the mentality was largely the same, and Salo was Nokia’s Golden Child. The head of production and logistics, Juha Putkiranta, had referred to a new way of working in February 2010. This was piloted in Salo, because the factory was a forerunner in all inventions related to Nokia’s material flow. He explained how the market and Nokia’s strategy had changed. Smartphones were to be delivered, ready and tailored, to the network providers and large distributors. The software package would already contain applications and market-specific content, for example, maps. Salo concentrated on manufacturing phones, fast and efficiently, in small batches. Generic large-scale manufacturing, producing components and the setting of the printed circuit boards was centralized to the large factories in Asia. The production cycle needed to be sped up, because customers’ plans kept changing all the time.In September 2010, the future of Salo looked bright. A month earlier, the temporary layoffs for the fall had been called off, and the factory was hiring new staff, because the N8 smartphones were being produced for the Christmas market. The deputy chief employee representative, Marjo Kallio, announced how satisfied she was because the remaining staff in Salo was fully occupied.Elop had a major chip on his shoulder with Salo. He understood the symbolic value of the factory and wanted to avoid, to the very end, upsetting Finns in their home territory. After having spent half a year at Nokia, in an internal personnel meeting he assured that Salo factory would remain. Nokia needed more production capacity. The employees were satisfied with the news. Many said Elop’s speech had strengthened their faith in the future. Union representatives said they had interpreted the overall sentiment to be such that the personnel could look forward with relative confidence to keeping their jobs.In April 2011, when Nokia announced the massive layoffs as a result of the Windows strategy, the 3,800 staff at Salo sighed in relief. Executive Vice President of Markets Niklas Savander, announced that manufacturing is a critical competitive advantage in the future business. Salo would remain as a factory manufacturing smartphones, even if the new factory in Vietnam was already in the planning.When the hammer fell in February 2012 and the layoffs were announced, Elop still highlighted how important the Salo factory was for Nokia: “Despite the reductions that are underway, the Salo factory and the product development done in Salo will continue to play a significant role”, he formulated. In hindsight, the statement cannot be said to have been very honest. Internally, Elop had begun to make it clear that phone manufacturing must be made faster. If he had announced publicly that phone manufacturing in developed countries is no longer profitable, he could have saved face. With the path he took, he managed to both anger the Finns and got a label of goal-oriented, foreign, restructuring man.All it took was four months before the whole Salo factory was on the kill list. The last 870 workers were made redundant, and the only thing remaining in Salo was the research and development of Lumia phones.According to a director who knew the factory well, June 14, 2012 was the saddest day of his life. He also remembers the head of production Juha Putkiranta to have been thrown off by the news. “Still, from a purely business angle, it was the right decision. If the phones are not selling, how can you keep up the factory?” The director also estimates that if Nokia had been just a little better off, financially — not even profitable, but if the losses were smaller — and taking into account the strengths of the factory, Salo would have remained a part of the production chain. But the whole production model of Nokia had become old-fashioned. There was simply no money left to keep the factory running. It was Nokia’s largest asset.The last mobile phone manufactured in Salo was made on Wednesday, July 25, 2012. It was either an N9 or Lumia 800, the chief union representative could not publicly say exactly which.The city of Salo, once the symbol of Nokia’s growth, had taken its crown jewel for granted all these years. The city elders had built town halls, daycare centers and schools with the tax money, and the influx of people into Salo had been downright chronic. On top of the municipal services, the city had also invested in housing. The tables, however, had already turned a few years earlier, and problems had started to accumulate. Nokia’s downfall manifested itself as increasing health problems. Temporary and permanent layoffs were clearly taking their toll in the demand for health services. Increasing alcohol abuse was visible in the extent of support families needed. Child welfare services could no longer handle all the cases within the statutory time limits.In January 2012, the city’s Chief Financial Officer, Seppo Juntti, took a very grim view in the local newspaper Salon Seudun Sanomat, and stated that he believed the tax income would not turn into a growth path ever again. Nokia’s gradual disappearance was even visible in the amount of waste. When Nokia Salo factory’s waste compactor had been emptied at least daily, now it was emptied only once per month.In February, Salo town hall hosted a low-spirited information sharing session. Smartphone production at Salo factory would cease. In the information sharing session, there were Finnish minister of economic affairs, Jyri Häkämies, minister of labor, Lauri Ihalainen and city mayor, Antti Rantakokko present. Both Nokia and representatives of the Finnish state expressed their wishes that the mostly female labour force, now made redundant at the Salo factory, could find new employment in social and health services. Häkämies saw potential in bringing IT knowhow into the health and the energy sector. The chances of finding new employment were slim, however. Most of the 500-odd people to be laid off, had no qualifications. The city planned to start a business park into the factory premises left empty by Nokia, in the same way as forestry company UPM had done in Kajaani.Salo was already identified as a city impacted by industry restructuring. The minister of economic affairs, Jyri Häkämies, said that the Finnish government would start a rescue programme during the same spring to save the Finnish IT sector, because altogether 5,000 IT jobs were disappearing from Finland within the time span of one year, and on top, 1,000 people were made redundant from Nokia Salo factory. As engines to drive the rescue mission, the Finnish state had requested that Nokia, Nokia Siemens Networks and Accenture would join in.Nokia tried to encourage the people made redundant by saying that on previous rounds, the people who were made redundant had already given birth to 100 new enterprises. This time the situation was different: The majority of people made redundant from Salo factory were women who had no or very little qualifications. Many had worked nowhere else than Nokia.Nokia, the city of Salo, local work and economic development office, and Yrityssalo, a business incubator owned by the city of Salo, launched an information sharing office in Nokia premises. Regional Centre for Economic Development, ELY-keskus, estimated that most of the people made redundant from Salo, would have to refresh their skills and even retrain into a new profession.For the mayor Antti Rantakokko, the summer and early fall were spent extinguishing flames in the smoking ruins. After the factory had permanently closed its doors, the city council came together in a crisis meeting at the town hall. At that time, the unemployment rate was estimated to rise to 20 percent. The goal was to create 1,000 new jobs to replace the 2,000 lost Nokia jobs.It took an additional six months to bring the whole production down. Nokia started selling the Meriniitty facilities in Salo and in October, a pharmaceutical company Orion announced that they would purchase a part of the facilities and start a packaging and logistics center in the premises.Lumia research and development continued to employ 1,500 staff. In May 2013, the Nokia Conversations blog wrote that for example, the new model Lumia 925 was largely designed by them.The demise of the Nokia factory was a huge blow to Salo. The unemployment rate was 11.3% at the end of 2012. Temporary layoff numbers were up to 3,500. The local work and employment center estimated that unemployment figures would rise to 15.5%. In September 2013, when Nokia announced that they would sell the mobile phones business to Microsoft, Salo representative of senior salaried employers, Mika Paukkeri, frankly stated he was afraid the decision would deliver a death blow to the entire city. At that point in time, Salo had 1,200 employers left. It sounded a bit grotesque when Paukkeri continued to say that the Nokians in Salo still had trust in soon-to-be former CEO Elop.Nokia did not manage to bring down Oulu as profoundly as Salo, because the people who were laid off there had higher education levels. When the engine started coughing and finally stopped running, almost completely, the northern university city of Oulu was faced with a different kind of problem: Where and what is the new road to success? Nokia had started a research program together with Oulu University in the beginning of the millennium and the growth had been phenomenal. The northern Shangri-La had experienced the first crack in the veneer only in July 2010, when Nokia announced the sales of the wireless modems business to Japanese Renesas Electronics. 1,100 former Nokians moved to Renesas, 450 of which worked in Oulu. The personnel information sharing session was held in one of the university lecture halls, and the Chief Operations Officer of Renesas, Shinichi Yoshioka, came to the session in person. People transferring from Nokia to Renesas were in shock, but the overall sentiment was relief: Operations would continue with familiar people and nobody would be laid off.In August 2010, Nokia and Intel established a joint research centre in Oulu. A few dozen researchers were employed there, and this was seen as a sign of Finland still being a potential incubator of high technology. The centre developed 3D mobile applications, and, for example, games, and holograms which were aimed to improve the usability and the user experience of mobile phones. The CTO of Intel, Justin Rattner, and Nokia’s CTO, Rich Green, praised the 3D knowhow of the personnel in Oulu. They both saw big potential for this skill in, for example, the clothing industry.In February 2011, Oulu saw not only a record-breaking cold spell, but also the advisor to Microsoft CEO, Orlando Ayla. The event was held, apparently by chance, a day after Elop had said that Nokia had chosen Windows. In Oulu, people thought that the local IT coalition had a million dollar opportunity to start jointly developing software with Microsoft. The American company was interested in the 3D and cloud applications developed in Oulu. Microsoft hoped that Nokians would train their staff quickly in these fields.Nokia employed approximately 2,000 staff in Oulu at that point and additionally, 300 IT companies in the region were dependent on Nokia.On February 21, 2011 Elop made appeasing visits to Tampere and Oulu. The visits were shrouded in secrecy. Elop quickly slipped away from the press in both places. Elop took a private jet to Oulu in the afternoon, quickly got out of the cab and went in by the side door to Nokia’s premises in Peltola. Those present in the information sharing session got very little out of the man.At the end of April, 2011 the big bang came. Symbian and MeeGo development in Oulu would cease, and hundreds of people would be laid off. The development of the basic S30 and S40 phones would remain. Oulu mayor Matti Pennanen bravely commented on Nokia’s decision to outsource Symbian development to Accenture. Pennanen said that the decision would give a chance to develop new business. He emphasized that the knowhow had not disappeared anywhere and now would be the chance to make room for new business.In June, the next wave of crushing news hit. 500 to 600 jobs would be at risk. The executive vice president of human resources, Juha Äkräs, calmed down Nokians by ensuring that Salo, Oulu, Tampere and capital region would have strategic significance, also in the future.Elop gave an interview to the Oulu newspaper, Kaleva, saying that Nokia would remain in Oulu also in the future. Oulu would be central to the development of feature phones. This statement is interesting when considering how Nokia had already planned to move the development of feature phones to China since 2008. The well-oiled information machinery was rolling away even if the world was crumbling around it.A rumour started spreading in early August 2012 that Nokia will cut more jobs in Oulu than anticipated. Some of the staff had already reacted by starting their own companies or leaving Nokia. Some 500 were left in the development of feature phones. Jolla, a Finnish phone manufacturer starting business at the same time, announced that they were considering setting up shop in Oulu, and the recruitment event they organized was a success.In July 2013, the last of Nokia Oulu had to go into statutory negotiations preceding layoffs. The layoffs were targeted at the Mobile Phones unit manufacturing feature phones. The staff were afraid that Nokia operations in Oulu would come to an end.On September 4, 2013, one day after Nokia had sold the mobile phones business to Microsoft, Ballmer and Elop, who had moved to Microsoft, were doing rounds appeasing the people in Oulu again. After the years of continuous layoff negotiations, the Nokians were not shaken by the newest piece of news. Newspapers commented that Ballmer’s and Elop’s visit to Oulu was altogether carried out in pleasant atmosphere. The engineers remaining in the North did not lose their calm even at this stage. Everything had been done before, seen before.Contrary to what happened in Salo, where Nokia left only smoking ruins, the city of Oulu started placing Nokia people in new companies. Among others, city-owned BusinessOulu and business incubator, Oulun Yritystakomo, took care that laid-off Nokians spent no time crouching on their sofas. Ex-Nokians started dozens and dozens of start-up companies, most of which working with mobile services. The miracle engineers of Oulu were pitched even in Silicon Valley.Invest in Finland, an organization working for the Finnish ministry of economic affairs and employment, had salesmen traveling the world, advertising that Finland had top experts on offer. Invest in Finland gave praise for the engineers in Oulu — how they would be more loyal to the employer compared to Indian engineers. Oulu’s efforts were recognized world-wide. American Intelligent Community Forum listed Oulu as one of the seven most intelligent communities in the world.Elop’s Nokia shook not only the impacted cities, but also the Finnish national economy. In 2,000, Nokia created 4% of the gross domestic product of Finland. The Research Institute of the Finnish Economy ETLA estimated that taking the subcontractors into account, the company’s share of GDP was 8%. One quarter of the economic growth of the entire country was attributed to one company.The figures are staggering. The Economist magazine has listed ten companies that have been exceptionally important to their home countries. For example, according to Economist, Royal Dutch Shell brought 56% of the gross domestic product of Netherlands, and China Mobile attributed 34% of the gross domestic product of Hong Kong. These companies embellish the gross domestic products of their home countries only accounting-wise, because most of their operations are abroad.When the list is purged of companies registered in certain countries because of technical reasons, there is only one company resembling the case Nokia: Taiwanese electronics manufacturer Hon Hai. Even compared to Hon Hai, there is one essential difference with Nokia. Nokia made 27% of all the patent applications in Finland in 2011, Hon Hai 8% in Taiwan. Also Samsung’s position in South Korea is different. The gigantic corporation is one of the biggest companies in the country, but the economic landscape of South Korea is more diverse than that of Finland.Nokia’s share of community income tax was 17% in the best year. Nokia’s share of exports was the biggest in the beginning of the millennium, over 20%, much greater than paper, pulp or forestry equipment. In research and development, Nokia rose to the top of Europe in the year of 2008 with a 5.2 billion euros ($7.6 billion) budget. There were only eight companies worldwide that year that had a research budget of over 5 billion euros. In 2009, Nokia used almost 38% of all the research investment in Finland. At best, Nokia employed 24,500 staff in Finland which was equivalent to 1% of the whole workforce.By 2012, all indicators had collapsed. Elop’s actions had caused massive losses and the value-add provided by Nokia was negative, as was also Nokia’s impact on gross domestic product. The company became a dead weight to national economy. The share of exports dropped down to 5–10% and the proportion of Finnish workforce employed by Nokia dropped down to 0.5%.But during the years of 2001 to 2008, Nokia had contributed 11.3 billion euros ($16.5 billion) to the Finnish national economy. In recent years, the billions have not been there which has further worsened the Finnish economy, already weakened by the economic downturn. The Research Institute of the Finnish Economy ETLA’s research director, Jyri Ali-Yrkkö expresses it clearly. Finland should have done what Norway is doing — put the money in funds, like Norway does with its oil money. On the one hand, Nokia has served as an international business school for thousands of Finns. What is even more remarkable, Nokia people accumulated expertise in the consumer product business, which has traditionally been a weak point for businesses in Finland. The Finns hardened in the global racing fields of the business are now putting their expertise to good use in companies like Rovio.And finally: In 2013, Nokia’s impact on the Finnish national economy returned to black: +0.5%.Abroad, Elop’s death blows were equally dramatic as in Finland. The first victim was the Cluj factory in Romania. Closing the factory made 2,200 workers redundant. In September 2011, Nokia stated that it was re-evaluating the long term future roles of the Salo factory, as well as Komarom in Hungary and Reynosa in Mexico, because the poor sales meant that even after closing Romania, Nokia still had over-capacity in production. The factory in Mexico served the American continent, Hungary served Europe with Salo. Both factories were cut, along with 3,000 staff, 2,300 of which were in Hungary. The cuts had a major impact on the labour market both in Hungary and in Slovakia, where approximately 1/3 of Nokia Hungary workers came from.In Germany, the Meltemi unit employing 700 staff was closed down, and that made the locals angry. Nokia had promised more jobs to Ulm only three months earlier. Nokia’s bumpy decision-making was a source of wonder.Elop also reorganized research and development. The research network, in his opinion, was geographically too widely spread and far inbetween. Because one product or piece of software had been developed in several places, the ax was swung again. The Copenhagen product development center was closed down. In the United Kingdom, the number of sites were cut. In the US, the White Plains office was closed down and operations were centralized to Sunnyvale, California. Windows Phone product development was centralized in Tampere, Salo, Beijing and San Diego.The list is long and the amount of human suffering is unmeasurable. Nokia could be one of the companies in charge of the biggest layoffs in the world economy in the recent years.Compared to American-listed companies or Chinese sweatshops, Nokia still took care of the layoffs in an exemplary manner. In 2011, Nokia launched a world-wide program called ‘Bridge’ to find new work for the people who had been laid off. The initiative for the program came from the executive vice-president for corporate relations and responsibility, Esko Aho. Elop immediately supported the idea. The idea was to soften the blow and it was considered to be a part of Nokia values to support the personnel being made redundant.The personnel could take one of the five paths. They were re-employment within Nokia, re-employment outside of Nokia, becoming entrepreneurs, take training or “create your own path”.Those opting to set up a start-up could get up to 25,000 euros ($36,500)/future shareholder in Nokia-funded support with the maximum amount of support per startup set at 100,000 euros ($146,000). Those who were laid off got an additional 1–1.5 year’s pay as severance pay. Nokia also would guarantee companies’ credit accounts in banks. The programme was exceptionally altruistic in the world of business. In an interview with the Finnish newspaper Taloussanomat, various employee representative organizations could not remember any company who would have supported the new companies founded by laid-off personnel as much.Training involved consultation in finding a new job, change coaching, and training in new professions. Nokians also got their own recruitment service which sought suitable work for the laid-off personnel from external companies and offered experts to other companies.Tens of millions of euros were spent on the Bridge programme.A study on the Bridge programme published in February 2014 stated that the programme was a success. Altogether 18,000 Nokians were in the scope of the programme, of which 5,000 in Finland. In a year and a half, 70% of them had found new employment. Some 400 startup companies had been created and 550 Nokians were involved in these. Aalto University made a study according to which 43% of the people concurred with the statement: “I have wanted to start my own company for a long time, and now I got the chance.” 170 of the new companies were launched in the capital area, over a 100 in Oulu, 80 in Tampere, and 65 in Salo. The most well-known of the startups is Jolla, that could not have been born without the money from the Bridge programme. Half of the new companies are operating in the software industry.Nokia gets credit not only for the financial support but also for the flexible attitude for the people who were made redundant.“The most important things was that they encouraged us and did not try to stop us. They could have stopped the launching of the new company by referring to legislation on non-compete clauses or something like that. They were, however, open with us, and we with them, and that is how we were able to continue with MeeGo” states Marc Dillon, one of the founders of Jolla.Some who chose to the path to entrepreneurship undoubtedly used the possibility for their advantage and bought more time to think about their future. By the end of 2013, when the financial support ended, most of the companies employed only 1–3 people and some of the companies have undoubtedly ceased to operate. This does not diminish the well-earned value of the Bridge program. Approximately a half of Finnish startups fold during the first three years.The Bridge program gained fame abroad. The European Commission started exploring, based on the Bridge program, if a similar model could be applied to situations in which people are laid off from other companies in Europe operating in the IT and communications industry, as well as the supply and demand problematics of workforce. Esko Aho took the Bridge program into the curriculum at Harvard University, where he later went to work. The Bridge program is an example of how a major corporation can carry out its social responsibility in massive layoffs.21. Nokia spirit evaporatesStephen Elop came from a world where a job gives you financial security and status — but nothing beyond that. If you were to tire of your Silicon Valley employer, you would simply walk across the street and join another startup or high-tech company. In comparison, Nokia was more significant for Nokians, as the job market in Finland is so much smaller. There are only a handful of global public companies in Finland, and nearly all of them operate on a business-to-business market. If you got tired of your Nokia job, there was only one other successful international company in Espoo where your talent could be put to good use — the elevator manufacturer Kone. The mental barrier to leave Nokia for industrial companies such as Outotec, Metso or UPM-Kymmene was very high in 2010, and many still believed Nokia would bounce back.The situation in smaller cities such as Salo and Oulu was even worse. Nokia had practically eliminated all other manufacturing jobs in Salo as any talent in the region flocked to Nokia’s 5000-strong campus. Similarly, Nokia was by far the largest employer in Oulu, a northern economic center with a major technical university, and it wasn’t uncommon that a new university graduate starting work at Nokia would continue to work there for over ten years.One such graduate was Mikko Merihaara. “For many, there were no other practical employment alternatives in Oulu beyond Nokia. There was a clear focus to ensure that the Oulu site would not be lost to competition between other sites, and the desire was always to ensure Oulu was the best site in Finland.”The life of young Nokians was sweet in the early 2000s. Most employees were in their late 20s and didn’t have families of their own, so it was natural to hang out with colleagues after work. Nokia organized a continual stream of parties and social events for its employees — so much so that you could pick and choose the best events to go to. The company was growing quickly, and spending on business class flights or a Playstation for the break room was not a problem. What more could a young professional hope for?Salla Jämsä, an HR veteran who worked in many different Nokia units, believes that Nokia’s good team spirit was a result of successful recruiting, and could only recall a few people throughout her Nokia career who were genuinely difficult to work with. Nokia colleagues got along very well, and it was only natural to spend time together after work as well. Nokia was a second home of sorts — people would work, marry, have kids, and then return to work at Nokia. Most importantly, when you worked for Nokia, it was a matter of national pride. Regular middle-class Finnish men and women were designing and making phones that were used all over the world. Connecting People, from Finland.Ville Valtonen, who was head of human resources at Nokia Finland, explains how the management style during the growth years of Nokia gave plenty of freedom and responsibility, and most employees embraced it wholeheartedly. Young professionals were given a lot of responsibility and tangible targets, and they would work very hard to meet them. Work felt meaningful.Merihaara, also the chief shop steward of the Oulu site, explained how Nokia was favored by students who had gotten through their studies quickly and were eager to make their mark on the world — no further motivation or incentive program was required. Nokia was a world-class business school for new recruits, as international consumer electronics is a fickle market where few companies survive at the top for very long. In contrast to other Finnish international success stories, Nokia worked in the consumer business. Marketing gurus in the US and UK looked to far-away Finland in awe: Now that’s how you build a mobile phone brand.Nokians had learned how to work with difficult partners. Network providers were notoriously tough customers and negotiators. Managing a mostly Asian subcontracting operation efficiently was an absolutely essential skill when competing in global consumer electronics. The result was that Nokians were actively pursued by headhunters — but most chose to remain, out of loyalty to the company that had given them so much.Elop realized this slowly. It wouldn’t be enough to turn around a faltering phone business. He would have to save the whole nation.As Nokia’s financial performance weakened, many attempted simply to survive. It wasn’t easy for many to find motivation. As early as in 2010, people on the shop floor knew that changes were on their way and fast.Statutory negotiations upon personnel reductions had become commonplace. The push for cost savings had started in 2008 when the company financial results plummeted after the all-time high results of the year 2007. Investors liked the personnel reductions. For Olli-Pekka Kallasvuo, just like for Jorma Ollila before him, the share price had been the most important success factor so the personnel had to adapt to a constant threat of layoffs. This was a big change in a company that was used to good revenues. Statutory negotiations hampered both the employee spirit and execution. Anyone can imagine how effective an organization can be if it is rebooted 4–5 times a year.Personnel motivation was also hurt by duplicate work. Suddenly people might have realized that two similar products were about to be launched at the same time. The smartphone development unit in Oulu had been working on a new product and for some reason the program had been delayed by nine months. At the same time the Copenhagen development team was working on a similar product and their progress was faster. One of the programs had to be killed and usually the axe hit the program that was late. A printed circuit board designer in Oulu recalls how all their product programs were killed over a period of five years. One can imagine how this impacts job motivation.Program slowdowns and cancellations eventually led to significant loss of job motivation, says Merihaara. The attitude among employees shifted towards feeling that there’s no reason to work overtime any longer or stress about one’s work. You only did what you were told to because the program would most likely be killed soon anyway .It felt like the big bosses in Espoo were very far away. The company headquarters could have easily resided in the US. Both Kallasvuo and Ollila had been distant figures so Elop’s nomination had not sounded very different. It did not really matter what language the CEO was speaking.But actually Elop was different. He hopped onto the plane to Oulu fast, came to the R&D unit and took a phone from a test engineer’s hands: Can I test this out too? It sounded like Elop was listening to improvement suggestions and was actually acting on the topics people were complaining about. Merihaara is grateful for Elop for this. “Nokia had been managed by a strong top-down culture. Even if we saw how things were on the shop floor, there were no channels to report these to the upper management. Elop was trying to change this. He had built a team with 10–20 plain engineers. They were telling him what is going on in the organization.”Almost every Nokia person interviewed for this book said they were impressed by Elop’s abovementioned style to respond to emails personally and quickly. Many said that this felt especially good because Kallasvuo and Ollila had withdrawn into their ivory tower. It sounded like the middle management had felt to be the most detached, so when the new CEO arrived, cheered people up and remembered their names, they were thrilled. Nokia people enjoyed the style of the new CEO, Elop kept the communication channels open and was almost jovial.Merihaara received 15,000 emails from Nokia people during his stint as the shop steward during the 2009–2013 period. He has done a statistical analysis of the emails and the single topic that came up most frequently was the continuous statutory negotiations to reduce headcount. During these four years, the Oulu R&D unit went through statutory negotiations 30 times. “When the Symbian headcount reductions really took off, we had five parallel personnel negotiations ongoing. I guess that was not legal either.”Product development was stopped by the continuous statutory negotiations. The former shop steward says that from the day the negotiations were announced, projects languished for at least two months. First, people waited for two weeks for the official negotiations to start, then the negotiation period took six weeks, and then it took two weeks to plan how to execute the layoffs. Product development was idling the whole time. Because of the continuous personnel negotiations there was perhaps six months of productive time in a year. “It felt silly to announce statutory negotiations just in case. The understanding was that we had to initiate negotiations even when there was an organizational change that would not have required statutory negotiations.”Nokia people in Oulu learned to recognize signs of impending statutory negotiations: External recruitment was stopped, future plans became even hazier, project progress information sessions were cancelled, travel restrictions were imposed and project schedules were frozen.People came to the workplace but did only the bare minimum. They turned a knob or two or built some piece of code so that the next month’s salary was guaranteed. People were no longer submitting internal improvement ideas or filing patent applications. Managers were spending their days finding ways to motivate their team members: Let’s try to finish this task because it’s the only thing giving us hope. Merihaara stresses that it was not about reduced work morale: “We did want to improve the situation and we were looking into ways how this could be done. Some people spent five hours a day into this. We sent probably 10,000 emails to the upper management. Maybe a hundred of those had some impact.”When projects were lingering, people enjoyed long lunch hours and coffee breaks. People at their desks were surfing job openings or checking their unemployment benefits. People were busy calculating the impact of losing one’s job on the family’s financial situation. Many were in the middle of building new homes and had children going to daycare.Sometimes cost savings and the quest for operational efficiency went overboard. The former head of HR Finland Ville Valtonen thinks that efficiency seeking often underestimated the amount of work required by the operational changes. The intent was good but the end result of operational changes often included increased amount of work and thus also higher cost. Some layoffs were also poorly planned. Panic started when Elop announced that R&D costs need to be cut by one billion euros ($1.3 billion). Teams were terminated, functions were killed. Experts were laid off and later the remaining organization realized that the company was totally lacking this expertise. People who had been laid off were hired back after a couple of months. Some went through the firing and hiring cycle multiple times. Cuts were done based on numbers and not considering what kind of competences the new Nokia will need. When tough choices had to be made between employees, personal preferences or organizational nepotism also came into play. Personal relationships mattered and the closest friends were able to keep their jobs.During the earlier growth years, the Nokia culture had been based on teams with strong leaders but a lot of freedom for employees to execute. The team leader had often been from Finland and the team members knew each other over a long period of time. The key progress indicators were clear: More sales, more new products.After the Windows strategy announcement, the management’s attitude towards remote work became more stringent. Individuals and small teams started to be pooled together in the same physical locations. Big teams in two locations were merged to an even larger team in one location. Some people resigned from the company due to these changes since they did not want to move to Oulu or Ulm in Germany. The Beijing office had severe problems with recruiting because few people wanted to move there.Middle management was one of the first to notice the working climate getting tougher. There were large numbers of Symbian developers because each new Symbian phone required a new tailored variant of the Symbian software, as described earlier. The traditional divide between hardware and software developers got worse when the budgets became tighter. Symbian developers in Oulu were envious of the MeeGo people being able to choose 12 euro ($17) microphone components for their smartphones while the Symbian team was asked to change their 0.40 euro ($0.56) microphone to another component that was five cents cheaper. Bitterness was spreading in the organization.One middle manager described how mediocrity was spreading among management. Managers were managing upwards: “They were able to look good in the eyes of their own managers but they were not able to manage their own organizations efficiently. The company would have been able to take a totally different course with more competent managers.”One middle manager from the services business recalls how conflicted it felt in the organization to go through the layoff phase. You saw how a colleague had to go and you were satisfied and happy that you got to keep your own job. People felt they were working on the right things until the guillotine fell on themselves. One person working in the strategy unit described the Nokia of 2010 as North Korea. Propaganda was plentiful. According to an internal joke, the most accurate HR news one could read was from the Helsinki and Salo area newspapers Helsingin Sanomat and Salon Seudun Sanomat, respectively.People had conflicting feelings also about Elop. A former director describes how people were astonished when the CEO was walking around in the building and asking people to cheer up — while the burning platform around him was spreading damage. A person who worked in the communications team said that people were also unhappy with Elop bringing in the new style of systematically replacing Finnish managers with Americans and Brits.Leslie Nakajima, who worked at Nokia during 2007–2012, describes the dramatic changes in the Nokia company culture. She had joined a company full of self-confidence and one that felt more like a family than an employer. The great layoffs destroyed the company spirit known for humanity, solidarity and optimism. Bitterness was the unanimous feeling when Elop fired Nokia employees, sliced the company and received a massive personal bonus at the end. Most of the shop-floor employees had not received any bonuses over the final Nokia years. Even more insulting was how some of the short-lived Nokia executives like Jerri DeVard received 7-digit bonuses.A senior executive from a telecom network provider describes the Nokia company culture developing into something that was exceptionally competitive. Everyone was competing against everyone else and backstabbing each other the best they could. Many noticed how twisted the culture was only after leaving to a “more normal organization” or to a startup company. The network executive thinks that excessive internal competition is no longer normal: If you cannot trust your colleagues any longer and everyone is busy driving only his or her own agenda, the prerequisites for effective execution are simply not there.Nokia’s employee retention rates used to be high and this had helped in building the Nokia spirit. However, as the industry was changing this soon became an issue. Mikko Merihaara thinks that Nokia made a big mistake when the company did not hire more new people when the business was still going strong. The same teams who had joined Nokia in the early 2000s being under 30 years old were in their positions ten years later. Merihaara’s statistics indicate that only about two percentages of the one thousand Nokia employees in Oulu left the company by their own initiative. This ratio is too low for a company in the need of a continuous renewal.“We should have hired 10 percent new people all the time. Renewal stopped completely and this just did not happen. The same people who had been designing phones in the early 2000s were still designing phones when the world had changed. The average age in our teams was closer to 40 but we should have had younger people designing phones for young people. We often wondered why we stopped hiring from the outside so that we could rotate the more senior employees in the company or — and a shop steward should not say this — out of the company.”At the same time new people were joining and old people were leaving Apple and Google — the world’s most talented workforce was available in Silicon Valley. In the Nokia Oulu offices, people did also have talent but they did not change. The management layers did not change either, and this was noted by the employees as well: “The same executives were simply rotated to relieve them from their earlier responsibilities or when a new initiative was kicked off. Usually a Nokia person was nominated from the organization. It was always the pieces from the same jigsaw puzzle. This is what our people complained about.”The managerial rotation did introduce its problems too. During five years Merihaara worked in seven different organizations. “After two of these changes I did not know who my manager was. Once I had the annual development discussion with a manager I had never seen before.”Many other things were noticed on the factory floor level. Employees were worried about the way Nokia was treating the most important customers of the company, the network providers. The worry stemmed from Nokia’s desire to reap maximal margins from every product, says Merihaara. Competitors like Samsung were seen to offer some of their products to network providers with a lower margin depending on the market situation. Merihaara says that this drive towards maximal cost efficiency was a recurring topic in corridor discussions in Oulu but the salary situation also created bad blood between teams and people. The MeeGo project came to an end in the summer of 2011 and when the best developers were leaving the company, some people were offered double salaries to make them stay and start developing Meltemi. People moved officially to the program that was paying a higher salary. Double salaries generally eroded work morale: “It looked like the ones who were offered a double salary were picked randomly. It was not so that the company was paying more for the work in programs that were estimated to bring in higher revenues. You should treat people equally when they are in the same jobs. Your motivation will crash when you are having lunch in the company cafeteria with someone who has the same job as you do but is making a double salary.”The Nokia bonus system had worked as planned during the good years. When the financial situation in the company changed, the system went haywire as well. People started to receive the best bonuses from programs that were canceled. The second best bonuses were paid in programs that got significantly delayed. Phones that were shipped in time and without defects did not necessarily bring anything for their engineers and designers. According to Merihaara, managers set targets incorrectly. Even when the times were tough, the goal was to make every program a successful one. Employees with top talent who were hunted for many programs could miss all potential bonuses because of the wrongly defined bonus system while a newcomer received a large bonus just by being lucky to work on some other project.One Symbian program that was delayed in Tampere once took away all bonuses from the Oulu team. Bonuses that were on their way were often cut because of the Nokia Funding Factor (NFF). NFF was a factor calculated from the company’s overall financial performance and despite hard work, good feedback, and great achievements on an individual or team level, a poor NFF could nullify the bonus payment. Incentive targets that were set wrongly led to sub-optimal results, recalls a former Nokia manager: “Many people had wrong incentive targets. They were looking only at their own pay slip, seeing how they can reach their maximum bonus. It was more beneficial for them to work on tasks that were not optimal for the company.”In one project, people had calculated how the total sales could improve with a new service that would support the device business. Then came the next reorganization and the project with the budget and profit and loss responsibility was moved to a new department but the new leaders could not add the service originally developed in the old unit to their own bottom line figures. The service was killed. Many other good products were terminated after they landed in the wrong units in the continuous reorganizations.What was perhaps the most ironic is that work became a lot easier after an organization had been told that it will be terminated. One MeeGo director recalls how this happened in MeeGo: “We were continuously losing people when they were fired one by one. Our work became easy and efficient after all the changes stopped. The situation was completely different compared to the time when the organization still had some future, and changes were announced every week. It is obvious you face delays when the plan needs to be changed. You need to move people from one task and get new people to another one. Besides, the new people don’t yet know their new jobs, and you need to train them for the new context.”When Elop joined Nokia, the old Nokia sites in the cities of Salo, Tampere, and Oulu had been drifting alone. They were mentally separate from the company headquarters in the city of Espoo. People working in Salo, Oulu, and in Tampere often had their team managers on some other site or even in some other country. The fromer HR chief Valtonen says that the physical location had gradually become less relevant: The reporting structures were no longer primarily local but global. The sense of belonging to a community had been lost. There were no longer common objectives nor local leaders to tell their troops the what and the why.“It was a clear goal when Pekka Ala-Pietilä told our team in the early 2000s that we plan to become the world leader. During the later years the direction setting and leading by example would have been specially important. Now that this was missing, the psychological contract you had made between yourself and the employee kind of vanished.”The feeling of disconnect had been amplified by the failed Nokia company values renewal. The old Nokia values ​​introduced in the 1990s — Customer satisfaction, Respect for the individual, Continuous learning and Achievement — were changed in 2007 to better reflect the modern times. According to Valtonen, the values ​​of the 1990s had had a great impact. People had implemented them through processes, leadership, and training. For example, respect for the individual truly had meant that team members were respected. The old values ​​had acted as a compass to the Nokia people.The new values ​​were Engaging you, Achieving together, and Passion for innovation. [17] Fine, lofty thoughts with a lot of wisdom baked in. According to Valtonen, the rationale behind the renewed values was good, but the compass was missing and the sea was getting stormier. Many of the interviewees say that the new values did not really speak to them. When problems arose, the motivation to take the resignation package on offer and leave the company grew.The mobile phone company has its muscles in sales and marketing, its brain in research and product development. New inventions are born when the researchers are appropriately motivated.The R&D model was radically changed in 2005. Tero Ojanperä had assumed the responsibility of the Nokia Research Center (NRC) and he had had a strong pressure to turn the downwards-spiraling innovation curve upwards again. At Nokia it had been understood that the mobile phones business cannot be forever profitable. The Research Center was given a mandate to show the rest of the Nokia organization where to go.In the former Nokia structure, the Research Center had been a long-term research unit that maintained daily communication with the mobile phone product development units. Cooperation was continuous and fruitful. Initiatives came from the R&D units that had better knowledge of consumers’ preferences and technical constraints than what the researchers possessed.After the change, the researchers were required to show the direction to Nokia. Disconnected from the rest of the organization, it was difficult if not impossible. Wordings like “technology transfer” and “bringing innovations into products” were introduced. A new multi-faceted organization was created to achieve the fine-sounding objectives. The Nokia Research Center was entrusted with the development of innovations. It was supported by a newly founded unit whose mission was to productize the potential inventions.A researcher who worked in the Research Center says that the change was a step into a worse direction. “It did not really ever start to work, I think. Innovations were identified but we kind of tried to push them forward with a rope. There was no traction on the business side. Also, the motivation at NRC deteriorated when people began to wonder how useful they are in the new setup.”The link to the shop floor — people designing and developing the phones — was cut off. The incentive scheme had been changed so that the goals no longer came from product development but from the Research Center management. The sense of inwardness increased. Layers of bureaucracy had increased by one.“We who worked in research did not have our own mandate or the ability to create products. We had to look for the people who can create products. Often we were even bounced back with the message that there’s no time to work on these kind of irrelevant ideas.”The researcher thinks that the Research Center restructuring was one possible reason why Nokia started to lose its potential.It was hardest for the one who remained as the last person in team. A senior salesperson who had joined the company really young recalls: “2010 was the year when things started to change. Many of those trusted people, who I ranked as the world leaders, left the company. Between 2000 and 2009 no one of such caliber had left and now ten people left in one go. Something must had changed. For me it was tough to be the last of the Mohicans.”“I really was a small-town boy and I believe carrying the Nokia badge gave me an extra inch or two. Suddenly we were bumping into obstacles for which there were no rational reasons. No travel authorization was granted when you should have gone to fix something. Metrics were changed on the fly.”Another long-term Nokia employee recalls how the motivation of many at Nokia fell decisively when it was discovered that the Lumia sales were not developing as anticipated. The return to the winning path had felt to be possible because the Lumias received some good ratings in technology blogs and product reviews. Now even the most stubborn believers who had been pushing hard until the end, in the Nokia style, were paralyzed. The culture of doing and executing had survived in Nokia for amazingly long but now people were standing at the edge.The last drop for the sales executive was the closing of Nokia’s nicely growing online business. He was also baffled because the sales support tool developed for retailers was never launched even though Elop was complaining in public how retailers are not selling the Lumia phones to consumers actively enough. “That was such a political game. But I do not think Elop had a role in this. On the contrary, he was asking for the right things. There were people between Elop and myself who forbade me from coming to a particular decision-making meeting. They blocked us from succeeding. This was very regrettable. That was the point when I finally lost my faith.”According to the sales executive, many recruitments from abroad failed big time: “Nokia was the pride of Finns but there was no similar driver for people joining from abroad. Leaders who were hired from the big European countries did not care so much of the interests of Nokia and Finland, while many of the top Finnish executives wanted in their hearts for Nokia and Finland to succeed.”As always, good performers got the most attention. According to a person in the middle layers of Nokia, the management style was often the “who shouts the loudest”. People with good argumentation skills could override others in internal meetings and get their agenda through even if there was no business rationale. The common denominator for the atmosphere problems was leadership. Many felt that the leadership culture had deteriorated since the early 2000s when they had joined the company. The person in the middle management recalls: “When I started at Nokia, I thought it was well-managed company. The Dream Team was still around although already starting to break down. I kind of joined Nokia because it was an organization with good leadership.”In 2010, the employees in Nokia MeeGo thought they have the best Nokia spirit. Despite the fact that the unit had grown to two thousand people, the atmosphere was like in a startup company. People in the Keilaniemi headquarters in Espoo were dressed up in business suits while in the MeeGo house in Helsinki Ruoholahti people wore sandals. Managers were fewer and top coders plentiful. This ratio was correct for work satisfaction and efficiency, at least in the minds of employees.According to a manager who had worked both in Symbian and in MeeGo, the topmost product developers were undoubtedly on the MeeGo side: “Other teams spent months or a year before they could implement changes to a phone. In Symbian, I could not even go and talk directly to programmers. When I went to speak with a MeeGo developer, the change was done the next day.”One MeeGo director says that elsewhere in Nokia teams and units were suffering from the boiling frog syndrome: The water is gradually getting hotter and hotter but you get used to it until it’s too late. “Sure, I too could have been able to hit the table with my fist and say that we will do this or I’m out. The situations were insane. In one event in 2010, we were told that we have 60–70 phone models. Someone of us could have said that this is totally absurd, we must end this. However, by that time there were two different truths: Money was pouring in from all directions even though everyone must have seen that one day this will be over. At that point we should have taken the blow and risk the money making machine. Elop was chosen to end this madness. Pause the game, make the necessary hard decisions.”Former HR chief Valtonen says that the feeling of responsibility and sincere desire to help were prevalent among the managers who had to lay off their team members. Middle management was in the toughest spot. Middle managers had to execute the layoff decisions made by the top management without having any chance to influence the big picture nor visibility into the future. Peer mentoring was arranged for mid-level leaders to relieve pressure and benchmark their experiences.The strongest joint efforts to assist in re-employing people who had been laid off were held in the Salo production unit. According to the ex HR chief, people who had decided to shut down the Salo production unit had a sincere desire to retain the function in Salo, until the end. He firmly shoots down any allegations that Salo’s fate had been sealed as soon as Elop started as the new CEO. According to Valtonen, the role and added value of the Salo factory was considered very carefully. The decision to refocus the Salo plant to a customization center for high-end products was made as a result of long consideration. Valtonen reminds that the world was changing fast at that time, and the old plans had to be scrapped in the summer of 2012.22. Why didn’t the Lumias fly?Stephen Elop had no idea the dramatic consequences that would follow his decision in February 2011. The Nokia board, who had blessed the Windows choice did not quite know that the lifeline of the phone business would become totally dependent on the sales of Lumia.Why didn’t Nokia succeed with Lumia? We gather the reasons for this in this chapter.The Valley of Death Drags OnA “valley of death” is a state a company gets into when business has stagnated and new business is not growing fast enough to compensate for the losses. No other mobile phone manufacturer had tried on a similar scale to move on the fly from a long-standing legacy operating system to a new one. This was a question of how deep and wide the valley of death is.Fortune had turned its back on Nokia, unfortunately. Windows Phone was chosen at a bad time because a new version was already under development. Windows Phone 7.5 was being replaced in a year or two with Windows 8. Shortly after customers had gotten their hands on brand-new Lumias, they learned that their devices would not get updated to the latest version. Especially when lots of new apps would appear only for Windows Phone 8.Potential buyers opted to wait for the new version, which slowed crucial market growth.Did Nokia know beforehand about this discontinuity? Our sources give conflicting information. According to one reliable source, already when they chose Windows Phone, Nokia and Elop knew that Windows Phone 7.5 and Windows Phone 8 are based on different technologies and it is not possible to update from the older version to the newer one. Another equally reliable source was of a different opinion. According to the source, this was unclear even to Microsoft.In part, this was what Microsoft wanted. Updating would have only been possible with a lot of programming work. Nokia’s value and hopes took a back seat when resources were allocated.This happened at exactly that vulnerable moment when Nokia should have been creating a credible ecosystem together with Microsoft. According to Nordea analyst Sami Sarkamies, the failure with the update cast a shadow over Nokia for a long time. The consumers started doubting Nokia’s ability to bring viable and long-lasting smartphones to the market.One big reason for the Lumias being late was a shortage of components.The component business was a merciless world, where old merits mean very little. When Nokia was the market leader, subcontractors had danced to Nokia’s drum with very little profit, so that they could get massive orders. Nokia had gotten phone parts and raw materials first, and for a good price.When Lumia first began, in the eyes of the subcontractors, Nokia had dropped to being a second-class customer.During Nokia’s peak years, big volumes had also attracted network providers. They were prepared to agree to worse terms in order to get Nokia models in their product portfolios. Because of Nokia’s position, it was able to operate with negative capital. Money was coming into the account faster than it was going out. The component manufacturers were paid within a three month payment period, while the network providers paid Nokia within a month. External money was unnecessary.In 2011, that position had been lost, and Nokia could only dream of being able to operate with negative capital. When there was a shortage of components, the Koreans and Americans got their deliveries first. Apple used the power of money: It bought its needed components half a year in advance hand with hard cash. Samsung was able to cut in line because it could guarantee huge volumes. Nokia’s position had also weakened because of its financial cutbacks and thus needed to demand very precise agreements with its subcontractors. Component manufacturers preferred to serve Samsung and Apple, from whom they could get money more easily and reliably. Qualcomm was especially a bottleneck for Lumia. There was a constant shortage of the company’s chipsets, and the demand caused by Lumia could not be met with satisfactory speed. Other smaller manufacturers, like HTC and Sony were in the same boat. They had to wait too long for components, and the train sped past in the smartphone market.The possibility of a valley of death was certainly on Elop’s mind when he chose Windows Phone. What strengthened the hope for success was that the best phone manufacturer and the best software company were teamed up in this effort.However, it is also speculated that Elop did not take the possibility of a death valley sufficiently seriously because Elop’s previous experience was in business-to-business settings. There is little need to worry about the consequences because customers are committed for years.Microsoft didn’t keep its promisesMicrosoft had failed so many times in the mobile market, that starting cooperation with Nokia was critically important. They painted a rosy picture of the features of Windows Phone and of the speed of development. While working on the agreement, they promised Nokia more than what they could realize.In the months after the signing, Nokia realized the truth. For example, business apps and adequate data security were missing from the first Lumias. Robin Lindahl, who was responsible for Nokia’s network provider relationships, wondered why Microsoft, while developing Windows Phone, concentrated more on competing with Apple and user experience, like increasing the number of apps, instead of going for customers where it would have been easy to get them — competing with Blackberry in the business world.None of our interviewees believed that Microsoft deliberately deceived Nokia by exaggerating the abilities of Windows Phone. It was a question of bad organization and under-resourcing. Nokia and Microsoft were large technology companies, where things were done in an overlapping manner or ever in parallel. Microsoft, with its huge resources, had developed X-Box, Windows, and Windows Phone in parallel. At Nokia, duplicated work was caused by the transition period. Many interviewees claimed that during the Windows cooperation, both companies did unusually large amounts of duplicate work and wasted resources. Too much of Nokia’s meager resources were used to patch up the gaps in Windows Phone, nor did Microsoft’s resources suffice for everything promised.The cheap Lumias didn’t arrive in timeAs noted earlier in this book, in the beginning, Nokia only had permission to install Windows Phone on the most expensive phone models. Microsoft wanted to ensure that Windows Phone competed in the same price category with iPhone and Samsung Galaxy. Nor would it even have been technically possible to run Windows Phone on cheap models. When Nokia brought the cheap Lumia 520 and Lumia 620 to market, it was too late. There were too few Lumias available and too late.Network providers compared the offering to Android, where with one operating system, there were devices ranging from under 100 euros ($135) up to 700 euros ($950). Nokia had a few expensive Lumia phones, which had a limited number of apps, as well as feature phones, where the apps, user interface, and design were from a whole different world than Lumia.Suspicion slowed cooperationElop was confused by the poor Symbian cooperation at internal events, and in one place, even swore at the teams by name.When Microsoft came to the sandbox, the game became much more complicated. Figuring out how much could be disclosed to the other company was a matter of daily uncertainty. Things which were obvious to Nokia employees were not obvious to Microsoft employees. For example, the week-view in the calendar, which Finnish users were accustomed to, was an unfamiliar concept in Microsoft. The calendar week-view was not available in the first Lumia phones.Cooperation was sometimes so difficult that the details were negotiated using lawyers.Distribution didn’t workFor many people, Windows means their work environment, and it was not appealing as a phone brand. When the natural attraction is missing, the importance of the distribution network is emphasized. A less attractive product can be sold if the network provider subsidizes the price.The largest American network providers AT&T, Verizon, and T-Mobile were of course partners with Nokia on paper. In practice, efforts to sell remained weak nonetheless. The floor-level salespeople in phone stores cared little about Lumia. They recommended iPhone and Samsung, from which they got nice fat commissions, and which were easy to present to the customers. A person working in Nokia sales boils it down to this: iPhone sold in 30 seconds, because the consumer wanted it. Android sold in 10 minutes, because the consumer had to choose a model. Lumia took 30 minutes, because the consumer had to be told what Windows Phone is, and their prejudices had to be overcome.During Elop’s time, the information systems using which the network providers and other dealers could be in real-time contact with Nokia were ramped down. Through this digital system, retailers would have been able to get quick answers to questions about Lumia, and material to support sales and marketing. According to the person who developed the system, Elop stopped the solution at the worst possible time. Nokia had been able to use that same idea successfully in the US since the year 2000.In China, China Mobile got Lumia into their portfolio, like in the United States, but only for show. Elop took all the publicity possible from the deal with the world’s largest network provider , but in actuality the deal produced only meager results. The phone’s presence in China Mobile’s huge product portfolio does not mean anything, if the network provider does not target money for marketing, subsidize the price, and instruct the sales staff to sell it.Nokia also did not have any direct sales channel where it could interact with buyers. The online store, which was built up with great diligence, was also stopped during Elop’s time. Elop was afraid that having Nokia’s own online store would upset the network providers. In a time where online shopping was becoming the main distribution channel, the decision seemed odd.In summary: During the sales campaign of Lumia, Nokia had lost direct contact with the consumers, nor was the value chain any longer in Nokia’s hands.Wrong things were done in R&DIn 2011, Nokia was one of the world’s top companies in terms of investment in research and development. Even if the profitability had taken a plunge, the R&D budget stayed at the same level as with the previous year. Nokia put a gigantic 6 billion euros ($8.1 billion) into R&D.Only Toyota, Samsung, Intel, Microsoft, General Motors, and pharmaceutical companies Novartis, Roche, Pfizer and Merck invested more money into R&D. A comparison by Forbes magazine reminds us that not one of these giant investors are known for their innovative products. Money was used on an astronomical scale without breakthrough inventions. Forbes also reminds us that none of these companies featured on the Fast Company magazine’s “50 most innovative for 2012” list, where all the most innovative companies are listed. The frontrunners on the Fast Company list, Apple, Facebook, Google and Amazon, used money with more care on R&D. They did not justify historical programs, like upkeeping long standing research programs. Rather, they invested on-the-fly in solutions that they believe will change the markets. According to Forbes, Nokia and other big investors in research experienced problems with the logic of diminishing returns: When more money is spent in a certain area, then more money is needed in that area to find anything new.Clayton Christensen refers to the same phenomenon in his book, The Innovator’s Dilemma. A company that was once successful in an area can easily become a prisoner in that area where profits were previously found. Competitors who are capable of new ways of thinking overtake them.Application developers failed to get inspiredThe media content and services play a decisive role in phones, when the smartphone customer changes their brand or recommends their device to others. For many people, using Facebook, Twitter, LinkedIn, Instagram and other well known apps is enough, but early adopters also require more specialized apps. By the end of 2011, the number of apps had become an intrinsic metric in the competition between ecosystems. Apple’s App Store and Android Market had half a million of them, and about 1 billion were downloaded from each of them each month.In the same period, the Windows Phone Marketplace had 50,000 apps. It is also a huge number, but nothing compared to the competitors. This was a chicken-egg phenomenon. App developers wanted to make apps for phones whose app markets had a lot of downloads. Buyers wanted phones which had lots of apps. In between were the network providers, who wanted to maximize data traffic.The most embarrassing deficiency of Lumia was the lack of Instagram. At the time of the announcement of Lumia in the fall of 2012, Instragram was the fastest growing social media in the US.Nokia desperately tried to get app developers excited. Windows Phone did not have enough appeal. Nokia even paid developers. Windows Phone still failed to attract. It was like trying to get blood from a stone.The marketing was off targetIn many people’s opinion, Nokia made a mistake by concentrating on selling Lumia in the United States, even if the largest and fastest growing smartphone markets were in China. For Elop, succeeding in the US was an obsession. It was difficult to understand why a little bit of money was invested in the United States, while at the same time a complete collapse was happening in China.The Lumia flagships came first to the US markets, where Nokia was almost unknown and its possibilities to differentiate were marginal. At the same time, Nokia’s most faithful customers in Europe and Asia had to continue waiting for their Lumias.The advanced camera was given so much glowing praise, even if it wasn’t a buying criteria for the customers. Maps had become an assumed part of smartphones. During the time of Chief Marketing Officer Jerri DeVard, Lumia was marketed as “the return of Nokia”. Starting from zero is perhaps not something you want to emphasize, when the consumers are wondering if Nokia can do anything right.Samsung used $14 billion a year for marketing. That is the GDP of a small country. According to leaders of large European network providers, Samsung’s market muscle was overwhelming. According to many assessments, Samsung also used questionable methods to strengthen its market position. There is a phenomenon known as spiff. It refers to money given to retailers. It is considered unethical, and for example, Nokia and Apple are not known to have used this method.It is estimated that a fifth of a Samsung smartphone’s sales price was spiff support. This is, nonetheless, a very well kept secret, so not much public information is found. In the smartphone sector, one talks also about “soft dollars”, which are in practice briberies given to phone retailers: T-shirts, coupons, discounts on devices. In developing markets, these kinds of benefits can have a decisive effect on the sellers, bloggers, and other opinion influencers. In India it is rumored that Samsung had bribed phone sellers to leave Nokia out of their product portfolio.Nokia did not use enough money for marketing, because there wasn’t any money. In Nokia’s internal events, CFO Timo Ihamuotila compared Nokia to Spain. Nokians were told that Nokia’s chances of getting a reasonably priced loan were weaker than Spain’s, where the national economy had collapsed into ruins. Nokia was able to scrape up enough money from its meager funds to total hundreds of millions of dollars in the complete marketing effort for Lumia. The investment was unreasonable, when considering how many Lumias were sold, which moved in quantities of millions. For example, the Lumia 900 phone sales in the United States were smaller than the money used in its marketing.Jerri DeVard was a total flop as the Chief Marketing Officer, also witnessed by many Nokians interviewed for this book. DeVard was hired because she had good relations to American network provider Verizon, and has an otherwise handsome CV, but in far away Finland, she did not operate on the same wavelength with her staff.The money ran outNokia had great difficulties changing the predominant thinking patterns of the company to a smaller scale.Nokia had ordered huge quantities of components and assembly services for Lumia 900 from the Taiwanese manufacturer Compal, because it was imagined that Lumia 900 would sell as well as the Lumia 800. Nonetheless, the new Lumia did not arouse interest in the same way, and there was a big problem ahead: A large amount of materials needed to be bought from Compal, even though the phones sold poorly. Compal was paid as agreed, and Nokia had to sell the Lumia 900 at a ridiculously low price, so that it could get rid of them. The consumers who had bought a Lumia 800 the previous year for 500 euros ($680) noticed that the Lumia 900 is now selling for 200 euros ($270).In terms of capital, Nokia was in the wrong ball game. Apple was a money making machine: It made two thirds profit on its mobile phone business. Samsung made a super high 20 percent profit, in other words one fifth from every phone sold stayed with the Korean company. Nokia’s smartphone gross margin was 20 percent at the time. The fixed costs are deducted from the gross margins. So less money came in than with the competitors, but the costs were the same.Even if Nokia finally, in 2012, unveiled the lower priced Lumia 520 and 620 models, the cash flow stayed the same. In 2013, when the cheap Lumia models were being shipped, the cash flow stayed more or less the same, at one billion euros ($1.35 billion). The launching of the cheap phones also did not help the phone business out of its profitability crisis. The cheap Lumias were so cheap, that they brought noticeably less money into Nokia’s coffers per phone than the expensive Lumias.Windows 8 floppedBusiness customers were considered to be Microsoft’s and Nokia’s territory, because the previous leader in that world, Blackberry, was in difficulties. However, the competitive edge in the business world was not realized. The problems were caused by how the widely used PC operating system in business, Windows 7, was in practice, not compatible with Windows Phone. The vision of the Windows chain between businesses’ devices remained, in the beginning years, a daydream of Elop and Steve Ballmer.The biggest disappointment was with the new Windows on the PC. Because Windows 8 was based on the “tiles” thinking, it was thought to make it easier for consumers and business users to switch to Windows Phone.Especially Risto Siilasmaa praised the combination. Windows Phone and Windows 8 would, according to him, make the PC and phone user experience the same.The new PC Windows uptake took away all hope. Windows 8 flopped. The predecessor, Windows 7, achieved 20 percent share of PC usage. A year after Windows 8 went public, it had a share of 10 percent. For Nokia, the end of the decisive year of 2013, the statistics showed some shocking facts. The Windows 8 market share had grown only 0.05 percent, when Windows 7 had grown 0.22 percent. So windows 8 lost market share to its four year old predecessor.Siilasmaa’s reasoning is easy to criticize. The tiles in Windows Phones and in the new PC Windows worked in different directions and the usage logic differs, for example in the direction of the swipes, among other things. Rather, Windows 7 and the Android desktop with its icons were closer to each other.The markets were the most difficult in the worldThe smartphone markets can be compared to a storm or a desert. This area of tough international competition is hard to describe. Investments in factories, people, components, and materials are huge and they would need to be predicted correctly according to trends and cycles. A testament to the difficulty of this market is shown by how almost all the PC manufacturers have tried but few have succeeded: Acer, Dell, Hewlett-Packard, Asus. All of them tried, but only memories remain, if even that. Many Android manufacturers have disappeared into the dark haze of history.The markets were so tight that network providers complained about the royalties that Nokia was paying to Microsoft. The leader of a large European network provider said that he complained to Microsoft many times about its royalty practices. In the leader’s opinion, Microsoft saw itself being like Apple, who because of its desirability charged a high price, when Google gave its software to manufacturers for free. The royalty-free Android phone made more money for the network provider than the license-encumbered Windows phone.The decisive blow to the Lumia phones was made, after all, by the consumers. They did not want Windows phones. All the tricks and dancing around did not help. A phone is bought with emotions.23. Tough choice for Mr. SiilasmaaOne day in September 2013, on the morning TV of the Finnish national broadcaster YLE, there was a news ticker at the upper edge of the screen. A freshly awoken financial reporter stopped in his tracks to watch. These kinds of moments only happen once during a person’s lifetime, like in September 1994, with the even more dramatic and sad news of the sinking of the passenger ferry, Estonia.Sadness was on the forefront also with this news. Sadness about two things. That the Finnish self-esteem which had arisen from winning the ice hockey world championship, together with the company’s mobile phone operations having come to the end of the road. And that the reporter who had been following Nokia’s phones for nearly 20 years had, in a moment, lost his professional identity. September 3, 2013 will always be remembered for the rest of his life.The worst was that it seems that light had begun to appear in the midst of this darkness. Between July and September, Windows Phone’s market share went past 10% in the most important countries of Europe, and in a year had doubled. In Italy, iPhone had been left behind. In Latin America, the same had happened before summer. The third ecosystem had emerged a few months earlier, when Blackberry was left behind. In the summer, new fast-selling models were launched, and the Lumia lineup was comprehensive. Why just now? Why so cheap?In hindsight, it is easy to notice certain facts which get blurred in all the wishful thinking. Worldwide, the market share of Windows Phone was still less than five percent. Nokia’s money was running out, and the losses continued.When the markets opened on that September morning and the share price jumped from three euros ($3.95) up to over four ($5.25), eyes were finally opened. Nokia had encountered a random stroke of luck. Or rather two.The turning point happened in the spring of 2012. Jorma Ollila resigned (finally, in the opinion of many), after holding the role of chairman for 13 years. Especially the big domestic shareholders had wanted Ollila out of the picture. Ollila should have originally left in 2010, but he was asked to continue because of the company’s difficulties. In spring 2011, he announced that it would be his last term.The considerations for identifying a replacement for Ollila’s role as chairman of the board, were intense. Because the CEO was from overseas, only domestic candidates were even considered for chairing the board, especially because there had been no pressure from foreign shareholders for this nomination. Possible candidates from within the board were Risto Siilasmaa, Jouko Karvinen from the paper and pulp company Stora Enso, and possibly Kari Stadigh from the Finnish bank Sampo.The chief editor of the Finnish business newspaper Kauppalehti, Hannu Leinonen, could tell already in September 2011 that the name would be Risto Siilasmaa. According to Leinonen, the speculation outside the company that the Nomination Committee would be making a genuine decision, was no longer valid. After the summer, according to Leinonen, it was clear that Ollila had chosen his crown prince. “Nokia would no longer be Ollila’s Nokia, if he left a matter of such magnitude for others to think about”, Leinonen formulated.Ollila’s connections to Siilasmaa were well known — their shared businesses included at least the online marketplace Fruugo. The nomination was anything but simple. Siilasmaa was by trade an entrepreneur and business angel. There were doubts whether he could fill such big boots of the governing body of a large corporation. His strengths were his technology background and experience in working on the board. He was with Nokia during big changes from the beginning, and understood their background.When the candidate for the chairman of the board was announced in January 2012, the reactions were mostly positive. Siilasmaa’s choice was interpreted as emphasizing the anchoring of Nokia in Finland. There were also critical voices. A well-known Nokia analyst from the investment bank Nomura, Richard Windsor, considered Siilasmaa as suitable for the role, but not necessarily the best. According to Windsor, Nokia’s problems were gigantic. They should increase sales, improve their results, take over new smartphone markets, fight for their position in developing countries, defend their relationship with Microsoft, and so on. Siilasmaa’s experience in these areas had gaps. His skills had not been tested yet in these playing fields.The test began in the spring of 2012 at the general meeting on May 3. Ollila, who was leaving the company, told the full Helsinki Messukeskus convention center crowd that this moment was special for him. He wished that he could leave behind a thriving company. Despite the difficulties, Ollila said that the board stood fully behind CEO Elop without wavering, and would continuously support him. When his speech had concluded, Ollila wished his successor luck and officially opened the general meeting.The successor went on the stage once. On behalf of the Audit Committee, he presented the auditor’s selection. The board chose the chairperson only at the organizational meeting after the general meeting.The new chairman quickly made it clear that his ways of working were different from his predecessor’s. The fourth floor office where Ollila worked was removed. Siilasmaa started working without his own office. He had a desk in an open office.“When people walked by, they could always exchange a few words. It is a brilliant way of doing work. No one ever comes into an office”, he told Suomen Kuvalehti.Risto Kalevi Siilasmaa was born in 1966 and spent his youth in Helsinki. In school, it is said that Siilasmaa did not stand out from the other students. Already before high school, he worked evenings at the Valintatalo department store in Tapiola in Espoo, and later at the Sesto grocery store in Lauttasaari in Helsinki. In December 1982, when Commodore 64 computers started selling in Finland, Siilasmaa and his friend Ismo Bergroth knew that they wanted one. The obstacle was the price. They saved their salaries for months and bought one device and took turns using it.In high school, Siilasmaa spent more and more time on the computer. He worked as a counselor in the Bittileiri computer camp, among other things. When others were cursing about syntax errors, Siilasmaa stayed calm. Many remembered his face. The dark, helpful guy, who was later successful.Despite his hobbies and his helping out at computer magazines, his studies remained on track. After high school, Siilasmaa got into Helsinki University of Technology to study industrial engineering and management. This major is nicknamed “vuorineuvos line”. [18] It is difficult to get into and the studies can be applied to a variety of different areas, unlike other areas of engineering. For example, areas of application include how to shorten the lines at theme parks, increasing the effectiveness in operating rooms, management of distribution of products, and production of cheaper and more reliable cars. Bars were not included in the young man’s study program. He preferred being in the Taekwondo studio.Still, his graduation didn’t happen till 2009. On May 16, 1988, as a 22 year old, he started a company together with his student friend Petri Allas. They named the company Data Fellows. The company started gradually changing focus from providing computer training to cybersecurity. Products included antivirus software and software that encrypts data traffic. The final lucky stroke was the coming of the internet, which led to an explosive growth in the demand for cybersecurity. In November 1996, Data Fellows won the EU IT Grand Prize, which finally brought the company into the forefront of Finnish news. The value of the award was 200,000 euros ($234,000). What was interesting was that the Nokia Communicator was in the same competition, with poor results.When Data Fellows was listed on the Helsinki stock exchange luckily before the internet bubble burst in 1999, Siilasmaa noticed suddenly that he was the second richest person in Finland. He was still the largest owner of the company that had since changed its name to F-Secure. He left his position as CEO in 2006, and his invitation to the board of Nokia came in 2008.Siilasmaa is described as analytical. He gets into things very deeply and questions the viewpoints of the leadership. He is called sharp, balanced, and a good listener. His character is considered by evaluators as pedantic. When Siilasmaa starts to do something, he keeps going till it is done. After a decision was made, he has supported the board with all his energy. He is also said to be patriotic in a good way. His agenda at Nokia has always been a bit broader than the narrow view of only Nokia’s benefit.His tendency to rather be an introvert than an extrovert is a counterbalance. His charisma could be called weak. Many will remind you that despite his background in F-Secure, he is not a real software specialist. He does not have hands-on programming experience.In his Keilaniemi fourth floor open office, Siilasmaa quickly started to question the strategy which he had started to develop. What could be done differently? What could we give up? Were there other possibilities with smartphones than Microsoft? The focused strategy work began at the latest in the previous summer, when Microsoft announced its Surface tablet. It stepped on the toes of the computer manufacturers who used Microsoft software. If this happened with computers, anything could be ahead with phones, the reasoning went. Especially now that Microsoft had brought forward hardware manufacture in their strategy.Nokia’s board started thinking about scenarios. What will happen when Microsoft does this? How will it affect us and Google? What if Microsoft really does that? Or if Google’s next move is this, how can we add possibilities for us to succeed? Should we sell NSN so we could buy more time for Lumia? Could we find companies to buy? Could we sell Navteq? What about patents — could we get money from them to alleviate the cash flow crisis? The work included the unbiased evaluation of the Nokia-Microsoft agreement, and the possibility of changing it with negotiations.Android was on the table continuously. It was installed in Nokia devices, and was proven to work acceptably. But would the coffers withstand the loss of the Microsoft support payment and the penalty payments of breaking the contract? And the employees, with starting over again with a new platform from scratch?The analysis, which lasted many months, has been described as being so thorough that afterward, nothing could come as a surprise.Materials for the extraordinary general meeting, where the decision to sell the phones business to Microsoft was taken, contain a detailed description of how the negotiations which led to the sale started and how they proceeded. The text has gone through a thick column of lawyers, so its truthfulness is credible. Nokia would hardly have taken a risk that the decision to sell would be seen to be based on mistakes in the materials for the meeting. Also, reporter Ina Fried had given such a detailed explanation on the website AllThingsD, that the source would have to have been Ballmer, Siilasmaa or both. Siilasmaa clearly wanted to downplay the allegations that the deal was done by Elop.In February 2013, Ballmer called Siilasmaa and expressed his concern with three words: “Can we talk?” Ballmer called, even though he knew that evening was turning to night in Finland. For him, it was morning. The time zones of Seattle and Finland cross in such a way that work is never done during the same normal work time. During the five minute call, the two agreed to meet soon at the Barcelona Mobile World Congress. The topic was set as “strategic partnership”.Now would be a good time to pause for a moment. Siilasmaa was the chairman of the board and Ballmer was the CEO. Why did Ballmer contact Siilasmaa and not the current chairman, Bill Gates? Was Nokia considered a second rate target?Siilasmaa has never commented about this mismatch. The normal practice would have been to approach the CEO, when feeling out the possible sales of a business unit and not the whole business. Siilasmaa probably understood from the beginning that Ballmer was wise enough to avoid hygiene problems. Had he directly approached his counterpart Elop, it would have seemed suspicious, due to their common background. On the other hand, the roles of chairmen of the board in the two companies were different. Siilasmaa worked full time. Gates had already been concentrating on his namesake foundation and on his family, and had withdrawn from practical leadership.Siilasmaa accepted the invitation without becoming offended because of the circumstances. Before the meeting, both of them reviewed what was working well in their partnership and what was not.The two of them talked for an hour in hotel Rey Juan Carlos, named after the Spanish king. The options were combed through without prejudice. The option of terminating the participation was accorded the same foreground status as the option of deepening it. Ballmer put the basic arrangement on the table, that Microsoft had to get more money from each phone that was sold. If Microsoft put $20 into marketing each Windows Phone device, then a return of $10 was too little. According to Ballmer, marketing money was used ineffectively for two brands — Lumia and Windows Phone. Software engineers were doing double work. In other areas, the cooperation was hitting bumps. The work would proceed better if the phone manufacture was transferred completely to Microsoft, Ballmer estimated.Microsoft really became concerned about Nokia’s financial situation. There was good reason to be afraid that Nokia would jump ship over to Android.Siilasmaa announced that Nokia did not have any intention of selling. Should it further be examined what could be done with the cooperation?This announcement put a train in motion that almost derailed many times, often went in strange directions, required a lot of luck, and eight months later, led to one of the most dramatic acquisitions in Finnish business history. The first step was evaluating the state of the cooperation, which Siilasmaa had demanded before agreeing to concrete discussions. Nokia used a lot of time, for example, to study how things looked from Microsoft’s perspective. The board also agreed that Siilasmaa would handle the negotiations. Elop would have been unsuitable for the job, due to his Microsoft background.The negotiations could very well lead to a corporate restructuring, according to a source from the board, therefore Nokia started scanning for options outside of Microsoft. For a few months, they considered whether to keep going in the same direction, or try to change the agreement with Microsoft. Should a new platform be adopted for smartphones? Is it better to sell the entire mobile phone business or only part? Should the location services business HERE be sold? Completely or partially?Already in the early stages of the evaluation, Nokia decided that it would be good to start negotiations with Microsoft. The first meeting was, as agreed, a month after the Barcelona Mobile World Congress, at the end of March, 2013. At the end of April, the parties met in the premises of Nokia’s legal office in New York. Nokia was represented by Siilasmaa and Elop, who as a member of the board and CEO, was present in the process, along with Louise Pentland and Timo Ihamuotila. On the other side of the table sat Ballmer, along with Terry Myerson, who had transferred to head of the Windows Phone unit, Chief Financial Officer Peter Klein and Chief Legal Officer Brad Smith.When the discussion got going, Microsoft opened the game. They started talking about purchase offers. When Nokia heard the proposals, they withdrew to prepare an answer.When they met back together, Siilasmaa made a ten minute speech, where he calmly and politely explained that the parties’ price evaluation of the phone business are from different planets. Ballmer answered that it was good to know where we were going. A new meeting was considered unnecessary.The initiative, which came to be known as Project Gold Medal — Microsoft used the name Edwin Moses, Nokia the name Paavo Nurmi — was made of tougher stuff than the gentlemen had imagined.On the following morning, Siilasmaa sent Ballmer a text message and suggested that the companies explore whether or not the analysis should be continued. Maybe the topic of money came up too soon. Perhaps Microsoft had lacked knowledge of the many parts of Nokia and did not understand their value. Perhaps the companies were closer to each other than they realized.A series of telephone negotiations followed. It led to a meeting at Microsoft’s legal office in London in May, 2013. AllThingsD describes the dramatic events on the evening of the 24th. The groups from Nokia and Microsoft were the only ones in the building. The parties were on different sides of the floor considering tactics, when the building shook with a huge roar. A roar so huge that it could have only come from the huge lungs of the Microsoft CEO. The Nokia team was startled, and guessed that Ballmer had reacted to their proposal differently than how they had hoped for. In the Microsoft room, they were wondering what was happening because Ballmer had left the room just a moment prior.After a moment, they heard a person running, which added to the restlessness. Gradually, it became clear that Ballmer had not noticed the glass coffee table and had tripped. He had hit his head and gotten a knot on his forehead. Myerson sent the Nokians a text message and explained what had happened. Even as he was being fixed up, Ballmer continued the negotiations with Siilasmaa and Elop. At the end of the evening the group went for dinner, where Ballmer arrived with his head wrapped in a bandage.The next morning, the coffee table had been moved to the middle of the floor’s lounge area beside the window, and by afternoon, it had been removed completely.Nokia’s map services Navteq, which had gotten the name HERE became a bone of contention in this discussion. Siilasmaa was unshakable. Here was mandatory for Nokia’s future. Ballmer was of the opinion that Microsoft could not succeed in the mobile world without control of the maps and navigation platform. On June 14, 2013 he flew to Finland, together with his Chief Legal Officer. This time, the flight went without any problems, and they met the Nokians at the Båtvik manor house in Kirkkonummi, which was owned by Nokia. The results were unimpressive. Nonetheless, they concluded that they wanted to continue negotiations.During that time, Siilasmaa had gotten a special reason to continue the negotiations. Nokia had, after a long time, gotten a stroke of luck. Two major developments were coming together just at the right time. Siilasmaa came to know that the German Siemens’ share of their shared networks company, NSN, was up for sale. They felt extremely lucky that their negotiations had remained outside the eye of publicity. Siilasmaa and the CFO, Timo Ihamuotila, started to develop some downright brilliant business.Siilasmaa notified Ballmer that Nokia had a list of prerequisites that have to be fulfilled before any serious negotiations about the sale of the phones could take place. The first one was a convertible bond. Nokia had to get it whether or not the sales of the phones business took place. The other involved the maps. HERE had to be taken off from the negotiation table. Nokia would keep it.The partner figure skating began. Nokia was promised a convertible bond of 1.5 billion euros ($2.2 billion). Nokia now had money to buy Siemens out, without Microsoft being able to use that as leverage in the sales of the phones business. On the other hand, Siemens thought that Nokia had empty coffers, and agreed to make a payment plan for part of the sales price. The analysts had appraised the value of NSN at 6–9 billion euros ($8–12 billion). Nokia got half at 1.7 billion euros ($2.2 billion). The price probably would have risen had Siemens known that Nokia was getting money from Microsoft. The NSN deal was announced on July 1, 2013.Nokia’s second stroke of luck occurred during 2009, when Nokia Siemens Networks had appointed Indian Rajeev Suri as CEO. Suri had trimmed the joint venture’s workforce with a heavy hand, and concentrated on mobile broadband, especially LTE technology. The company, which had been facing losses, recovered and so quickly turned profitable in 2012 that Nokia gave a positive profit warning. Because of Suri, Nokia was able to get its other support pillar in shape. The future could possibly be built on something other than phones.Suri was born in 1967. He has a bachelor’s in electronics and communications engineering from the Mangalore University.He came to work for Nokia in 1995 in Singapore. When Simon Beresford-Wylie transferred to head Nokia Networks in 2005, Suri moved to his place. Suri was later responsible for NSN’s services business activities and worked in India, Britain, West Africa, and Singapore.Suri followed Beresford-Wylie to lead NSN, and moved from New Delhi to Finland in the fall of 2009. Suri is married and has two children. He is living in Finland once again. His family previously followed him, but now the two boys are studying, and his wife lives mainly in Singapore, where one of the boys is studying.Suri’s goal has been to raise NSN to be the world’s second largest networks manufacturer after the Swedish Ericsson. The goal has remained just a dream. In 2013, Ericsson’s market share, according to the market research company Dell’Oro, was 36 percent. In second place was Chinese Huawei, which had 23 percent. NSN’s market share had shrunk to 17 percent. In the fourth place was Alcatel-Lucent at 13 percent. NSN’s profitability had nonetheless remained good. It concentrated on money-making projects at the cost of market share. NSN had, on average, 64,000 employees in 2012. Nokia had 48,000 in the other units.According to a source from the board, in summer 2013, they arrived at the conclusion that Nokia had good reason to sell off the phones business completely. It would get the accounts in shape and get rid of the uncertain future of the nest of losses and would be able to build a nice support pillar, thanks to the NSN deal. The board was continuously active. They had meetings at a furious pace in 2013. The meetings added up to 34, and including the committee meetings, 60.David J. Cord explains in his book that Nokia used the Huawei card during those times. According to Cord, Huawei and Lenovo expressed interest in buying Nokia, after Wall Street Journal had hinted that there were discussions underway. If Huawei was interested, it would have constituted a good weapon for Nokia, because Huawei might be interested in both phones and networks, because it was succeeding on both sides, and was targeting aggressive growth. When Nokia presented this possibility with Microsoft, the answer was blunt. Microsoft reminded them that they had 55 billion euros ($70 billion) as liquid cash. They had enough to buy any phone manufacturer, if things don’t work out with Nokia.The breakthrough occurred at the end of July in New York. The maps question was solved. The deciding factor was that since it is a question of software, the source code could be shared. The intellectual property rights would stay with Nokia, but Microsoft would get a special license which gave it equal rights with Nokia to modify the map services, and permission to do with the source code as it pleased. The road opened. After the meeting, Siilasmaa and Ballmer shook hands. A PowerPoint presentation with the main points had been collected together, which would be fleshed out.During the following weeks, the lawyers created contracts from the PowerPoint presentation, and the companies started a due diligence process, where they would check the points of the deal. The date for sealing the deal was agreed to be September 3, 2013. The business sale would become very demanding. From Microsoft’s perspective, it could only succeed if the company cultures were suitably close to each other. The decision to buy could be interpreted that, in Microsoft’s opinion, the cooperation had gone well.In a few interviews, Siilasmaa and Ihamuotila were congratulated that they succeeded in selling the feature phones in the same package with the smartphones. It is true that Microsoft expressed interest in buying only the smartphones, and that was what they were primarily interested in. The negotiators quite quickly concluded that the parts could not be separated. They had so much in common, among other things sales, logistics and management, that separating them would have given birth to two lame ducks.On the first Sunday of September, Ballmer finally flew to Finland, and on Monday the deal was finalized and the papers were signed. Risto Siilasmaa called Jorma Ollila and the Finnish prime minister Jyrki Katainen that evening to convey the news. Nokia employees got news the next morning via text message, and after a few moments, the morning TV was able to report their news.The press conference was called together at Dipoli in Espoo at 11:00. Siilasmaa took 9 minutes to tell how the decision was the most demanding and complicated in his life. He described it as rationally correct, but emotionally difficult. Nokia’s board had gone through all the options, and had come to the conclusion that the deal was in the best interest of Nokia shareholders.Next, Ballmer stepped up on stage. He concentrated on calming the Finns. Siilasmaa had obviously told him that how Microsoft is viewed in Finland is important for the continuation of the phones business. Microsoft would start a data center in Finland and invest at least 190 million euros ($250 million) in it. Microsoft promised to be a good corporate citizen in Finland, and assured that the development of mobile phones would continue in Finland. According to Ballmer, the deal was a win-win. The time it took for him to say all this was five minutes.After the details from Siilasmaa and Ihamuotila had been handled, Siilasmaa invited Elop up on stage. Elop waited a short moment, and climbed the few steps onto the stage with familiarity, and started.His expression was serious. One could see in his face that he was troubled. Instead of waving around, his hands stayed at his sides or clasped in front of his stomach. The presentation was colorless and subdued, even if the content was supposed to appeal to people’s emotions. According to Elop, Nokians had every reason to be proud of their work and achievements. He told them that he was proud of them, even if he was frustrated at being left behind in market share by the competitors. He said he was also sad, because the word Nokia in its former form had meant so much to so many people.Finally, Elop addressed the people of Finland. The pride felt by Finns toward Nokia has been an inspiration to him and a source of strength. He thanked the people for the support he had. Now a new chapter was beginning in Nokia’s life, according to him. Some things would still not change. “As you see the bright yellow Lumias in peoples’ hands overseas, continue to be proud. The phone is still made by your friend, colleague, or even family member. We will stay in Finland to win.”Ballmer didn’t want to stay and answer the questions from reporters. Ihamuotila, Siilasmaa, and Elop stood up in a row. Elop stood next to Siilasmaa like an ice hockey coach that had just lost his game. It seems his hands had difficulties finding a place. One’s attention shifted to the sleeves on his suit, which were too long. He was a defeated man.The winners stood next to him. Siilasmaa’s radical decision garnered thanks where it was important. The share price had risen over 40 percent, even if many thought the sales price was too cheap. Nokia had become a real company. One whose cash flow was transparent, which was predictable, and whose future looked bright and with whom one could expect new business.One representative from a large shareholder was satisfied with the result. “We had high expectations for Siilasmaa, and they were fulfilled. His board met unbelievably often. The alpha and omega of everything was that Siilasmaa could use his time for Nokia. From our point of view, Siilasmaa is the hero of this story.”The Nokia–Microsoft agreement was multifaceted and complicated. The most relevant parts and the changes that were announced in Nokia were the following:Microsoft pays the sum of 5.44 billion euros ($7.2 billion) in cash for the sale. From the sum, 3.79 billion euros ($5 billion) are for the mobile phones division (Devices and Services). 1.55 billion ($2 billion) are for the licensing of patents for 10 years. On top comes 100 million euros ($130 million) for the exclusive right to modify the license agreements to be continued. Money for the patents therefore comes continuously when ten years are up.The Nokia brand remains under the control of Nokia. Microsoft receives a license to use it in its feature phones for 10 years.Nokia can use the Nokia brand on its mobile devices at earliest on December 31, 2015.The Lumia brand is transferred to Microsoft.Microsoft becomes a strategic license holder of Here. It will pay separately for the license to Nokia.Around 32,000 employees are transferred to Microsoft, out of which 4,700 are in Finland.Elop will step down immediately from his responsibilities as CEO and will transfer to Microsoft when the deal is confirmed. Until then, he is responsible for the Nokia mobile phones division.Siilasmaa will become temporary head of Nokia, and Ihamuotila will become temporary CEO.Elop will be paid the amount specified in his CEO contract in case of the sale of the business.The purchasing cash flow was about 15 billion euros ($19.8 billion) in 2012, which was nearly half of Nokia’s cash flow.The deal is finalized by the end of March, 2014.On September 3, in Keilaniemi and Redmond, the difficult job of finalizing the deal was begun. This period was one of the most difficult in the history of business. What made it especially difficult was that a part of a large company was being split off, instead of selling an independent unit. Information systems, business services, bookkeeping, and so on — everything had to be rethought. The new Nokia needed a new strategy, organizational structure, leadership, and capital structure. The unit had to still be able to start its activities immediately when the deal was finalized. The part that was being split off needed to sit well in the buying organization. And everything needed to be reversible in case the deal fell through.The biggest stumbling block was seen in the approvals of the antitrust officials. In the US and the EU, they came in the beginning of December 2013, but in Asia, especially in China the decision was drawn out. The local manufacturers, Google and Samsung told the officials that they were concerned about their patent licenses. They feared that Nokia would start to be a patent troll.Patent disputes had been everyday life for years in the mobile device world. What has kept the prices down is that many patents are based in reciprocity, and the owners of the patents are themselves mobile phone manufacturers. When Nokia was giving up its mobile phone manufacturing, it was reasoned that it might raise the prices of its patents and terms. In South Korea, the organization of electronics manufacturers made a similar complaint to the antitrust officials. Even the European Union warned that it would be following the patent licensing practices when it accepted the Nokia sale.In India, tax disputes chafed at the deal. The dispute was about license fees that needed to be paid based on software development. Nokia had paid taxes to Finland, so the worst that was ahead was double taxation of billions of euros. The Indian officials told that they would block the transfer of the Chennai factory to Microsoft until the mess was sorted out. The dispute took on absurd characteristics, when the Tamil Nadu state tax officials claimed that during 2009–2011 Nokia had sold 275 million mobile phones without paying value added tax on them. According to acting CEO [19] Ihamuotila, the phones had been exported overseas. “If the phones had been sold in Tamil Nadu, every resident of of the state would have bought four phones in three years. We certainly don’t have a 100 percent market share there”, Ihamuotila said publicly.While waiting for the final signature for the deal, the circle was closed with the three year old events in a surprising way. Nokia unveiled three phones which were based on Android. It was as if Nokia acknowledged its mistakes, even though the phones were based on free Android, and remained outside the Google ecosystem.“Hell freezes over, Nokia unveils an Android Phone”, rattled the web publication Mashable. Freely translated, cows had begun to fly. It was asked whether Nokia was pushing Microsoft with the Android project? Or was Android plan B if the Lumias failed?Or was it after all what Nokia said it was, a gateway from Asha to Lumia?The best answer is probably a combination of everything. The phones certainly caused Microsoft a scare: If the phones known as Nokia X would start to fly, Nokia might pull out of the Windows Phone deal, despite the penalties. The X series was, in that way, used as a price leverage for selling the phones. Microsoft had to buy Nokia’s phones if they wanted to get a decent life expectancy for Windows Phone at all. This idea is supported by how the head of Windows Phone, Joe Belfiore, made some sour statements about Nokia’s move. The X series might have also been a way to force Microsoft to buy the feature phone business along with the smartphones in the same package.The gate theory also has credibility. Nokia had built their own operating system for the X series, which combined the Windows Phone tiles together with features from the Ashas. The most important applications were Microsoft Skype, Outlook, OneDrive, as well as Nokia’s Music, Maps, and Drive. Apps could also be downloaded from app stores that were independent of Google. The bundle led Asha users into the world of Lumia and Nokia, as well as Microsoft services.Why was the X series unveiled, even if the phones business had already been sold to Microsoft?Nokia needed to be prepared for the possibility that the deal might fall through. The companies needed to act as if the deal was not even going to happen until the time the antitrust officials had approved it.On March 24, 2014, Nokia announced that the deal with Microsoft would be moved to April. They had not received all the approvals from the antitrust officials. Those with weak nerves began to have doubts: Was the deal in danger of falling through? The penalty for the deal was 700 million euros ($922 million). It is a big sum, but it would have still left a big hole in Nokia’s finances with the loans included in the deal.It was finally ready on Friday, April 25. The price of the deal rose slightly from the original. The Chennai factory remained outside the deal. According to Nokia, this had no real effect on the terms of the transaction, and Nokia would receive compensation from Microsoft for responsibilities that were not transferred. Furthermore, Nokia notified that it would be closing its factory in South Korea, therefore it would not be transferred to Microsoft.When the deal was delayed, it looked month by month better for Nokia. In 2013 during the last quarter, Samsung’s and Apple’s profitability decreased. HTC and LG were fighting for their lives. Lumias were selling poorly, and in January of 2014 they had clearly plummeted. The phone markets were heading for bloodier competition, where Nokia had no reserves to work with.Friday, April 25, 2014 was still a sad day. Jorma Ollila had hired, with great expectations, a foreign CEO in 2010 to lead Nokia. A CEO with a software background. A CEO, who combined Finnishness and Americanness. A CEO who was supposed to save Ollila’s life’s work. A CEO who was supposed to lift the crown jewel of Finland to a new level of prosperity.Three and a half years. Only three and a half years and that operation — which we call Operation Elop on the cover of the book — had ended in a perfect belly flop.[18] ‘Vuorineuvos’ is an honorary title granted to leading lights in Finnish industry by the President of Finland.24. The bonus brouhaha boils overRisto Siilasmaa has said he just knew that Stephen Elop’s payoff would cause a brouhaha. A clause in the Microsoft-Nokia agreement requires Elop to have his stock compensation vested in an accelerated manner along with a 4.2 million euros ($5.5 million) cash payment including salary, severance pay and bonuses adding up to a total of 18.8 million euros ($24.7 million). Siilasmaa says he thought he could soften the blow and do a favour for Nokia shareholders by asking Microsoft to pay part of the payoff. He turned to Ballmer and again implored him to consider how important the perception of Microsoft in Finland would be for the future of Microsoft mobile devices in Finland.It was a bad idea. The value of Elop’s stock awards — totalling in 14.6 million euros ($19.2 million) at the current rates at the time — went public from a filing of the extraordinary meeting voting on the sale of the Nokia mobile device business. The media went crazy. The payment was considered outrageous since the general impression was that Elop had brought Nokia to ruins. And when the buyer, Elop’s former and future employer, was to foot part of the payout bill, many considered it to be an additional reward for selling Nokia to Microsoft.For instance, the Wired magazine had a bold headline: “Microsoft Brings its Trojan Horse Home”.On top of everything, Siilasmaa blundered in his communication. According to him, the reward was based on Elop’s CEO contract, which essentially was the same as Kallasvuo’s. But Finland’s largest newspaper Helsingin Sanomat dug out the truth. Kallasvuo’s contract did not have the controversial clause entitling him to an immediate share price performance bonus in case of “change of control”.The controversy reached political proportions. The then finance minister Jutta Urpilainen considered the judgement of the payoff justified. She called for consideration of new rules that would allow shareholders to decide on executive compensations at the general meeting. She also insisted upon introducing a clause in collective labor agreement requiring “responsibility and moderation in all reward practices”. Even the American Forbes magazine quoted the then prime minister Jyrki Katainen who called the reward “quite outrageous”.On top of that, Elop refused to take a cut in the payoff for marital reasons. Elop had separated from his wife Nancy in October 2012 and filed for a divorce in August 2013 after 26 years of marriage. The family had never moved to Finland but stayed in Seattle, and Elop had decided to sell the family residence.Since there was no prenuptial agreement, Nancy would be entitled to half of Steven’s assets. Allegedly, Elop had justified his refusal to Siilasmaa by claiming that Nancy’s divorce lawyers would accuse him of destroying Nancy’s assets.The imagination of speculants was flying sky high. One thought that Elop was hired to prepare Nokia’s mobile phone unit to a sellable condition. Another speculates that the accelerated stock grant “change of control” clause was included in Elop’s contract on purpose, because Elop and Nokia both knew at the time that Nokia’s mobile phone unit would soon be taken over by Microsoft.The media, as well as the speculants, forgot about one crucial fact. If you want an American executive, you must compensate them on American terms.The median compensation for CEOs of US publicly traded companies on Standard & Poor 500 index (S&P500) in 2012 was about $10 million per year. In 2012, Elop’s tech peers like Honeywell’s David Cote received $56 million, Qualcomm’s Paul Jacob $36 million and AT&T’s Randall Stephenson $26 million.The type of stock award previously commonly used in Nokia, viz. stock options, are not so common in the US anymore. Their bad reputation and taxation practices have made corporations change the way they incentivize executives: Restricted stock units. It is not uncommon for CEOs nowadays, also within Nokia, to be awarded performance-related stock. The CEO is incentivized to stay on. Should he/she resign during the three-year performance period, he/she would lose the stock reward.Restricted stock units have a major advantage. They align the interest of CEOs to those of the shareholders. A big salary comes out of the shareholders’ pockets. In the US, there is also a corporate tax law limiting the employee salary deduction to $1 million per employee. For example, Qualcomm’s Jacobs had an annual base salary of 1.2 million euros ($1.65 million). The rest of his 36 million euros ($49.4 million) payment was awarded in stock.However, restricted stock units raise three questions:What to do in the event of change of control within the performance period and the company’s direction changes?What if the responsibilities of a CEO diminish so much so that he/she can no longer influence actions to meet the performance criteria?What if the CEO is fired?Wouldn’t it be unfair for the CEO to lose his/her stock grant in cases like this? He/she might just be on the verge of reaching the set targets. Wouldn’t it be right to take protective measures against situations like these?Americans seem to think so. This is in the core of the reward controversy. In the US, almost without exception, CEOs have stock “change of control” clauses in their contracts requiring their stock compensation to be vested in an accelerated manner should they resign following a change of control. The number of stock units they receive is calculated as if their targets were met. It is impossible to hire a top American executive to Europe without this clause even though it is uncommon in Europe. This is the reason why Elop’s contract differed from Kallasvuo’s. In the event of “change of control”, Elop stood to have all unvested stock vested in an accelerated manner should he resign. The takeover triggered the accelerated bonus.It would have been in Elop’s interest to resign following the takeover. Nokia, however, wanted him to stay through the transition. Hence, the amendment to the employment contract. Elop was granted the same compensation if he was to stay on. He did, and made a smooth transfer back to Microsoft.Elop’s CEO employment contract was reasonable by American standards. He himself commented on it at the time of the appointment: “The Nokia Board of Directors, particularly the chairman, are fully aware of contractual matters”.Some commentators were of the opinion that the CEO contract created a strong incentive for Elop to take substantial risks with Nokia. One of the many conclusions reported that Elop stood to gain a huge reward ifthe share price drops deeply as the company is driven to a cash flow crisisNokia sells the mobile phone business under pressure to raise cashshare price rebounds sharply on a takeover bid but still remains far below where it was when Elop joined the companyAnd that is exactly what happened between 2011–2013, much to the amazement of the speculants.Restricted stock grant may well encourage a CEO to sell the business. BlackBerry, formerly RIM, CEO Thorsten Heins was perceived to have had this specifically in mind. He stood to make 40 million euros ($54 million) if the company was sold and he is ousted. Should he be forced to step down without the company changing hands — which he was — the compensation would only be half of that. Heins and the BlackBerry board were criticized the same way Nokia was: What is the point of rewarding the CEO for failing to turn the company around?This was of no concern to BlackBerry. The new CEO John Chen was awarded restricted stock units valued at 61 million euros ($82 million) on top of his annual salary.Historical data shows that even this sum is small. The Finnish business magazine Tekniikka & Talous reviewed earlier cases. General Electric’s CEO Jack Welch is at the top. He collected a severance payment of 300 million euros ($417 million) when he was fired in 2001. Viacom’s CEO Tom Freston was sacked in 2006 after holding office only for 9 months receiving a severance package for nearly $100 million. Heinz’s William Johnson stands to gain over $200 million in the event of change of control. If any consolation to the Finnish people, Sanjay Jha received a severance pay of 50 million euros ($63.7 million) when he resigned as Motorola’s mobile phone business was taken over by Google in 2011.In Elop’s case, we are now approaching the main point. Did the “change of control” clause set an incentive for him to sell a Nokia business unit to Microsoft?Yes. It may well have done.Nokia shares had plummeted, economy in ruins. Elop knew he was unlikely to meet the performance criteria, which meant that he would be left without stock grants. By triggering the change of control, he would stand to gain a substantial amount without meeting the set performance criteria. Furthermore, the takeover might increase the value of stock options, which were also part of the compensation package.It would still be unreasonable to claim that Nokia Board of Directors were fooled. An expert on executive compensation packages said that corporate executives are like top athletes. They are genuinely passionate about the task in front of them and will only agree to take it on if they feel excited about it. The change of control clause is a minor detail in the employment agreement, more like a safety valve. It would be highly suspicious if a CEO were to prioritize that issue high among other requirements.Now for the grande finale: Elop had no reason to deliberately damage Nokia. Not even with the change of control clause. He would have gained a lot more by achieving the set goals. Every rational being would have set out to reach for the rewards through Nokia’s success.The truth is that a company on the verge of a bankruptcy is of no interest to anyone. What if Microsoft had not been keen to purchase the Nokia mobile device business? Elop would have been left with nothing. Above all, Elop did not sell the device business to Microsoft. That was done by Risto Siilasmaa and the Nokia Board of Directors.Elop’s huge compensation package was a follow-up of American corporate culture and the common practice of overwhelming compensation rewards. The business school giant has it all figured out regarding risks in the use of restricted stock awards and the change of control clauses. Despite that, these kinds of compensation packages are widely used.Then, a human argumentation as the icing of the cake. Elop chose Nokia although he stood to gain more had he stayed in the US. More than monetary benefits, he was driven by ambition.Naturally, Elop was paid a salary, too. His employment contract outlined the following:1.05 million euros ($1.47 million) annual salary.2.3 million euros ($3.22 million) as compensation for loss of income at Microsoft.Allegedly, Elop earned 9.7 million euros ($13.3 million) in 2013. Along with the stock gain, the severance pay rose to 24.2 million euros ($33.12 million) which indicates Nokia having compensated Elop 52.8 million euros ($72.3 million) in total.Wrong. The media in Finland got the figures wrong from the start. The biggest mistake was to interpret the 24.2 million euro reward to be an additional severance pay. It was not. Those appalled were not aware of the actual details behind the figures. The reported yearly payments included theoretical calculations on his equity awards. Nobody actually knew what their value would be in a year’s time. International Financial Reporting Standards require the stock grant units to be reported in closing share price calculated as if the targets were met.By going through four years of Annual Reports, it turns out that by the time of the change in his position, he had not been granted any equity awards. Neither options nor restricted stock. Why not? Because the performance period was still ongoing. For that reason, the whole controversy surrounding the reward was partly unreasonable. Elop was not paid 24.2 million euros as additional severance pay as many commentators and politicians imagined. [20] He received the money that Nokia had already reported as having paid him.We have calculated Elop’s total earnings based on Annual Reports. The sum is 34.7 million euros ($47.5 million) including 18 months of base salary and management short term cash incentives 14.7 million euros ($20.1 million), equity awards, i.e. restricted stock grants 12.7 million euros ($17.4 million) and stock options 7.3 million euros ($10 million). Particularly profitable were 2013 options of which the closing price was 2.73 euros ($3.73). Elop received 4.6 million euros ($6.3 million) for those.Huge sums, but still smaller than what many of the politicians agitated by the controversy were criticising.We want to emphasize that we are not taking a moral stand on the size of the rewards. We feel it is more important to understand that without this, it would have been impossible for Jorma Ollila to hire an executive from North America, and that compared to North-American practices of reward, Elop’s compensation was reasonable.As Nokia CEO, Elop was expected to purchase Nokia shares himself, too, worth three year’s annual salary according to the board’s recommendations. Elop was fairly lazy in this, partly due to stock market regulations. He purchased his first shares in February 2011 with 1 million euros ($1.36 million), i.e. roughly his annual salary. At the same time he reported having sold his Microsoft shares that had come under criticism. A top up of Nokia shares worth 500,000 euros ($780,000) followed six months later, after which he had 425,000 shares in total. By the end of 2013, there was no change, meaning that the recommendation for 3 million euros was not reached even after the stock gain by the takeover.The board’s recommendation does not define any time frame for share purchases. The members are expected to increase the number of their shares with half of the reward profits but as mentioned before, Elop did not have any stock compensations vested until he stepped down.[20] Presumably a typo in the original text, €20.1 million changed to €24.2 million.25. The world’s worst CEO?Stephen Elop, in many respects, is one of the worst CEOs in the world, if not the worst. The fall of Nokia’s mobile phones was one of the most dramatic ever among the companies listed in Global Fortune 500.Let’s play a little number game. A day before Elop started, the market value of Nokia was 29.5 billion euros ($37.5 billion). When Nokia announced that it would sell its mobile phone activities to Microsoft, the value was only 11.1 billion euros ($14.7 billion). Elop’s term as CEO lasted 1,020 days. Every day Elop was at work — counting 7 day work weeks — ate away 18 million euros ($23.8 million) from the shareholders’ assets. This achievement is mind-boggling.Still, some defend him and blame the failure on Olli-Pekka Kallasvuo.This conclusion is shaky. When Elop started, the sales of Nokia smartphones grew. Elop’s task was to plug the leaks. When Elop accepted the task, he believed that things could be fixed. It was futile to try to explain afterward. Elop failed in his task all by himself.The “Burning Platform” speech has become a legend how a CEO can destroy almost everything with one stroke. Consultant and ex-Nokian Tomi Ahonen has created the fitting term: “Elop Effect”. Elop combined two different CEOs cardinal blunders: The Osborne and Ratner Effects.In 1983, the computer manufacturer Osborne announced several new models of computers, which they said would be launched in sales after one year. In the meanwhile, sales of the old models plummeted because the consumers were waiting for the new models. Osborne ended up in bankruptcy. Gerard Ratner, on the other hand, was the CEO of the jewelry company Ratners. He gave a speech in 1991, where he said that Ratners products were so cheap because they were “total crap”. The consumers believed him and stopped buying.Elop announced that Nokia is giving up on Symbian before any Windows Phone smartphone was ready (Osborne effect) and with his “burning platform” speech, expressed that Symbian and MeeGo were trash (Ratner effect).In parallel with the “burning platform” speech, another serious mistake was made with the binding Microsoft agreement. It was senseless to lose freedom in the most dynamic sectors of the business world. How would anyone know what the world would be like in 5 years? It is ironic, as we showed earlier, that Nokia was afraid of becoming Google’s slave through Android. A prisoner or a slave, it is the same, but as Google’s slave, one could always run away from their master at any time. With Microsoft, Nokia was cornered without any alternatives.Elop chose a daredevil one-path policy, even when the fast-moving internet era demands more. For example, Google does all sorts of experiments, sometimes even when logic defies them. By experimenting, the company confirms whether or not it has not overestimated its possibilities in its chosen path.Elop also made quite many little mistakes. Many foreign recruitments went bad. Foreigners do not have the same kind of commitment to Nokia as Finns do. In the organizational upheavals, entire teams were lost: The best left and were replaced by people with the wrong skills. US-centricity backfired in other parts of the world. Sales of the Lumia phones launched at the wrong times. The effect of the ultra-cheap Android prices was underestimated. The list is long.And as one of the interviewees reminisces, there were many PowerPoints and initiatives to save money during Elop’s time. It was very seldom taken into account what the consumers wanted and how to get the sales back to the old levels.And what does Elop himself think about his mistakes?The answer comes from Elop’s long time leadership coach, Stephen Miles. “The greatest thing about Stephen is that he never gets down or gets stuck in regret. Never. He is a machine.”Apparently, even when 20,000 people have lost their jobs as well.After over a hundred interviews, wee are completely convinced about one thing: The talk about any conspiracy behind Elop is without any basis. Elop was not a Trojan horse. Microsoft did not smuggle him into Nokia with a plan to later buy Nokia’s phone business for a low price. Our interviews gave no indication — none, whatsoever — that would have hinted that this could have been even possible.A Trojan horse is a war stratagem from ancient Greek mythology. In the Trojan war, Greek troops entered the enemy city of Troy by hiding inside a hollow wooden horse. Trojans dragged the wooden horse inside their city walls because they thought it was a gift. Inside the wooden horse, the Greek soldiers waited until the night and then took over the city.The conspiracy theory is absurd because this time the wooden horse was not a gift but was selected by Jorma Ollila and the Nokia board. It was not possible for Microsoft to influence this process.The conspiracy theorists then say: Elop turned into a mole when he was injected into Nokia by chance.It is difficult to understand why Microsoft would have wanted Nokia to fail. A successful Windows Phone ecosystem was of utmost importance to Microsoft — and it is good to keep in mind that in those days, device manufacturing was a side note in the Microsoft corporate strategy. And above all: why would anyone pay one billion a year for a company hoping it to fall? Risto Siilasmaa may be childlike in appearance, but based on our interviews he is a pedantic, prudent, and rigorous negotiator. He involved Elop in the negotiations with Microsoft. Siilasmaa has been very convinced that Elop worked only in the interests of Nokia.The pension fund Ilmarinen is one of the large shareholders of Nokia. Deputy CEO Timo Ritakallio of Ilmarinen thinks that the claim of the Trojan horse is totally absurd. Ritakallio says that the conspiracy theorists seriously underestimate the Nokia board of directors if they believe that the board had stood still when an outsider was preparing the phone business for sale. He says that no actions by the board or by Elop support this claim.The last nail in the Trojan horse theory was revealed in March 2014 when the news agency Bloomberg disclosed details of a June meeting with Ballmer and the Microsoft board. Several members of the board objected to Ballmer’s intention to buy Nokia’s phone business, according to Bloomberg. Even Ballmer’s longtime supporter Bill Gates was against him. The division hinged on whether Microsoft should expand to hardware manufacturing or remain as a software company. Ballmer’s yell was heard outside of the conference room, according to the news agency. He claimed that he could not act as CEO if the proposal was not accepted. Several members in Microsoft’s executive team also expressed their objection to the deal, including the future CEO Satya Nadella.Conclusion? In 2010, when Elop was “smuggled” into Nokia, Microsoft did not want to become a phone manufacturer. In 2013, when it was time to turn the Trojan horse plan into action, the desire was almost as weak. Where would the horse have been needed?When the result was what it was, it must be asked, was it all Elop’s fault? If a movie was made about Nokia’s phones, would Elop be the bad guy?The plot of a movie requires that there is good against evil. In real life, things are much more multicolored.The big strategic plans of a publicly listed company are made by the board. The highest responsibility fell on the chairman, Jorma Ollila. Nokia’s board was professionally run, but looking back, when choosing the CEO, it also outsourced the company’s strategy. To a large degree, it is a question of the ways of working and ethics of the board. In Google’s board, the decision making power rests on three people (founders Sergei Brin and Larry Page, and the CEO Eric Schmidt). They have concluded that there are other factors pressing in decision making than economic metrics. Apple has the same principle. You have to say “no” to Wall Street. This point is also made by Clayton Christensen, in his book The Innovator’s Dilemma: If we obey money, it does not solve the bigger problems. Nokia’s board had begun to optimize. There were shortcomings in the ways the operative leadership was monitored.Elop got an important role to act in favor of Windows Phone, while preparing his choice. The choice was made sincerely, and the decision was justified.Let’s listen for a moment to a CEO of one of the largest mobile network providers in the world. In his opinion, time has shown that Windows phone was the best choice of all the alternatives available in 2011 for Nokia’s owners. He raised a relevant question: If Nokia had gone with Android and was in the same boat as before the sale to Microsoft, who would have bought it then? An Android mass-producer that is in financial difficulties? No one. Because of the Microsoft decision, Nokia could get a price for its phones. And a new start as a networks company.He even defends the “burning platform” speech. It was surprising that Symbian lived as long as it did, he says. A year before Elop, according to him, everything at Nokia had begun to decline and that people were depressed. No one was really able to do their work, when no one knew what was happening. After the burning platform, Elop had to let people go, but then people were reinvigorated and the energy level was high. The network provider CEO remembers Nokia first as very strong, then very weak, and finally strong again, a manufacturer with its pride back, whose market share was increasing. The return of the pride was because of Elop, according to him.“Elop made one very big mistake. He let Microsoft pay some of the rewards when transferring to Microsoft”, the network provider CEO says.That mistake, as pointed out by the CEO, was not made by Elop, but by the chairman of the board, Siilasmaa. The decision was ethically intolerable and gave birth to unnecessary speculation, according to the network provider CEO.Another CEO of a mobile network provider says he talked to several of his colleagues about Elop’s future after the Microsoft deal. Almost all of them are of the same opinion: They would have wanted Elop to continue leading the phone division at Microsoft, and they were happy with Elop’s ways of handling cooperation.If we give the Trojan Horse theory a possibility, Elop was excellent in that role. As a Canadian, he gave a convincing impression that he really cared about Finland and Nokia. To top things off, he was modest and dressed like a Finn without false Bohemianism (Apple) or false relaxedness (Google). He fit in with Finns, even if he was a real talking-machine. Sugar-coated flattery and bravado remained in check. An employee of the communications department vividly remembers seeing him at the Heathrow airport in London. He looked fed-up and tired with a little backpack on his shoulder, waiting in security. It was hard to believe this was a CEO of a globally listed company.People who worked with Elop were invariably in agreement that he did his work like a machine. Many said that he was the hardest working person they had ever met. He flew over 60,000 miles a month on business, or more than twice around the world. He used a lot of energy to motivate people and to take care of business relationships, and raised the level of work to a new level. Nokia got its humility back, started making decisions, implemented them effectively, and started to think about the markets from a consumer’s perspective. Not just anyone could have made this happen. It required an exceptional leader, and an extraordinary work effort from him. Why be bothered if the goal was to destroy Nokia?Nokia’s phones were not killed off by a murderer from Canada. What killed them was the arrogance born in Nokia’s own country, concentrating on costs, unclear responsibilities, and bad decisions made by the company’s board.Elop’s role is summarized here: He failed in his attempt to save Nokia. He made gigantic mistakes — but in good faith. Inspired by his success with Macromedia Flash, he put all his eggs into one basket at great risk. He pushed ahead like a Finnish small business entrepreneur, into whose head was driven the teaching that you can only succeed if you believe in yourself. Sure, success requires belief, but many who have believed have ended up bankrupt. Belief does not guarantee success. It is a requirement of success. For Elop, everything was all or nothing. If there was even a small possibility of contributing to the success of Windows Phone, it was chosen, even if another option would have been more useful elsewhere. When Elop arrived, Nokia was arrogant and thought it knew how things were done, didn’t listen, and made decisions slowly. Because of Elop, everything had to be decided very quickly.Now let’s give a chance to speak to a former American executive of Microsoft. He tells us that he was shocked by Nokia’s choice of Windows Phone, even if he worked at Microsoft. The only explanation was that Elop subconsciously wanted to do a favor for his former employer and boss, Ballmer. Microsoft leaves an impression on people as an employer, according to the American leader, which is hard to get rid of. It becomes like part of the DNA.“Many former Microsoft employees go through the same phase”, he said.According to a large shareholder, someone from a small company should have been chosen to lead Nokia, rather than someone like Elop, who was a division leader of a large company. At Microsoft, Elop continuously had his boss nearby. The work of a CEO is a thoroughly lonely job. It is important to have networks, where you can throw ideas around and get new perspectives. If one’s network is former colleagues from Microsoft, the ideas get stuck.The final conclusion is simple. Elop was the wrong man for leading Nokia. Someone else would have been able to save Nokia.There was only one person who claimed to have known with certainty that Elop was the wrong choice. Hindu astrologer Shyamasundara Das states on his website that he has consulted in the selection of leaders of many large companies. The results are good, according to him. He says how he knew beforehand that Carly Fiorina was the wrong person to lead computer manufacturer Hewlett-Packard. The effectiveness of his method is proven by how the astrologist would have advised Nokia to hire someone other than Elop.Should Ollila have listened to him?26. What if…Speculating about the fate of Nokia phones has been a popular national pastime in Finland, as of late. [21]The news about Microsoft closing the Oulu mobile phone R&D unit has added fuel to the fire. The same fate is threatening the Tampere and Salo R&D units. Microsoft seems to be ramping down the phone activities it bought from Nokia, after Satya Nadella took the reins of CEO from Steve Ballmer. Stephen Elop was not able to get profitable growth in the phone business neither at Nokia, nor at Microsoft.Hindsight is the cheapest of all the forms of wisdom. One of the most pathetic forms is “what if” speculation. We still dare to think about what if Nokia had done differently, what should it have done, and what would have happened.Most of the people interviewed in this book believed that Elop would have ended up with a strategy like Samsung, that is having multiple software platforms. The Korean company sells, besides Android and Windows phones, phones made with its own software platform, Bada. Many of those interviewed were of the opinion, that if Nokia had announced a strategy of having multiple software platforms, and had gone with Symbian and MeeGo in parallel, Microsoft would have had to flex and give the Windows Phone bundle with more enticing terms.It remains a mystery to us, whether having multiple platforms was ever an option for Elop at any point. At first, he praised MeeGo. Was it manipulation done by a talented actor? Or did Elop change his mind as the situation got worse?Elop knows the answer best himself, but the hours of thought behind his reasoning seem to be clear. Splitting the poker chips would, in his mind, have weakened the possibilities of creating a real ecosystem out of MeeGo, likewise with Windows Phone. The efficiency would have suffered. One member of the board reminds us that the decision to change the strategy to drop Symbian took 24 months. After focusing, it took 6 months to create the first Lumia. The same would not have been possible in the world of multi-platform.One can try to throw the ball in the other direction as well. Symbian would have still made billions if its ramp-down had not been so dramatic. With that money, it certainly would have been possible to get phones with different operating systems into sales.How would a multi-platform model have worked?It would have been difficult. At any rate, the majority of those interviewed for this book were of the opinion that the phones had left Nokia behind. There was certain death ahead. Nokia made phones which were unsustainable for its profitability. It would have required massive changes.Don’t get us wrong. We are not knocking the choice of Windows Phone. It might have been the best strategic option of all those available. Perhaps the same thing had happened as before, the execution of the strategy failed.In hindsight, it is easy to say that the exclusivity of the Microsoft agreement should have been revealed later. The most important things in Finland have been done using secret meeting minutes. It should have been done without announcing burning platforms, that Lumias would come side-by-side with Symbian. If the devices were done with proper hardware, the combination would have survived a while. When the Lumias were ready, then the exclusivity agreement could have been announced. Even better, if they had launched directly with Windows 8 phones.A managed transition. It would have opened up a possibility for Lumia. In the shops, Lumia phones would have replaced Nokia’s own phones instead of Android phones. Everything rested on this point. Dropping Android from places where it had already taken over proved to be impossible.What if Elop had chosen Android and hopped onto the winner’s train?Let’s back up a little. What if Nokia had chosen Android in 2007, when there would have been a chance? If Nokia had become the first ruler of the Android world, something beautiful could have been created. Maybe even a renaissance of Nokia phones. But that did not happen. Samsung got there first.But let’s think still for a moment, what would have happened if Elop had chosen Google instead of his previous employer in the beginning of 2011.We can start from the speech of Risto Siilasmaa, chairman of the Nokia board, in the exceptional general meeting in the fall of 2013. Small investors chastised him about the choice of Windows Phone and how, through that, Nokia ended up in the lap of Microsoft. He was moved by the criticizers. He asked: What could have been done differently?The question is indeed relevant. Was Android an option?According to Siilasmaa, it was not. We have a different opinion. Android was a real option, and a better option than what Nokia had publicly stated. Managed transition is the keyword here as well. With the help of Android, Nokia would have been able to replace its cheap phones before the competitors.Elop and Siilasmaa defended Windows Phone with the argument that Samsung’s key position strangled the other Android manufacturers, and it would have been impossible to fit in. Elop and Siilasmaa had left the consideration of this important question halfway: Why did Samsung and Android get such a strangling position? Because Nokia opened this opportunity up to them with its Symbian catastrophe. At least in part.Choosing Google in 2011 would have undoubtedly been a bold move. Android would still have been a sure choice, and in the long run, fewer people would have lost their jobs than with the Microsoft choice. Especially when the combination would have enabled Nokia to compete in the lower mid-price category without all the image and device restrictions from Microsoft. We believe it to be very possible, that by choosing Android, Nokia would have stayed in the game longer.The Elop way of putting all the eggs into one basket might have even worked with MeeGo as well. Especially if Symbian had been ramped down to make way for MeeGo already before 2010, the world today might look very different. In our interviews, there were a lot of people in support of MeeGo. It would have been possible to build an ecosystem around it, and time would have opened new possibilities in the free world of Android.We still want to emphasize that we have understood the nature of opinions. Some of them are expressions of bitterness from former employees and conclusions made from their own narrow point of view. MeeGo is also romantic and stirs up feelings of longing, and bypasses rational thought. We also understand the idealism of Free Open Source Software, and that also blinds people from the truth. Our mission here is to sort through what is relevant, and hopefully we have succeeded in this.We have also understood that the people who have agreed to being interviewed by us are more likely people who have left Nokia than people who have stayed, and people who have left MeeGo are numerous, which has certainly colored the interviews.More speculation: What if someone else had been chosen, rather than Elop.If Anssi Vanjoki had become CEO, MeeGo would have lived. More resources would have been put into it. Symbian would have been ramped down, but in a controlled manner, spanning 2–3 years. In parallel, Android might have also been taken, but not Windows. Vanjoki would have probably chosen a multi-platform strategy.If Symbian had been ramped down in a controlled manner, it would have been compensated with an increasing amount of MeeGo and Meltemi phones, and Nokia could still be in the phone business. The crash of Symbian would have been less sudden and new solutions could be brought in at a reasonable pace. We must also keep in mind that the smartphone markets were growing. If the market share had been stabilized in some form or another, the sales would have increased. With Meltemi and MeeGo, Nokia would have also been a solid player with tablets as well. Nokia could have caught up with the growing markets, because both cheap and flagship tablet models would have been developed at a fast pace.If the MeeGo/Meltemi ecosystem would not have been created, there would still have been Android apps. So they could have chosen the Jolla way. It is its own ecosystem which also operates as a parasite on another ecosystem, Android. The consumer doesn’t care if an app is a Qt or Android app. The important thing is that Facebook, Instagram and other key apps work.The above paragraph might sound technical, but that is a topic related to the crash. The brand was lost, because the wrong operating system and ecosystem were chosen. Even if money was being made on mid-price and cheap phones, their image should have been used to help strengthen the world’s’ best and most advanced mobile phone.The decisive blow to the profitability of Nokia’s phones was the smartphones becoming cheaper.Neither Vanjoki nor anyone else would have been able to stop this chain of events. Speculation around the idea that Ollila’s original first choice candidate would have come to far away Finland to lead Nokia leads to a short chain of reasoning. He would been subject to the same set of rules as Elop: A large business made impossible by its lack of profitability. This super savior would have needed to make massive, correctly targeted cost reductions, without losing the ability to innovate. This would have been a tough cookie for anyone, when they were up against very powerful players. Nokia would have certainly achieved a sustainable growth in the phone business if the Samsung–Android pair had encountered some unexpected setbacks. Even if Apple had collapsed, it would not have automatically saved Nokia.There are yet more topics of speculation. What if Nokia had not sold its phones to Microsoft? The experts unanimously agree: Nokia would have ended up in a cash crisis and the existence of the entire company would have been at risk. The phones had become a dangerous burden that needed to be dropped. Nokia got a good price for its phones and turned its activities into a profitable direction.Now finally: What if Jorma Ollila had stepped down earlier as chairman? The board led by Ollila got a CEO from the New World, but Nokia itself represented history. A CEO cannot work wonders by himself. He needs the support of the board, who have enough understanding to bless the strategy and other critical decisions. Many uphold Siilasmaa as the hero of this story, who managed to save Nokia from certain disaster at the last moment and sell the phones. Many who decried the sales price of the phone activities, gave praise.A leader of a large American hedge fund says directly, that he underestimated the desperation of Microsoft, upon which Siilasmaa managed to cash in. The exclusive agreement with Nokia had left the American giant vulnerable. The sales price of Nokia phones was, in his opinion, far too high. Why? Because the sales volume of the Lumia phones, which they were following, was continuously decreasing. The right price, according to him, would have been 1–1.5 billion euros ($1.3–2 billion) or even less.Nokia under Siilasmaa’s leadership got a new start. Even if the signs look optimistic, the world can change suddenly. It may be that the situation in the company can undergo extreme changes. Siilasmaa’s real final value still remains to be seen.The change of rhythm in the board and the commitment of the chairman in bringing Nokia upward, in any case, made a huge impression on large shareholders. If Siilasmaa had replaced Ollila earlier, the process of recuperating would have started more quickly. The decisive thing from Nokia’s stance would have been who Siilasmaa’s board had chosen to be CEO.In summary, Nokia was in such deep water in 2010 from long procrastinated solutions and because of the quickly growing market, the board made the wrong decision: To hire Elop as CEO. The Microsoft man lashed up Nokia as a beast of burden for Windows, and then tied its hands. The result was a historical loss of market share and sales, a cash crisis and the end of the legendary Finnish mobile phone industry.27. EpilogueWe interviewed 102 people for this book. We heard the sentence “Nokia’s biggest mistake was …” almost as many times. Each time the ending of that sentence was different.There are as many versions of Nokia’s recent history as there are storytellers. So we don’t even try to insist that our book illustrates the ultimate truth. The book is a synthesis of multiple views and interpretations; as such we have tried to make it as accurate as possible. Economic history will reveal a more accurate truth after sufficient passage of time since these events, Nokia will open its files, and the people involved will start to publish their autobiographies.Rumors and conspiracy theories will likely continue to thrive, although as we have described, they do not have any solid basis.During the course of this book project we have reviewed multiple rumors and some partially imaginary claims.One such claim is that Jorma Ollila was the Trojan horse. Seriously: We saw no reason to even investigate this further. Dear conspiracy theorists, please cool down! Jorma Ollila and Stephen Elop came to their decisions based on the best available information they had at that time, acting sincerely. They wanted to save Nokia, and they worked relentlessly towards this goal. Kudos to them for this.The fact that decisions made turned out to be wrong does not make the decision makers traitors.Another rumor has it that Steve Ballmer’s superyacht was in the Helsinki harbor in the summer of 2010 and that Ballmer was in Finland to negotiate the deal with Jorma Ollila. Except that it was not Steve Ballmer’s superyacht. It belonged to Paul Allen, a Microsoft co-founder who had left the company in 1983.A third rumor claims that after the Nokia Board of Directors had decided on the Windows Phone strategy, Microsoft people waiting outside the meeting room were ready to take over. Wrong. According to sources in the board, neither Microsoft nor Google representatives were ever seen in or around those meetings.According to a fourth rumor, after a year at Nokia, Elop realized that the Windows strategy would be a dead end. He supposedly contacted Steve Ballmer and told him that Nokia’s Windows phones would not have a chance. Ballmer was then said to have contacted Risto Siilasmaa. This is a grave accusation. It means that Elop bypassed his own superiors to reveal Nokia company secrets to Microsoft — even if these might have been construed as his personal opinions.We were not able to have this rumor confirmed or denied but it is hard to believe that Elop had come to such a conclusion before the Lumia sales had even started. He had been working for Nokia for one year by October 2011. If there is any truth behind this rumor, it must have leaked from Microsoft since for Elop to disclose this would have been tantamount to playing with fire.We thank everyone who has been interviewed. You have invested your time altruistically to benefit our writing project.The takeaway message after all these pages can be summarized as: Nokia’s phone business did have a chance. Yet, it was still merely a chance that would have required a long series of right decisions, luck and a lot of skill. The end result could still have been the same as what actually transpired, but the probability of this might have been smaller.What is gone is gone and will not come back. Great memories will remain, as do the ample amounts of world-class know-how and lessons for posterity. Goodbye Nokia Mobile Phones!Stephen and Nancy were officially divorced on July 3, 2014. The family residence in Redmond, Washington, was still on sale in September 2014. The price had been dropped from the original $5.8 million to $4.5 million. In April 2013, Stephen moved a little closer to the Microsoft headquarters. The new residence in Grousemont Estates in Redmond cost $1.1 million and Elop is probably still living there.Events have followed a familiar pattern. In July 2014 Microsoft announced the layoffs of 18,000 employees, of which 12,500 were former Nokians. In Finland, this will impact 1,100 employees. According to media reports in July 2014, Microsoft is planning to discontinue the Asha and S40 feature phones inherited from Nokia in the next 18 months. The Android-based Nokia X product line will switch to Windows Phone even though it seems to have been a success: Information from September 2014 indicates that Nokia X series was the best-selling smartphone in the under-$150 price category in 40 countries. Nokia’s smartphone market share had roughly tripled in those countries. The Lumia share had remained stagnant.It looks like the brand still has appeal as long as the phone is inexpensive enough and the operating system presents the right image. In particular, the X series did beat cheap Android phones from local Asian manufacturers and seems to have been performing well against the cheap models by Samsung as well.The story in our book reached its final conclusion one day before going to press — Microsoft announced that it will stop using the Nokia brand for its smartphones from the beginning of 2015.We would like to thank journalist Katja Boxberg, software developer Antti Koivisto and managing director Timo Salminen for their support and assistance. We also thank editor-in-chief Arno Ahosniemi for arranging the time to work on this book and for his support. We thank our spouses and children for their patience.Appendix 1: Where are they now?Please note that this reflects the situation in October 2014 when the original book was published.MARKO AHTISAARI left the Nokia head of product design job in October 2013. He is a Director’s Fellow at the MIT Media Lab. The Media Lab at MIT tries to combine technologies, multimedia, arts, and design.JERRI DEVARD who led marketing at Nokia quit in July 2012. She worked in her own company until March 2014, when she started as the Chief Marketing Officer of the US company ADT selling security services to homes and small companies.STEPHEN ELOP stepped down from his Nokia CEO role after the Microsoft deal was announced to become the acting head of the Nokia phones unit. He was one of the candidates to succeed Steve Ballmer as the next Microsoft CEO but lost to Satya Nadella. Elop is currently in charge of the Microsoft devices unit, including the phones bought from Nokia, Xbox gaming consoles and Surface tablets.COLIN GILES quit his Nokia head of sales job in September 2012. In July 2013, Giles started at Huawei, heading phones marketing and moved forward in May 2014 to another Chinese company, Lenovo, as their Vice President leading sales.MICHAEL HALBHERR continued in the new Nokia in his old job but his title changed from the Executive Vice President of Here to the CEO of Here. He quit in August 2014 presumably after having differences in opinion with Nokia CEO Rajeev Suri regarding the future of Here.JO HARLOW who was leading the smartphones operation at Nokia, transferred to Microsoft in the deal and is in charge of the phone operations.TIMO IHAMUOTILA held the role of Nokia interim President after the Microsoft deal was announced. Ihamuotila is the CFO in the new Nokia.OLLI-PEKKA KALLASVUO quit his job as the CEO of Nokia in September 2010, and later started as a board member and is the vice-chairman of the Board of Directors of TeliaSonera and construction company SRV. He is the chairman of the board of the Swedish Zenterio company developing software for digital television receivers.MARY MCDOWELL left her job as the head of Nokia feature phones in June 2012. She has been working as a board member of the event company UBM and software company Autodesk.JORMA OLLILA left his post as the Nokia chairman of the board in the spring of 2012. Ollila is the chairman of the board at Shell and Outokumpu, the chairman of the board of the EVA thinktank in Finland, and an advisor-partner of the consulting company Perella Weinberg Partners.JUHA PUTKIRANTA transferred from his job as the Nokia head of manufacturing and subcontracting to Microsoft to lead the two company integration operation.NIKLAS SAVANDER left Nokia from his job as the head of the Markets unit in August 2012. In April 2014, he started as the CEO of the Swedish Elekta company manufacturing radiotherapy equipment, after having worked as an advisor to several venture capital funds.MARJORIE SCARDINO left the Nokia Board of Directors in the spring of 2013. She had quit her job as the CEO of the media company Pearson before that, and after Nokia she has worked primarily as a board member, including the board of Twitter.RISTO SIILASMAA was the Nokia interim CEO after the Microsoft deal was announced. Siilasmaa is currently the chairman of the board of the new Nokia.RAJEEV SURI is the CEO of the new Nokia from May 1, 2014.TIMO TOIKKANEN moved from his job as the head of Nokia feature phones to Microsoft and continues to be in charge of feature phones.ALBERTO TORRES left his job as the head of Nokia MeeGo in March 2011, and started as the head of Hewlett-Packard mobile business in September 2012.ANSSI VANJOKI started in August 2013 as a Professor at the Lappeenranta Technical University in Finland. Vanjoki is the chairman of the board of the sporting goods manufacturing company Amer Sports and a startup investor.CHRIS WEBER transferred from his job as the head of Nokia sales and marketing to Microsoft where he is currently in charge of phone sales.JUHA ÄKRÄS transferred to the new Nokia to lead Human Resources.KAI ÖISTÄMÖ did not move to Microsoft in the Microsoft deal. Öistämö’s permanent employment with Nokia ended in April 2014, and he works as an advisor to Nokia.Appendix 2: Glossary3G (3RD GENERATION) Acronym to denote third generation cellular mobile networks. First generation networks used analog standards like NMT (nordisk mobiltelefon) and second generation networks were digital such as GSM (global system for mobile telecommunications). 3G enabled faster data communication. The first 3G networks were deployed in the early 21st century.4G (4TH GENERATION) Acronym for the fourth generation cellular mobile networks that follow 3G. 4G enables faster data communication. The definition is somewhat unclear: Network providers also market their fastest 3G data networks under the 4G moniker. The first 4G networks were deployed at the end of the last decade.ANDROID Free smartphone and tablet operating system developed by Google. Android is open source software based on Linux. The Google version of Android requires Google services on the phone, such as the mobile app store Google Play. If a manufacturer makes modifications on its version of Android, it won’t be able to have Google services in their phones.CDMA (CODE DIVISION MULTIPLE ACCESS) Roughly one third of the world’s cellular mobile networks are based on this second-generation cellular mobile network technology, especially in the United States and Asia. CDMA is a competitor to GSM, and it is often seen to include also the 3G version called cdma2000.ETHERNET Local area networking technology for computers. Local area networks include computer networks in individual buildings or corporate networks in a single office location.FEATURE PHONE (Finnish: PERUSPUHELIN) An inexpensive mobile phone lacking some smartphone functionalities, such as fast data transfer and a large variety of applications. With feature phones, one can make phone calls, send and receive text messages and access the internet in a limited fashion.GOOGLE DOCS Free office application suite for word processing, spreadsheets, and other applications. The applications are used with an internet browser and the documents are stored in Google servers. Google Docs is a competitor to the more expensive Microsoft Office suite and free OpenOffice.GRAPHICS ACCELERATOR (Finnish: GRAFIIKKAKIIHDYTIN) Microprocessor to allow computing devices to produce graphics on the display faster. Offloading graphics software operations to a graphics accelerator frees capacity from the other components in the computer for other tasks. Many smartphones and computers have graphics accelerators.GSM (GLOBAL SYSTEM FOR MOBILE TELECOMMUNICATIONS) Second-generation cellular mobile network technology developed in Europe during the 1980s. GSM introduces a SIM smartcard to identify the user, allowing billing to happen based on phone numbers instead of devices. The first GSM networks in Finland were built in the early 1990s.iOS An operating system developed by Apple that is used in iPhone smartphones and iPad tablets. Apple does not license the iOS operating system to other manufacturers. The first version of iOS was released in June 2007.JAVA Programming language developed by Sun Microsystems. Java is being used in about 3.8 billion devices, from phones to supercomputers. It is used in developing applications for low-end phones.LINUX Operating system initiated by Linus Torvalds in Finland, based on open source software that is available for free and allows further modifications. Google’s Android is based on Linux just like the MeeGo operating system Nokia was developing for a long time.LTE (LONG TERM EVOLUTION) New cellular mobile network technology. Basic LTE is often considered to be 3G, while the more advanced versions like LTE Advanced are part of 4G. LTE significantly improves data transfer speeds. It is becoming the first truly global network technology because both GSM and CDMA technology network providers can migrate to LTE.MeeGo Mobile device operating system developed by Nokia and Intel by merging their earlier operating system endeavors (Maemo and Moblin). MeeGo was planned to become the Nokia smartphones operating system but the plan was scrapped when Stephen Elop announced that Nokia will start using Windows Phone.MELTEMI Operating system for feature phones and mid-tier smartphones developed by Nokia in secrecy during 2011–2012. The development was terminated when Android smartphones price points reached the same level as the planned Meltemi phones. Nokia also had plans for Meltemi tablets.MULTITASKING (Finnish: MONIAJO) Functionality of an operating system that allows the device to execute multiple applications in parallel, making it more convenient for the user to switch between applications.OPEN SOURCE (Finnish: AVOIN KOODI) Method to develop computer software allowing anyone to freely access and make further modifications of the original software. Open source licensing terms often dictate unrestricted availability of software modifications. Enthusiastic developers or companies often drive the development of open source software that is made available without license fees.OPERATING SYSTEM (Finnish: KÄYTTÖJÄRJESTELMÄ) The central software program in a computer or smartphone that is required for other applications to work. Smartphone operating systems include Symbian, Android, and iOS.OPERATOR BILLING (Finnish: OPERAATTORILASKUTUS) Users pay for their purchases via their phone bills even if the money eventually goes to some other party than the network provider.PLATFORM (Finnish: ALUSTA) In this book, platform primarily denotes either the combination of the smartphone operating system and the required electronics and hardware or only the operating system. The first platform in Nokia smartphones was Symbian, then Windows Phone. Platform can also denote approaches where the software or mobile phone technology is based on one single baseline version that is modified to develop new products.QT Software development framework developed by Trolltech in Norway, bought by Nokia in 2008. Qt simplifies mobile application developers’ work. An application can be developed for multiple operating systems in one go. Digia in Finland has continued to develop Qt further after Nokia abandoned the framework.S40 Operating system developed by Nokia for its feature phones. S40 is the world’s most widely used mobile phone operating system. Nokia had sold over 1.5 billion S40 phones by 2012.S60 User interface platform developed by Nokia that was built on top of the Symbian operating system. S60 is a user interface platform because it has broader functionality than just the user interface.SMARTPHONE (Finnish: ÄLYPUHELIN) Mid-tier or expensive mobile phone with a rich set of applications, graphical user interface, and decent internet connection. Smartphones usually have an open operating system for new applications developed by third parties.SYMBIAN Smartphone operating system initiated and offered to other manufacturers by Nokia. The heyday of Symbian ended in the year 2010 when Android became more popular than Symbian. Nokia turned Symbian into open software but Stephen Elop terminated Symbian as Nokia’s primary smartphone platform and chose Microsoft Windows Phone instead.SYSTEM-ON-CHIP (Finnish: PIIRISARJA) Small piece of silicon containing an immense number of small electronic components. In this book, system-on-chip denotes one silicon chip that contains all the most important electronic components for a mobile phone, including the microprocessor. Lumia phones use Snapdragon chips by Qualcomm. Synonyms for system-on-chip include microchip and integrated circuit.TD-LTE (TIME DIVISION LONG TERM EVOLUTION) Fourth-generation cellular mobile network technology based on LTE. LTE variants also include LTE FDD (frequency division long term evolution). The TD-LTE standard was developed by China Mobile, Huawei, Nokia Solutions and Networks, Samsung, Qualcomm, and ST-Ericsson.TIZEN Operating system for mobile devices that was born after Nokia ended its MeeGo development and Intel together with Samsung continued the work. Tizen is used, e.g., in wearable devices of Samsung.USER INTERFACE, UI (Finnish: KÄYTTÖLIITTYMÄ) Control devices and software for the user to control a product. In phones the UI consists of elements visible on the phone display and the methods, such as the keyboard or touch gestures that are used to control the device.WINDOWS 8 Microsoft operating system for computers that succeeded Windows 7. Windows 8 became available in October 2012. It includes the start screen optimized for tablet computers and visually resembles the Windows Phone user interface that is recognized by its “live tiles”.WINDOWS PHONE 7 The Microsoft smartphone operating system and the successor of Windows Mobile. Windows Phone 7 became available at the end of 2010. Microsoft collects a license fee for Windows Phone 7.WINDOWS PHONE 7.5 An improved version of Windows Phone 7, also known by the codename Mango. Windows Phone 7.5 introduced new features and brought new languages. The first Lumia smartphones used Windows Phone 7.5 but Nokia had no chance to influence what is in it.WINDOWS PHONE 7.8 A version of Windows Phone 7 that was created to remediate the fact that phones running Windows Phone 7 were not upgraded to version 8. One could adjust the size of the tiles like in Windows Phone 8 but Windows Phone 8 applications did not work.WINDOWS PHONE 8 The successor of Windows Phone 7.5 as the operating system for Nokia smartphones. Windows Phone 8 deviates from its predecessors so much that applications developed for it did not work in its predecessors and the earlier phones could not be upgraded to the new version.Addendum to the glossary in the English translationWe provide the following additional clarifications specifically for the English translation.NETWORK PROVIDER (Finnish: OPERAATTORI) The Finnish term “operaattori” refers to companies that provide network access and communication services to subscribers. Within Nokia, the standard English translation was “operator”. In North America, other terms such as “network provider” and “carrier” are more commonly used. In this translation, we generally use “network provider” throughout for the sake of consistency, except when reporting verbatim original English quotes.DIRECTOR, MANAGER (Finnish: JOHTAJA, PÄÄLLIKKÖ) The book refers to multiple named and anonymous people with their titles in Finnish being “päällikkö” or “johtaja”. Through some detective work with Google and LinkedIn we found out the more accurate English titles for some of the people mentioned in the text but in many cases, and especially with the anonymous references, we generally use “manager” for the original term “päällikkö” and “director” for “johtaja”.STATUTORY NEGOTIATIONS (Finnish: YT-NEUVOTTELUT) Finnish labor law requires that an employer planning job reductions (or other major changes affecting employees) have a series of negotiations with employee representatives on how the reductions/changes are implemented. Finnish sources sometimes translate this as “cooperation negotiations”.The final final epilogueThis online book is a pro bono effort by a team of former and current Nokians to translate the original Finnish Operaatio Elop book from Finnish to English. It all started when the original book was published in October 2014. By that time, Nokia’s phone operation had been going through difficult and turbulent times; it had been sold to Microsoft, and Nokia was focusing on its telecom infrastructure business. There was bitterness in the air and conspiracy theories abounded—after the crown jewel of Finland and champion of the whole European technology industry had crash-landed. Many readers praised the original Finnish book for its objective treatment of the affairs at Nokia, thanks to the background research done by the authors Merina Salminen and Pekka Nykänen through interviewing a large number of Nokia employees, executives, and other stakeholders. After reading the book reviews, non-Finnish readers were asking in social media if the book is going to be available also in English.Like many other former and current Nokians of that time, Harri Kiljander bought the book as soon as it was available. After spending the following night reading the book, Harri sent an email to the publisher asking them to deliver a thank you message to the authors for a well-written book. To Harri’s surprise, Merina Salminen and Pekka Nykänen soon responded to his email, and over the next couple of emails a loose idea was born: Gather a team of Nokians to bootstrap an English translation of the book and eventually publish a proper English version of the book. Harri introduced the idea to Janne Parkkila who invited Timothy Jasionowski to help in the effort. The team agreed with the authors to translate a set of chapters as a teaser to expedite their task finding a publisher for the English version. By the fall of 2015 a sample set of chapters was ready and the authors had signed a deal with a publishing agent.Triggered by Microsoft announcing the death of Windows Phone in October 2017 Harri again checked the English book status again with the authors. There was no progress with publishing the English version, so a deal was made with Merina and Pekka: If their agent cannot get a contract for them by the end of the year, the pro bono team can translate and publish a non-commercial English version of the book.Beyond Nokia is a closed Facebook group for ex-Nokia employees globally with over 27,000 members in February 2018. Stories have been written of what keeps the ex-Nokia community together and this forum is one manifestation of the old Connecting People mission statement of Nokia still going strong. On October 13, 2017, Harri sent out a call for contributors message in Beyond Nokia:Call for contributors!Windows 10 Mobile was axed this week and as we all know, Nokia’s path crossed with Microsoft’s mobile platform endeavours. Stephen Elop was in a key role in that development, and after Nokia’s handsets were sold to Microsoft in 2014, two Finnish journalists Merina Salminen and Pekka Nykänen wrote a book of what had happened in Nokia’s handset business during 2010–2013 when Stephen Elop was the CEO of Nokia. The book “Operaatio Elop” (“Operation Elop”) came out in Finnish but no English version was published.I spoke with Pekka after the book was launched and we came up with an idea to crowdsource some of the book chapters in English to expedite the English version of the book. However, the publishing agent they had selected did not do her job properly so no English version ever came out. The agreement with the agent has been prohibiting Pekka and Merina from proceeding with any alternative paths to publish an English version.The contract period with the agent ended some time ago and Pekka and Merina just let us know that it’s ok for them if we want to translate and launch a free English version of the book!Janne Parkkila and myself feel there might still be interest towards an English version of the book. We have some chapters translated and proofread and we think that Medium would be a good platform to launch the book e.g. under an appropriate Creative Commons license.Ex-Nokia volunteers started to sign up both as translators and proofreaders and eventually there was a team of 20+ enthusiastic people, from San Diego, California, to Ulm in Germany to Oulu in Finland to Batam in Indonesia, and in many other locations. People had worked at Nokia — and some are still working — as engineers, marketers, designers, and managers. Some team members are or have been professional wordsmiths in their working lives but most were simply interested in working on a small new chapter in the Nokia story. Nokia’s corporate language was internally humorously said to be ‘broken English’ and our team screening criteria was “good enough broken English so that one can actively contribute to a credible business book”. Some of our team members are among those who were initially interviewed to the Finnish book by Merina and Pekka. At some point in history we all have worked for Nokia, some also for Microsoft, some of us were even colleagues, but most of us actually have never met each other in real life. We decided to use Google Docs [22], Facebook Chat and Medium for the translation project.Some people have asked us why we want to dwell in the past. They say they want to leave Nokia behind and move forward. Also we want to move forward — we are moving forward. We also enjoy working as a team in the Connecting People spirit, even if no longer a full-time Nokia team, hopefully sharing some Nokia learnings with a wider audience so that those may help them in their new jobs and lives. We know a number of other books have already been written about Nokia’s mobile phones, see e.g.:Ours is a translation project. So please try to remember that we are not the authors. All that credit goes to Merina and Pekka. We took what they had published in Finnish and translated it to English as a team. Our team learned a lot in this project: From spelling the em dash “ — ” to the use of footnotes and how to do currency conversions in the past, how to refer to “palkkajohtaja” and “vuorineuvos” in US English, how to do handovers between translators and proofreaders in Google Docs, and what the difference is between “basic phones” and “feature phones” in 2018, if any. We decided to write primarily for the North American audience, and Nokia’s official language used to be US English so we wrote this text in US English. The original book was mostly using the Euro currency for financial numbers; to help the non-European reader we decided to add US Dollar equivalents using the exchange rates dating back to the original context in the storyline. Many of the interviews had been originally conducted by Merina and Pekka in English and many of their written sources were also in English. We did not have access to these interview notes. So whenever we couldn’t locate the original sources, we had to translate their Finnish translations in the book back into English. Likewise, we have translated people’s titles to English in Appendix 1 and in the People index section, knowing that in this translation we’ve not been able to refer to everyone with their accurate titles. Apologies for that.We had fun translating and editing the book and we hope you enjoyed reading it. If you dislike or disagree with something in the book, we ask you to consider not shooting the messenger.We would like to thank Merina Salminen and Pekka Nykänen for their highly collaborative attitude with this non-commercial translation and publishing project. We would also like to thank the legal and typography experts in the wider ex-Nokia community who gave us valuable guidance for free in this project, and Jari Ijäs for the stunning cover photo, taken from the old Nokia headquarters. Please do note that Merina and Pekka own all commercial rights to the book; so others do not have permission to use the material for commercial purposes.",Operation Elop
9932,3903878,2018-02-28 17:00:22,"Hangouts Chat, Google’s Slack competitor, comes out of beta0Hangouts Chat, Google’s take on modern workplace communication, is now generally available and is becoming a core part of G Suite. Hangouts Chat was first announced at Google Cloud Next 2017, together with Hangouts Meet. While Meet went right into public availability, though, Chat went into an invite-only preview. Now, Google is rolling Chat out to all G Suite users over the course of the next seven days (so if you don’t see it yet, don’t despair).For all intents and purposes, Hangouts Chat is Google’s take on Slack, Microsoft Teams and similar projects. Since Google first announced this project, Atlassian also joined the fray with the launch of Stride. Like its competitors, Chat is available on iOS, Android and the web.All of these companies are essentially riffing on the same theme, but all of them put their own spin on it. For Google, that means a strong emphasis on AI. The best example for this is probably the @Meet bot that helps you schedule meetings and the @Drive bot that keeps you abreast of when files are shared with you or when people request access to one of your own documents.Chat currently supports 28 languages and each room can have up to 8,000 members. What’s maybe just as important, though, is that Google has already built an ecosystem of partners that are integrating with Chat by offering their own bots. They include the likes of Xero, RingCentral, UberConference, Salesforce, Zenefits, Zoom.ai, Jira, Trello, Wrike and Kayak. There’s even a Giphy bot.Developers can also build their own bots and integrate their own services with Chat.Deep integrations with Google’s own products are, of course, also part of Chat. These include the ability to start Hangouts Meet video conferences from Chat, built-in file uploads to Drive, Docs collaboration, etc.Hangouts Chat is launching into an increasingly crowded field. There’s still plenty of space for growth, but a lot of enterprises have already placed their bets. Google’s advantage, of course, is that Chat is part of G Suite and companies that already use Google’s services won’t need to pay extra for it. Google surely also hopes that its machine learning-powered features and deep integrations with popular services will give it a leg up on the competition.For consumers who just want to use Hangouts on their phones, this launch just adds to the confusion of which Google messaging services to use. Google’s overall positioning has long been clear: Duo and Allo are its consumer video and text chat apps and Hangouts Meet and Chat are their equivalents on the business side. But Allo never caught on and Hangouts happily lives on millions of smartphones (and there’s also Android Messages, but that’s more of a carrier and hardware OEM thing and all about traditional text messaging and RCS).As for the rest of G Suite, Google also today announced the launch of a smart scheduling service that helps you find a convenient meeting room for your bi-weekly status meeting, as well as the launch of Quick Access in Docs, which uses machine learning to automatically surface the most relevant files. That’s similar to the Quick Access feature that already exists in Drive.","Hangouts Chat, Google’s Slack competitor, comes out of beta"
9933,3903957,2018-02-28 18:01:29,"TNW SitesTwitter adds new features to bookmark and share your favorite tweetsTwitter today announced it’s rolling out a new Bookmark option for tweets, as well as a new way of sharing them with friends.First spotted last October, the Bookmark option lets you save tweets in a private list. This way, you can save tweets, links, and threads for later perusal — particularly useful if you’re like me and find twenty interesting links while browsing Twitter during a five-minute break and can’t afford to go down the rabbit hole in the moment.Credit: TwitterUp to now, Twitter users had the “Like” heart, which would technically save your tweet to a list on your profile. But in practice the Like functions more like an endorsement, since that list is part of your public profile. Only you’ll be able to see your Bookmarks, making it more useful for users who don’t feel like putting every tweet they like out for everyone to see.Twitter is also adding a new “Share” option to tweets. Replacing the envelope icon — which was for sharing via DMs — with a more standard “arrow in a box,” the Share option now includes sharing via DMs, saving tweets to Moments, and sharing via outside apps such as email. This menu is also where you’ll find the Bookmark option.","Twitter adds a Bookmark feature, and makes saving tweets miles easier"
9934,3906265,2018-02-28 18:30:00,"Please, Nokia, bring back the 7110 nextIt's becoming something of an MWC tradition that HMD Global, the company that builds phones under the Nokia brand, offers a gift to sentimental Europeans. In 2017, the manufacturer rebooted the 3310 while 2018 saw the arrival of a similarly refreshed version of the 8110. When HMD/Nokia returns to Barcelona in 2019, I hope that the company chooses to unveil a new version of the 7110, because I'd be first in line to buy one.Most British teens who clattered into adolescence at the turn of the millennium have a passion for Nokia phones. Like the car for earlier generations, the Nokia 3310 represented freedom, independence and rebellion. The 3310 was tailor-made for kids, since it had a ring-tone composer that let you craft tunes that kinda/sorta sounded like the pop hits of the day. The Xpress-on covers let you alter the shell of the phone to suit your style, and at its peak, you could buy thousands of third-party cases.Ever the angry nonconformist, my desire for the 3310 rapidly evaporated while I was sitting in Lowestoft's premier/only fleapit, the Hollywood Cinema. In the summer of 1999, seated next to my friend in a near empty theater, we saw The Matrix, a movie that forever altered our adolescent brains. The customized 8110 used in the film, with its spring-loaded cover, became the most desirable device we'd ever seen. After all, it just sprung open, so you didn't even have to hit the respond button. How cool was that!?The only problem was that the 8110 was a phone for hard-nosed business types and was priced well beyond the wallet of a dirt-poor 15-year-old. Oh, and it didn't have a spring-loaded mechanism in real life; it was doctored by production designer Owen Paterson for added cinematic flair. The Wachowski siblings were apparently fixated on ensuring that the film's stars didn't fumble with their phones on-screen. Thankfully, Nokia was already working on the real thing.""I wanted a spring for the 8810,"" Nokia's former chief designer Frank Nuovo told Engadget, ""but we just couldn't do it."" It wasn't until the team commenced work on designing the next phone in the range, the 7110, that Nuovo's team solved the issues that enabled the slide to work. Nuovo doesn't know if someone from the production team was shown a 7110 prototype at the time or if ""art and reality happened in parallel.""The result, however, was that my plan to save up and buy a 3310 was ditched in favor of saving up for the 7110. It would only be another couple of months before the 7110 arrived in the UK, and I worked twice, possibly three times as hard to earn the £130 ($210 in 1999 money) that I needed. But what a phone I was getting in return: one that was the first to come with a WAP browser, the first running Nokia's Series 40 OS and a NaviRoller, a clickable scroll wheel that eliminated the up-and-down buttons.But it's not really about the specs or the fact that my 7110 can still kinda/sorta work, even now. It's the fact that it was damn cool. Admittedly, ""damn cool"" in my vernacular meant that I could shoot open the phone and begin talking as if I was an enigmatic cyber hacker from the 21st century. In retrospect, it's fortunate I couldn't afford a leather trench coat, because you can be damn sure I'd have worn one. It's the 7110 that got me interested in weirdly designed phones, because it was put together with so much love.My one concern is that nü-Nokia would do what it has done with the 3310 and 8810 and omit many of the original reasons to love the 7110. It's an anxiety shared by its original creator, who feels that the reimagined shape is a little bit more sterile than his original. ""There was a romanticism in our design,"" Nuovo said. His flourishes included cladding phones in an oil-slick black plastic that, in certain lights, tinted green. Trust me, this was cool in 1999.Then there's the fact that the redesigned 8810 lacks any of the faux-chrome flair that made it easier for your thumb to find specific buttons. Now the keypad is uniform and ultimately rather bland, a fate that could befall the 7110 as well. It would be a shame to rob the 7110 of the techno-fetishism it demonstrated so well. Look at the way the the menu buttons pinch in to accommodate the slider -- like oblique muscles on a bodybuilder. The buttons on the keypad flare, with spacing around each one, make it easier to text without looking. Nuovo feels like this can be avoided. All HMD/Nokia would need to do is hire him back to help with the redesign.My colleague Jamie Rigg says that HMD's policy of rebooting classic Nokia phones is little more than a ""calculated marketing ploy."" I'm sure that the company welcomes all the nostalgia-stoked think pieces that help remind folks that Nokia is alive and kicking and still making phones. But if we live in an era when companies can spend not too much cash to rebuild a two-decades-old phone and get some user love, what's wrong with that? I'd happily tote around a 7110 as my backup, something to show off at parties and otherwise keep in the car for emergencies. I'm still a nonconformist, and my teenage self still lusts after a daily phone with a mechanical slider.After training to be an intellectual property lawyer, Dan abandoned a promising career in financial services to sit at home and play with gadgets. He lives in Norwich, U.K., with his wife, his books and far too many opinions on British TV comedy. One day, if he's very, very lucky, he'll live out his dream to become the executive producer of Doctor Who before retiring to Radio 4.","Please, Nokia, bring back the 7110 next"
9935,3906267,2018-02-28 17:00:00,"Google's Slack alternative is available starting todayIt's been almost a year since Google first mentioned Hangouts Chat -- a totally redesigned messaging service that's more like Slack than the Hangouts most consumers know today. As of now, Hangouts Chat is out of its ""early adopter"" program and will be available to all G Suite users over the next week, assuming their company enables it, of course.To be clear, Hangouts Chat is a totally separate and distinct service from Hangouts proper, which still lives in your Google mail inbox. And while we'll forgive you for rolling your eyes at yet another chat service from Google (the number of different apps the company has built is legendary at the point), Hangouts Chat does offer something potentially valuable to companies using G Suite -- assuming they're not on Slack already.Just like Slack, Hangouts Chat features virtual rooms for different parts of a company's team or to organize people around a specific project or task. Each room can hold up to 8,000 people, and Chat works with 28 different languages total. Naturally, it has direct messaging and threaded conversations as well as apps for iOS, Android, macOS and Windows; there's also a web interface, of course.Hangouts Chat includes a total of 25 bots that interface with other G Suite apps — you can talk with @Drive to get updates on shared files, or use @Meet to check people's calendars and schedule meetings. Many of those bots are also built by third-party companies to interface with their own services, like Xero accounting or Jira and Trello for project management. As you'd expect, Google is keen on having other companies build their own integrations for Hangouts Chat, so it is also offering developer resources for anyone who wants to add their software to this new platform.Given Google's focus on AI across basically all of its products, it's no surprise that Hangouts Chat will use machine learning to try and figure out what users might need. Specifically, Google says AI will help book meeting rooms, find files ""and more."" Specifically, a link between Chat and Calendar will learn how to suggest locations to book by analyzing attendees' ""building and floor location, previous booking history, audio/video equipment needs and room capacity requirements."" It's hard to say how well this will work — but anyone working in a semi-large company also knows that booking a meeting room likely can't get any worse than it is right now.As for files, Google Docs will now feature the same ""quick access"" feature that predicts what files you might want to access in Drive. When you're working on a specific document, clicking the ""explore"" section will reveal related files that Google thinks might be useful. It's not clear yet how this will play into Hangouts Chat specifically, but give the tight ties between all of Google's various services it seems likely that these suggestions might be accessible right through the chat interface.On one hand, it's easy to roll our eyes and yet another attempt at Chat. But Google has been saying ever since it launched the consumer-focused Allo messaging app that Hangouts was destined for businesses. And at this point, despite the derivative name, Hangouts Chat is a messaging service that's pretty unique in Google's portfolio. The success of Slack has made it clear that more robust messaging apps than what the old Hangouts offered is important. Google might not have an easy time convincing companies to change their workflows to accommodate Hangouts Chat, but getting new customers on board is another story. Since G Suite customers get Hangouts Chat by default, giving it a shot will probably make both logistical and financial sense for many businesses — assuming, of course, that Hangouts Chat can get the job done.",Google's Slack alternative is available starting today
9936,3906361,2018-02-27 17:10:55,"Barely seven months ago, a senior Chinese official promised that artificial intelligence could one day help authorities spot crime before it happens.In the country's far western Xinjiang region, it's already happening, with the establishment of a system that critics call ""Orwellian"" in scope and ambition, and which is being used to place people in political re-education.Called the Integrated Joint Operations Platform, or IJOP, it assembles and parses data from facial-recognition cameras, WiFi internet sniffers, licence-plate cameras, police checkpoints, banking records and police reports made on mobile apps from home visits, a new report from Human Rights Watch finds.Story continues below advertisementIf the system flags anything suspicious – a large purchase of fertilizer, perhaps, or stockpiles of food considered a marker of terrorism – it notifies police, who are expected to respond the same day and act according to what they find. ""Who ought to be taken, should be taken,"" says a work report located by the rights organization.Another official report shows how reports generated by IJOP are used to send people to an ""Occupational Skills and Education Training Centre"" where political re-education is carried out.""We have documented the connection between a big-data program and detentions,"" said Maya Wang, senior China researcher at Human Rights Watch. ""We are no longer saying that mass surveillance is deeply and widely intrusive when it comes to privacy rights, which of course is a big alarm. It goes further than that. People are being detained in an arbitrary manner because they are put in these political-education facilities.""Such re-education can involve forcibly detaining people for months at a time without charges to inculcate them in political doctrine considered acceptable by the Chinese state.The system is being used in Xinjiang, a region whose largely Muslim Uyghur population has been accused of committing acts of terror in China and abroad. Uyghurs have fought in Afghanistan and Syria, and China has launched a series of ""Strike Hard"" campaigns in response.The widespread use of political re-education is the latest attempt to root out what China calls extremism. Critics call it a racially motivated campaign directed at Uyghurs, who are being forced to pledge fealty to the Chinese state, study Mandarin Chinese and participate in cultural customs of the majority Han Chinese population.Big data in policing ""often exacerbates some of the biases,"" Ms. Wang said.Story continues below advertisementStory continues below advertisementChinese police theorists have identified specific ""extremist behaviours, which include if you store a large amount of food in your home, if your child suddenly quits school and so on,"" she said. Train a computer to look for such conduct, and ""then you have a big data program modelled upon pretty racist ideas about peaceful behaviours that are part of a Uyghur identity,"" she said.The report ""adds some pieces to the puzzle"" over what is happening in Xinjiang, where it became clear over the last year ""that tens or perhaps hundreds of thousands of Uyghurs were disappearing without having done anything illegal,"" said Rian Thum, a historian at Loyola University in New Orleans who has travelled extensively in Xinjiang.""No Uyghur in Xinjiang today, not even the most submissive party loyalist, can go to sleep feeling certain that they won't be taken to the re-education camps,"" he said. ""The notion of 'predictive policing' would go some way to explaining how people can disappear without having crossed any obvious line.""Chinese officials, however, have boasted that their new skill in sifting through information allows them to prevent the personal and societal damage that comes from crime.The big-data platform in Xinjiang's Jiashi County, for example, ""covers all sorts of information, such as geography, the migrant population, fertilizer purchases, gasoline and vehicles. Once finding abnormal data, the system will automatically alarm,"" a police officer named Xu Linglei told China's Nanfang magazine.He added: ""Before the application of big data, police often only arrested people after they had committed wrongdoings and the victims suffered losses as a result. Now, relying on information technology, they can take preventive measures in advance.""Story continues below advertisementJiashi County was seen as a template for the rest of Xinjiang, the article said.A report on a website maintained by the Communist Party's Committee of Political and Legal Affairs further describes how ""public security organs throughout Xinjiang have built an integrated information prevention and control circle, intensifying and increasing the information collection of citizens' 'eating, living, travelling, consumption and entertainment.'""Real-time data collection and analysis across Xinjiang provided ""an effective means for timely detection of the whereabouts of those who may be involved in terrorism activities,"" the report said.China's Ministry of Public Security has formed a leading work group with a focus on big data. At a national conference held this January, Minister of Public Security Zhao Kezhi described China's ambitions.The conference described efforts at all levels to ""realize the acute perception and accurate prediction of various hidden dangers and crime,"" China Youth Daily reported.Elements of the policing system in Xinjiang are being set in place elsewhere in China, too, including the collection of data and integration of systems. But Xinjiang appears to be unique in the use of artificial intelligence to detain people in political re-education.Chinese companies are also boasting about their big data prowess abroad.At the Winter Olympics in South Korea, for example, Alibaba built a large pavilion to describe its capabilities and offer its services to the outside world. Its ""ET City Brain,"" for example, can be used to improve timing of traffic lights and employ artificial intelligence to quickly route emergency services to an accident.But Alibaba also boasts about the system's value in ""social governance and public security,"" saying in an information presentation: ""With video recognition technology and location-based services, authorities can respond to incidents precisely and quickly.""Alibaba provided a person to describe the company's pavilion on the condition that it be off-record. The company declined interview requests at the Olympics.",China using big data to detain people before crime is committed: report
9937,3906363,2018-02-24 07:00:39,"How much money do people need to be happy?Written byShareWritten byTake three people. All are unmarried, 33-year-old women who live in the United States. One makes an annual salary of $40,000, another makes $120,000, and the third makes $200,000. Who do you think is the happiest?According to a recently released study (paywall) in the burgeoning field of happiness research, the two higher-earning women are likely to report more satisfaction with their lives than the one who makes $40,000. But, perhaps surprisingly, the psychologists who conducted the study find that the one making $200,000 is probably no happier than the one making $120,000. This is because both the $120,000 and $200,000 women have incomes above $105,000, which according to their research is the point at which greater household income in the US is not associated with greater happiness. The technical term for this cutoff is the income “satiation point.”The study is based on a life-satisfaction survey conducted on over 1 million people as part of the Gallup World Poll. Respondents across the world were asked to rate their lives on a scale of 0-10, where 0 is the “worst possible life” and 10 is the “best possible life.” (This author would give himself an eight.)The researchers analyzed the relationship between this score and household income. They find that in every region of the world, after accounting for a person’s age, gender, and marital status, people with higher incomes are happier. But they also find that there is a level of income at which happiness no longer increases with more money. This varies by region, with Australia and New Zealand the highest and Latin America and the Caribbean the lowest. They even find some evidence that in certain places, when incomes rise above the cutoff level, life satisfaction gets lower.The chart below shows the “satiation point” for different areas of the world. The incomes are converted to US dollars and adjusted for variations in spending power across countries.These psychologists, from Purdue University and the University of Virginia, are not the first to study how income relates to life satisfaction. In 2010, the Nobel prize-winning duo of economist Angus Deaton and psychologist Daniel Kahneman, famously found that the satiation point for US households was about $75,000 (about $84,000 in 2016 dollars). This new research improves on Deaton and Kahneman’s work, because the data is able to account for the number of people in a household, has more detailed income numbers, and includes responses from many more countries.Dan Sacks is an economist at Indiana University who studies the relationship between income and subjective well-being. He tells Quartz over email that he finds the new research compelling, but far from definitive. The primary strength of this paper, Sacks says, is that the researchers have access to a huge dataset that, unlike many previous studies, includes a large number of high-income people. His main concern is that the research relies on flawed survey questions.The surveys rely on self-reported income, and previous research shows that just because people say they make a certain amount of money, it doesn’t mean they actually do (pdf). “It could be true that on average, people who say they have income of $150,000 are no happier than people who say they have income of $100,000,” writes Sacks. “But I’m not convinced that people who actually have income of $150,000 are no happier than people who have income of $100,000.” Also, it’s possible that rich people have a tendency to underemphasize their happiness compared with poorer people.People also tend to answer questions about their happiness differently on different days. While a person’s answer on any given day is predictive of what they say a month later, it’s not that statistically robust. Today, I say my life is an eight, but ask me tomorrow and it might be a seven. This measurement error makes it difficult for researchers to assess the income-happiness relationship with great accuracy.But let’s assume that the research is right, and there is some point at which higher incomes don’t predict greater happiness. Does that mean that if you already make $120,000, you wouldn’t be happier with a $30,000 raise?Not at all. Research suggests that the average person who makes $150,000 is no happier than the average person who makes $120,000. But it could be that the sort of person who makes $120,000 is different in some fundamental way from the sort of person who makes $150,000. Perhaps, the people who make $150,000 would be less happy if they made $120,000, so their satiation point is higher than the sort of person who is happy with $120,000 and doesn’t want for anything more.",How much money do people need to be happy?
9938,3906365,2018-02-28 18:16:55,"DigiCert Statement on Trustico Certificate Revocation“Trustico requested revocation of their Symantec, GeoTrust, Thawte and RapidSSL certificates, claiming the certificates were compromised. When we asked for proof of the “compromise,” Trustico did not provide details on why they were requesting the immediate revocation. Trustico’s CEO indicated that Trustico held the private keys for those certificates, and then emailed us approximately 20,000 certificate private keys. When he sent us those keys, his action gave us no choice but to act in accordance with the CA/Browser Forum Baseline Requirements, which mandate that we revoke a compromised certificate within 24 hours. As a CA, we had no choice but to follow the Baseline Requirements. Following our standard revocation process, we gave notice via email to each certificate holder whose private keys had been exposed to us by Trustico, so they could have time to get a replacement certificate.In communications today, Trustico has suggested that this revocation is due to the upcoming Google Chrome distrust of Symantec roots. That is incorrect. We want to make it clear that the certificates needed to be revoked because Trustico sent us the private keys; this has nothing to do with future potential distrust dates.The upcoming Chrome distrust situation is entirely separate. We are working closely to help customers with certificates affected by the browser distrust, and we are offering free replacement certificates through their existing customer portals. That process is well underway.”DigiCert Statement on Trustico Certificate Revocation was last modified: February 28th, 2018 by DigiCertDigiCert is a leading provider of scalable security solutions for a connected world. The most innovative companies, including the Global 2000, choose DigiCert for its expertise in identity and encryption for web servers and Internet of Things devices. DigiCert supports SSL/TLS and other digital certificates for PKI deployments at any scale through its certificate lifecycle management platform, CertCentral®. The company has been recognized with dozens of awards for its enterprise-grade management platform, fast and knowledgeable customer support, and market-leading growth. For the latest DigiCert news and updates, visit digicert.com or follow @digicert.Follow us on social media:AboutDigiCert is the go-to provider of identity, authentication, and encryption solutions for the web and IoT devices. We help enterprises of every size deploy PKI security that aligns with industry standards and best practices. Our SSL tools and enterprise-grade platform simplify management, automate certificate tasks, and give organizations the power to customize workflows to best fit their needs.",DigiCert Statement on Trustico Certificate Revocation - DigiCert
9939,3906523,2018-02-28 19:46:32,"Spotify has filed to go public0Music streaming service Spotify is going public and it just unveiled its filing.The documents state that it is targeting a $1 billion IPO, but this is just a placeholder. The company actually plans to go public without the standard fundraising event. In other words, Spotify isn’t selling its shares on the stock market. Instead, the event known as a “direct listing” will be a series of transactions from existing shareholders (like employees and investors) selling shares to stock market investors. Spotify’s filing even acknowledges that this unusual process is “risky.”Its public debut is likely to happen in late March or early April, but it is unclear how much shares will cost when it lists under “SPOT” on the New York Stock Exchange. Spotify says that for 2018 its shares have traded on the private markets for between $90 and $132.50, valuing the company at $23.4 billion at the top of the range. But that these transactions “may have little or no relation to the opening public price of our ordinary shares on the NYSE.”Spotify says it is present in 61 countries and its platform includes 159 million monthly active users and 71 million premium subscribers.Total MAUs SpotifyPaying users SpotifySpotify Ad supported usersThe filing shows that the Swedish company had 4.09 billion Euros in revenue last year (or close to $5 billion), compared to 2.95 billion Euros (about $3.6 billion) the year before. 2015 saw 1.94 billion Euros in revenue (about $2.37 billion).Losses for last year were 1.2 billion Euros ($1.46 billion), which compares to 539 million Euros ($657 million) the year before.The filing shows that CEO and co-founder Daniel Ek has voting power that represents 23.8% of the company. However, some of this voting power is on behalf of shares owned by Tiger, TME Hong Kong and Image Frame. Ek technically owns closer to 9% of the business.The prospectus has an additional note about Ek’s compensation, which says that he doesn’t receive a base salary, but is eligible for $1 million annual bonuses based on metrics like subscriber growth and active users.Martin Lorentzon, who co-founded the business, owns 12.4%. Other large shareholders include Tencent, Tiger Global, Sony Music and Technology Crossover Ventures (TCV).There are some key obstacles to the business, which Spotify acknowledges in its risk factors.Some are concerned that Spotify is beholden to music rights owners like record labels who could try to raise rates during periodic re-negotiations if they think the service becomes too profitable. There are also administrative agencies like the Copyright Royalty Board and trade groups like the American Society of Composers, Authors and Publishers who could seek to increase the rates Spotify has to pay. Control of rights is heavily concentrated within a few major labels and organizations. Universal Music Group, Sony Music Entertainment, Warner Music Group, and Merlin hold rights for music that accounted for 87% of Spotify’s streams in 2017. They could potentially wreck Spotify’s margins by demanding higher rates or deprive it of content in ways that would drive away listeners.Spotify’s costs could continue to increase as it pays for content, creates its own, builds new features, and markets the service in the face of competition. Spotify’s licensing and royalty agreements are complex and could lead to litigation costs if it doesn’t hit milestones or guaranteed minimum payouts, or doesn’t properly license all the content it streams. Spotify has already been hit with numerous lawsuits for failing to find and pay all rights holders. Its competitors also hold larger patent portfolios that they could use to attack Spotify for intellectual property infringement.Spotify faces competition from all sides and formats. There are traditional formats like CDs and Vinyl, digital files like MP3s and iTunes downloads, terrestrial and satellite radio, online radio like Pandora, and competing on-demand subscription services including Amazon Prime, Apple Music, Deezer, Google Play Music / YouTube, Joox, and SoundCloud. Since Google and Apple own the top mobile app stores, they could potentially bury Spotify and already charge it a tax that doesn’t get applied to their music services. Interestingly, Spotify lists Facebook as a potential competitor, given that it’s building a smart speaker and has struck deals with record labels, although it offers no music streaming service currently.The company writes that“a key differentiating factor between Spotify and other music content providers is our ability to predict music that our Users will enjoy.” Features like Discover Weekly are what keep hardcore listeners on its service, and it will have to find a way to stay ahead of the recommendation engines of its competitors if it wants to win.",Spotify has filed to go public
9940,3906525,2018-02-28 14:55:32,"Porsche’s EV lead takes shots at Tesla while hyping the Mission E0Porsche is readying its Mission E for launch, with a 2019 target sales date. The all-electric Tesla Model S competitor has a lot of car fans excited, and has been drawing covetous looks since the concept’s unveiling back in 2015. Now that launch is drawing closer, however, we have some new info about the car, how it charges and its performance.Porsche EV lead Stefan Weckbach told a group of journalists that Porsche, unlike Tesla, is developing its car as a performance vehicle that can maintain top speed and reproduce acceleration reliably, specifically calling out Tesla’s vehicles’ ability to do 0 to 60 in under 3 seconds “only twice – the third attempt will fail,” reports Autoblog.He’s likely referring to a software-based restriction on use of Tesla’s Launch Control and aggressive acceleration features, which was limited both in terms of total uses and times it could be used successively to limit its impact on vehicle powertrain parts, as well as the Tesla’s battery. Tesla lifted this limitation in response to customer complaints, however.Besides throwing some mildly outdated shade Tesla’s way, Weckbach also talked about how the vehicle will quick-charge, aging back 250 miles of range during a 20 minute charging session, though that will take a lot of infrastructure expansion in the U.S. and abroad. Tesla’s investment in its Supercharger network begins to look like a smart hedge and early lead in this context.Porsche’s Mission E arrives next year, as mentioned, so it’s basically time to get excited. Tesla fans likely be swayed by cheap shots at their beloved cars, but a battle on performance merits does look to be brewing.",Porsche’s EV lead takes shots at Tesla while hyping the Mission E
9941,3906603,2018-02-28 20:40:59,"TNW SitesDemon’s Souls is a perfect example of why preserving online games is so importantRecently, we reported on the conflict between game publishers and game preservationists — specifically, their disagreement over whether online games should be preserved in their original state. When speaking about the kinds of online games historians might want to keep, we used MMORPGS as an example. But there are multiple kinds of online interactions, even in single player games — and some of those are starting to die off as well.As an example, the servers for From Software’s Demon’s Souls shut down today, after 8 years of being online. Now players won’t be able to use any of the game’s online features.Attention all Slayers of Demons: As previously announced, at 09:00 UTC on Feb 28, 2018, the Demon Souls servers will go offline.Our humble thanks for your support over the last 8 amazing years. Please make the most of the remaining time so you may live without regrets.Demon’s Souls, the precursor to the popular Dark Souls series, doesn’t, on the surface, seem like the kind of game that needs online features: It’s a linear, story-driven action RPG that can be finished by a single player, even now that the servers have shut down. All of the online features are, in theory, completely superfluous.That said, Demon’s Souls multiplayer gave the entire game world atmosphere it wouldn’t otherwise have had. Players could invade each other’s games, assist each other in combat, and leave messages for each other. It gave the impression that players had each other as a tether to sanity in an insanely brutal world.Without that interaction, the game is still technically playable, but it loses something. If, for example, the Museum of Art and Digital Entertainment could maintain a server, future players could see what the game is like for those who originally played it.If the Museum doesn’t get the chance to preserve Demon’s Souls‘ online features, then we can only hope From Software’s sophomore effort will wind up on the Nintendo Switch, just as Dark Souls has — and with new servers.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Demon's Souls is a perfect example of why preserving online games is so important
9942,3906604,2018-02-28 20:34:58,"About TNWTNW SitesApple patent details dual display laptop with no physical keyboardA recently granted patent shows Apple is exploring the idea of a “dual display” laptop or iPad that eschews physical keys. Titled “dual display equipment with enhanced visibility and suppressed reflections” the patent details a device that could act, essentially, as a giant Touch Bar — switching between a keyboard and content-specific controls.The documentation shows two different configurations: one with a permanent hinge and another that allows the second screen to be removed and used separately, a la Microsoft’s Surface Pro and other two-in-one devices. It also says the device’s main screen would be an OLED display with the secondary device being an LCD screen.It’s an interesting idea, one that could turn the space once reserved for typing into a highly configurable workspace with numerous options for artists, video editors, and casual web browsers. Apple’s Pencil, for example, could be used as a primary input device on a MacBook, and artists could make use of digital tuning knobs, color palettes, and drawing tools that make better use of the space below the main screen.But it’s also forcing users to type on a flat surface without the tactile feel of physical keys. So, there’s that.As usual, it’s worth reminding everyone that a patent is just an idea, and one that is equally likely to end up in the trash as on a future Apple product. And this one, honestly, seems more likely for the former than the latter — at least until someone figures out a way to make flat keys “feel” like real ones.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Apple's new patent may hint at the death of a physical MacBook keyboard
9943,3906605,2018-02-28 19:49:58,"About TNWTNW SitesGoogle teaches AI to fool humans so it can learn from our mistakesFooling robots into seeing things that aren’t there, or completely mis-categorizing an image, is all fun and games until someone gets decapitated because a car’s autopilot feature thought a white truck was a cloud.In order avoid such tragedies, it’s incredibly important that researchers in the field of artificial intelligence understand the very nature of these simple attacks and accidents. This means computers are going to have to get smarter. That’s why Google is studying the human brain and neural networks simultaneously.So far, neuroscience has informed the field of artificial intelligence through endeavors such as the creation of neural networks. The idea is that what doesn’t fool a person shouldn’t be able to trick an AI.A Google research team, which included Ian Goodfellow, the guy who literally wrote the book on deep learning, recently published its white paper: “Adversarial Examples that Fool both Human and Computer Vision.” The work points out that the methods used to fool AI into classifying an image incorrectly don’t work on the human brain. It posits that this information can be used to make more resilient neural networks.Last year when a group of MIT researchers used an adversarial attack against a Google AI all they had to do was embed some simple code into an image. In doing so, that team convinced an advanced neural network it was looking at a rifle, when in fact it was seeing a turtle. Most children over the age of three would’ve known the difference.The problem isn’t with Google’s AI, but with a simple flaw that all computers have: a lack of eyeballs. Machines don’t “see” the world, they simply process images – and that makes it easy to manipulate the parts of an image that people can’t see in order to fool them.To fix the problem, Google is trying to figure out why humans are resistant to certain forms of image manipulation. And perhaps more importantly, it’s trying to discern exactly what it takes to fool a person with an image.According to the white paper published by the team:If we knew conclusively that the human brain could resist a certain class of adversarial examples, this would provide an existence proof for a similar mechanism in machine learning security.Credit: GoogleLeft: original image of a cat. Right: The same image successfully manipulated to fool humans into thinking they’re seeing a dog.In order to make people see the cat as a dog the researchers zoomed in and fudged some of the details. Chances are, it passes at-a-glance, but if you look at it for more than a few seconds it’s obviously a doctored-up image. The point the researchers are making is that it’s easy to fool humans, but only in some ways.Right now people are the undisputed champions when it comes to image recognition. But completely driverless cars will be unleashed on roadways around the world in 2018. AI being able to “see” the world, and all the objects in it, is a matter of life and death.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",Google is teaching AI how to fool humans so it can learn from our mistakes
9944,3906606,2018-02-28 20:00:46,"About TNWTNW SitesOnePlus 6 ‘leak’ shows glass back and iPhone-like notchIt was inevitable. After the iPhone X, other manufacturers were bound to copy the camera notch, and now it seems OnePlus may be one of them.A couple of images showed up on Chinese site ITHome showcasing what’s reportedly a OnePlus 6 (or perhaps OnePlus 7, according to the translation). Of immediate note are the iPhone X-like notch and glass back, and the fingerprint sensor has taken a more oblong shape.Also note that super tiny bottom bezel, which should make it easier to reach the top of the screen. Spec-wise the device appears to have 6 GB of RAM and 64 GB of storage – the same as the most recent two OnePlus devices.The glass back is an unusual choice for OnePlus, which has used metal or plastic on all of its devices except the short-lived OnePlus X back in 2015. The move to a glass back could suggest the company is finally considering adding wireless charging – or maybe it just wants to switch things up. But I’d hope it didn’t opt for a more fragile material without a good reason.Of course, this could very well just be a prototype, so take it all with a grain of salt – if it’s even a legit leak in the first place. The OnePlus 6 wouldn’t be due for several months according to the typical release cycle, so anything can change.Still, the notch isn’t entirely unreasonable; Android P is expected to include native support for formatting apps around the notch. If this leak holds up, the next OnePlus might be the most radical design departure for the company yet.",OnePlus 6 'leak' shows glass back and iPhone-like notch
9945,3922961,2018-03-01 14:30:00,"A promising car that’s a work in progress.Model 3Get more infoEngadgetUsersI was standing next to the Model 3 when a guy on a bike rode by and yelled, ""How is it?"" My typical interactions with people who ask about Tesla's affordable sedan (so many people ask me about the car) typically take about five minutes. I point out the highlights and issues I've encountered while driving. Without thinking, I threw him a thumbs up. It was a gut reaction to a car I've come to adore but have also been confused by. I should have yelled, ""It's complicated!SummaryGallery: Tesla Model 3 review | 39 PhotosTo say the $35,000 Model 3 is important to Tesla would be an understatement. Judging by its pre-orders (the highest the industry has seen), it's already the most anticipated car ever. It's the culmination of CEO Elon Musk's nearly 12-year-old ""master plan"" to bring an affordable EV to market. Now that the Model 3 is here I can positively say it's a joy to drive, but it's also frustrating to do some of the simplest things in the cockpit. During a single trip I went from having an electric-motor-fueled grin on my face to throwing my hands up in exasperation while trying to adjust the cruise control follow distance. So yeah, it's complicated.The relationship (and if you follow any Tesla forums or even hang out at a Supercharger station, you know the owners are in a relationship with their cars) starts out like all new affairs: filled with excitement. Behind the wheel, the Model 3 is the best Tesla. I prefer it to the Model S (yes, even the P100D) as well as the Model X. It's quick but more importantly nimble. It handles corners and is far lighter on its tires than the S or X. It doesn't have the neck-snapping speed of Ludicrous mode, but the Long Range version's 307 pounds of torque coupled with 271 horsepower make for a car that's just asking the driver to punch the accelerator and hit a few curves.The Model 3 also has a very smooth and nearly silent ride. It's quieter than most other EVs, and it glides almost as well as a mid-tier luxury sedan. With a range of 220 miles for the base model and 310 for the Long Range edition, you're going to be spending a good amount of time behind the wheel instead of sitting at a Supercharger.Yet no matter how engrossed you are in the act of driving, the interior of the Model 3 will probably manage to confound you at some point. Some things are just too complex.The act of driving is more than just pressing the accelerator and pointing a car toward its destination. While you're on the road you turn things on and off and adjust various settings. At any given moment, I was changing the temperature or the volume of the radio, adjusting mirrors, using navigation, fine-tuning cruise control and sometimes even adjusting my chair. It's a dynamic experience. As you'll quickly find out, Tesla has moved some of these controls to submenus on the Model 3's giant 15-inch display.To be clear, there are hardware controls in the car, including two buttons on the steering wheel and two stalks on the steering column. The right stalk controls the transmission and turns autopilot on and off. The left stalk is the blinker control and is used for a single wipe and cleaning the windshield. The buttons are programmable by Tesla, but the left one is mostly for controlling your media. But that's it, and the hardware situation (or lack thereof) creates a bunch of issues pertaining to usability.The first time I noticed this I was using Tesla's semiautonomous Autopilot feature. The controls for adjusting the top speed and follow distance of adaptive cruise control are on the wheel and steering stalk on the Model S and X. They're easy to adjust without looking. On the Model 3 you have to tap the display to adjust the cruise control speed. Even worse, you have to go into a submenu to adjust the adaptive cruise control follow distance. First you tap on the car icon, then find and tap on Autopilot in the screen that pops up. Finally, you adjust the follow distance by tapping plus or minus. That's not even close to a better experience, and it's potentially dangerous, because instead of using muscle memory to find and adjust a physical item, you're reaching and tapping while looking to the right.Plus, a few times instead of tapping Autopilot, I hit something else because of a bump. That's an ongoing issue with in-car touchscreens that every automaker should be aware of. But it didn't stop Tesla from making that display the car's primary input device.Adjusting the mirrors also requires a few taps before they become adjustable via the two steering wheel buttons. Typically these are fixed before you start driving, so it's not that bad. But the wipers, that's a different story.As noted earlier, you can enable a single swipe or a cleaning of the windshield via the left stalk. But anything beyond that requires using the touchscreen. The Model 3 does have automatic wipers, which work well most of the time. During a rainstorm, however, it took longer than I was comfortable with to engage and didn't wipe fast enough for my liking. To speed them up, I had to reach over and tap the speed I wanted.If this were any other car or automaker, that would be it. I'd tell you that these usability issues make an otherwise amazing car a pain to drive. But Tesla's cars evolve while sitting in your driveway, and apparently, it's already working on these issues.The ability to adjust the adaptive cruise control speed is coming to the right hardware button on the steering wheel. The wipers are also getting an update that puts a shortcut to start them on the left stalk. Both of these will arrive in the form of over-the-air updates. No word on whether the adaptive cruise control follow distance will also receive hardware-button support.This is where the company shines: the ability to take feedback from owners and reviewers like myself and fix things with a software update. But I can't review a car based on what it'll do in the future. I have to make my assessment based on what's available now. And right now, I'm bewildered as to why these decisions were made in the first place.I'm also annoyed by the lack of a dash cluster behind the steering wheel. The display that was in front of the driver on the Model S and X has been relocated to the left side of the touchscreen display. So you're looking to the right not only to see your speed but also what gear you're in and what the sensors see (which is especially important while parking) as well as to check on blinkers. It's weird for the left blinker to be flashing in your right eye.The lack of a display ahead of the driver is even more noticeable at night. After decades of having something glowing behind the steering wheel, complete darkness from that part of the car is eerie.Tesla isn't the first automaker to move the speedometer to the center of the dash. Mini did it decades ago. In fact, I own a Mini with the speedo in the middle of dash. I hate it. I'm not a fan. It wasn't enough to dissuade me from buying and enjoying my car. But it's something to keep in mind when you're thinking about the Model 3.These are dramatic changes from the Model S and X. Thankfully, though, some things do carry over from those two vehicles. Even though this is the affordable Tesla (its starting price is less than half the price of the base model Model S), if you plunk down $5,000 for the Autopilot option, you're getting the same experience as the other models. Tesla continues to refine its semiautonomous system, which is one of the best in the industry. The Model 3 had very few problems staying centered in its lane and tracking cars. The auto-lane-change option felt a bit more aggressive than the last time I drove a Tesla. Most of the time it was fine, but in one instance it quickly changed lanes, then immediately started braking enough to catch everyone in the car and the vehicle behind me off guard. It's a reminder that none of these systems are ready to be left on their own.Another place where the Tesla excels is its voice control. Even when I flubbed the names of locations, it figured out what I was trying to say. ""Umpire State Building"" was quickly changed to ""Empire State Building"" based on context. It did an outstanding job handling building numbers and parsing the names of friends I wanted to call. Currently, the feature controls navigation, the phone and media playback. But Musk said on Twitter that in the future, voice control would be integrated more into the vehicle's control, which is great news. But it'll be better when that actually happens.Definitely. You will be able to do pretty much anything via voice command. Software team is focused on core Model 3 functionality right now, but that will be done soon, then we will add a lot more features.That's what's so infuriating, because being behind the wheel of the Model 3 puts a smile on my face. It delivers on so many levels, and yet when I have to do things that would be simple in any other car, here it involves tapping and swiping when I should be looking at the road. That's frustrating. If this were a horrible car, it would be fine: You chalk it up to an overall failure of the automaker. A bad vehicle is a bad vehicle. But it's not. It's a great car held back by weird decisions. Tesla says it's working on these issues. That's great, but I can't judge a car I haven't driven yet.So I have to caveat the hell out of this car. It's great on the road and the tech is top notch, but using some of the most important features are a pain and potentially dangerous to operate. The plus side (but not really) is that if you order the car today it'll be 12 to 18 months before it's delivered. By then, hopefully, Tesla will have a user interface that's worthy of a car as great as the Model 3. Because right now, it's complicated.",Tesla Model 3 review: the fast and infuriating
9946,3922962,2018-03-01 15:00:00,"The best cameras under $1,000Enthusiast cameras like Sony's A7R III, the Nikon D850 and Fujifilm's X-H1 get a lot of attention. The good news is that nowadays, you can spend less than $1,000 for a camera body and get almost as much as you would with a model with three times the price. Cameras like Nikon's D5600, the Sony A6300 and Fujifilm's X-T20 handle both photos and video superbly. As you'd expect, though, each model has a different combination of strengths and weaknesses. This guide is meant to help you figure out which best fits your needs.The basicsBuying a camera revolves around three decisions: Do you want a fixed or interchangeable lens camera? If it's the latter, would you rather have a mirrorless or DSLR? And finally, what size sensor do you want: full-frame, APS-C, Micro Four Thirds or smaller?If you want the lightest camera possible for travel or street photography, consider a compact, fixed-lens model. Many, including Panasonic's Lumix ZS200 and Sony's RX100 V, are highly pocketable but provide excellent quality. And just because you're getting a fixed lens doesn't mean you have to settle for a small sensor. Panasonic's excellent LX100 comes with a Micro Four Thirds chip and costs less than $1,000.DSLRs offer a direct, optical viewfinder and generally focus more quickly than mirrorless models. That makes them ideal for sports and wildlife photography, but many folks might find them too heavy to take on vacation. Unfortunately, Canon, Nikon and Sony don't sell any DSLRs with 4K for less than $1,000.Those looking for the best video quality will thus need a mirrorless camera. Luckily, there are great midrange models from Panasonic, Olympus, Sony and (finally) Canon. Mirrorless cameras are better than DSLRs if you're looking to travel light, as many models are highly compact when equipped with small pancake lenses.If portraits, videos and art photography are your passion, you'll probably want the largest sensor possible. That makes it easier to separate your subject by blurring out the background, creating a bokeh effect. Full-frame sensors are best for that, but there's only one model under a grand, Sony's original A7. APS-C is the next-best choice and creates more bokeh than Micro Four Thirds.Best midrange mirrorless camerasThe mirrorless midrange category can be confusing, because there are so many excellent models. Unlike with DSLRs, it's easy to find 4K and in-body stabilization, and models from Sony, Olympus, Panasonic and Fujifilm are now crammed with innovative features.For instance, mirrorless cameras are closing the speed gap with DSLRs in terms of autofocus, thanks to a new generation of phase-detect sensor models from Sony, Fujifilm and other companies. Electronic viewfinders (EVFs) are better than ever and arguably superior to optical viewfinders because you can see the final result. Basically, then, most camera innovation is happening in the mirrorless category, making it the most worthy of your consideration, in my opinion. Here are the best of the best.Fujifilm X-T20Olympus' O-MD EM-10 Mark III is a stellar camera, but Fujifilm's $900 X-T20 wins on several counts. First off, it has a larger APS-C 24.3-megapixel X-Trans III sensor that instantly buys you better bokeh and low-light performance. It offers a 2.3-million dot OLED EVF and touchscreen, which its pricier X-T2 sibling doesn't have, giving you tap to focus and shoot plus swiping and zooming to review images. It can focus in just .06 seconds and shoot in burst mode at 5 fps.There's also 4K video at 30 fps, a microphone input and various creative filters for the amateur crowd. And don't forget Fujifilm's impressive and reasonably priced lens collection. Most importantly for many of its fans, the X-T20 has classic, retro good looks and is easy to operate, thanks to no fewer than five dials for mode, exposure compensation, aperture, ISO and shutter speed. On the downside, the X-T20 lacks in-body stabilization.Olympus OM-D E-M10 Mark IIISpeaking of beautiful, easy-to-use and impressively specced cameras, let's talk about the $650 Olympus OM-D E-M10 Mark III. It beats the X-T20 in one key area: Namely, it has 5-axis in-body stabilization, something sadly lacking in all Fujifilm models except the new X-H1. The OM-D E-M10 Mark III also has some drawbacks like the smaller 16.1-megapixel, Micro Four Thirds sensor and lack of a microphone port.It equals the X-T20 with 4K video and features a 2.36-million-dot viewfinder and the same image processor as the one on Olympus' super-quick OM-D E-M1 Mark II. It's also small and light, and Olympus has a wide and impressive selection of Micro Four Thirds lenses, including the new f/1.2 prime lens lineup with enough bokeh to help you forget about that smallish sensor. As with the X-T20, the E-M10 Mark III is lovely and packs manual dials, saving you from plunging into menus.The drawbacks are a clumsy menu system, lack of mechanical dials and poor battery life. All told, though, shooters who do both video and audio will be better off with the A6300 than many of the pricier models out there.Honorable mention: Sony A7Sony's original A7 is still available for $800 (body only) and less than $1,000 with the 28-70mm f/3.5-5.6 kit lens. So why would you buy a circa-2013 camera? Well, it has a 24.3-megapixel sensor and offers 1080/60p/24p HD video with both microphone and headphone jacks, crucial for videographers. Most importantly, it's a full-frame camera, giving you maximum bokeh and support for a growing number of full-frame lenses from Sony and others.Our top DSLR picksDSLRs are the weak point of the midrange-camera category, as they lack features like 4K and in-body stabilization that can be found in mirrorless and even compact models. Nikon finally released an APS-C model with 4K, the $1,300 D7500, so let's hope that trend continues in 2018. If you're looking to get a current midrange model for less than $1,000, however, you'll need to forget about those things and decide which cameras handle best, shoot the fastest and have the best-quality images for your needs. Here are our top three picks.Canon EOS 77DCanon's $800 EOS 77D and Nikon's D5600 are the best midrange DSLRs released recently and are close in specs, but the 77D gets the nod as our top pick. Released last year, it inherited some nice features from Canon's higher-end models. That includes a fast, accurate 45-point phase-detection system and Canon's Dual Pixel AF -- the best in the business for live view and video modes. Max burst shooting is 6 fps, and ISO tops out at 51,200 (extended), though shots beyond ISO 12,800 are extremely noisy.If you can live without 4K and settle for 1080p at 60 fps, the 77D actually has a lot to offer for video. You get surprisingly good electronic stabilization that works in concert with the optical lens IS, a flip-out screen for easier self-shooting and vlogging, and fast, accurate Dual Pixel autofocus. It also has WiFi, Bluetooth and NFC, so it's easy to remotely control the 77D and download photos.Nikon D5600Engadget and plenty of other sites have given Nikon grief for its lack of innovation compared to rivals like Sony. The $650 D5600 is a solid case in point, being merely a minor refresh over its predecessor, the D5500, and lacking features like 4K and in-body stabilization. But that's where the DSLR world is right now, and the D5600 still has enough to recommend it.Namely, you get excellent dynamic range and detail from the 24-megapixel APS-C sensor that, don't forget, is a touch larger than those on Canon cameras. The 34-point autofocus system for regular photo shooting is excellent, though Nikon is lagging well behind Canon's Dual Pixel autofocus for video and live-view shooting.Most importantly, perhaps, the D5600 is about as light as a DSLR gets, so it's feasible to lug it along on your vacation. It also has a nice touchscreen that flips around, making selfies and vlogging manageable. Finally, Nikon offers decent wireless connectivity for the D5600 via its SnapBridge option.Hopefully, 2018 will be a better year for Nikon. The D850 was a promising start, and the company needs to put features like 4K and five-axis stabilization into lower-end models to match Olympus, Sony and other rivals.Canon EOS Rebel T7iTrying to find the third-best DSLR shows the problems with this category. A few cameras, like Nikon's D7200 and Canon's 80D, are worth considering, but they were released in 2015 and 2016, respectively. In a market where things change so quickly, I chose Canon's $750 EOS Rebel T7i.While it doesn't ooze excitement, the T7i does key things well. It's nearly identical to the EOS 77D, with 6 fps shooting speeds, WiFi, NFC and Bluetooth connections, a top ISO of 51,200, and a flippable screen. It focuses quickly and accurately whether in regular or live view/video mode, thanks to the phase-detect and Dual Pixel autofocus systems. And yet it's much smaller and lighter, making it a good travel camera for folks who must have a DSLR.Compact camerasAnyone who's wielded a Sony RX100 or Leica Q can understand why photographers are passionate about fixed-lens, compact cameras. Being able to whip out a camera and instantly yet discreetly snap great photos is a godsend for travelers and creative photographers.Manufacturers keep upping the ante in this category in order to sway photographers away from smartphones. As a result, there are some astonishingly great models for less than $1,000 (and many that cost more, like Fujifilm's X100F and Canon's G1X Mark III, but I'll save those for my forthcoming enthusiast guide).Sony RX100 VAs with the mirrorless category, it's tough to choose a winner here. But the $980 Sony RX100 V's pure technical competence and tiny size make up for its operational minuses, which is why it's often used as a second camera by professional photographers.Where to start? The 20.1-megapixel, 1-inch sensor RX100 V can shoot an astonishing 24 RAW photos per second and handle 4K, 30 fps video with a full sensor readout. Low-light capability is great for a compact, aided by the f/1.8-2.5 24-70mm lens, and it uses Sony's latest BSI sensor tech with 315-point phase-detect AF.That said, the RX100 V is pricey, and meanwhile the handling is clumsy compared to models from Panasonic and others. Sony needs to redesign its menu system, in my opinion, but seems to have resisted doing much with it so far because sales have been so good.Panasonic Lumix ZS200I picked Panasonic's Lumix ZS100 as one of the top compact cameras for my recent camera-buying guide. Since then, the company has released the ZS200, which succeeds and improves on it in most ways. You still get a 1-inch, 20.1-megapixel sensor; 5-axis in-body stabilization; a 2,330K-dot EVF; and 4K video at 30 fps.What the new model brings is a new L.Monochrome black-and-white shooting mode and a 15x, 24-360 zoom (35mm equivalent), a big jump from the last model's 10x, 25-250x lens. In exchange, unfortunately, you lose a stop of speed, jumping from f/2.8-5.9 to f/3.3-6.4. Nevertheless, the wider zoom range will probably be more useful for the ZS200's intended tourist market, helping vacationers snap distant wildlife or beach action.Panasonic Lumix LX100Panasonic's aging LX100 gets on this list because it's the only model for less than $1,000 with a sensor larger than one inch. It packs a Micro Four Thirds, 12.8-megapixel chip and f/1.7-2.8 24-75mm equivalent lens, letting you easily separate subjects from backgrounds with plenty of bokeh.Despite hitting the market in 2015, the LX100 also offers 4K video, embarrassing more-recent models from the likes of Canon. It also looks great and has physical dials and buttons for easier handling. The LX100 is a bit large and the zoom range is limited, however, so it may not be the best option for travelers. On the other hand, it's a perfect street- and people-photography camera.Wrap-upFive hundred dollars to $1,000 is a sweet price spot for camera buyers right now, and you can't go wrong with any of my picks. That said, unless there's a particular reason you need a DSLR, I would avoid that category and stick with mirrorless models instead. The latter are more technically competent, especially for video, and are lighter and easier to carry, to boot.Meanwhile, there's no longer a need to look down on fixed-lens compacts. The models mentioned here are not a hassle at all to take on holidays but are capable of putting smartphone photos to shame.Steve should have known that civil engineering was not for him when he spent most of his time at university monkeying with his 8086 clone PC. Although he graduated, a lifelong obsession of wanting the Solitaire win animation to go faster had begun. Always seeking a gadget fix, he dabbles in photography, video, 3D animation and is a licensed private pilot. He followed l'amour de sa vie from Vancouver, BC, to France and now lives in Paris.","The best cameras under $1,000"
9947,3922963,2018-03-01 20:45:00,"The first Android Go phones blend low prices with lots of promiseThe world's first Android Go phones debuted here at Mobile World Congress, and they just might change the way Android works for the developing world. We took some time to get acquainted with some of these new devices, but in case you're not entirely up to speed on Go itself, we've prepared this handy primer.What is Android Oreo Go Edition?Easy. In short, it's a customized build of Android Oreo meant to run better on inexpensive phones. (It's also more generically referred to as Android Go, and I'm going to stick with that name.) Google announced it at its I/O developer conference for devices with 1GB or less of RAM, but it's appealing for more than just the way it manages performance.Android Go doesn't take as much as space as typical Android, for one, so you don't have to worry as much about running out of memory. Data usage is more of a concern in some markets than others, so Google has also released a handful of Go-optimized apps (think YouTube, Google Maps, Gmail and more) that attempt to minimize data costs. Hell, even the version of the Play Store on Go phones has been tweaked -- it highlights apps Google knows will run well on these more limited devices.YNETWhy does this matter?Let's say you wanted to buy a brand new Android phone for $100 -- there's a strong chance you'd wind up with a phone running a slightly dated version of Android. Those phones (obviously) lack some of the features found in up-to-date models. Even though everyone (more or less) has access to the same apps, the experience of using Android can differ pretty wildly depending on the hardware you're working with.Android Go, then, is an attempt to level the playing field a little and ensure that the Android experience is more consistent across all kinds of devices. Android Go won't make a $100 phone feel as fast as a flagship, but it'll bring important Oreo features to a wider audience and help make sure inexpensive phones aren't awful to use. That's the idea, anyway.What's using an Android Go phone like?Now we're getting to the fun stuff. Brands like ZTE, General Mobile, Micromax and Lava have brought the first Go phones here to MWC, but I spent most of my time playing with two phones in particular. We saw Alcatel's 1X early on in the show, and it's fascinating because it squeezes a 5.3-inch 18:9 screen into such an inexpensive body. That display ran at what Alcatel calls ""VGA+"" so it's not the crispest thing you'll ever see, but for €99 (or about $120) that's no great sin.Driving the phone was a quad-core MediaTek chipset with 1GB of RAM, and it ran generally well... at least until I started firing up too many apps and jumped between them a lot. Alcatel later told me this was because the phone's software was far from final, but it was a reminder that Android Go's existence doesn't obviate the need for good software engineering.Gallery: Meet the first Android Go phones | 11 PhotosThe 1X is also a great example of the kind of control Google doesn't exercise over its Android Go partners. Alcatel's Go phone uses a non-stock launcher and has a handful of preloaded apps, so nothing but good sense is preventing device makers from painting over (and potentially slowing down) stock Oreo Go.I was more fond of the Nokia 1, a small, charming device with a surprisingly nice 4.7-inch screen. When I first walked into Nokia's booth to take a look at the thing, I thought the company had put dummy phones up on display because the 1's screen looked so crisp and nicely saturated. The joke's on me, I guess. The Nokia 1 uses a quad-core MediaTek chipset and 1GB of RAM as well, but overall it felt much more stable and smooth-running than the Alcatel 1X. Launching apps sometimes took an extra moment, but it was never frustrating and I couldn't trip it up by jumping between multiple apps the way I could with the Alcatel. The new version's 5-megapixel camera is obviously not great and its polycarbonate design can feel lacking, but all told it's a strong (and cute) package for just $85.Chris Velazco/EngadgetAnd there are Google's Go Edition apps. They're low-impact versions of existing apps and in the case of Gmail and YouTube, they seem to work just as well as their standard counterparts. YouTube Go worked fine as well, though I didn't get a chance to try its best feature: the ability to download videos and share them with other devices. I put one relatively new Go-optimized app through its paces, though: Google Assistant. Originally exclusive to Google's Pixel phones, the company announced it would expand Assistant support to other Android phones at MWC last year. The catch? Among other things, Android phones needed at least 1.5GB of RAM to use the feature.Not anymore, though. Since most of Assistant's work happens in the cloud, it functioned just as well on this $85 phone as it does on my Pixel 2 XL. The only real difference is that you can't invoke it with an ""OK, Google"" -- it requires a tap on the app icon. Even so, it's pretty great.So, what happens next?That's a tough one -- none of these devices have gone on sale yet, and the actions of the companies that make them depend on people's response to them. Google, however, told the Indian Express it would make Android Go versions of all future Android builds since it's core to the company's commitment to entry-level devices. You might have only just heard of Android Go, but it's not going anywhere.You can also expect some super-cheap Go phones to appear, too -- Google suggested we'd see some sell for as low as $50, but the least expensive we've seen so far is the $80 ZTE Tempo Go. (Turkey's General Mobile India-based Lava and Micromax haven't said how much their Go phones will cost.) We're cautiously optimistic. This is early days for Android Go, and it may well change how people around the world get to experience Android. The lack of control over what OEMs can do to Go Phones may mean some problems low-end phones face are never resolved. We'll just have to wait and see.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",The first Android Go phones blend low prices with lots of promise
9948,3922964,2018-03-01 13:36:00,"An LG G7 showed up in Barcelona after allAfter revealing an AI-powered version of its late-2017 flagship, LG seemed to be having a quiet MWC. As it turns out, the real action was happening elsewhere. Israeli news site YNET posted a brief hands-on of a smartphone called the G7 (NEO) that appears to pack a Qualcomm 845 chipset and a 6-inch, 19.5:9 OLED screen with a notch carved into the top. YNET's report further suggests the device could come with either 4GB of RAM and 64GB of storage, or 6GB of RAM and 128GB of storage, as well as a 16-megapixel dual camera. Given its name and its apparent horsepower, this just might be the new flagship phone LG was planning to reveal after MWC, though the company stopped well short of confirming that.An LG spokesperson told Engadget that the phone in question was a ""concept,"" and that LG never had this particular device at its main MWC booth. (That sadly means we couldn't talk our way into some hands-on time of our own.) That spokesperson also added that ""carriers and business partners always get to see early concepts"" at trade shows like this one, which may indicate LG is trying to firm up its sales channel partners ahead of an upcoming announcement and launch.Had LG shown this device off at MWC proper, it would've been in good company. Quite a few smartphone makers embraced notched screen designs this year. While some are relative unknowns -- like Noa and Leagoo -- ASUS notably paired a notched display of its own with a Snapdragon 845 in its new Zenfone 5Z. Like it or not, it looks like 2018 is going to be the year of the notch.Given the G7's concept status, though, it's possible LG may not join the notch club after all. VentureBeat's Evan Blass noted in a tweet that this might be a version of the G7 that was ultimately scrapped in favor of the ""Judy"" design and its super bright, low power display. If the rumors hold true, the Judy flagship will be announced sometime in June -- looks like we won't have to wait too long to start getting some actual answers.",An LG G7 showed up in Barcelona after all
9949,3922965,2018-03-01 18:30:00,"Pico CGet more infoEngadgetCriticUsersPicoBrew has made a name for itself with a range of beer-making appliances that promise a ""set it and forget it"" experience. The company covers the gamut of brewing levels, from the Zymatic (now Z Series) for pros to the Pico Pro and Pico C for intermediate folks and beginners, respectively. While PicoBrew's entire product line offers WiFi connectivity for remote monitoring of the process, its PicoPaks make the whole process even easier by bundling all the ingredients into a simple, self-contained pod. However, as I found with the $499 Pico C, that simplicity has one big trade-off.Gallery: PicoBrew Pico C review | 20 PhotosIf you're not familiar with PicoBrew's gear, every model works more or less the same. The devices are the size of kitchen appliances (think Instant Pot for the smaller C and large convection oven for the Zymatic) and have a clear drawer that holds all the ingredients and works as a mash tun to create your wort. Each connects to a keg that serves as both boil kettle and fermentation tank. Once you put in your ingredients and add the proper amount of water, you select the recipe and hit start -- the machine does the rest. And a couple hours later (when your wort has cooled), you add your yeast and start turning that sugar water into beer through the magic of fermentation. It's this type of all-in-one setup that makes PicoBrew's line of products a compelling option -- at least for those who don't like tending to a brew kettle during the whole process.The Pico C's all-black exterior won't look too out of place if you leave it on your kitchen counter all the time. I actually prefer it to the silver/chrome combo on the Pico Pro, but I have black appliances, so it fits right in. The only real issue with leaving it out is that it takes up quite a bit of counter space: It's 16 x 12 x 14 inches (H x W x D). However, it did fit under my cabinets, so that means you can tuck it in a corner between brew sessions. You'll likely want to store the kegs and other gear elsewhere.SetupOnce you unpack everything, setting up for your first brew takes about 15 minutes. You'll have to connect to your home WiFi and register the unit before you do anything else. Then you run a 10-minute rinse cycle with distilled water and a brew keg. When it's complete, drain the water from the reservoir on top of the Pico C, dump it and rinse both the clear drawer (Step Filter) and brew keg. The system is now ready.PicoPaksIn order to make beer with the Pico C, you'll need a PicoPak. These prepackaged-ingredient containers debuted on the Pico Pro, and they make the process super easy for someone just wading into homebrewing. However, they can be incredibly limiting for someone with a bit of experience like myself. Yes, they are super convenient, but you have to order them from PicoBrew or a licensed retailer and wait for them to arrive. Depending on your shipping method, this could take about a week. That may not sound like a long wait, but it does mean you can't have a spur-of-the-moment brew day.Since the Pico C doesn't work with loose grains and hops, you can't just run out to your local homebrew shop on a Saturday morning. PicoBrew says ""a limited number"" of physical locations stock PicoPaks, but unless your local Best Buy has them on the shelf, you're out of luck. A few other retailers -- Bed, Bath & Beyond, Home Depot and more -- sell the PicoPaks online, but if you're having them shipped, you might as well order direct from the source.Basically brewing with the Pico C requires you to plan ahead -- something I'm not good at.If you're ordering from PicoBrew's website, you can choose from a selection of more than 100 packs based on recipes from professionals and homebrewers. There are versions of commercially available beers like Rogue's Dead Guy Ale and 21st Amendment's Brew Free or Die IPA, just to name a couple. You can also use a recipe of your own to create a so-called Freestyle PicoPak. Or if you need a jumping-off point, you can pick a base beer and adjust the hops and grains as you see fit. For example, if you wanted to make a single-hop IPA with Citra, you could start with the American IPA base and tweak the hops before ordering.There are some constraints though. I'm a big fan of Mosaic hops, so I initially wanted to use that base IPA recipe to make a single-hop beer when I hit a snag: PicoBrew doesn't offer Mosaic for the Freestyle option. In fact, there are only eight hop varieties available for those going the DIY route. I understand not being able to stock every type of hops, but the lack of popular ones like Chinook and Mosaic is disappointing. The company says it is constantly adding new recipes and will add to the Freestyle option as well. New brewers will eventually want to experiment, though, so expanding those DIY packs will be key.BrewingWhen you're ready to brew, all you have to do is turn the machine on and place your PicoPak in the Step Filter (the clear drawer). The Pico C automatically recognizes the Pak and pulls in the proper times and temperatures for that recipe. PicoBrew's instructions are clear and easy, but the device itself will walk you through the steps on the digital display, just in case. It'll even guide you through the cleanup process.Once everything is in place, simply hit start. The display will keep you updated on which step it's on and how much time is left. You can also track the process on PicoBrew's web-based BrewHouse interface. It isn't quite as handy as a mobile app, but it works just fine. The company says it's working on a dedicated app, but the software is currently in the testing phase, so there's no word on a release date yet.One thing you need to be aware of when brewing with the Pico C is how loud it is. During the brewing process, the machine is about twice as loud as my dishwasher. This means if I'm watching Electric Dreams in the next room while brewing in the kitchen, I have to really crank the volume on the TV. It's certainly not a dealbreaker, but it's something you should know before investing in the device. Once the brewing is complete and you've pitched your yeast, the fermentation process and the wait begin.Fermentation and the PicoFermThe PicoFerm is an optional $59 WiFi sensor that attaches to the top of a brew keg so that you can monitor progress remotely. You don't need this to make beer with a Pico (it's only available for pre-order right now), but it does allow you to track temperature and air pressure (attenuation) in your keg using the web app. The site will also tell you how many days are left in the fermentation process, though since it doesn't monitor gravity, there's some room for error.I used the PicoFerm on two of the three batches I made. If you don't use the sensor, you lose some of the guidance of the PicoBrew system -- and a big part of its appeal. You can still hit ""ferment"" on the website and it will estimate when the process is complete, but it's not actually monitoring anything. For some, that peace of mind is worth the extra cost. I will say that the PicoFerm worked well for me, consistently sending data while the beer was fermenting. The only time I had an issue was when I forgot to reconnect to WiFi after a couple weeks between batches, but that was pure user error.Kegging and bottlingWhen the beer is finished, which takes about 7 to 14 days depending on the recipe, it's time to keg or bottle. The Pico C comes with a three-liter serving keg that makes packaging your brew super easy. Just select the correct mode in the menu and the device will pump your beer into the mini keg or bottles for you with the included tools. If you've homebrewed before and used a racking cane and bottling wand, this is similar but a lot less messy.There's a force-carbonation kit that you can use to infuse your beer with CO2 in about 36 hours, or you can go old school and use priming sugar to condition your beer naturally over the course of about two weeks. Carbonation can be fickle, and even after a few batches, I wasn't able to dial in the levels I wanted. Both methods worked okay but weren't precise. That's not necessarily a knock against PicoBrew; sometimes these things just need to be fine-tuned over time, especially when you're using a new package size and brewing system.I'll admit I was skeptical of what the Pico C could produce in terms of overall quality, but I was pleasantly surprised when I tasted the results. Each batch was pretty solid, but the batch based on Tallgrass Brewing's Buffalo Sweat Oatmeal Cream Stout and my Freestyle PicoPak single-hop Citra IPA were the standouts. The Pico C makes brewing easy, but most importantly it makes good beer.CompetitionThe Pico C is limited, though, and the initial $499 investment is pretty steep. For as little as $60 you can put together a beginner's brewing kit of your own or buy a prepackaged one. Yes, it's a bit more labor intensive to brew in your kitchen or on a burner in the garage, but you could certainly use that extra money to buy more ingredients or expand your system to make more beer or higher-quality beer. Of course, if you'd rather not futz around with it, there are plenty of semiautomated options.If you have a draft setup, consider splurging for the $799 Pico Pro. Instead of a proprietary Pico Keg, the Pro uses the more standard cornelius kegs (ships with two). But it brews the same amount per batch as the C and still requires you to use PicoPaks.If you'd rather not bother with the PicoPaks, the Zymatic (which will soon be replaced by the Z Series) will cost you. The Zymatic is priced at $1,999, and the modular Z Series will start at $2,500 (discounted to $1,499 during the pre-order phase). Both options whip up 2.5-gallon batches, but the Z Series can scale up to 10 gallons total in 2.5-gallon increments.If you're a little more serious about brewing but still want some automation, there's the Grainfather. You'll have to supply your own fermenter, but glass carboys are cheap and even a bucket from Home Depot will do in a pinch. But otherwise the Grainfather is an all-in-one setup where you can mash, boil and cool your wort with precise temperature control. You can make batches up to 25 liters (6.6 gallons) and control everything from an app. You can even set a timer: Fill it up with water before bed and wake up in the morning ready to start brewing. But at $1,000, it's definitely targeted at experienced homebrewers. Brewie is another automated all-grain option that lets you use whatever ingredients you like, but it too is very expensive ($1,900), so it's made for serious homebrewers as well.Wrap-upPicoBrew set out to make a Keurig-like device for brewing beer, and it has largely succeeded. The Pico C is an easy-to-use machine that makes good beer on your kitchen counter while you watch TV or work on other things. Beginners will definitely appreciate the simplicity and hand-holding, but if you get the urge to experiment, the C could get frustrating. Currently, the PicoPaks have a very limited selection of ingredients, especially hops. Plus you have to really plan ahead. If you're good at that, then you won't have too many headaches. I, however, am not, so relying on PicoPaks is a major inconvenience. Even though the machine works great and it consistently made good beer, I just couldn't get past that limitation.A tech writer by day and a graphic designer by night, Billy was ushered into the gadget world by an Atari and its vices: Frogger and Grand Prix. Little did he know that this was training for the undefeated seasons he would string together in NCAA Football (RIP). He covers the audio beat, spanning everything from headphones to streaming. He's also a Cheez-It expert.",Pico C review: Making homebrew beer easy for beginners
9950,3923055,2018-02-26 00:00:00,"How Modern Art Serves the RichMore art is being produced and sold than ever before, at ever higher prices.February 26, 2018During the late 1950s and 1960s, Robert and Ethel Scull, owners of a lucrative taxi company, became fixtures on the New York gallery circuit, buying up the work of then-emerging Abstract Expressionist, Minimalist, and Pop artists in droves. Described by Tom Wolfe as “the folk heroes of every social climber who ever hit New York”—Robert was a high school drop-out from the Bronx—the Sculls shrewdly recognized that establishing themselves as influential art collectors offered access to the upper echelons of Manhattan society in a way that nouveau riche “taxi tycoon” did not.Then, on October 18, 1973, in front of a slew of television cameras and a packed salesroom at the auction house Sotheby Parke Bernet, they put 50 works from their collection up for sale, ultimately netting $2.2 million—an unheard of sum for contemporary American art. More spectacular was the disparity between what the Sculls had initially paid, in some cases only a few years prior to the sale, and the prices they commanded at auction: A painting by Cy Twombly, originally purchased for $750, went for $40,000; Jasper Johns’s Double White Map, bought in 1965 for around $10,000, sold for $240,000. Robert Rauschenberg, who had sold his 1958 work Thaw to the Sculls for $900 and now saw it bring in $85,000, infamously confronted Robert Scull after the sale, shoving the collector and accusing him of exploiting artists’ labor. In a scathing essay published the following month in New York magazine, titled “Profit Without Honor,” the critic Barbara Rose described the sale as the moment “when the art world collapsed.”In retrospect, the Sculls’ auction looks more like the beginning than the end. By today’s standards, the then-record-breaking prices—even adjusted for inflation—sound almost quaint. In November 2013, the Post-War and Contemporary evening sale at Christie’s totaled $691.6 million, setting a new record for the most expensive work by a living artist—$58.4 million for Jeff Koons’s sculpture Balloon Dog. Christie’s proceeded to break its own record auction totals twice more in the next year, with the November 2014 Contemporary sale raking in $852.9 million in a single evening. Meanwhile, this past fall, Christie’s sold a recently rediscovered painting attributed to Leonardo da Vinci for upwards of $450 million, making it the most expensive work ever sold at auction. It’s telling that Christie’s chose to include it in their Post-War and Contemporary sale rather than one devoted to Old Masters where it logically belonged, attesting to the privileged place of contemporary art within today’s art market.What was once a niche trade has expanded into a global industry bound up with luxury, fashion, and celebrity.In her 2014 book Big Bucks: The Explosion of the Art Market in the Twenty-First Century, the veteran art market reporter Georgina Adam surveyed the forces that propelled the stratospheric rise in the market for contemporary art, attempting to explain why, for instance, one version of Andy Warhol’s 1963 screen print Liz could sell for $2 million in 1999 and another from the same series for $24 million in 2007, only a few years later. What was once a niche trade overwhelmingly based in the United States and Western Europe has expanded into a global industry bound up with luxury, fashion, and celebrity, attracting an expanded range of ultra-wealthy buyers who aggressively compete for works by brand-name artists. “When I started out, 30 years ago, millionaires had boats and jets—but didn’t necessarily have any art at all,” Thomas Seydoux, the former chairman of Impressionist and Modern art at Christie’s tells Adam. “For the very wealthy today, it’s not fine not to be interested in art.”In her follow-up Dark Side of the Boom: The Excesses of the Art Market in the Twenty-First Century, Adam, a longtime editor at the Art Newspaper and contributor to the Financial Times, considers the negative effects this influx of money has had on the art itself. As contemporary art is increasingly viewed as an asset class—alongside equities, bonds, and real estate—Adam sees artworks often used as a vehicle to hide or launder money, and artists encouraged to churn out works in market-approved styles, bringing about a decline in quality.Art’s imbrication in networks of money and power is hardly a contemporary phenomenon. Many of the great masterpieces of Renaissance art, for instance, were commissioned by members of the nobility. The origins of the modern picture trade date arguably to the 17thcentury Dutch Republic, where, in the absence of monarchical or church patronage, artists began producing domestically-scaled genre paintings for sale on the open market. In the early 20th century, the art dealer Joseph Duveen—later the 1st Baron Duveen of Millbank—made a fortune selling Old Master paintings he acquired from cash-poor European aristocrats to wealthy American industrialists like Andrew Mellon. As Duveen famously quipped, “Europe has a great deal of art, and America has a great deal of money.” What has changed is speed and scale: There is, Adam argues, more art being produced and sold than ever before, as artists, galleries, and auction houses attempt to keep up with the demand of a new class of international “UHNWIs,” Ultra High Net Worth Individuals attracted by the lure of profit and prestige.DARK SIDE OF THE BOOM: THE EXCESSES OF THE ART MARKET IN THE 21ST CENTURY by Georgina AdamLund Humphries , 208 pp., $34.99As Adam describes, two significant changes at the end of the 20thcentury set the stage for today’s inflated contemporary art market. The first was the expansion of the base of potential buyers: The fall of communism in Eastern Europe and economic liberalization in countries like China and India created a new wave of billionaires eager to flaunt their wealth. In China, which has consistently ranked among the top three largest art markets by value since 2009, demand has also been boosted by a government-sponsored museum-building boom. Over 1000 new museums, a combination of state-run and private institutions, have opened in the past decade; as of 2017, there were approximately 200 privately owned museums devoted to contemporary art. Crucially, building private museums serves not only as a status symbol for the country’s elite, but a means of gaining state approval for lucrative real estate development deals.The second major change was the shift away from Old Masters and Impressionists as the core of the auction business. Historically, selling contemporary art had been the province of galleries and private dealers; the work of living artists went to auction only infrequently. But the major auction houses, Sotheby’s and Christie’s, recognized that promoting the contemporary market could open up vast new revenue streams. They began to function more like luxury brands. Christie’s was in fact purchased in 1998 by François Pinault, the owner of the European luxury retail conglomerate Kering whose brands include Gucci, Saint Laurent, and Balenciaga. The houses began aggressively hyping a never-ending flow of new inventory, and with it, a jet-set lifestyle of multi-million dollar auctions, exclusive gallery dinners, and VIP art fair vernissages.Auction houses also began to expand into financial services for this expanded range of “UHNWI” clients, offering collectors lines of credit, allowing them to borrow against the value of their collections, and sometimes selling works with third-party guarantees, in which the house effectively pre-sells a lot before the auction and may split some share of the proceeds with the guarantor if it ultimately goes for more than the agreed-upon price. The result is that the prices for contemporary art have risen higher and higher, which in turn attracts new buyers, many of them drawn in by the money alone. “I know some Rothko buyers—some of the key market makers today—who didn’t know who Rothko was until he reached $40 million,” says former Phillips auction house chairman Simon de Pury, quoted in Dark Side of the Boom. “When they saw a good work could sell in the $40 to $60 million range, they suddenly said, ‘this is really interesting!’”Critiques of the art market’s rise often revolve around anxieties about art’s commodification: What happens to the notion of art as a public good when its value is primarily measured in dollars? But that isn’t the only thing at stake. In the book’s final section, Adam examines the ways in which the notoriously secretive art business, coupled with lax regulatory oversight, has enabled vast sums of money to change hands without public scrutiny. She cites a number of high profile scandals involving money laundering, stolen property, and shady self-dealing. There is, for instance, the so-called “Bouvier Affair,” a legal dispute between the Russian oligarch Dmitry Rybolovlev and the Swiss “Freeport King” Yves Bouvier, the owner of an art shipping and storage empire, which is still unfolding in multiple international jurisdictions.The houses began aggressively hyping a never-ending flow of new inventory, and with it, a jet-set lifestyle.Like the doormen in Park Avenue co-op buildings, art shippers are frequently privy to the secrets of the extraordinarily rich. After meeting Rybolovlev in 2002, Bouvier offered to use his connections and insider knowledge to help him build a world-class collection by discreetly locating artworks and negotiating deals. In 2008, in the midst of a messy divorce and facing difficulties with the Russian government, Rybolovlev began pouring money into art with Bouvier’s assistance, shifting some of his wealth into portable assets. It later emerged that Bouvier had charged Rybolovlev multi-million dollar markups on his purchases. In one particularly egregious example, Bouvier charged Rybolovlev $118 million for Modigliani’s 1916 painting Nu couché au coussin bleu, but had acquired it for only $93.5 million. Rybolovlev accused Bouvier of fraud; Bouvier insisted, as Adam puts it, that “he was acting as a dealer, that this was clear to both sides, and that he was entitled to make what he could.”Yet the most troubling examples of the exploitation of art for financial gain are perfectly legal. As Adam outlines, collectors and their agents have continually found creative ways to use their art holdings to defer paying taxes, including the establishment of private museums and foundations, storing artworks in offshore freeports where they can be exchanged without incurring customs duties or VAT, and loopholes in the tax code such as “like-kind” exchanges. Originally set up in the 1920s to aid farmers by enabling them to defer taxes on livestock trades, “like-kind exchanges” are now regularly invoked by art collectors in order to avoid paying taxes on the sale of artworks: So long as a collector uses the proceeds of the sale of one work to purchase another within 180 days, the tax obligation can be perpetually kicked down the road.Adam brings both an insider’s perspective and a well-sourced reporter’s eye to Dark Side of the Boom, using legal documents, auction records, and firsthand interviews with art-world players to shed light on the seemingly inscrutable workings of the art market and the various tertiary industries—art advisors, investment funds, collection management software, storage, shipping, insurance—that have opportunistically sprung up around it. She opens chapters with scene-setting anecdotes—the unveiling of a freeport in Luxembourg, a lavish party celebrating a new Shanghai art fair—offering a privileged, behind-the-scenes view into the places where art’s value is produced and maintained.But while Adam paints a detailed and convincingly dire picture of the art world’s excesses, she never fully probes its implications. Perhaps ironically, its central weakness is her narrow focus on the activities of the art market itself: Her book largely brackets an exploration of the art market from the broader context of rising income inequality, economic exploitation, and staggering concentrations of wealth in the hands of the very few, all of which have enabled activity at the its upper reaches to continue unabated despite global downturns in other financial sectors. According to the sociologist Olav Velthuis, the art market ultimately benefits from an unequal distribution of wealth, as newly minted billionaires turn to blockbuster art purchases as a means of announcing their arrival.Moreover, the question of where the money comes from and where it ultimately goes is only a passing concern here. It’s no coincidence that the world’s most prominent art collectors include Walmart heiress Alice Walton; the Sackler family; Poju Zabludowicz, whose family fortune has its origins in arms dealing; and hedge fund founder Daniel Och, whose firm paid millions of dollars in bribes to government officials in several African countries in exchange for mining rights. No doubt they’d rather be remembered for their patronage of the arts than for profiteering off human misery.",How Modern Art Serves the Rich
9951,3923221,2018-03-01 20:03:32,"Facebook’s next money-maker: Messenger Broadcasts0Users might hate it, but Facebook is now testing a self-serve sponsored messaging tool for small businesses that aren’t sophisticated enough to build bots. TechCrunch first reported back in November Facebook internally building a prototype of the Messenger Broadcast tool that let companies blast a message to anyone who’s already started a conversation with them. Now Facebook is starting to test the Messenger Broadcast Composer externally, beginning with a small percentage of Pages in the U.S., Mexico and Thailand.Currently, the tool is free to use, but a Messenger spokesperson tells me that’s for a limited time only, and that eventually this will probably become a paid product that charges businesses. Messenger is capping the number of messages businesses can broadcast to deter spam, and won’t let them ping anyone who hasn’t voluntarily talked to them first. But some users still might find it interruptive, especially if less tech-savvy local merchants blast out low-quality promotions.Now with 1.3 billion users, Facebook is eager to monetize Messenger. And some use cases like being able to asynchronously text a customer support rep instead of waiting on hold on the phone make perfect sense for Messenger. But with display ads injected into the inbox, sponsored message ads from big brands and now Messenger Broadcasts, Facebook risks its chat app becoming our new spam folder.Using the Messenger Broadcast Composer, small businesses with no coding skills can choose a subset of people who’ve messaged them to hit with a text blast. They write up a title and body text, add an image and select a call to action, like a button people can hit to visit their website or a choice of pre-written replies.This screenshot of the old Messenger Broadcast prototype shows that for now it’s free to use, but Facebook says it will likely charge in the futureBusinesses can then do a basic segmentation of their potential recipients by manually labeling conversations with tags like “interested in shoes” and then sending the message to all threads with that label. After the free trial, Facebook likely will charge depending on how big of an audience a business wants to hit with a sponsored message. The original prototype noted that businesses could ping a few users for free, with the implication that they would be able to pay for more reach.Facebook tells me that people are increasingly messaging small businesses on its platform, often when they seek store or product information, or need customer service. In 2017, 330 million people started conversations with small businesses. “But we got feedback from small businesses that they don’t have the know-how or the tech resources to build experiences on the Messenger Platform (bots) that would enable them to reach their entire audience,” a Facebook spokesperson tells me.“This is why we are testing the broadcast composer; using this tool, small business Pages can message all or a segment of the people who have initiated conversations with their business at once, directly from their Pages Inbox,” says the spokesperson.Both Facebook’s News Feed ads and Snapchat’s business grew significantly when they added self-serve ad-buying interfaces. While there are plenty of big brands paying for huge campaigns through ads APIs or account reps, small businesses around the world can greatly benefit from self-serve ads that leverage their more familiar connection with customers. Sponsored messages from Nike or Walmart might immediately seem like spam. But from the shop around the corner, ads could feel more at home within Messenger if they keep it casual and conversational.",Facebook’s next money-maker: Messenger Broadcasts
9952,3923222,2018-03-01 15:26:18,"The new Light Phone 2 keeps things basic but adds e-ink and ‘essentials’0Light is back with a new twist on its anti-smartphone phone. But this time, instead of doing just one thing, the Light Phone 2 does a few, and exists somewhere between the original Light and your overwrought iPhone – though still far closer to the first-generation Light phone overall.The new design features a matte finish e-ink display, which occupies most fo the front face of the device and can show text, act as a virtual keyboard for sending messages, show your contacts and alarms and more. The phone uses Light’s own proprietary operating system, which is heavy on the text and limited on the total number of options and features, and you use physical keys on the side of the phone to navigate through menu options.The Light Phone 2 has 4G LTE connectivity and, since it’s not yet finalized but is instead kicking off its Indiegogo crowdsourcing campaign, could add features including directions, ride-sharing specific apps, playlists, weather reports, and voice commands according to the company’s founders, on top of the basic call, messaging, contact book, alarm and auto-reply features that are definitely going in. Whether those other add-on features make the cut will depend in some part on backer feedback.But with those potential additions, plus the larger, device-commanding active display, the Light Phone 2 is starting to sound a lot more smartphone-y and a lot less “Just a phone.” But LIght’s creators say that it’s definitely not, under any circumstances, going to add social media, advertising, email or news features to the phone.Really, those are the things that truly turn our mobile companions into huge time sucks and mood altering devices. Light Phone 2 is definitely more of a compromise than a purist dumbphone like the original, but it still also sounds like it fits the company’s chosen tagline of being “a phone for humans” better than your average flagship smartphone does today.Light’s been out of stock of its current generation device for a while now, which was probably because it was looking forward to this launch. The phone’s Indiegogo campaign has $225 as the early bird price for the device, with $400 as the target retail cost, and estimated shipping is April of next year (yes, over a year away) so the company also seems to have learned a lesson or two about manufacturing and shipping hardware, and is giving itself ample buffer for this redesign.Nokia is relaunching its ‘Matrix’ slider phone and other high-concept simple phones like the Punkt MP01 are out there trying to wean people away from their smartphone habits. It’s an appealing dream, but it’s hard to tell if it’s just a brief hiccup due to information ennui, or a real movement in the early offing. How Light Phone 2’s campaign does overall might be another indicator as to which it ends up being.",The new Light Phone 2 keeps things basic but adds e-ink and ‘essentials’
9953,3923223,2018-03-01 14:00:33,"Uber launches Uber Health, a B2B ride-hailing platform for healthcare0Uber’s launching a new business line called Uber Health on Thursday that will provide a ride-hailing platform available specifically to healthcare providers, letting clinics, hospitals, rehab centers and more easily assign rides for their patients and clients from a centralized dashboard – without requiring that the rider even have the Uber app, or a smartphone.The Uber Health value proposition is similar to that of UberCENTRAL, the company’s ride-booking service aimed at business customers who want to provide rides for their clientele, but it’s also tailored for the healthcare industry with HIPAA standards compliance, as well as the ability to use the service on the client-side from even just a landline.Uber Health’s creation was rooted in some alarming statistics about patient care and healthcare client absentee rates. Uber Health General Manager Chris Weber explained on a call that some 3.6 million Americans miss medical appointments owing to a lack of available, reliable transportation. Plus, he noted that nearly a third of patients fail to show up to medical appointments every year in total.“Uber’s endeavors into health care trace back to 2014, when Uber first offered on-demand flu shots in large markets across the U.S.,” he said, regarding the genesis of the focus on health within Uber. “Since then there have been similar efforts throughout the world, from diabetes and thyroid testing in India, to subsidized rides for breast cancer screening in the U.S., to many more. That said, all of these efforts have been pop-ups.”Layout1PORTLAND_WOSINSKA_1016Riders 2Phone + DesktopWeber notes that this led them to investigate how they might do something more permanent, lasting and impactful in the healthcare space, and building on the reliability and efficiency of their consumer service, they thought they might be able to have an impact in terms of reducing that number of missed appointments.Uber Health is designed to do that, by allowing clinics and other medical facilities to book rides on their clients’ behalf, using a simple web dashboard where you input the client name, number and pick-up and top-off location, then select from Uber’s range of ride hailing vehicle type options. The client then receives a text message on their device alerting them that the company or organization has booked an Uber ride for them, along with a terms of service link, and then you get a notice of wo will be picking you up and when, with a contact number and a link to a live, web-based map showing you where they are and where they’re picking you up.As mentioned, Uber is also working on making patients aware of this information via voice call, which will work with both basic feature phones and landlines, and there’s a printout that clinic and facility staff can fill and provide to the client when they’re leaving their site and going home, too.Uber Health stores all of the trip information but only in client-side, HIPAA-compliant servers, and that data is never stored on Uber’s own, Weber points out. The ability to view and export the records is key for the organizations in terms of billing and reporting, and provides basic patient info (name and number) along with trip star and end point data.As for the business model, Uber Heatlh charges its healthcare organization customers only for the cost of individual rides, which are at par with what those rides would cost via the consumer app. Access to the dashboard and the reporting tools are included free. Uber already has over 100 healthcare organizations on the platform, thanks to a private beta that began last summer. The company also created an API to make it easy for those organizations to build the service into their existing patient management software.Uber Health is not a replacement for emergency service vehicles including ambulances, despite recent attention paid to instances where patients have hailed Ubers while in need of urgent care because they’re quicker to respond than ambulances in some cases. But it is a way that Uber can open up a promising new line of business while also providing a solution to a problem in the healthcare industry backed by the reliability of the ride-hailing company’s consumer service.","Uber launches Uber Health, a B2B ride-hailing platform for healthcare"
9954,3923224,2018-03-01 14:00:28,"DoorDash raises $535M, now valued at $1.4B0Restaurant delivery service DoorDash is joining the unicorn club with its latest round of funding.The company is announcing that it’s raised $535 million in a Series D round. And while it’s not saying anything about valuation, a source close to DoorDash told us that the new funding values the company at $1.4 billion (post-money).Asked whether that level of funding means that DoorDash is starting to think about an IPO, CEO Tony Xu said, “It doesn’t really change anything for us. Instead, we’re saying it adds more flexibility, giving us the optionality in terms of where we want to invest and how we want to think about financing for the company.”Latest Crunch ReportThe round was led by SoftBank Group, with participation from Sequoia Capital, GIC and Wellcome Trust. SoftBank’s Jeffrey Housenbold and GIC’s Jeremy Kranz are joining Sequoia’s Alfred Lin and Kleiner Perkins Caufield & Byers’ John Doerr on the company’s board of directors.“DoorDash’s technology advantages, exceptional management team and relentless merchant focus are reflected in their stunning growth and impressive unit economics,” Housenbold said in the funding announcement. “Food delivery is just the first chapter. Tony and team have a bold vision to create the world’s best logistics company, and we’re thrilled to partner with them to help accelerate their progress.”The company says it’s now working with almost 90 percent of the top 100 restaurant brands in the U.S., including Wendy’s, IHOP, and The Cheesecake Factory.Perhaps even more impressive: Xu said the company became “contribution margin positive” in the last year, which means that it’s profitable on a per-order basis. In fact, DoorDash has become profitable in its earliest markets.CrunchbaseOverviewDoorDash is a technology company that connects customers with their favorite local and national businesses in more than 600 cities across the United States and Canada. It empowers merchants to grow their businesses by offering on-demand delivery, data-driven insights, and better in-store efficiency, providing delightful experiences from door to door. By building the last mile delivery infrastructure …BioTony Xu is the CEO and Co-founder of DoorDash, an on demand-delivery company that enables delivery in areas where it was not previously possible. Starting with restaurants, DoorDash is building the infrastructure and logistics platform to allow any local merchant to deliver. Prior to co-founding DoorDash, Tony worked in product at Square, led business development at RedLaser (eBay), and began his …OverviewSoftBank is a multinational telecommunications and internet corporation focused on broadband, fixed-line telecommunications, e-commerce, internet, technology services, finance, media and marketing, and other businesses. The company operates through four major segments. The mobile communications segment provides mobile communications services; produces and distributes online games for smartphones …All of that might seem like an obvious necessity for a delivery business, but it addresses concerns that on-demand models only work when they’re subsidized by venture capital. Xu has said in the past that DoorDash’s logistics platform is sophisticated enough to make the business sustainable, and the latest numbers seem to bear that out.“That really was the driving force for this financing,” he said. “We figured out that our thesis was proven out. Now is the time to take this operating playbook and accelerate growth.”Those growth plans include expanding from 600 to 1,600 cities (DoorDash is still focused on the U.S. and Canada), as well as hiring 250 more people.Xu said DoorDash will be making big investments in its Drive platform, which allows restaurants to offer DoorDash-powered deliveries through their own websites and apps, so they can serve “all of their customers in all of their channels.”And, as Xu has long hinted, he also wants to turn Drive into a platform that serves a broader range of businesses, not just restaurants. He declined to get specific about those plans, except to say that we can “expect to see a few of those deliveries this year.”","DoorDash raises $535M, now valued at $1.4B"
9955,3923225,2018-03-01 11:53:58,"48 hours with the Samsung Galaxy S9: our first impressionsAn early look at the new GalaxySharesWe’ve got our hands on a Samsung Galaxy S9 out at MWC 2018, so the first thing we’ve done is take it out and about to check out what it’s all about.As you know, TechRadar loves to put a phone through its paces so we haven’t stress-tested the new Galaxy as yet - but we’ve spent two days with the phone, so here are our first impressions of the Samsung’s great new hope for the smartphone industry.CameraThe camera doesn’t look like it’s changed much, but there’s actually a fair amount of difference under the lens surface - the dual aperture sensor is supposed to make the Galaxy S9 immense in low light.And it is - we were really impressed with what it can capture when there’s very little light around. We haven't managed any stunning low light shots yet, but we've not stress-tested it yet.There are some other impressive new flourishes in the camera from Samsung on the Galaxy S9 - for instance, the software works out when there’s an object or person in the foreground, and gives the background a slight blur, which is really appealing to the eye.Image 1 of 5Image 2 of 5Image 3 of 5Image 4 of 5Image 5 of 5Super Slo MoWe’ve played with this mode extensively on the Sony Xperia XZ Premium last year, and the version Samsung has brought out is technically identical to the one the Japanese brand produced.However, the software change of capturing slow motion when action is seen in the central square is interesting - we kept getting told to hold the phone steady when playing with the mode. We were doing it in darker conditions, and it’s clear that the Super Slo Mo mode is best used in bright lights, so we need to try that out.Here's a video taken late at night - the darker conditions, like with Sony, aren't great with the Super Slo Mo, but it's more designed for brighter scenes, which we'll play with more this week:AR EmojiOK, these are weird. We firstly thought that what we got looked nothing like us - and it’s annoying that the costumes on offer are a bit limiting. That said, the phone is good at getting the general idea of what you look like, and some of the automatic GIFs it generated are pretty funny and accurate (and they’ll freak your friends out, trust us).It’s really nice how Samsung has embedded the ability to use the AR Emoji throughout the phone - for instance, it’s already available as a sticker on WhatsApp that you can send to chums.The AR Emoji are interesting... but a little on the weird side.You can create little videos of yourself with your AR Emoji from the front-facing camera, but the sensor doesn’t seem super accurate, only picking up a few flickers of the face - it’s not something that you’d really want to share with friends (unless we find a bit more practice makes it more usable).The Galaxy S9 generates a lot of AR Emoji in different scenarios that you can use as GIFs to your pals, filling your gallery with them instantly. They’re fun, if a little strange. We’re not sure if this is going to be a massive feature or a novelty...Design and biometricsOK, there's barely anything new in the design on the S9... this is just the Samsung Galaxy S8 with a fingerprint scanner in a different place.The look and feel is identical - if you owned both phones then you’d easily pick up the wrong one time and again, and probably not notice which you had until you found it hard to unlock.What is nice is that the biometric security already seems way better than the Galaxy S8 - Intelligent Scan is so much more rapid than the iris / face scan, and you can also have the fingerprint scanner running at the same time.There were multiple occasions when the scanner fired up and were in before even realizing what had happened. It doesn’t work very well when placed down on the desk, but it’s still a big step up from last year.The dual speakers are also really nifty - so much louder than last year and offer a wider, more expansive sound. We’re still working out how best to test the Dolby Atmos abilities on the phone, but again, this is a small upgrade that looks like it’s making a big difference.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",48 hours with the Samsung Galaxy S9: our first impressions
9956,3923226,2018-02-28 23:48:27,"This is our first glance of the LG G7, and it looks like an iPhone XThe notch-toting handset was caught on camera at MWCSharesUpdate:Previously reliable leaker Evan Blass had speculated this wasn't the LG G7, but he has now said he was wrong and he believes this is LG's next flagship phone.He said, in an article for Venture Beat, ""According to several sources — and judging by the icy reaction Ynet reporter Guy Levy received from LG representatives following the video’s publication and viral spread — the phone depicted is in fact LG’s next flagship.""Original story: LG all but confirmed that it wouldn't be revealing its 2018 flagship phone at MWC 2018, so it seems to have caught many by surprise that the LG G7 was found on display in one of the halls at the event with the codename 'Neo'. It was filmed by Israeli publication Ynet, which posted a video of the device.Ynet has confirmed that the LG G7 is powered by Qualcomm’s new Snapdragon 845 chip and includes 64GB of onboard storage with 4GB of RAM. A second, higher-end version of the handset will offer double the storage at 128GB and include 6GB of RAM.LG has upped the ante in terms of design, too: the vertical rear dual-camera setup is centered, with a fingerprint sensor below it. The 6-inch OLED display features an iPhone X-like notch at the top with a 19.5:9 aspect ratio, making it slightly taller than the 18:9 displays on the previous LG G6 and V30.At certain times in the leaked video, the notch does disappear and seemingly become hidden in a black notification bar, suggesting users may be able to hide the notch if they're not fans of the controversial feature.Some questions remain as to whether this is actually the final LG G7 design, however, with VentureBeat reporter and prolific leaker Evan Blass suggesting in a tweet that this ""may be the device LG scrapped"".This is interesting. Article/video from an Israeli journo at MWC showcasing the LG G7 (Neo). May be the device LG scrapped in favor of Judy. [h/t: @Hanan_haber]https://t.co/fNng7jw9AsFebruary 28, 2018MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicated MWC 2018 hub to see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.","This is our first glance of the LG G7, and it looks like an iPhone X"
9957,3923228,2018-02-28 17:00:00,"The best 13-inch laptop 2018: the top 13-inch laptops we've reviewedRegardless of your need, we've found a 13-inch laptop for youSharesIt would frankly be an understatement to say that the best 13-inch laptops are the best size of laptops. In 2009, when Apple stopped manufacturing 17-inch laptops, we knew that the norm for laptop displays was going to continue shrinking as time went on. Fast forward to 2018, and all of the best laptops are between the 12-inch MacBook and the 15-inch Surface Book 2. However, the most popular variations are the many laptops between the 13 and 14-inch mark.At a time when our top laptop choice altogether is the Dell XPS 13 and even gaming laptops are beginning to adopt similarly compact form factors, the choice is clear... or is it? Despite the fact that the best 13-inch laptop is considered a flagship by its manufacturer, there are factions within the 13-inch laptop category that make buying one a more complicated process than initially meets the eye.Many of these laptops are distinguished from each other due to their operating systems, but there is also a wide breadth of different form factors to keep an eye on these days. Some of the best 13-inch laptops can convert into tablets while others are mainstays of the traditional laptop economy. Whichever you prefer, we’ve listed the best 13-inch laptops that we have personally reviewed over the last year or so, explicitly ranked in order of awesomeness.Its changes are subtle, and yet the Dell XPS 13 is still the best 13-inch laptop you can buy. On the high end, it now offers a 4K display, for a sharper picture across the board. But even if you can’t afford higher tier configurations, the beautiful design, lengthy battery life and even the SD card slot are still there – plus you’ve got a quad-core processor no matter what poison you pick. Better yet, the 13.3-inch display of the XPS 13 has been squeezed into a smaller frame, which explains Dell’s marketing line: ‘the world’s smallest 13-inch laptop.’ Now stew on that, as you admire the Dell XPS 13’s gorgeous, albeit more expensive, Alpine White finish.You wouldn’t expect a laptop made by one of the biggest names in PC gaming to be thinner and lighter than a MacBook Pro, and yet here we are. The Razer Blade Stealth – in its all-new gunmetal finish – boasts not only a gorgeous and practical aesthetic, what with its 400-nit display brightness and full-size USB 3.0 ports, but it also has a few tricks up its sleeves to make it perform faster. At long last, the 8th-generation Intel Kaby Lake Refresh have made it into the newly 13.3-inch version of the Razer Blade Stealth. Though it received a ding on the battery life, an Ultrabook this fast is worth a 16-minute shorter use time on a single charge.There’s a new entry to Asus’s Zenbook lineup with the UX360, featuring a lot of the same specs as its predecessor but with the flexibility of a 2-in-1 laptop. While the ability to bend over backwards may diminish the build quality somewhat, it allows for new levels of functionality for computer consumers who want a laptop and tablet all in one. Thanks to an all-day battery, the Zenbook Flip UX360 really can go with you for everything you do. And, with a wide variety of internal customization, you can deck out the Zenbook Flip UX360 for your own personal needs, saving you some cash or boosting its performance.For those who prefer premium quality parts over exceptional affordability, the HP Spectre x360 contains everything you could ever ask for from a 2-in-1 without compromise. Weighing only 2.78 pounds (1.26kg) and measuring just over half an inch thick, it’s as light as it is thin. Plus, with up to a 4K screen and 8th-generation Intel processors paired with integrated UHD graphics, it’s one of the best ways to experience high quality video streaming as well as 720p gaming on a hybrid. That goes without mentioning the ports, of which there are plenty. You’ll get two USB-C Thunderbolt 3 connections in addition to USB 3.1 Type-A, a welcome variety to say the least.Samsung may not have made much of a dent in the laptop scene with much more than its Tab Pro S convertible, but Samsung's follow-up to the 2012 Series 9 notebook doesn’t just feature impressive specs – it’s got a competitive price as well. Marketed as an Ultrabook, it's certainly disheartening to know the battery life lasts only five hours, but given its sleek and sexy design, it almost doesn't matter. What's more, with the Samsung Notebook 9, you won't have to deal with the bloatware that makes many other Microsoft the target of angry forum posts.Everyone hates change. However, while the 15.6-inch frame of the Samsung Notebook 7 Spin we once knew will soon be erased from our memories forever, the new 13.3-inch model doesn’t seem like a terrible upgrade. It’s bounced two generations into the future in terms of processing power, and although there are no discrete graphics present, we’re glad to see a keyboard that can compete with some of the heavier hitters. It’s nothing revolutionary, yet as far as hybrid notebooks go, this one ain’t too shabby (plus it’s cheaper than a weaker MacBook Pro).The Surface Laptop is Microsoft’s first stab at a 'traditional' laptop, if you can even call it that knowing full well that its PixelSense touchscreen and Alcantara keyboard are anything but conventional. Featuring a full stack of U-series 7th generation Intel Core i processors, the Surface Laptop beats out Apple’s 12-inch MacBook any day of the week, and for a lower starting price at that. Despite the ports and operating system being limited, the Surface Laptop is appealing for its laudable design, beautifully vivid screen and impressive performance.As 2-in-1 laptops have become more prevalent in recent years, their manufacturers have been improving them at a nigh-equal pace. This rings true for few laptop makers more than it does for Lenovo, however, who has taken the liberty of crafting drop-dead gorgeous convertibles with standout watch band-like hinges, much like that of the Yoga 920. This rendition of Lenovo’s increasingly popular brand of hybrids sees the integration of USB-C ports, a centered webcam and, perhaps more interestingly, an 8th-generation Intel quad-core processor regardless of which configuration you opt for. It does purr more loudly than expected, and tablet mode could use some work, but the Lenovo Yoga 920 is ultimately worth its price of admission.In a market densely populated with slim-line laptops from a massive range of manufacturers, Apple's MacBook Air fights on admirably – though it started showing its age on the outside a long time ago. It has Intel's fifth-generation Core-series processors rather than the newest Skylake variants, but it's still a capable machine; even more so since Apple made 8GB of RAM standard across the line.The latest iteration of Apple’s seminal MacBook Pro series is here, and as you would expect it makes a number of notable improvements over last year’s offering. While it might not exactly feature the strongest battery life in the game (scoring under an hour less than last year’s offering), it does offer increased performance by way of a new CPU and faster RAM. Add that to Apple’s continued dedication to simplicity and beautiful design and you have a laptop that is sleek, portable and reliable.",The best 13-inch laptop 2018: the top 13-inch laptops we've reviewed
9958,3923301,2018-03-02 12:22:53,"TNW SitesGoogle’s Song Maker lets you easily compose music within your browserGoogle has added a new tool to its Google Music Labs effort, called Song Maker. As you’ve probably figured out, Song Maker allows you to make songs. Everything happens in the browser. You’re able to define a sequence of notes from a chessboard of sounds, and Song Maker loops over them repeatedly.Yeah. It sounded terrible.Think of it as a bit like a massively pared-down Yamaha Tenori-on, but running within the confines of the browser. Song Maker also lets you connect a MIDI keyboard, add percussion, sing over your track, and share it with your friends.As someone with precisely no musical skill, I do appreciate the fact that I don’t need to know anything about music to write a song. I can experiment by throwing notes together, and hoping for the best that they work.",Google Song Maker lets you easily compose music within the browser
9959,3923302,2018-03-02 10:54:05,"TNW Sites10 ways my robot vacuum cleaner is a way better version of my catIn at least ten ways, having a robot vacuum cleaner is like having a more useful cat. Ever since I got sent the DEEBOT N79S, a cheap robot vacuum cleaner made by Chinese manufacturer Ecovacs, I’ve been taking note of the similarities and differences – ending up with more of the former than the latter. Unfortunately the robot leaves my cat eating its dust.It reminds me to clean up things it can potentially choke on, but in a polite wayLast week I caught my cat in the act of eating a piece of string. He got quite far, about halfway through the two meters he’d pulled off a curtain. I was able to gently extract it, avoiding a poopy strand dangling out of his ass for days.My new DEEBOT handled a similar piece of string very differently. Instead of eating the whole thing, it stopped its rotating suck hole and started politely beeping at me to come free it. Now I scan the room it’s about to clean for anything that could possibly choke it, and clean up. Unlike my cat, who’d just go ahead and eat the string, DEEBOT teaches me to be a better person.I have to provide it with nourishment, but can do so at my leisureEvery morning, the moment my alarm goes off, a hairy black and white face concurs loudly that it’s time to get up. The alarm I can snooze, the cat I can’t.The robot vacuum cleaner feeds itself though. When its battery is running low, it finds its charging base and returns safely to load up on that sweet, sweet, electricity.It makes me check on it when it makes unusual noises, without being manipulativeThe first time I had DEEBOT clean a room with stairs in it, I was terrified. Every time I’d hear it get near the stairs, I’d run up to make sure it didn’t tumble off them. It never did. Thanks to a drop detector, it can avoid throwing itself off steps or even furniture. I learnt this after a while and relaxed.My cat, on the other hand, has learnt to abuse this worry by blurting out an unusually disconcerting meow from somewhere upstairs whenever he feels like a head scratch.It eats things it’s not supposed to eat, but not my human foodMy cat loves human food. Also, glasses of water or chocolate milk I just got for myself. With my cat around, I can’t leave my food unattended. The robot ignores human food, but might eat a hairpin or two.It can be petted, without necessarily liking itAs far as I can tell, the DEEBOT doesn’t posess sensors that register petting, so it’s not bothered when I reach out to touch it while it whizzes by minding its own business. It’s comforting in its predictability. With my cat, I never know if he’s going to appreciate a nice stroke or bite me. DEEBOT also doesn’t have any teeth.There are even little games you can play in your mind while watching; where will it turn next, will it sweep that piece of dirt there into its aspiration cavity, or will it hit that chair leg?It forces me to move things it could break, but not foreverWe’ve all seen videos of asshole cats shoving things off tables on purpose. My cat enjoys chewing on my poor plants while looking me straight in the eye. His dickishness has already made me move things from somewhere where they look good, to somewhere he can’t reach. Or I can see.DEEBOT also requires this, but in a way more subtle way. He just loves getting stuck in curtains, rugs, and under certain chairs. This means I have to move those, but only when it’s doing its work. Not all the time. And not to places where they’re permanently out of site.I have to clean its litter container, but can do it when I wantFor whatever reason, my cat loves taking a stinky dump the moment I’ve sat down for dinner. It might be because I normally go to the bathroom after I’ve fed him in the morning, and he’s just returning the favor. Anyway, after he’s done scooping litter onto his gift, I have to clean out his poop.The DEEBOT also needs me to clean the box where it deposits its litter, but allows me to do it at my leisure, and without spoiling my appetite.It can be left alone, and be civil about itLike a cat, the robot vacuum cleaner is comfortable being alone for a while. I can even set it to automatically clean at certain times on certain days. Unlike a cat, it doesn’t do other things like turning the couch into a hair repository or find out tea bags make for great toys, especially if you scatter its content all over the floor.It endears, but not in a fuzzy wayThe DEEBOT’s dumb persistance works endearingly, like watching a baby animal flail around. My girlfriend even gave it a name: Dinand, after the lead singer of Kane, our local Dutch version of Nickelback.",10 ways my robot vacuum cleaner is a better version of my cat
9960,3923303,2018-03-02 10:31:24,"About TNWTNW SitesThe magic behind the EU’s most lit Twitter accountThe EU isn’t exactly known for its ability to relate to young people, being the massive and cumbersome governmental institution it is. That’s why I was pleasantly surprised when I came across this tweet the other day:I follow a lot of official EU social media accounts, but I had never seen any of them use pop culture GIFs like EU Trade does — the official Twitter account of the EU Commission’s International Trade Department. I started imagining that somewhere, deep down in the bureaucratic catacombs of the EU, there was a text filled with institution jargon outlining ‘cool’ ways to use Graphic Interchange Formats to reach the continent’s younger demographics.To find out more, I tracked down the person behind the EU Trade’s GIF and emoji usage, Ariel Delvecchio, who is one of the most adamant emoji/GIF evangelists within the EU.“I’ve been systematically promoting the use of emoji and GIFs since 2015 and when I started there was nobody else doing it,” explains Delvecchio.“GIF use is a conscious part of our communication strategy here at DG TRADE, and that’s just the latest incarnation of what we’ve been doing for the past three years. You could say that we have a tradition of being ahead of the institutional curve when it comes to digital communication — it started with a very engaging account of the EU TTIP Team, and continued with us being the first institutional account to systematically adopt the use of emoji in a communication strategy — probably the first governmental institutional account in the world. We’ve also been systematically using GIFs since 2016.”The first GIFs were much tamer and uninteresting, but it’s obvious that Delvecchio and his team have grown bolder lately and have started embracing GIFs with more pop culture references — like normal people use them.The two unofficial EU languages: Emoji and GIFsThe EU’s use of emoji and GIFs actually makes sense within the union’s broader vision. The EU is all about communicating in and translating between multiple languages, whether they’re traditional are not.“You can see emoji and GIFs as two extra languages,” says Delvecchio. “They’re perhaps not official but I’d like to argue that they’re equally important as the official languages. When you use emoji or GIFs, almost any person from any country — regardless of which languages the speak — can relate to it and understand that message.”This actually makes sense as most of us use visual ways to express ourselves every single day. Even my grandmother uses emoji, so why shouldn’t the EU? Especially when being fluent in these languages really pays off.Delvecchio has seen it clearly in the data that embracing emoji usage has pumped up engagement numbers considerably and that’s why the use of GIFs should be increased as well. GIFS and emoji don’t dilute the message at all, in fact, the opposite is true: They amplify it.“The tweets containing emoji — we have more reliable data on that than the GIFs — had up to six to eight percent engagement rates in 2015/16 and around four percent afterwards — contrasting that with the average for normal tweets being one percent. Interestingly, since then the engagement trend in emoji tweets is slowly getting lower, possibly due to a more widespread use of emoji and hence saturation from all sides by the followers.”GIF/emoji friendly account dwarfs othersBut if these visual ways of communicating are able to transcend borders and engage people more, does Delvecchio believe we’ve found the new Esperanto? A new universal language?“I believe that emoji/GIFs are not the new Esperanto, they are the old Esperanto, much older than writing itself. I see them as a technology-enabled digital form of the primordial visual language that doesn’t even require knowing a language to understand it because it’s already embedded in us — to a large extent. That’s where the true power of emoji, GIFs, and memes in general lies.”The Emoji (and GIF) Union and the final frontierTo my great dismay, I found out that the EU doesn’t in fact have official guidelines tucked away somewhere, detailing which emoji to use or what kind of GIFs are likely to reach millenials. However, Delvecchio did confirm that he and his team have some internal ones — and some other EU institution might as well — where they collect their knowledge on what type of tweets and emoji/GIF usage has proven to work.Unfortunately he couldn’t share the full guide with me — highly classified emoji business and all that — but Delvecchio says people at the EU are trying to be more open and forthcoming about their work as an institution and that emoji and GIF usage has helped them do it. But if there are any EU employees out there that want to leak emoji/GIF guidelines, you know where to find me.Delvecchio also hosts official workshops for other EU employees, being the evangelist he is, to get more people to push the envelope of institutional communication by pumping it full of emoji, GIFs, and hopefully someday, memes.“I’d definitely say that using memes at some point is one of our goals. We’re trying to get into it, but it’s not going to be easy as it’s a more sensitive issue. We mostly tweet about bilateral relations, like trade deals, so getting approval from all parties could be tricky,” says Delvecchio. Unfortunately he doesn’t have the support right now within EU but memes are definitely a part of his long term strategy.It’s understandable that it’s difficult to implement meme usage, as there’s definitely a fine line between ‘how do you do, fellow kids’ and a ‘sick meme game.’ So we might need to wait a few years before seeing a fresh meme dump from an EU institution.However, before crossing the final meme frontier, the GIF usage needs to be perfected, but could it be that the EU itself would stop that from happening? I asked whether the EU’s current rigorous copyright reform — which people are already worried about — might restrict the usage of pop culture GIFs, like Delvecchio’s beloved Disney ones.“I only wish that there will be enough prudence to make sure we can keep on using these timeless GIFs — when it comes to meaning, archetypes, and symbolism, there’s little that surpasses the versatility of Disney GIFs.”Fortunately, there’s no way that it will effect emoji, which are still close to Delvecchio’s heart. He loves it every time new emoji are released but there’s one in particular that he feels needs an upgrade — that would be especially useful for accounts like the EU Trade — the handshake emoji. 🤝“I really like the handshake emoji because it symbolizes straightness and the handshake ‘seals the deal.’ But I wish that the handshake was designed a bit better, at least on Twitter. I think the hands should be a little bit bigger and made like they’re actually ‘coming together,’ instead of being this illegible little emoji blob,” Delvecchio explains.I personally am really glad that there’s somebody within the massive infrastructure of the EU fighting to make it a bit more human. It’s made up of people like us, so it should also relate to us. And if there’s one thing we’ve learned from our constant online presences is that all humans speak GIF and emoji.However, I’m also somewhat saddened by the EU’s humanity. I was really hoping to find a dreary legislation-like text determining what constituted as ‘lit’ or ‘woke’ GIFs and emoji. Maybe someday my bizarre dream of institution speak merged with internet culture will become a reality.","EU official: Emoji and GIFs aren't the new Esperanto, they're the old Esperanto"
9961,3923304,2018-03-02 09:01:09,"The aim is to provide 3D audio cues that not only help you find your chosen destination – whether that’s a university building or a coffee shop – but also to make you aware of exactly where you are and what’s around you as you walk.Microsoft says that Soundscape achieves this by pairing with a stereo headset and calling out the names of roads and landmarks that you pass by. It also lets users find their way to a chosen destination using spatial audio instead of verbal instructions, and can call out points of interest around them so they can familiarize themselves with their surroundings.Soundscape is designed to be used either on its own or in conjunction with other wayfinding tools and methods, as well as guide animals. It was created in partnership with Guide Dogs UK, and Guide Dogs for the Blind, and the LightHouse for the Blind nonprofit that promotes independence of people who are blind or have low vision.Hopefully, folks will find this useful and provide Microsoft with feedback to improve and make the app available to more of the 300 million people around with vision impairments.The company previously released Path Guide, an app for indoor navigation that doesn’t rely on GPS of Wi-Fi to help you find destinations in malls, hospitals, and office buildings – instead, it works with crowdsourced directions for those locations.",Microsoft's Soundscape app helps blind people get around town with 3D audio cues
9962,3923305,2018-03-02 07:15:37,"If Snapchat’s system is anything to go by, people will be able to call the Instagram users they follow, unless they restrict the feature to only allow pings from people they follow back. Instagram previously added the ability to join Stories via video a few months ago, so perhaps we should have seen this coming.I’m not a fan of this trend of baking every possible functionality into apps just because you can do it – and with Instagram, it feels like it’s all too easy to mar the experience by tacking on too much stuff.I’m happy to broadcast my photos and video there knowing that my audience includes a mix of friends, acquaintances, and strangers who enjoy my posts – meaning that I’m comfortable with the persona I put out there, and with the way I interact with people in the app.I also like using the messaging features to share visual content with a small bunch of friends and groups that have come together to share our common love for things like watches.However, I certainly don’t want people to call me on the app I use for when I want to simply kill time, or when I’m looking for recipes, or looking for gift ideas. And I certainly don’t want strangers to be able to video call me just because they’ve found my public profile.It’s also worth thinking about how this opens up a new avenue for harassment. Even if you can restrict who can call you, the responsibility of preventing harassment now falls upon the user. And unlike Snapchat, most Instagram users leave a record of what they’re about in the form of their standard posts, which malicious actors can mine for ammunition they can bully their victims with.Call me old-fashioned, but I’d like to be able to give my consent before being contacted online. It’s bad enough that anyone with my phone number can ping me on WhatsApp and the like. C’mon, Facebook, just let me have this one.But of course, that’s not going to happen. Facebook won’t rest until Instagram is every bit the same as Snapchat, only with a larger user base and a better interface. Whatever happened to not fixing what isn’t broken?The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Instagram might add audio and video calling features that I didn't ask for
9963,3923306,2018-02-28 17:33:50,"TNW Sites10 strategies to help your business keep up with the AI revolutionFrom smartphones and kitchen gadgets to cars to manufacturing equipment, artificial intelligence and automation have become firmly embedded in modern everyday life. As these useful technologies gain steam in the workplace, businesses everywhere will have to adapt their processes or risk falling behind.To help leaders prepare for the growing presence of AI and automation in the business world, we asked a panel of Young Entrepreneur Council (YEC) members to answer the following question:The AI/automation revolution is here. How can companies keep up?Their best answers are below:1. Get an internal expert onboard.Make sure you have internal expertise. Hiring consultants to ‘check the box’ normally results in overpaying for under delivery. Understanding how to properly apply artificial intelligence and machine learning to your business is just as important as the implementation. – Hongwei Liu, mappedin2. Turn to AI to improve your security.While once upon a time we made a choice to move forward, innovation and advancement are now crucial to survival. One of the most popular ways companies can use AI technology is to fight fraud and financial crime. Criminals have elevated their game in today’s digital world, but AI-powered solutions are able to help businesses protect themselves against sophisticated fraud schemes and cyber crimes. – Stephen Ufford, Trulioo3. Use chatbots for customer service.AI-powered chatbots promise intelligent digital assistants that are available 24/7 to resolve your customer requests affordably, consistently and very quickly. Companies can use predefined rules in a decision tree to understand and solve customer queries. Eventually, they can apply machine learning to learn from each interaction and escalate more complex issues to human agents. – Raad Ahmed, LawTrades4. Research and read.Pay attention to what is going on by subscribing to sites that provide AI industry research, analysis and updates. This information is critical to know what is going on and when and where to get involved. – Angela Ruth, Calendar5. Foster an environment of entrepreneurship and creativity.One thing AI and automation are unable to replace is the value of a strong business relationship. Particularly in the startup world, many businesses can live or die by the partners they make along the way. As long as business leaders can maintain top-notch instincts and encourage and foster an environment of entrepreneurship and creativity, they can stay one step ahead of AI. – Zohar Steinberg, token payments6. Understand how AI can be applied to your industry.Start by understanding AI’s application specific to your industry. Then make sure you have a business problem that needs to be solved. AI is just a tool to solve that problem. Applying or partnering with companies that can more efficiently solve that problem with AI is how you keep up. If the right partner doesn’t exist, you can design and build an AI solution. – George White, Pavia Systems7. Start small.Start by finding ways automation can work for your business on a small scale. You can begin with automated website chats or save time by automating parts of your workflow. We use an application that automatically reminds us when to follow-up on tasks so we don’t have to continually scan to-do lists. Then, we use the time we’ve saved to work on new products and innovative marketing strategies. – Jessica Gonzalez, InCharged8. Learn from others’ mistakes.AI is an extremely powerful tool that is only becoming more powerful over time, but there have been several speed bumps on this road. Learn from the mistakes made by others. Microsoft had to drastically revamp their AI chatbot after a very public debacle, as did Tesla after unintentionally causing a fatality with a self-driving car. – Bryce Welker, CPA Exam Guy9. Augment, rather than fully automate.AI and machine learning technology is still in its infancy. I don’t suggest creating automated call centers or leaving your PPC campaign to a robot if you’re a small business. I say augment; don’t automate. Identify weak points in your organization, from the marketing department to finances, where AI-powered technology can augment and improve the existing workflow. – Kristopher Jones, LSEO.com10. Use your imagination.It’s not so much AI/ML technology itself that is preventing more widespread adoption, but rather slow implementation and a lack of imagination from business leaders. Don’t get left behind. Think hard about how AI/ML can transform the way you do business, researching the best tools out there that would help automate processes and reduce operational costs, depending on your business. – Thomas Smale, FE InternationalThis post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",10 strategies to help your business keep up with the AI revolution
9964,3923307,2018-03-02 05:26:27,"A DDoS attack is one of the most common methods employed by hackers to take websites down: it involves bombarding sites with more traffic than they can handle, so as to overwhelm their servers and cause enough crashes to take the site offline temporarily.In this case, GitHub faced a whopping 1.35 terabits per second of traffic, which caused its service to go down for a total of 10 minutes. After detecting the attack, it requested helped from Akamai Prolexic, a service that mitigates such incidents by routing traffic through its larger network and also blocking malicious requests. The company told Wired that it had never handled that much traffic all at once – but because it had designed its infrastructure to handle five times the traffic from the previous largest attack recorded, Akamai managed to bring GitHub back online in just a few minutes.Interestingly, no botnets were involved in this attack, as is usually the case with DDoS incidents. Instead, the hackers went with what’s known as an amplification attack. They spoofed GitHub’s IP address, and sent queries to several memcached servers that are typically used to speed up database-driven sites. The servers then amplified the returned the data from those requests to GitHub – only, amplified by 50 times.It’s good to know that even with this much traffic, the attackers couldn’t do much harm besides interrupting GitHub’s service for a few minutes. Clearly, network infrastructure providers are getting better at handling DDoS attacks – but they’ll need to do more to stay a step ahead of hackers in the future.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",How GitHub braved the world's largest DDoS attack
9965,3923308,2018-03-02 02:12:52,"About TNWTNW SitesFacebook pumps the brakes on its secondary News Feed, ‘Explore’One of the largest changes to Facebook’s News Feed in recent years is finally getting the axe it deserves.Last October, The Guardian first reported that Facebook intended to split its algorithmic News Feed into two: one for non-promoted posts from Pages and publishers (called “Explore”), and one for your friends and family. In a blog post, Facebook’s head of News Feed, Adam Mosseri, said the experiment was motivated by “consistent feedback” that people wanted to see more from friends and family and less from media organizations and businesses.The change, predictably, didn’t go over so well in the six countries Facebook used as a testing ground: Bolivia, Cambodia, Guatemala, Serbia, Slovakia, and Sri Lanka. Aside from the backlash by news organizations, Facebook found that separating the feeds didn’t actually lead to increased connections with friends and family as it had intended. Although this could also be blamed on how the company chose to roll out the change.Without warning, Facebook made a substantial change to the News Feed. And according to some users in the test markets, few had any idea what had happened to their News Feed, or how to best utilize Explore.Today though, Mosseiri had this to say about the failed experiment:To understand if people might like two separate feeds, we started a test in October 2017 in six countries.You gave us our answer: People don’t want two separate feeds. In surveys, people told us they were less satisfied with the posts they were seeing, and having two separate feeds didn’t actually help them connect more with friends and family.To be clear, this in no way impacts changes to the News Feed announced last month. In January, Facebook announced it was prioritizing “meaningful social interactions,” a move that would feature more posts from friends and family, and fewer updates from media organizations and business.","Facebook pumps the brakes on its secondary News Feed, 'Explore'"
9966,3923309,2018-03-02 02:07:09,"About TNWTNW SitesFacebook’s ad policies are infuriatingly sexistFacebook is again under fire over policies meant to curb adult content in ads. The company has recently faced scrutiny for scattershot enforcement of its own rules, seemingly penalizing women for showing skin that men get away with.Several people reported to the New York Times that they’d had pictures taken down or rejected from ad campaigns for featuring female nudity, despite the fact that much more explicit photos of men have remained up.Facebook’s policy on adult content in ads states that it prohibits:Nudity or implied nudity, even if artistic or educational in nature, except for statuesExcessive visible skin or cleavage, even if not explicitly sexual in natureImages focused on individual body parts, such as abs, buttocks or chest, even if not explicitly sexual in natureContent portraying excessive nudity or alluding to sexual activityFacebook has five examples of non-compliant ads on its page. Four of those have women as the primary subjects — the fifth has two people of ambiguous gender in bed together. While it could be argued that two of them might be inappropriate — a woman in bed, maybe; a woman miming a blowjob, sure — the explicit prohibition against artistic implied nudity and cleavage feel backwards.Facebook doesn’t say why it prohibits cleavage, and it seems like nothing more than a low blow aimed at women. Infuriatingly, one picture shows a woman leaning forward slightly and looking into the camera. This image apparently “shows someone in a sexually suggestive pose.” Really, that’s apparently all it takes, even though she looks more like she’s modeling for hair conditioner.It’s fairly common for women to be accused of being flirty in situations that, to their minds, are perfectly innocent. It’s the reason men like VP Mike Pence say they won’t be alone with a woman they aren’t married to under any circumstances, because no matter how professional or banal the setting, they seemingly believe women are seductive temptresses.It’s reductive; it’s insulting; it’s not the least bit flattering; and the knowledge that our bodies are unwelcome or being watched is really damned uncomfortable. So when Facebook says a woman is sexually suggestive for possessing cleavage and a haughty stare, it doesn’t help one bit.At best, Facebook enforces the rules unevenly.Recently, Facebook removed a picture of the curvaceous Venus of Willendorf statue, despite their ad policies explicitly making an exception for statues (but not other forms of art — Michelangelo’s canon must really confuse them). Facebook later reversed the decision, saying the ad the picture was in “should have been approved.”On the flip side, when the NYT asked Facebook about a specific ad of a man’s rippling obliques posted on a romance novel page, a Facebook spokesperson said it shouldn’t have been approved. To reiterate, Facebook’s ad policy prohibits “Images focused on individual body parts, such as abs, buttocks or chest, even if not explicitly sexual in nature.”So while Facebook might claim its policies are in place to prevent any inappropriate material getting through, it seems fairly obvious it’s more focused on women in ads over men.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Facebook's examples of inappropriate sexual ads are aimed at women
9967,3923310,2018-03-01 22:53:33,"About TNWTNW SitesResearch shows when your Apple product is likely to bite the dustIf you want to know how much life you’ll get out of your new Apple product, an analyst for Asymco has estimated the average life of your next device.Researcher Horace Dedlu determined lifespan by using the number of active devices and the cumulative total of products sold. The former number, revealed during Apple’s Q1 financial call last month, has only been disclosed once before.When you subtract the number of active devices from the total number sold, the remaining number is the amount of products that have been retired in a quarter. Dedlu contends that figuring the average lifespan is as easy as calculating when the number of devices sold at one time equals the number of devices that have been retired at a later time.To use his own results as an example, the number of products sold in Q3 2013 equals the number of products retired in Q4 2017 — and from that you can infer that was the moment the 2013 devices died and their owners went out to buy new ones.Based on these factors, Dedlu puts the average lifespan of an Apple device at just over 4 years. That number has also increased over time. In an earlier article on Asymco, Dedlu estimated that two out of every three Apple devices were still in use at any given time.Almost all of Apple’s toys were included in this data, including the iPhone, iPad, Apple Watch, and Mac. I’d be eager to see breakdowns for each individual product, as the lifespan of a Mac is probably different from that of an Apple Watch.TNW 2018 is coming soon, and we want to see you there. Read all about our conference and how you can attend here.",Researcher pinpoints average Apple device lifespan
9968,3926086,2018-03-02 09:15:34,"Machine learning enthusiast, I spend my time reading scientific papers, replicating work and waiting for my own creativity to kick in!Mar 2OpenMined logoContributing to OpenMinedSumming up 3 months with the communityIf you are new to the OpenMined community, this article provides an overview of the current situation. I hope it will help you see more clearly the purpose and challenges of the community so you don’t get overwhelmed by the project complexity👍🏻As OpenMined is evolving fast, the date of the publication of this article is very important — March 01, 2018.TL;DR:OpenMined is a very ambitious community, building OpenSource tools to ease the developments of applications leveraging machine learning on private/confidential data, automatically complying with legal restriction thanks to cryptography and automatically remunerating data/compute/expertise contributors thanks to cryptocurrencies.What is OpenMined?OpenMined is a project that has been started 8 months ago by 👉🏻 Trask 👈🏻 when he made the observation that no OpenSource tools existed to do private ML. From this simple observation to its current state, the OpenMined community had to face a mountain of technical challenges which gave birth to the current incredibly ambitious project.Many developers are contributing every day on the different repositories, Hell many repositories have been created and/or deprecated in the last 3 months! It’s been pretty wild and that’s amazing, it shows how lively the project is!What are we trying to do?OpenMined is currently concerned with 3 problems surrounding artificial intelligence:How to deal with sensitive/confidential data?How to deal with the explosion of the computational need to train Machine learning models?How to handle remuneration for any contribution would it be data, computation or expertise?Those 3 concerns are at the heart of OpenMined and shaped the project to its current form one after the other. The community realised pretty fast that a project lacking any of those 3 pillars would not fit the bill and not be ambitious enough to support the overall vision:Building OpenSource tools to ease the creation and training of machine learning pipeline for private/confidential data while automatically remunerate the different contributors.Think about it, companies and researchers will soon face a lot more regulation from governments (we can already see many initiatives like #GDPR and growing concerns in general about AI) and this will hinder their capacity to innovate if they don’t have the right tool to strive in the upcoming new regulatory space.3 problems, 3 solutionsThose 3 problems can be solved with 3 overall solutions:Encryption: which allows one to securely keep models and data private against malicious actorsFederated learning: which allows one to train computationally greedy machine learning models in a decentralised manner on less computationally efficient devices.Gradient marketplace: which allows one to be remunerated for its contribution to the training process.Each one of those 3 pillars support the whole ecosystem in its own way by giving OpenMined the opportunity to create a self-reinforcing positive loop:The encryption is beneficial to the whole ecosystem, it works as an insurance for the data providers and the model owner in a decentralised and anonymous environments. Also it alleviates the concerns one might have about sharing confidential information.Federated learning provide model owners without enough computational power and/or enough data the access to a decentralised cluster of computational nodes and data provider to train their model.The gradient marketplace creates financial incentives for data owners, expertise holders and computation providers to contribute to a training pipeline.3 solutions, dozens of possible technical implementationsThe exciting part of those 3 pillars is that they are actually 3 whole research fields. One could see that as a burden but we don’t. It would be legitimate to ask why on earth one would try to merge 3 research fields and their respective state of the art solutions instead of going for proven and solid existing ones.The answer is pretty simple, Machine learning itself is a field of research where many parts still need to be understood. Good luck finding proven engineering solutions that scale on this field when you start to add constraint like privacy on it.This is why OpenMined is still in an exploration phases for the underlying technology that will power the ecosystem. And yet, this is exactly what makes OpenMined valuable as all those research fields have made some very interesting advances in the last few years. Maybe it’s time to merge all those advances don’t you think?Let me point you at some articles that are valuable to get familiar with the different subjects:Federated learning: OpenMined is currently investigating IPFS and their pub sub features to build its own federated grid. To get a good overview of federated learning, I invite you to read this Google blog post.Gradient marketplace: OpenMined is currently investigating ways to incorporate a friendly and powerful remuneration scheme. This is one of the most unique and interesting parts of OpenMined. Let me detail it for you.A gradient marketplace for everyoneAll kinds of machine learning practitioners should have an interest in OpenMined, actually even non-practitioner (like non-ML companies sponsoring Kaggle competitions).Every technology used by OpenMined is there to power this “Gradient marketplace”. It is at the same time, the core of the decentralised training process and the accessible part for end-users. It is gluing together all our technology: IPFS, Blockchain, MPC, Trillian, Unity, etc.As a high level abstraction we have 4 different profiles that will interact in the marketplace:Computation providers: They provide computation power (but only for the case of public model and public data, more on this below👇🏻)Nothing prevents some overlapping on those 4 categories, this is just the most generic case. Task sponsors and data scientists could belong to the same organisation, that would not make much of a difference.Let’s explore the process in more details now.First, task sponsors (startups, big Co., random individuals, etc.) needs a model and they have no expertise, no data, no computational capacities but they usually have a business incentive for the model. In that case, they provide a bounty to the network so the different other actors get interested in contributing. They must also provide a validation set so we can have an objective metric to compare contributions.Second, data scientists propose some models architectures (including training and initialisation process) that are linked to the task. All those architectures are now ready to compete for the bounty.Every data owners that have data compliant with the task can contribute to the training. 2 different possibilities here, either the data is public and data owners can just provide their data to the network. Or their data is private and in this case they download a model, train it on its on private data and upload back the updated model.Notice that in the private case, the data never leaves the data owner even if it’s encrypted. This is why computation providers cannot play a role in the private setting.Technically, this process creates a tree structure of models for which the root node is that task, the first layer of the tree is the model descriptions, all other layers are different level of trained models and leafs represent the last trained models.Occasionally, the task provider compute the validation accuracy on all existing leaves to check if a given threshold has been reached (could be a duration for the competition, an accuracy threshold, etc.)If the threshold has been reached, the “model winner” is declared and all contributors to the given “model winner” are remunerated:for data through improvement of the validation accuracyfor computation and RAM usage, billed per operationfor expertise by the numbers of model architectures proposed to a given problem.To do so we use micro-payments and cryptocurrencies. As a first step we will use an existing crypto-platform API to simplify the process and leverage existing ecosystem. In the long term, OpenMined will just work in a fully decentralised manner leveraging existing cryptocurrencies directly.All of this happens, if needed, in a fully encrypted manner assuring all the parties to be compliant with any privacy laws.Ease of use, a core valueWe’ve explored the 3 core technology of OpenMined but I would like to highlight one of its core values. Even in the most technical environment, ease of use is usually a deal breaker for potential users and I’m really amazed by how OpenMined try to keep the balance between the user experience and the technical challenges.Remember that the overall goal is to simplify the development process of private ML application. This simplicity is the bond that unifies the whole architecture: abstracting away all the complexity at every step of the development is a necessity for us.I could list all the efforts that have been made in the past in the different repositories, but that would mean very little. The important message I want to convey is that we want to give access to private machine learning to everyone, avoiding any too steep learning curves that would left non technical people out of the loop.The first and simple step is to be able to use the overall workflow designed for the gradient marketplace on the very simple case of one user holding the data and one user training a model. Consider this achieved!2. Federated learning: open grid for public models on public distributed datasets (on going)Implementing the first federated learning algorithm, everything is public for now. The focus is on handling all the technical details brought by asynchronous and decentralised machine learning pipelines (asynchronous SGD, delayed gradients, etc.). This is currently happening!3. Open grid for public models on private distributed datasets (soon)We add the cryptographic layer to the whole technological stack! As a first step on that side, we encrypt only the data, arguably the most important and crucial part of the machine learning training process on confidential data.4. Adding reputation and remunerationAs things start to get private, the question of trust starts to kick in. A reputation system (closely linked to the remuneration system) using the blockchain will be bootstrapped. The more you train/provide data/provide funds in an honest way on the network, the more reputation you get, the more nodes are willing to work with you!5. Open Grid for Private models on private dataThe final step of the cryptographic setting. Models get encrypted too, improving the incentive for data scientists and private companies to train their models on OpenMined.6. Backend API for private ML applicationsFinally, we can achieve the grand vision and build a set of plug-and-play API for many different contexts (games, web apps, mobile apps, etc.)7. Meet at your own local pub worldwide on live stream to celebrate the work done 🍻Arguably the most important step of the OpenMined community!To finish this piece of writing, I would like to highlight the fact that we keep facing technical challenges and open questions on the different possible implementations of the above technologies. We are currently investigating many papers on those subjects to help us go forwards but if you have any knowledge in those areas, please reach out to our slack!If you read so far, thank you for your attention. Hopes it was enlightening! Have a good day sir ☕️",Contributing to OpenMined
9969,3926091,2018-03-02 00:00:00,"Uber, Lyft Drivers Earning A Median Profit Of $3.37 Per Hour, Study Says : The Two-WayResearchers at MIT said 30 percent of Uber and Lyft drivers are actually losing money after taking car expenses into account, while most drivers earn less than minimum wage.An Uber car waits for a client in Manhattan in June 2017. A study from MIT says most drivers are making less than minimum wage. Spencer Platt/Getty Images hide captiontoggle captionSpencer Platt/Getty ImagesAn Uber car waits for a client in Manhattan in June 2017. A study from MIT says most drivers are making less than minimum wage.Spencer Platt/Getty ImagesThe vast majority of Uber and Lyft drivers are earning less than minimum wage and almost a third of them are actually losing money by driving, according to researchers at the Massachusetts Institute of Technology.A working paper by Stephen M. Zoepf, Stella Chen, Paa Adu and Gonzalo Pozo at MIT's Center for Energy and Environmental Policy Research says the median pretax profit earned from driving is $3.37 per hour after taking expenses into account. Seventy-four percent of drivers earn less than their state's minimum wage, the researchers say.The conclusions are based on surveys of more than 1,100 drivers who told researchers about their revenue, how many miles they drove and what type of car they used. The study's authors then combined that with typical costs associated with a certain car's insurance, maintenance, gas and depreciation, which was gathered in data from Edmunds, Kelly Blue Book and the Environmental Protection Agency.Drivers earning the median amount of revenue are getting $0.59 per mile driven, researchers say, but expenses work out to $0.30 per mile, meaning a driver makes a median profit of $0.29 for each mile.""While the paper is certainly attention grabbing, its methodology and findings are deeply flawed. We've reached out to the paper's authors to share our concerns and suggest ways we might work together to refine their approach.""The newspaper also noted, ""Other studies and surveys have found higher hourly earnings for Uber drivers, in part because there are numerous ways to report income and to calculate costs and time and miles spent on the job.""MIT authors also calculated that it's possible for billions of dollars in driver profits to be untaxed, because ""nearly half of drivers can declare a loss on their taxes."" Drivers are able to use the IRS standard mileage rate deduction to write off some of the costs of using a car for business. In 2016, that number was $0.54 per mile. ""Because of this deduction, most ride-hailing drivers are able to declare profits that are substantially lower,"" researchers write.""If drivers are fully able to capitalize on these losses for tax purposes, 73.5% of an estimated U.S. market $4.8B in annual ride-hailing driver profit is untaxed,"" they add.The MIT researchers said 80 percent of drivers said they work less than 40 hours per week. An NPR/Marist poll in January found 1 in 5 jobs in the U.S. is held by a contract worker; contractors often juggle multiple part-time jobs.Uber and Lyft both have ""notoriously high"" turnover rates among drivers. A report last year said just 4 percent of Uber drivers work for the company for at least a year.NPR's Aarti Shahani reported in December that Lyft began a program to give drivers ""access to discounted GED and college courses online"" in a recruiting effort.It was only last year that Uber introduced the option to tip drivers into its app for customers. Recode listed the initiatives Uber rolled out in 2017 in order to appeal to drivers, including 24-hour phone support, paid wait time and paying drivers if customers cancel after a certain amount of time.Both Uber and Lyft have been fighting legal battles for years against initiatives to classify their drivers as ""employees"" instead of ""independent contractors"" — meaning drivers don't receive benefits like health care or sick leave.","Uber, Lyft Drivers Earning A Median Profit Of $3.37 Per Hour, Study Says"
9970,3926327,2018-03-02 14:12:29,"About TNWTNW SitesHow Sweden’s startups became Europe’s digital vikingsSweden is an Eden-like country for tech startups and that’s no news.If you want to start an innovative business, the land of Nobel Prize, Volvo, and Ikea offers top-notch infrastructures, simple regulations, and a smart tax legislation.The corporate tax rate is as low as 22 percent yet income taxes are relatively high which makes Sweden doubly appealing for new business ventures. On the one hand, the government doesn’t cannibalize your company’s profits while, on the other, it has the financial resources to guarantee extensive welfare packages that constitute a much-needed safety-net for groundbreaking yet risky business ideas.In frozen Sweden, unicorns do find pastures to graze. Over the years, five Swedish startups reached the coveted $ 1 billion evaluation: Skype, Spotify, King, Mojang, and Klarna. The country has 20 startups every 20,000 employees — compared to five in the United States. Sweden is so crazy about startups and innovation that even the Church — usually not the most progressive club in a country —invested $ 5 million in the sector.For our series of Growth Stories, I’ve been interviewing many CEOs of fast-growing Swedish companies. Different hallmarks of this idyllic ecosystem struck me but one, in particular, stood out. All those startups products were at the end variations on the same business theme: make content valuable again.Spotify is possibly the most remarkable example. Since its inception in 2008, the company defibrillated a dying music industry implementing what has been branded as a “sustainable business model for artists.” Despite good intentions, the company got its fair share of criticism from people that didn’t feel the compensations were high enoughThe company always replied that its product massively migrates users away from piracy. In the Internet age — they claimed — Spotify represents the best of all possible worlds for musicians. Spotify is now almost worth $ 20 billion and filed to go public.On a relatively smaller scale, dozens of other companies set on the same mission. Acast, for example, is a hugely successful podcast platform that’s spearheading the field of voice-first technology. Soundtrack Your Brand is a b2b version of Spotify (yeah, it’s illegal to stream music from Spotify in your store). Epidemic Sound produces music for online creators. And these are only audio-related companies.Acast officeThen, there are the legions of game publishers. Colossi like King, Mojang, Paradox, and Mag Interactive gave the world viral hits such as Minecraft, the Candy Crush saga, and Ruzzle. When Steam announced its first Steam Awards in 2016, 11 out of the 100 awarded games were brewed in Sweden. An astounding stat, given the country’s dimensions and its number of game development studios.And finally, there’s “newstech,” a sector busy with finding a sustainable business model for journalism. Stockholm hosts at least nine notable startups that are trying to make the news easier to monetize.Epidemic Sound’s officeA couple of weeks ago, I was chatting with Magnus Hultman, a founder whose company Strossle falls under this latter category. After he repeated once again that its company’s mission is to make journalism pay again, I couldn’t help but ask if he knew why this is such a common trait in Swedish startups.“Oh, I know why,” he quipped. “It’s because this is the land of Piratebay!” The answer begged further clarifications. “We were the first to pirate music, films, and books. We were the digital vikings out there stealing content! Then, we realized that stealing is not a sustainable business model. If you just keep plundering, soon there’s nothing left. The loot is gone. So we needed to fix what we broke. That’s why Spotify got started. And that’s why thousands of years ago we became Christians, leaving our hideous viking helmets and swords behind.” Magnus concluded the explanation with a decisively not viking smile.Magnus HultmanAt first, I felt that sounded a bit too simplified and sales-y, just a good marketing tagline without much reality behind it. The somewhat essentialist approach almost reminded me of Montesquieu’s The Spirit of the Laws where the French philosopher paired climate and society, claiming that Nordic men were braver, more honest, and less suspicious towards others because of the cold they endured.But then I realized that maybe Magnus explanation might actually have some truth to it. The “digital vikings” hypothesis seemed a viable way to get a good grasp of Swedish tech ecosystem.Swedes might have left their helms and swords behind, as Magnus explained, but the colorful metaphor of the “digital vikings” was just too good to not be used to illustrate the Swedish tech ecosystem and some of its aspects that are usually overlooked.We then launched our quest to find the habits that make Sweden tech startups the great vanquishers of the digital era. Here they are.7 habits of highly effective digital VikingsStrossle’s office1. Find new pools of wealthIf you don’t own it, then just take it. That’s what vikings used to do. But after destruction, there was always construction. Vikings conquered Normandy but then they implemented a sophisticated social system. In the Middle Ages, the loot was made of gold and gemstones, nowadays it’s content that — 20 years after Bill Gates famous essay— is still king. The digital vikings startups have found new, legitimate ways to appropriate content (music, news, podcasts,…) and make it valuable again. They understood that all the big digital platforms (Facebook, Amazon, Netflix,…) are useless without it.2. Sail across the seaIt doesn’t matter that Sweden is one of the best country in the world to live in, for your business it will never be enough. Swedish companies are always built to go international soon or later. And this has always been the case. This long and established tradition is a valuable asset in the contemporary digital world where business ventures must have an international mindset from the beginning.3. Learn new habits from the lands you conquerThe digital vikings are great conquerors but they’re also eager to pick up new habits and trends from the lands they go ashore at. “When you live in a country that it’s often dark and cold, you get curious about the rest of the world,” says Magnus Hultman.Swedes always keep an ear to the ground, staying abreast of innovation. “We’re a society of early adopters,” adds Therése Gedda, founder and CEO of Swedish startup 30minMBA, “for this reason, Sweden is an excellent test market for b2c companies.”4. Crush the hierarchyThe digital vikings are not particularly fond of hierarchies. Only results and performance grant authority. And also in that case, the leadership style generally consists of entrusting people with freedom and responsibility. In a lowly populated country such as Sweden, you always lack good people and you can’t waste them on bureaucracy and tedious tasks.“Flat hierarchies are a consequence of our decision-making process,” says Leandro Saucedo, Global Head of Strategy at Acast, “In the Nordics, decision making is about trying to get consensus. If that’s not possible, you try to get at least a qualified majority. This easily leads to flat hierarchies where every voice counts.”5. Collaborate with trustTrust is probably the essential value for digital vikings and it tracks back to their rural heritage. “This is our Jantelagen, the law of Jante,” proposes Malin Lundell, marketing manager at MAG Interactive, “it’s a term that describes a typically Nordic attitude. There’s a strong emphasis on the importance of growing together. If you want to achieve something great, it doesn’t mean you need to throw under a bus your colleagues to succeed.”6. Infrastructure as nation buildingWhen Norse warriors ransacked the British coast, they didn’t go on a single boat (as they do in Asterix comics.) Instead, they used a proper fleet that was big and strong enough to implement their vision. Digital vikings emphasize the importance of infrastructures.Examples proliferate. In 1995, the Swedish government launched a “Home PC offering.” Anyone who bought a computer with an Internet connection could deduct it from the income tax. A few years later, in 1999, the government issued a statement saying that everybody in the country required the broadband because that was the future.It’s not by chance that Sweden boasts one of the fastest average Internet speed in the world. Also in the case of 3G, the adoption was fast and widespread. This was because the government gave the licenses not to the highest bidders but to the companies that promised the best coverage.7. Vikings are a tight-knit community of innovatorsWhen you know that you have a community of vikings to fall back on, you dare to innovate. Sweden’s social welfare system has supported people in taking risks and make their dreams come true. “Universities and colleges are free,” reminds Björn Fant, co-founder of online store Hemmy.se , “in addition, students can get a low-interest loan from the government to cover their living expenses. Someone says that this is the best venture capital we have.”Johannes Schildt, CEO and co-founder of healthcare startup Kyr, adds: “It’s this dual dependency between state and industry that has laid out the foundation for the tech-oriented entrepreneurship that we see today.”MAG Interactive’s officeDespite all the benefits, digital vikings’ history presents its fair share of failures. “You should always remember that this is the second Swedish tech invasion,” comments Leandro Saucedo, “and the first one didn’t go particularly well. A lot of businesses went belly up. A spectacular example of failure was Boo.com, a kind of Lesara avant la lettre that burnt $ 135 million of VC capital in just six months.”But now digital vikings’ longships have the wind in their sails, right? “Nowadays, we have the opposite problem,” adds Leandro, “we’re already at the third wave of invasion. The ecosystem works like a charm but there’s also a lot of pressure that there wasn’t before. All the companies want to be the next Spotify but often they lack the patience and a long-term vision.”—This article is brought to you by TNW X and Adyen with the collaboration of Strossle. Fast-growing startup? We dare you to prove it. Tech5 is our annual competition that celebrates Europe’s fastest growing tech companies. Signups are open.",How Swedish startups became Europe's digital vikings
9971,3926328,2018-03-02 13:56:10,"About TNWTNW Sites5 easy-to-use tools to discover new ICO’s and cryptocurrenciesThe last few decades have witnessed the rise of people who made their fortunes off the internet.Some of these enterprising individuals built companies from an initial idea to global behemoths worth billions, while others became rich by investing in the stocks of internet-driven firms such as Amazon, Google, and Facebook.Now, a disruptive new technology is taking the world by storm and fortune is ready to favor the bold as Blockchain technology has slowly, but strongly gained mainstream adoption. Cryptocurrencies are here to stay and Wall Street investors are gradually warming up to the idea that there’s huge fortune to be made off cryptocurrencies.Bitcoin is probably the most popular cryptocurrency on the market as the coin has outperformed every other asset class. In the year-to-date, Bitcoin has rewarded investors with 666.2% gains and is up by an incredible 3,000% in the last three years.New opportunities for cryptocurrency investorsMany potential investors missed the opportunity to invest in Bitcoin in its early days; and now, not many people can afford to buy Bitcoin at the current price (at time of writing this post, a little over € 7,900) Other relatively newer cryptocurrencies such as Ethereum and Litecoin are also moving very fast as the cryptocurrency market continues to expand.Interestingly, potential cryptocurrency investors can still find opportunities to invest in the next Bitcoin by buying into newer cryptocurrencies just fresh from their ICOs. Yet with more than 900 cryptocurrencies available for trading on different exchanges, it might be somewhat hard and overwhelming for an investor to know where to start.This article provides precise and concise information on tools you can use to discover new ICO’s and cryptocurrencies in order to make informed decisions.1.Top ICO ListTop ICO List is your one-stop-shop for discovering potential blockbuster ICOs in the cryptocurrency market. Top ICO offers a list of all ICOs but it goes the extra mile to categorize the ICOs to give you a list of the best upcoming ICO.Top ICO also provides insight on some of the best currently active ICOs and it provides information about past ICOs so that you can have a base for benchmarking the performance of any ICO that interests you. Top ICO list also serves a platform where you can find curated whitepapers and one-pagers of ICOs instead of navigating through multiple web pages to access the whitepapers of different blockchain startups.2. ICO BenchICO Bench is a professional ICO grading website that utilizes crowdsourced ratings from expert crypto traders and crypto experts. These experts evaluate the project and look for many different parameters (such as the team, the hard cap, and the white papers) and grade it accordingly. In addition, an assessment algorithm provides its own rating, based on 20 criteria. An ICO can earn up to 40 points in this assessment. Afterward, this rating is combined with the experts’ ratings to calculate one final score.3. CoinGeckoCoinGecko is still in public beta testing but it has proven to be a priceless tool for cryptocurrency traders and investors that want to be ahead of the market.CoinGecko simply provides a real-time ranking of live cryptocurrencies across multiple exchanges. A single glance at CoinGecko provides insight into the cryptocurrency, price in real time, and the percentage gain/decline in the trading session. CoinGecko also provides information on the market cap of different cryptocurrencies. In addition, CoinGecko gives insight into the development and community activity of the coin to show if developers are still backing the stock and it shows if it has a strong community of users.4.Smith + CrownSmith + Crown is setting up shop to be the Bloomberg/Zacks/Financial Times of cryptocurrenciesSmith + Crown provide access to a suite of “indexed public research which has been tagged and summarized for reference and discovery.” The firm also maintains a repository of a curated and comprehensive list of upcoming and active ICOs. The company provides a simple summary for all of the ICO’s on the list (the name of the project, a short description of its ICO, and the start and end date of its token sale). For some of the listed ICO projects, Smith + Crown also offers more in-depth research (for instance a review of the whitepaper, research into the market, and commentary on the team’s strategy).In addition, Smith + Crown offers insight on the availability of detailed information on the founders of an ICO, the availability of its project code and whether the ICO is open to U.S. investors.5.CryptowatchThe aforementioned tools are mostly designed to help you in your research and due diligence before you buy into any ICO or cryptocurrency.Cryptowatch, however, is designed to help you improve the odds of your trading success after you might have bought any cryptocurrency. Cryptowatch provides a live feed of hundreds of cryptocurrencies across eight different exchanges. Cryptowatch’s live feed also tracks the cryptocurrencies across different currencies so that investors in other parts of the world can know how specific cryptocurrencies stack against their fiat currencies.Now that you have gotten your hands on useful tools that can help you discover new ICOs and cryptocurrencies, you’ll need an exchange that helps you get an edge in the market. EXMO is an excellent cryptocurrency exchange that will launch its own EXMO Coin token crowdsale on April 26, 2018, to power the margin loan service on its platform. Users will be able to trade their crypto assets with a leverage, and international token holders will receive another profitable asset on the cryptocurrency market.",5 easy-to-use tools to discover new ICO’s and cryptocurrencies
9972,3926329,2018-03-02 13:41:17,"TNW SitesThis startup is creating the cultural cryptocurrency for museums and institutionsCultural tourism is a big segment of the wider travel and tourism industry. While large online players like TripAdvisor provide a huge amount of information for travellers about museums, landmarks, and historical sites, there is still a need for innovation in the space.For example, the EU has a working group for digitally capturing, preserving, and exploring cultural heritage through new technologies. Austrian startup Cultural Places believes it has an answer by bringing cultural heritage and blockchain technology together. It wants to reinvent every aspect of the cultural industry, from ticketing to fundraising.At its core, Cultural Places is a social network for artists, curators, and patrons, built by Oroundo, whose founders developed the concept over the last three years, which is creating a cultural ecosystem connecting customers and suppliers.The first version of the app has already been deployed with more than 30 institutions and landmarks including the Stephansdom cathedral in Vienna and the Borobudur Buddhist temple in Indonesia.But now it is moving on to its next phase – the Cultural Coin. The token is the platform’s dedicated cryptocurrency for purchasing tickets to museums and theatres or supporting galleries and exhibitions. Cultural Places will be running an ICO to raise financing for further development and deployment of the platform. Long term, the founders envision Cultural Places as the go-to platform for discovering and booking trips.Streamlining ticketingTraditional ticket booking is dogged by high fees, sometimes up to 30 percent, not to mention the high levels of fraud and ticket touting on the secondary market.On Cultural Places, tickets are handled via smart contracts on Ethereum, which removes the need for intermediaries. Smart contracts also allow for sharing and reselling of tickets to avoid anyone being ripped off. Customers get reasonably priced tickets and artists know that their tickets are being sold to actual fans rather than ticket touts.This has been a massive pain point for ticketing across numerous industries and several startups are trying to solve it with the blockchain but Cultural Places is one of the first to design a blockchain solution specifically for this industry.Social network & crowdfundingCultural Places aims to be all-encompassing. It will be a social network for travellers and culture buffs where users can build connections with others that have similar interests and shop on marketplaces for physical and digital goods.The platform helps institutions to collect and analyse user data to refine and hone the content they share to users. Furthermore, this data will inform more precise advertising compared with other social networks.Cultural institutions, like museums, will be able to build offerings using Cultural Places’ API and receive payments using the Cultural Coin cryptocurrency. Beyond that, the platform allows institutions and bodies to digitally represent and spread awareness around their work.All content in the app is created by the institutions themselves so it’s always up to date and accurate. Museums for example are already able to integrate beacon and NFC technology in their buildings with the app to act as a walking tour guide when you’re actually at the location.Furthermore, the crowdfunding feature will help cultural businesses and artists to raise funds in a transparent way amid an environment where cultural investments are under strain. Much like ticketing, the startup claims that the crowdfunding process will be more transparent for all parties. This may be contributing to the costs of running an exhibition or helping to fund an archaeological dig.Lower costs for the userCultural Places promotes its model of transparency and lower fees compared to incumbents in the marketplace but its chief business model is still revenue from transactions like ticket booking.All purchases made on Cultural Places will incur a six percent fee. Of this, three percent goes to Cultural Places and other three percent is distributed across the community: One percent will be returned to the user in Cultural Coins as a sort of loyalty program. Another one percent of the transaction will be distributed to all Cultural Coin token holders as a reward for being part of the ecosystem. And a final one percent will be distributed to all institutions to encourage further participation.The Cultural CoinThe company is holding an ICO for the Cultural Coin, which is supported by the digital marketing agency Digitalsunray. The Cultural Coin is a utility token that will be operational across the platform, though payments will also be available in fiat.In the sale, 1.5 billion coins will be generated, with 900 million (60 percent) of these coins being made public in the sale, concluding on April 5 with all unsold tokens being destroyed. 150 million coins (10 percent) will go into a stability pool; 30 million (two percent) will be reserved for a bug bounty program; 75 million (five percent) will be distributed among existing Oroundo shareholders; and 345 million (23 percent) will be held for other early stakeholders, team members, and advisors in the project.The sale is taking place in five phases: a pre-ICO and four separate sale phases with the value of the coin going up at each phase. In the pre-ICO, the coins are valued at €0.015 and will then increase to €0.018, €0.021, and €0.024 before finishing at €0.030 on the final phase.Unlike many ICOs, the company has already launched a working product with iOS and Android versions of the app. The funds raised by the ICO will help further development of its blockchain features and growing its partner networks. It is working with startup Flashboys for the blockchain implementations. Flashboys is the creator of Wizzle and will list the Cultural Coin on its exchange. Cultural Places is also working with customer satisfaction rating company RateMyTate.The startup will first integrate the blockchain ticketing system this year with a view to building out the payments and crowdfunding features in 2019.This post is brought to you by The Cointelegraphand shouldn't be considered investment advice by TNW. Yes, TNW sells ads. But we sell ads that don’t suck.",This startup is creating the cultural cryptocurrency for museums and institutions
9973,3926330,2018-03-02 13:12:48,"About TNWTNW SitesTake your smart assistant with you with JinniJinni is a curious little device that combines the Alexa, Google Assistant and Siri (iPhone only) smart assistants into one small black box, paired to your phone by Bluetooth. When you want to access an assistant, pull Jinni out of your pocket, hold your finger on the appropriate button and off you go. Thanks to the API access for the first two platforms, you only lose out on minor functionalities from ‘official’ devices. I don’t own an iPhone so couldn’t test Siri, but as the platform is far less open, I assume it has fewer options available.As you might imagine with a multi-function device, setting up and using the Jinni is slightly fiddly, but the unit I tested is a prototype, so make some allowances. For all platforms apart from Siri you install a custom app that you use for settings and authenticating with the external platforms. It also displays notifications, but it was unclear to me as to which, as with my Alexa experimentation I received most notifications in the Alexa app anyway. At times it was fiddly to use with network timeouts and unresponsiveness, but it’s hard to tell if this is due to the device, the Bluetooth connection, or the assistant service, so your mileage may vary. When it did work, then as you can see from the video below, the audio is thin and quiet, but loud enough for short interactions.I wonder how long Jinni was in production, as now you can use all these assistants on a smartphone via apps, but minus the convenience of the Jinni’s size. The Jinni also lets you control calls and music, so adds an extra, more discrete control option for your devices. I asked the manufacturer what they considered the unique selling point of the Jinni and they highlighted convenience. Aside from some Android smartphones with Google Assistant, you typically need to pull out a phone and open an application. Ok, not massively inconvenient really, but the small discrete nature of the Jinni means you can hang it around your neck, clipped to clothing or on a car or bike, and access an assistant much quicker. Also, once you press an assistant button, it stays awake for 30 minutes so that you can re-access even quicker. I imagine Jinni as a potentially useful device for those whose clothing rarely has pockets, with Jinni paired to a phone in a bag, it’s a small, convenient and affordable device to use your phone without needing to have it your hands.The price point is much less than some official home assistants, or smartwatches, which offer some crossover functionalities. At worst, treat jinni as an opportunity to test the three major assistants on a semi-independent device and see if you would consider buying a dedicated device.",Take your smart assistant with you with Jinni
9974,3928725,2018-03-01 17:04:47,"Behind an abandoned military facility 40 miles northwest of Oslo, Norway built a surveillance base in close collaboration with the National Security Agency. Its bright, white satellite dishes, some of them 60 feet in diameter, stand out against the backdrop of pine-covered hills and red-roofed buildings that scatter the area.Classified documents describe the facility as “state-of-the-art,” with capabilities “previously not released outside of NSA.” Despite a hefty price tag of more than $33 million paid by Norwegian taxpayers, the Norwegian Intelligence Service has kept the operations at the site beyond public scrutiny.The station, code-named VICTORY GARDEN, was ostensibly built to support Norwegian troops serving overseas and to combat terrorism. But its dragnet has also secretly captured records of phone calls and emails transmitted between law-abiding Norwegians and their friends, families, or colleagues in foreign countries, an investigation by The Intercept and the Norwegian Broadcasting Corporation, known as NRK, has found.In 2014, the data collection at the base was central to a behind-closed-doors dispute between the Norwegian Intelligence Service and the oversight committee that monitors the conduct of the country’s spy agencies, according to sources with knowledge of the incident. The intelligence service argued that the surveillance was lawful and necessary. But the committee disagreed and claimed that the storing and searching of Norwegians’ communication records was legally dubious. The disagreement remains unresolved; meanwhile, the surveillance appears to have continued unabated.The station in Norway grew in the time period from 2004 to 2016.Images: NRK/Norwegian Mapping AuthorityThe cooperation between the Norwegian Intelligence Service and the NSA began officially in the early 1950s, when Norway and the United States signed an agreement called NORUSA. Due to its geographical proximity to the Soviet Union and its submarine bases on the Kola Peninsula, Norway was uniquely positioned to provide intelligence on Soviet submarines, missile systems, and military activity during the Cold War.The countries have since continued to cooperate closely. In 2001, Norway approached the NSA seeking to buy foreign satellite surveillance technology, known as FORNSAT, according to documents obtained by The Intercept from Edward Snowden. Two years later, the NSA provided the Norwegians with four specialist antennas. Each capability was given code names seemingly inspired by different types of gardens – WINTERGARDEN, FLOWERGARDEN, TOPIARYGARDEN, and so forth – together forming a VICTORYGARDEN.Norwegian intelligence sent employees on multiple trips to receive training and test equipment at the NSA, and a delegation from a now-defunct NSA Yakima facility in Washington state traveled to Norway. Meanwhile, NSA employees based in Oslo took delivery of more than 90 containers crammed with electronic equipment, which were sent by boat and airplane, according to an October 2005 article in SIDtoday, an internal NSA newsletter. Two months later, on December 15, 2005, the Norwegian Intelligence Service’s director, Torgeir Hagen, declared VICTORYGARDEN operational. An NSA article describing the base’s opening ceremony concluded: “We have only begun to see future possibilities to benefit both our nations and the free world.”Erik Reichborn-Kjennerud, a researcher at the Norwegian Institute of International Affairs, said that the documents provided a rare insight into how Norway’s relationship with the NSA has evolved. It “seems that Norway is asking for more and more capabilities, including training that enables them to better conduct surveillance,” Reichborn-Kjennerud said. “There’s been very little public debate about this in Norway.”Until recently, the surveillance station located near Oslo was so secret that the oversight committee could not mention it in unclassified annual reports. Cryptic references in Norway’s defense budget pointed to “modernization of the Defense Satellite Earth Station” without linking these to the intelligence service. Local media were told the large dishes were used for communications with NATO partners and Norwegian forces overseas.U.S. Secretary of State Rex Tillerson, right, talks as he stands with Norwegian Foreign Minister Ine Marie Eriksen Soreide, left, at the State Department in Washington, Thursday, Jan. 11, 2018.Photo: Susan Walsh/APDuring the country’s parliamentary elections in 2017, then-Minister of Defense Ine Eriksen Søreide visited the facility. “Satellite communication is extremely important,” she told Ringerikes Blad, a local newspaper, “when frigates at sea need to communicate with the command center.” She announced an additional 200 million Norwegian kroner ($25 million) funding to strengthen cyber defense and additional satellite equipment.But there is a lot more to the station than enabling “satellite communication.” The documents provided by Snowden state that VICTORY GARDEN can “see 130 foreign satellites” – indicating that it can tap into communications passing across them, including the contents of international phone calls and emails, as well as various types of metadata. (Metadata reveals information about a communication — such as the sender and recipient of an email and the time and date it was sent — but not the written content of the message.)VICTORY GARDEN’s antennas are aimed at countries from which the Norwegian Intelligence Service collects information in support of Norwegian interests and interventions in countries such as Afghanistan, Iraq, and Syria. But Norway’s spies have also used the base for a more controversial purpose, not previously publicly disclosed: For several years, they have collected metadata about Norwegians’ communications with people located in foreign countries and accessed those records through searchable databases. In 2014, the oversight committee found out about that practice and began raising concerns.“Our most important task is to ensure that the Norwegian Intelligence Service does not monitor Norwegians in Norway. It’s not their job,” said the committee’s chair, Eldbjørg Løwer, in an interview for this story. “We were uncertain whether the way they conducted these operations was sufficiently grounded in the law.”Antennas at the Norwegian Intelligence Service surveillance base in Ringerike, 40 miles northwest of Oslo.Photo: Norwegian Armed ForcesThe intelligence service told the committee that Norwegian citizens’ metadata is used to identify new targets. It denied that it used the intercepted communications for “mapping domestic relations or matters relating to Norwegian persons.” The country’s defense minister, Frank Bakke-Jensen, claimed that the spy agency “does not conduct surveillance against Norwegians in Norway.” He added, however, that “the legislation needs to be updated.”In response to questions for this story, Norwegian spy chief Morten Haga Lunde said that a new law currently in the works will solve the oversight committee’s legal concerns. The VICTORY GARDEN base, he added, served an important purpose and helped save lives in 2013 during a terrorist attack on an Algerian gas facility.It is unclear how many Norwegians’ communications have been swept up by the surveillance to date. A spokesperson for the intelligence service said he could not provide that information.",Norway Used NSA Technology for Potentially Illegal Spying
9975,3928726,2018-02-28 12:50:51,"Young Icelanders spend a large part of their lives in an almost entirely English digital world. Photograph: AlamyUnlike most languages, when Icelandic needs a new word it rarely imports one. Instead, enthusiasts coin a new term rooted in the tongue’s ancient Norse past: a neologism that looks, sounds and behaves like Icelandic.The Icelandic word for computer, for example, is tölva, a marriage of tala, which means number, and völva, prophetess. A web browser is vafri, derived from the verb to wander. Podcast is hlaðvarp, something you “charge” and “throw”.This makes Icelandic quite special, a language whose complex grammar remains much as it was a millennium ago and whose vocabulary is unadulterated, but which is perfectly comfortable coping with concepts as 21st-century as a touchscreen.How Iceland became the bitcoin miners’ paradiseRead moreBut as old, pure and inventive as it may be, as much as it is key to Icelanders’ sense of national and cultural identity, Icelandic is spoken today by barely 340,000 people - and Siri and Alexa are not among them.“It’s called ‘digital minoritisation’,” said Eiríkur Rögnvaldsson, a professor of Icelandic language and linguistics at the University of Iceland. “When a majority language in the real world becomes a minority language in the digital world.”Secondary school teachers already report 15-year-olds holding whole playground conversations in English, and much younger children tell language specialists they “know what the word is” for something they are being shown on the flashcard, but not in Icelandic.Because young Icelanders in particular now spend such a large part of their lives in an almost entirely English digital world, said Eiríkur, they are no longer getting the input they need to build a strong base in the grammar and vocabulary of their native tongue. “We may actually be seeing a generation growing up without a proper mother tongue,” he said.An Icelandic fisherman with his mobile phone. Photograph: AFP/Getty ImagesThe language has survived major foreign inputs in the past, under Danish rule for example. The impact of English, however, “is unique in scale of impact, intensity of contact, speed of change”, Eiríkur said. “Smartphones didn’t exist 10 years ago. Today almost everyone is in almost full-time contact with English.”The range and volume of English readily accessible to Icelanders has expanded exponentially, most of it more relevant and more engrossing than ever before, said Iris Edda Nowenstein , a PhD student working with Eiríkur on an exhaustive three-year study of the impact of digital language contact on 5,000 people.“Once, outside school you’d do sport, learn an instrument, read, watch the same TV, play the same computer games,” she said. “Now on phones, tablets, computers, TVs, there are countless games, films, series, videos, songs. You converse with Google Home or Alexa. All in English.”Icelandic’s relatively few speakers are also unusually proficient in English and enthusiastic early adopters of new technology. “The obvious worry is that young people will start to say: ‘Okay, so we can’t use this language abroad. If we’re not using it much in Iceland either, then what’s the point?” Eiríkur asked.In what amounts to a perfect storm for such a small language, it is also under siege in the real world. The wild north Atlantic island welcomed almost two million foreign visitors last year, four times the 2008 figure, and immigrants now make up 10% of the population, a five-fold increase in two decades.Mostly EU workers on short-term contracts in fish-processing or tourism, new residents rarely need to master Icelandic, with its three genders, four cases and six verb forms. In the bars, restaurants and shops of downtown Reykjavík, it can be a struggle for locals to get served in their native language.Online, however, is the biggest concern. Apart from Google – which, mainly because it has an Icelandic engineer, has added Icelandic speech recognition to its Android mobile operating system – the internet giants have no interest in offering Icelandic options for a population the size of Cardiff’s.“For them, it costs the same to digitally support Icelandic as it does to digitally support French,” Eiríkur said. “Apple, Amazon … If they look at their spreadsheets, they’ll never do it. You can’t make a business case.”Where Icelandic versions do exist, said Nowenstein, they are not perfect. “You can switch Facebook to Icelandic, but it’s not good at dealing with cases,” she said. “So people get fed up with seeing their names in the wrong grammatical form, and switch back to English.”Max Naylor, a UK academic also involved in the study, said he had emailed and written to Apple several times but had never received a reply. “We’re not expecting a fully-functioning operating system, but the hope is that they will at least open themselves up to collaboration,” he said.The Icelandic government is setting aside 450m krónur (£3.1m) a year over the next five years for a language technology fund it hopes will produce open-source materials developers could use, but the challenge – from apps and voice-activated fridges to social media and self-driving cars – is immense.Icelandic has survived almost unscathed for well over 1,000 years, and few experts worry it will die in the very near future. “It remains the majority, official language of a nation state, of education and government,” Nowenstein said.“But the concern is that it becomes obsolete in more and more domains, its use restricted, so it’s second best in whole areas of people’s lives. Then you worry about Icelanders understanding much less, for example, of their cultural heritage.”In the meantime, Naylor said, literacy rates among Icelandic children are falling as their vocabulary shrinks. “You could soon have a situation where Icelanders will be native in neither Icelandic or English,” he said. “When identity is so tied up with language … it’s hard to know what that will mean.”This article was amended on 27 February 2018 to reflect the fact that Icelandic people are typically referred to by their first name on second mention.",Icelandic language battles threat of 'digital extinction'
9976,3928731,2018-03-02 09:03:21,"We’ve said for some time that Uber and Lyft are exploiting the fact that their drivers don’t understand their own economics and don’t factor in the wear and tear on their vehicles. One former Uber driver did a back of the envelope work up and argued that you’d make more than minimum wage only if your car was more than six years old. The fact that only 4% of Uber drivers continue for more than a year suggests that working for these ride-sharing companies is an unattractive proposition.A large-scale study confirms these doubts about driver pay, and then some. A team from Stanford, Stephen M. Zoepf, Stella Chen, Paa Adu and Gonzalo Pozo, under the auspices of MIT’s Center for Energy and Environmental Policy Research obtained information from 1100 Uber and Lyft drivers using questionnaires and information about vehicle-specific operating costs, such as insurance, maintenance, repairs, fuel and depreciation.Results show that per hour worked, median profit from driving is $3.37/hour before taxes, and 74% of drivers earn less than the minimum wage in their state. 30% of drivers are actually losing money once vehicle expenses are included. On a per-mile basis, median gross driver revenue is $0.59/mile but vehicle operating expenses reduce real driver profit to a median of $0.29/mile.If you gross up the median hourly profit to gross revenue, using the same ratio for gross revenue versus net profit per mile, median gross revenue is only $6.86 an hour, still below minimum wage. These drivers would be better off doing almost anything else. Consider the safety risks. From Wired:Because the ridesharing industry is so new, and laws regulating it so patchwork, official figures are tough to come by, and the big companies don’t share specifics about incidents their drivers report. Still, online forums for drivers brim with descriptions of attacks on drivers by passengers, both verbal and physical, such as a driver posted a video of being spit on and punched.You might think ridesharing companies would be doing everything they can to ensure driver safety. But it turns out what they can do is limited by the kind of businesses they are. Because drivers operate as independent contractors instead of employees, the companies can’t offer true safety training. Under federal law, training is a signifier that someone is an employee, and both Uber and Lyft have fought bitterly against re-classifying drivers as employees. By the very nature of how on-demand businesses operate today, drivers in many ways have to go it alone…Still, if ridesharing companies don’t make their figures public, federal regulators do. “Taxi drivers are over 20 times more likely to be murdered on the job than other workers,” the US Occupational Safety and Health Administration said in 2010. In a 2014 report, the Bureau of Labor Statistics found that of 3,200 3,200 taxi drivers who were hurt or killed on the job, 180 sustained injuries caused by a violent person—about 5.6 percent.As if oh so charitable Uber would actually want to spend money on driver safety training….but you get the point.The CEEPR study also points out that the fact that so many drivers lose money also means governments lose out on tax revenues:For tax purposes the $0.54/mile standard mileage deduction in 2016 means that nearly half of drivers can declare a loss on their taxes. If drivers are fully able to capitalize on these losses for tax purposes, 73.5% of an estimated U.S. market $4.8B in annual ride-hailing driver profit is untaxed.More than 80% of the drivers work less than full time. That means most are doing badly despite presumably focusing on peak hours when they can benefit from surge pricing. It’s hard to see how anyone makes anything approaching a living or even an adequate supplemental income: “On a monthly basis, mean profit is $661/month (median $310).”As Lambert pointed out, this looks an awful lot like Marx’s immiseration of the proletariat. From Das Kapital:Within the capitalist system all methods for raising the social productivity of labour are put into effect at the cost of the individual worker […] All means for the development of production undergo a dialectical inversion so that they become a means of domination and exploitation of the producers; they distort the worker into a fragment of a man, they degrade him to the level of an appendage of a machine, they destroy the actual content of his labour by turning it into a torment, they alienate from him the intellectual potentialities of the labour process […], they transform his life into working-time…But as one colleague pointed out, the people who seem keenest about Uber are the affluenza and high-end professionals who can easily afford to pay more for car service. Her take was that this was another manifestation that they now see it as a matter of right to have a servant class at their beck and call. And not even a well treated servant class.Interesting that when the cab drivers were getting their financial throat cut by these uber drivers, they were fine with the situation. Now that they are becoming the low payed servants, crying is their feature. BooHoo……Cab drivers were ALWAYS getting their throats cut. The medallion owners were making bank. The only good thing to come from Uber/Lyft is the collapse in medallion values. And can we please stop JUST using Uber. It’s Uber and Lyft! Lyft has capitalized on the brand destruction of Uber and my friend who (unfortunately) drives for Lyft says they are just as evil. And I have minor quibble with Yves description that affluent professionals are the most defensive of the Uber/Lyft model. Many low income urban workers are ditching public transit and opting for door-to-door service that is only marginally more expensive. My friend says they are invariably the most difficult low fare passengers who never tip and are being conditioned and VERY dependent on the whole cheap fare industry model. I’m beginning to wonder if autonomous vehicles are really the goal or is ride-sharing targeting municipal budgets for public transit.Marco: Sounds from your description that ride-sharing has replaced jitney cabs in under-served neighborhoods. The poor keeping the poor poor, although it is hard to blame people of limited means. (And I am not.) Most people under the age of, oooohhh, 50, have never even heard of jitney cabs. So Uber and Lyft seem like “disruption,” when as the post points out, they are impoverishment.My buddy drives for Lyft and knows it’s a losing proposition. He got laid off and started driving when his unemployment was running out – he basically does it to get out of the house because fortunately he has a well paid wife. As you mention, the people he typically drives are well paid Amazon or other tech company office employees who are too good to take the bus and wind up with personal door to door service sometimes for less than the bus ticket costs. The only way that is possible is by completely screwing the driver.Prediction: If the neoliberals get their way, driving for Uber and Lyft will replace traditional government unemployment benefits. Get out there and make Travis rich or starve.So much of Uber’s popularity is just trendiness, there are so many who just jump on any new internet or app technology no matter how stupid or pointless because its seen as the new trendy thing. If you ask why not take a cab they look at you like your just some old Luddite who hates technology, there is a huge fear I think of people being behind the curve with tech. A form of old technology user shaming. You see a lot of that with social media as well. Not to mention the mainstream media pretending cabs don’t exist anymore.",MIT Study: Median Uber and Lyft Profits Less Than Half Minimum Wage; 30% of Drivers Lose Money | naked capitalism
9977,3928732,2018-03-01 13:00:00,"Burning Out: What Really Happens Inside a CrematoriumFour decades ago, less than 5 percent of American were cremated when they died. Now that figure stands at nearly half. This is how cremation actually works, and the story of what happens to a culture when its attitude about how to memorialize the dead undergoes a revolution.Rosehill Cemetery in Linden, New Jersey, is awash in small-town trappings: tree-lined roads, rolling lawns, and street signs at every corner. On this Wednesday midsummer morning, the familiar routine of loss plays out across the acres. A yellow taxi waits at the end of a row of graves for someone paying their respects. Men and women clad in church clothes line up their cars along the curb and make their way to a grave site. A backhoe digs out some earth, another spot for another resident.This is the textbook way we treat our dead. Someone passes, they’re buried, a headstone marks their place out among the rows in the borough of the departed. But today I’m bound for a different part of the cemetery, one fewer people see—though that fact is rapidly changing.This place is called the columbarium, and at first, the very existence of this vast chamber full of urns can come as a surprise. In the movie version of life and death, a cremated person’s remains sit up on the shelf at home, or friends scatter their ashes in the wind over a sacred locale. In the real world, many cremated people stay in the cemetery, just like their buried counterparts.Advertisement - Continue Reading BelowThere are certain words you’re not supposed to say in a crematorium.Rose-colored carpeting covers the floors here. The whir of a vacuum cleaner punctures the silence. Cubby holes or niches line the walls, and the varying sizes and styles of urns within them marks the passing of the eras. Older urns are ornate; one is topped by an eternal flame, while another is shaped like a Bible. One inscribed “Henrietta Leiber, 1866-1933” is shaped like an acorn. Next to it leans a photo of Henrietta, who’s standing behind a chair in a sleeveless white dress and long pearls, her hair fashioned in a bob like a flapper.More contemporary urns are boxier and cleaner in style. They’re also larger, and not for vanity’s sake. The cremation process recovers a lot more of the human body than it used to. Some families have packed their niche with flowers, family photos, or pictures of Jesus. Others skipped the niche entirely and entombed the cremated remains behind a marble plaque. It is a curious thing, as if the body was broken down into its smallest organic parts, then surrounded with stone to protect them.The doors at the back of the Rose Room.Getty ImagesCaren CheslerWe are seeing a fundamental shift in how we approach death and what comes after. Compared to just a few decades ago, vastly more Americans are foregoing the old-fashioned burial and turning to the alternative of cremation. This is what brought me here to Rosehill, and now my tour with Jim Koslovski, president of the Rosehill and Rosedale Cemetery, is about to go deeper into his world to see how cemeteries are dealing with America’s after-death revolution.As I follow him deeper inside the columbarium, we pass through the Rose Room. Urns here are not hidden in niches behind glass, but instead are on display in the open air. I prefer it this way. The glass cases remind me of the razors at the drug store—the ones you can only access by notifying a salesperson with a key. Deeper still, at the very rear of the room, lies a set of stained glass doors. Koslovski slides them open to reveal a hidden set of spy-movie doors, these made of metal. They are solid for a reason: Behind them lies the crematorium itself.The doors open, and we stroll onto what looks like the floor of a factory, but one dedicated to a certain kind of deconstruction.Socially AcceptableThe Nimtala Burning Ghat (Funeral rites) is the oldest and the most famous cremation ground of Kolkata. It is situated in Central Kolkata.Getty ImagesBack in 1980, less than 5 percent of Americans were cremated when they died. That figure now stands at about 50 percent, according to the National Cremation Association of North America. Changing cultural and religious standards are at play here, for sure. But if you want to see one event that accelerated the change, look no further than the Great Recession.“We saw a big uptick in cremation when there was the economic downturn in 2008, when people were losing their jobs. Cremation is a less expensive alternative,” Koslovski says.“Less expensive alternative” may be putting it lightly. Rosehill charges just $180 to cremate a body, although the urn, flowers, and service are extra. A grave, by contrast, can cost $2,500, plus an additional $1,500 to open the ground with a backhoe.Rosehill, located about a half-hour from Manhattan, now cremates about 25 bodies per day and has been expanding its facility to meet the growing demand. It already had three cremation machines, but bought an additional unit in 2013, another in 2016, and expects to have a sixth up and running by the end of the year.The cremator’s rule of thumb: 100 lbs. of human fat is the equivalent of 17 gallons of keroseneOf course, burning the dead isn’t a new concept—it was around long, long before the recession forced Americans to start pinching their pennies. Cremation began in the Stone Age, and it was common though not universal in Ancient Greece and Rome. In certain religions such as Hinduism and Jainism, cremation was not only permitted, but preferred.The rise of Christianity put the brakes on the practice in the West. As early as 330 A.D., around the time that Emperor Constantine adopted Christianity as the Roman Empire’s official religion, Rome outlawed cremation as a pagan practice. The theological reason for the ban was related to the resurrection—it was good to keep the body whole or in one place. Through the Reformation, the Catholic church frowned on or prohibited cremation, though it was used for punishment and hygiene reasons. Jewish law also banned the practice. By the 5th century, cremation had all but disappeared from Europe.The practice saw a resurgence in Europe in the 1870s, mostly because of public health concerns about curbing the spread of disease. The first modern crematory was built in the U.S. in 1876. A second came eight years later. By 1900, there were 20. The practice got another boost in 1963, when the Catholic Church reversed its opinion on cremation during the Vatican II reforms and said cremation was permitted (though ash scattering was not).Today, there are more than 2,100 crematories around the United States, and the cremation resurgence isn’t just about cost. There are other factors including fewer religious prohibitions on the practice and changing consumer preferences, such as the desire for simpler, less ritualized funerals. Our increasingly mobile way of life plays a part, too, says Robert Biggins of Magoun-Biggins Funeral Home in Rockland, Massachusetts “People aren’t growing up in Mayberry RFD and staying there their whole lives, We’re much more mobile. Generation X and Millennials, they stay in a job on average five to seven years.” Americans don’t want to be sedentary in death, either.Simply put, cremation has become socially acceptable. Acceptance varies by state and ethnicity, according to a report by the National Funeral Directors Association, but in places like California, Oregon, and Southern Florida, 60 to 80 percent of the dead are now cremated, while the number is much lower in the Bible Belt and among certain cultures, including Catholics and African-Americans.And there’s one more force pushing cremation as an alternative: Cemeteries are running out of space, Koslovski says. He estimates Rosehill has only 15 years before it’s out of room. It’s no wonder, then, that a lot of cemeteries have applied to build crematoriums—though there’s often opposition, particularly if they’re in a residential area.“There’s a stigma,” Koslovski says. “There’s still a segment of society that see cremation as gruesome or ghoulish, and they don’t want it in their backyard.”How Cremation WorksRosehill CrematoriumCaren CheslerKoslovski and I pass through the double doors. As we stand on the floor of the crematorium, a bell rings out.“What’s that for?” I ask.“That indicates that there’s a hearse probably backing up to the door,” he says. “So when the guys are in here operating, if they’re doing something and they hear the bell, they know someone is coming.”The bodies arrive in caskets, occasionally made of wood but more commonly cardboard. They remain in these containers during the entire stay. There are health reasons for this, such as protecting the technicians from infectious diseases. There are moral reasons—“the family would want them in something,” Koslovski says. There are logistical reasons, too. “It would be extremely difficult to load a set of human remains without a casket. Just think of a body, and trying to put it into a cremation unit.”The caskets go into the crematorium’s walk-in cooler, which is lined with shelves of them. One casket has a label on it from Delta Airlines that says, “Human Remains,"" and under it, ""Delta Cares."" Bodies typically remain a day or two in the cooler, because most states require a 24-hour waiting period between when someone dies and cremation can occur. When something is so final, you want to take a pause.Advertisement - Continue Reading BelowFive large cremation units occupy the floor, each covered in diamond-plated aluminum like you might see on a fire truck or a high-end tool box. It’s called a cremation unit, by the way, not an “oven.” And don’t call the process incineration, even though it is. There are certain words you’re not supposed to say in a crematorium.Ovens in the crematorium of ’Barrack X’ at the site of the former Dachau Nazi concentration camp in Bavaria, Germany.Getty ImagesRichard Blanchard“With ovens, you think of Auschwitz, and that definitely has negative connotations, so people shy away from that nomenclature,” says Brian Gamage, director of marketing at U.S. Cremation Equipment in Altamonte Springs, Florida.When a body is ready to be cremated, it is removed from cold storage and placed on a retractable table that looks like a gurney, then wheeled over to one of the machines. Cremation is the kind of business where an error would be catastrophic, unforgiveable, and so Rosehill actually uses two forms of ID to make sure the family gets back the right remains. A copy of the receipt is attached to the outside of the cremation unit, and a metal ID tag, similar to a dog tag, accompanies the deceased inside the unit.While the door can open about 30 to 35 inches wide, most operators open it only a foot or so, enough to accommodate the width of the body. Any more than that will let out too much heat, exposing the operator and the room to fiery temperatures. The body slides in, pushed with a tool or by hand. There are rollers on the gurney and sometimes on the floor of the cremation unit so the casket can slide with ease.A cremation unit has two chambers: the primary chamber, where the body goes, and the secondary or “after” chamber, where the gases generated are burned off.The primary chamber has brick-lined walls, and a floor and roof made of high heat refractory concrete. A burner descends from the roof and heats the chamber to about 1,200 degrees Fahrenheit, enough to break down a body into gas and bone fragments.The resulting gases and particulates travel into the after-chamber, a 30-foot maze designed to retain the gases for about two seconds. The after-chamber subjects the gases to a temperature of 1,700 degrees F to make sure the particles and odor are negligible before everything goes up the stack and out into the atmosphere. Gamage says you can think of the secondary chamber like the catalytic converter on an old car, which neutralizes the emissions of the exhaust system.“Any solid will turn to gas if heated to the right point. That’s essentially what happens to the body, when the tissue is heated to the point where it’s combustible and turns to gas,” Gamage says. “But just like in any combustion device, whether it’s a car or a backyard grill, when you burn something, there’s going to be emissions generated. The key is to design equipment that consumes most of the emissions so that they fall within the state environmental regulations.”The particulates emitted must be less than 0.1 grains per dry standard cubic foot, according to environmental agencies in most states. Problems arise when gases build up in the secondary chamber and begin to overflow. That can happen if the machine isn’t designed properly or if the operator overloads the primary chamber, which can happen for surprising reasons. For example, putting an obese person in the unit at the wrong time of day.As macabre as it may sound, weight is something crematorium operators must worry about. The machine doesn’t know the difference between a person who weighs 150 pounds and a person who weighs 400. It just does its job. The cremator’s rule of thumb is that 100 pounds of human fat is the equivalent of 17 gallons of kerosene. If you have a body that weighs 400 pounds, at least 200 of it will be fat that will burn rapidly. If you put that person into a very hot machine, as the cremation unit tends to be at the end of the day when it’s been running for hours, the chamber may emit smoke and odor out of the stack.“It’s just too much gas for the machine to handle,” Gamage says. “Most experienced operators will do those larger cases as the first cremation of the day, when the machine is colder.”Screens on the Rosehill units.Caren CheslerInside the Rosehill crematorium, I’m staring at a computer monitor that reduces this ritual into raw data. The body inside is a male, it’s the second case of the day, it’s in a cardboard container weighing 201 to 350 lbs, and it has already been there for an hour and 20 minutes. A diagram on the screen shows the machine’s various chambers. Three little blue flames are illuminated under one of the chambers, indicating that “hearth air” is now being blown in to the chamber to help cool it down. It’s currently between 910 and 1150 degrees inside, but moments earlier, the temperature had been 1600 to 1800 degrees.Altogether, it takes about an hour and a half to cremate a body, though that varies depending on the person’s weight and the type of casket they’re in. The time-consuming nature limits the number of bodies each can cremate. During my visit, all five of Rosehill’s machines were in various states of operation just to keep up with demand. Each needs to get five bodies done in eight hours. Rosehill’s cremation units run six days a week, standing idle only on Sundays.Advertisement - Continue Reading Below“For religious reasons?” I ask Koslovski.“No,” he says. “we just need a day off.”Close To HomeGetty ImagesWhen Lisa Tomasello was growing up in a large Italian Catholic family, the death of a relative was just the beginning of a grueling two or three days. Visitors would sign the guest book in the outer room, and form a line to get to the casket. People would sit in front of the body of the deceased, kneeling and praying and making the sign of the cross, before kissing them on the hand, face or lips. “The closer the relation, the closer to the lips,” she says.The immediate family sat in the front row receiving visitors in front of the dead body. Tearful outbursts and cries in Italian were commonplace. During the breaks in the wake, the family would go out for dinner and laugh and tell stories before returning to the funeral home for several more hours of crying. And all that was all before the funeral, which would start at a funeral home, resume at a church, and culminate at a cemetery before everyone was invited back for lunch.But once the body was buried and the headstone placed, then what? Tomasello, who I knew growing up, says it’s a question that nagged at her after every cycle of mourning. Maybe you visit the deceased a few times during the first few years. Maybe you don’t visit them again until another family member is buried in that plot. “My grandparents’ graves haven’t been visited in 30 years,” she says.When Tomasello grew up and her own parents passed away, she wanted something different. She and her siblings decided to have a small service and to cremate their mother’s body when she died. When her father followed a few years later, they dispensed with the formal service and made a toast to him with a shot of Jack Daniels, then had him cremated and divvied up the ashes.“I have my parents in my bedroom and I am comforted with them there,” she says. “There is no pressure or guilt of having to visit them in a cemetery, and they will stay with me until the end of my time.”“My grandparents’ graves haven’t been visited in 30 years.”Advertisement - Continue Reading BelowIt’s hard to let people go. We want them in an urn, to keep them near, sometimes even anthropomorphizing these objects as a way of bringing our loved ones back to life. The urn doesn’t contain mom’s ashes. The urn is mom.I bought a bench on the boardwalk in my town to memorialize my father. Now that bench is my father. When I watch the sunrise and see the bench’s silhouette, he is watching it with me.What Is Left BehindCaren CheslerThere is no easy way to say it: The physical attributes you picture when you envision a loved one—the eyes, the skin, the hair—disappear during the cremation process. Even after all we’ve been through—our experiences and memories, pain and suffering, tests taken, facts learned—one of the biggest parts of our cremated remains is the coffin. “Cremated remains are typically bone fragments and casket ash,” Koslovski says. “Remember, we’re 75 percent water.”Once the heating is over, cremated remains are put onto what looks like a silver baking tray. A technician runs a magnet over them to remove all of the ferrous materials that did not combust during the cremation process. These often come from a person’s staples, screws, hinges, and prosthetic joints. Someone then has to clean by hand any material the magnet missed—say, the bits of glass left behind because someone wanted their father cremated with a bottle of scotch. Those pieces are buried somewhere on the cemetery grounds.“What is that?” I ask, pointing to one of the silver trays of remains.“I don’t know why. It could be something the person was treated with. It’s hard to say. It could have been cancer.”The crematorium puts the bones and ash that remain into a pulverizer, not unlike a food processor. The remains are then put through a sieve and into a container for the family—but not always. Some Asian cultures want to be able to pick through the unpulverized remains to take bone fragments. A skull or hip bone is prized. They don’t want the bone fragments processed at all.Caren CheslerHindus often want the eldest son to commence the cremation process as a rite of passage, so he’s allowed down on the crematorium floor to turn on the machine. Other families just want to observe the process—about a dozen make that request each week. Rosehill allows them to do so, from an observation deck. To Koslovski, it’s about making sure people understand the process, that they’re not afraid or skeptical of cremation because of misinformation or rumor.“[Some people think] you’re cremating multiple people together. You’re reselling caskets. It can be anything. People watch the news.”I press him on the stories, the urban legends about crematoriums. Is any of it true? Do cremated remains from one person ever wind up getting mixed in with another? He explains that everyone is cremated separately and the units are swept thoroughly after every cremation.However, I remember Barbara Kemmis, the spokesperson for the Cremation Association of North America, telling me that while machines are swept or vacuumed between cremations—and that while operators do their best to remove recoverable remains—it’s possible that minute amounts could get caught in tiny wells and divots in the brick walls or concrete floor of the machinery and inadvertently wind up in another person’s cremated remains. Another part of the process that’s perhaps best not to think about.Advertisement - Continue Reading BelowThe Things We Cannot BuryCardboard casketsCaren CheslerCremation, like death, is final. But that doesn’t mean you won’t have second thoughts. Susan Skiles Luke, a marketing consultant in Columbia, Missouri, had her mother cremated and buried in a family plot. Now, she wishes it was her mother’s body and not just the cremated remains that was in the grave.“When I go there, which isn't often, I want to feel like her body is under ground, right alongside my grandparents and beloved great aunt, all dressed up in their Sunday best, not some heavy shoebox of something that looks like cigarette ashes,” she said.When her older brother, with whom she was very close, died tragically of a drug overdose 13 months later, skipping the old-fashioned burial in favor of cremation was a godsend. It allowed her to skip the whole public ordeal of a funeral. ""If you're still pissed—maybe they checked out like my bro did—you don't have to go through the public drama of a casket, dealing with the body, to display it not, discussing the circumstances of his death,” she says. ""[You can] deal with the logistics of 'the body' when you're ready.""That's one of the advantages of cremation: You can address your emotional issues with the dead on your own terms. The disadvantage? Now you’re left with the remains, this tangible object impressed with memories. After Luke’s brother passed away, she picked up his ashes on the way home from work, as if it were just another weekday errand. The funeral home was on the way home, after all. ""I was too stupid to ask someone else to pick (my brother) up, and had never done it before. I wasn't prepared for how personal it would feel,” she said. ""I threw my brother's ashes in the trunk with a thump and cried all the way home.”A few years later, when her stepfather passed, she couldn’t even bring herself to pick up the ashes, even as the funeral home kept calling. “I never spoke to them. I listened to one voicemail, politely reminding me to 'come get your dad,' It was the phrase, coupled with the fact 'my dad' was a bunch of ashes stuffed in a box, that just reminded me of that afternoon I picked up (my brother) Tom,” she said.One day, she returned home to find her dad's ashes sitting on her doorstep.She now has two boxes of their remains in storage somewhere, though she doesn’t know exactly where. She asked her husband to hide them somewhere so she didn’t have to look at them. “Not the healthiest reaction,” she admits.Ellen Herman, who sells digital advertising in Los Angeles, is in a similar situation. About nine years ago, her parents died about a year apart and were cremated. She went to a mausoleum in Florida, where her parents lived before they died, to see if there was a place she could put them in a drawer and honor them with some nice words.Advertisement - Continue Reading BelowShe meant to, anyway. She never followed through.Ellen Herman hung onto the remains of her parents. They’re in the bags marked ""Neptune Society.""Ellen Herman""They are in my house. Actually, in my bedroom! In boxes, under a bunch of other shit,” she says. ""I had them in the garage for a little while, but that felt wrong too.""Some of their ashes were scattered in various places, individually and combined, and her dad's brother has a bit of her dad, but the bulk are in the box in her house. ""Neither of my brothers wanted the stuff in their homes, and I didn't feel right scattering all of them,” she says. “I suppose the fact that families don't live as close together as they once did lessens the significance of a burial plot to visit, but I still find the remains sitting in a box in my bedroom less than ideal and respectful.""Sometimes, instead of burying people in the ground, we bury them among our stuff. We lose them among the emotionally charged paraphernalia of our lives. It is just too hard.Back to the EarthWe come from the Earth, we return to the Earth. That may be true, but the way we return to the Earth matters on more than an emotional level. It’s an environmental concern. As cremation continues to replace burial as a go-to way of dealing with the dead, the emissions that come along with this process are becoming a serious worry—so much so that people are starting to consider some wild-sounding alternatives for disposing of human remains.There is now a water-based process called alkaline hydrolysis, which is being marketed as a more environmentally friendly postmortem option because it produces less carbon monoxide and pollution. Alkaline hydrolysis involves placing a body in a chamber that is then filled with water and potassium hydroxide and heated to about 320 degrees F at high pressure. After three hours, the body becomes a green-brown tinted liquid and bones are soft enough to be crushed. The bones can be returned to the family, while the liquid can be sent into the sewer system.If this sounds rather dystopian to you, it’s partly because the process was invented as a way to dispose of animals infected with mad cow disease. When farmers in Europe had to put down herds of cattle infected by mad cow disease, their initial answer was to dig trenches, pour gasoline, and set the animal carcasses on fire. When alkaline hydrolysis was introduced in the 1990s, manufacturers made stainless steel vats about 20 feet across into which the carcasses could be thrown. The pressure of the alkaline hydrolysis process would kill the prion—the protein particles in the animal’s brain that are believed to have caused the disease.Advertisement - Continue Reading Below""People think, wow, you dissolved mom, and you’re putting her in the sewer.""In the years since them, some companies have proposed alkaline hydrolysis as a more environmentally friendly solution for human remains. “They took the technology and tried to apply it to the cremation side,” says Gamage of US Cremation Equipment. “It’s capitalism at its finest.”Perhaps unsurprisingly, this process has taken off as a popular post-mortem solution for people. It’s slower. The technology is more expensive: A stainless steel pressurized unit can cost from $175,000 for a basic unit to $500,000 for a high end unit, while a cremation unit costs about $80,000 to $100,000. There are legal issues, too, because the process is prohibited unless a state passes legislation specifically allowing for it.And then there is the “ick” factor: We’re talking about reducing a human body to a soupy mess that goes into the sewer. That may hold some allure for people put off by the idea of burning a body into nothing but bone and ash, but most people haven’t come around to the way byproducts are disposed after alkaline hydrolysis.Koslovski, ever the pragmatist about dealing with death, sees it another way.“People think, wow, you dissolved mom, and you’re putting her in the sewer. I understand that. But my thought is, with embalming, the fluids from your mother are being put in the sewer system as well. It’s the same thing.”The Missing MarkersMexican musicians play next to a columbarium during the Day of the Dead celebration on November 02, 2014 in Morelia, Mexico.Getty ImagesJan SochorIn the movies, characters are always scattering the ashes of a loved one over the side of a boat or off the top of a mountain. In reality, cremation rarely ends that way. The Cremation Association of North America estimates that 60 to 80 percent of cremated remains go home with people who intend to place them in a cemetery or scatter them at a future date. But while that may be their intent, scattering is not as popular as people think.“Based on recent media coverage of people seeking to recover cremated remains lost in fires, floods and mudslides, I suspect a high percentage of remains are in homes,” Kemmis says.There are actually laws dictating where ashes can be spread. In Massachusetts, for instance, the law says cremated remains may be “scattered with candor.” “What does that mean? It means you can’t just run down Main Street and throw them in the air or sprinkle them in your neighbor’s driveway. But there’s nothing to say you can’t sprinkle them in the golf course where your dad hit golf balls for 40 years,” says Biggins of Magoun-Biggins Funeral Home in Massachusetts.Advertisement - Continue Reading BelowWhile scattering ashes seems romantic, there’s something to be said about keeping your loved one in one place, and then marking that place with a name.“We mark the graves of our loved ones with a headstone to memorialize them, so we don’t forget them,” Biggins says. He lost his wife tragically when she was 57, and he visits her grave frequently and finds comfort in just seeing her name. “And when I see how many people remember her, they may leave a pebble or a coin, for me to go there on a weekly basis and see dozens of pebbles and coins, it’s heartwarming to see that people remember her.”The law says cremated remains may be “scattered with candor.”As I leave Rosehill cemetery, I decide to stop by the grave of my friend, David, who grew up in Harlem and was dealt a bad set of cards. His mother was an alcoholic. His father had left. And while he had a mother and grandparents, he still wound up in the child welfare services system. He was sent to a city-funded boarding school outside New York and managed to graduate with a football scholarship to the State University of New York at Cortland, but he lasted just one semester before landing back in Harlem. And like something out of a bad movie, he met a girl, was introduced to crack cocaine, lost his job, wound up with HIV and ultimately developed kidney issues that landed him on dialysis for well over a decade. He was on the kidney donor list and was near the top when he died of heart failure in 2015.I’d gone to his funeral, but hadn’t made it out to the cemetery—the one in which I now found myself. I decided to visit his grave. I followed the directions I was given, to Section 48, Row 24, Grave 83. It was a large cemetery, but when I finally found the section, it was easy to find the grave. I was surprised to see nothing marking the spot where he was buried. It was just a patch of dirt with a number “83” handwritten in concrete. There were big marble headstones on one side of him and on the other side, a pile of plastic flowers, bits of light blue ribbon and raffia, styrofoam crosses that said, “I Love You,” and a deflated white balloon, all contained in a little wire fence, as if there was a party in the neighboring grave site the night before, and David hadn't been invited.Caren CheslerAdvertisement - Continue Reading BelowThe lack of fanfare for my friend seemed unfair. He’d been so generous, to me, to his addict girlfriend, to his niece in Florida to whom he would send money despite having so little, himself. Without a headstone, no one would even know he was under there – or for that matter, that he’d been up here.Whether it’s a burial or a cremation, the hard part is letting a loved one just float off into obscurity. We need that physical marker, a headstone, a bench, an urn, to show that the person existed, that they once walked this Earth.I went to my car and found a trophy my son had found in the garbage and thrown on the floor of the back seat. It was a football player. I took a black Sharpie from the glove compartment and wrote on the front of the trophy, “David, April 23, 1954 to April 23, 2015.” I walked over to Grave 83 and placed the trophy at the top of the patch of dirt where a headstone might go. I then left a pebble, as people sometimes do, and walked back to my car.A Part of Hearst Digital Media Popular Mechanics participates in various affiliate marketing programs, which means we may get paid commissions on editorially chosen products purchased through our links to retailer sites.",Burning Out: What Really Happens Inside a Crematorium
9978,3928889,2018-03-02 10:48:06,"The Samsung Galaxy S9+ camera bumps the Pixel 2 for DxOMark’s top spot0You know the drill, right? A new flagship comes out and bumps the last big name out of the top spot. We’re still a couple of weeks out from the Samsung Galaxy S9/S9+ release date, but the premium handset just got the DxOMark treatment, and it seems the company’s got another feather to put in its flagship’s cap.The site posted a 99 for the S9+, edging out the Pixel 2’s very good camera by a point — and the iPhone X by two. Sure, that’s not exactly the “camera reinvented” message the company was pushing ahead of last weekend’s big announcement at MWC, but it’s certainly in line with the camera-first approach it was promising handset, besting the Galaxy Note 8’s score by five points (the site rated the S8, but not S8+, mind).The site gives the S9+ its highest photo score for a handset so far. The video score, while note quite tops, is still up there. According to DxOMark, “The S9 Plus comes with a camera that hasn’t got any obvious weaknesses and performs very well across all photo and video test categories.”The biggest addition this time out is a dual-aperture for low light shots, and indeed, the camera performs admirably on those. The S9+ also sports a zoom lens not found on the S9 (along with a few other features justifying the $120 price difference), which significantly reduces artifacts. The phone also gets high marks for its bokeh effect (for Portrait Mode), which was pretty solid to start with on the last note handset.There are a few small knocks, including artifacts in bright lighting situation, but all in all, the handset looks to be a standout.",The Samsung Galaxy S9+ camera bumps the Pixel 2 for DxOMark’s top spot
9979,3928890,2018-03-02 08:07:34,"The world’s largest DDoS attack took GitHub offline for less than ten minutes0In a growing sign of the increased sophistication of both cyber attacks and defenses, GitHub has revealed that it weathered the largest-known DDoS attack in history this week.DDoS — or distributed denial of service in full — is a cyber attack that aims to bring websites and web-based services down by bombarding them with so much traffic that their services and infrastructure are unable to handle it all. It’s a fairly common tactic used to force targets offline.A blog post retelling the incident, GitHub said the attackers hijacked something called ‘memcaching’ — a distributed memory system known for high-performance and demand — to massively amplify the traffic volumes they were firing at GitHub. To do that, they initially spoofed GitHub’s IP address and took control of memcached instances that GitHub said are “inadvertently accessible on the public internet.”The result was a huge influx of traffic. Wired reports that, in this instance, the memcached systems used amplified the data volumes by around 50 times.GitHub’s inbound traffic skyrocketed during the attackGitHub called in assistance from Akamai Prolexic, which rerouted traffic to GitHub through its “scrubbing” centers which removed and blocked data deemed to be malicious. Following eight minutes of the assault, the attackers called it off and the DDoS stopped.In total, GitHub was offline for five minutes between 17:21 to 17:26 UTC, with intermittent connectivity between 17:26 to 17:30 UTC.The service has become critical for any company handling code — very many, indeed — so while an outage is never welcomed, the response in this case is impressive and certainly bodes well. GitHub said it continues this attack, and others, to ensure it is suitably defenced.",The world’s largest DDoS attack took GitHub offline for less than tens minutes
9980,3928891,2018-03-02 00:05:52,"Uber co-founder Garrett Camp is creating a new cryptocurrency0Garrett Camp, best known for being a co-founder of Uber and founder of the accelerator/venture fund Expa, is launching his own cryptocurrency.The currency is called Eco, and Camp wants it to be a digital global currency that can be used as a payment tool around the world for daily-use transactions.There will be one trillion tokens issued initially, of which 50% will be given away to the first one billion verified humans that sign up. 20% will go to the universities running trusted nodes, 10% will go to advisors, 10% will go to strategic partners, and 10% will go to a newly formed Eco Foundation which will be responsible for creating and maintaining the network. Camp as well as a small number of partners affiliated with Expa will also donate $10M to seed the foundation with an operating budget.First, it wants to use only verified nodes for network support and transaction confirmations, meaning someone anonymous couldn’t run a node and confirm transactions like they could do on bitcoin’s network. While this essentially removes issues of 51% attacks or other acts of fraud, it also means it won’t be truly decentralized.Second, Eco will have a large token supply (one trillion, at least initially) and simple web and mobile apps. This is likely the project’s attempt to be more user friendly, meaning a smaller dollar to Eco token conversation rate so regular users aren’t scared away by high token prices, which often happens with bitcoin. Similarly, the web and mobile app directive likely means they want to make wallets easily accessible to anyone regardless of technical ability.The last improvement is that Eco wants to be energy efficient when it comes to transaction verification and token generation, meaning there won’t be a network of electricity-intensive miners supporting the network like some other cryptocurrencies have.Eco is an admirable concept, but not a novel one. Since the early days of crypto, groups have been “copy and pasting” Bitcoin to create their own digital blockchains, all with slight differences that they think will change the world.For example, Litecoin and Ethereum both have faster blockchain which have increased transaction times. Both Telegram’s upcoming token and Kik’s Kin token are trying to be mobile-first, daily-use tokens with a high supply and low token price. Ripple already eschews anonymous nodes and instead uses trusted validator nodes run by legitimate groups chosen by them. Ethereum is working on proof of stake to become more energy efficient, and NEO already has a working one in place.You get the point.There is currently no lack of existing cryptocurrencies with these improved features. Instead, we suffer from a lack of a developed ecosystem which is needed develop and support real-life everyday use cases for these blockchains. It’s not hard to launch a new cryptocurrency, but it is hard to onboard the hundreds and thousands of service providers, merchants and financial institutions that are needed to make a cryptocurrency actually useful.Maybe Eco will have better luck with this. And in fact, signing up partners to build an ecosystem is an area where this new cryptocurrency should thrive, considering Garrett Camp’s involvement and high-profile in the tech industry.Eco hopes to have a test net running by the end of the year, but users can sign up now to claim a username and reserve some tokens.",Uber co-founder Garrett Camp is creating a new cryptocurrency
9981,3928969,2018-03-02 15:35:46,"About TNWTNW SitesCoinbase has no immediate plans to add Litecoin Cash (LCC)A couple of weeks back the Litecoin (LTC) blockchain split in two to form the new Litecoin Cash (LCC) hard fork. But despite some requests from users, it appears that leading cryptocurrency exchange desk Coinbase has no immediate plans to add Litecoin Cash trading pairs on its platform – at least not for the time being.“We’re always monitoring potential forks from a security/stability standpoint, core developer roadmaps and customers perspective.” a spokesperson for Coinbase told TNW. “Ultimately, we want to do what’s best for customers while still being secure and practical.”While the company refrained from providing more details on its plans for the forked currency, it did not rule out the possibility it might get listed on the exchange desk, saying that it is actively “tracking” LCC’s progress.To give you some background: Litecoin Cash aims to replace Litecoin’s Scrypt encryption protocol with SHA-256. The tweak will make it possible to leverage old Bitcoin mining hardware to mine LCC, according to the developers behind this undertaking.The Litecoin Cash team has promised to distribute LCC to Litecoin holders in a 10 to one ratio. This means that if you currently have 10 LTC, you are entitled to 100 LCC. Needless to say, users are finding it difficult to decline an opportunity to score some free tokens.As pointed out by Inverse, the only way to claim LCC (assuming you store your LTC on Coinbase) is to move your funds to a private wallet like Litecoin Core or Jaxx. This way you will have access to your private keys – and thus to your LCC freebie.The reason is that Coinbase has a strict policy against sharing private keys with users.“Our hot wallet/cold storage is not setup in a way where we can release private keys,” Coinbase explained to TNW. “This is standard for exchanges.”Indeed, Coinbase has clarified these terms on its support page, saying “it’s not feasible to provide the private keys to individual wallet addresses.” The company further notes that “[t]his not only ensures your security but it also allows us to reduce the cost of sending digital currency to another Coinbase account.”Meanwhile, several less known exchanges have already opted to list LCC trading on their platforms. The list so far includes YoBit, CryptoBridge, SouthXchange, and Trade Satoshi.Despite some interest from the cryptocurrency community, Litecoin creator Charlie Lee has come forward to dismiss the Cash hard fork and its founders a “scam.”Since on the topic of scams, any fork of Litecoin, calling itself Litecoin something or other, is a scam IMO. Litecoin Cash, Litecoin Plus, Litecoin *… all scams trying to confuse users into thinking they are Litecoin.The statement is somewhat ironic, since Litecoin itself forked off from the Bitcoin blockchain. Still, his concerns are not entirely groundless.Lee has rightfully warned that the Litecoin Cash developers completed a pre-mining session ahead of the hard fork, accounting for about one percent – or roughly 5.5 million LCC – from the entire supply. For context, LCC stands at $0.91 at the time of writing, according to Coin Market Cap.Last December, Coinbase announced support for Bitcoin Cash – a popular hard fork of the main Bitcoin blockchain – after the currency surged up to crack the top five biggest cryptocurrencies by market share. However, the company took several months to green light this move.But as Coinbase already made it clear: the decision will ultimately come down to how the larger crypto-community fares in on Litecoin Cash. So if you’re hoping to claim some free LCC, better move your LTC to a private wallet now.",Coinbase has no immediate plans to add Litecoin Cash (LCC)
9982,3928970,2018-03-02 15:11:59,"Things are certainly good up north. But every paradise has its snake. The weird thing about this success is that Scandinavian people, in fact, hate success – this is called the Jantelaw.The social code that hates successThe Jantelaw was first described in the 1933 novel, A Fugitive Crosses its Tracks, by Danish author Aksel Sandemose. The Jantelaw was described as a sort of Scandinavian social code in ten points, a bit like the ten commandments of the Bible. The points are very similar and they basically all say the same: “Don’t stand out and don’t think you’re any better or different from the rest.”I’m from Scandinavia myself and I’ve always known about the Jantelaw and the fact that it’s something fundamentally negative. But it seems that our weird tendency to drag each other down might actually be one of the reasons the Scandinavian startup ecosystem is flourishing right now. I got in touch with three Scandinavian startups who all have become successful in spite, or because of the Jantelaw.Make sure you have your shit togetherHeikki Väänänen is the CEO and founder of the Finnish company, HappyOrNot. You have probably seen their distinguished smiley-faces at airports, as their product is present at over 200 airports around the world, where they are analyzing customer satisfaction in real-time.Väänänen thinks that the Jantelaw can, in fact, be beneficial for launching a startup. Since the Scandinavian social code stresses that you shouldn’t think you’re better than anybody, it kind of forces you to work harder and prove it wrong. As an entrepreneur, you need to be even more mindful of your product and to make sure that your idea is properly thought through before you execute it. Because if not, the reward is eternal social scorn.After all, it’s easier to ignore people who dislike your success, if you are actually successful. Having tried becoming successful and failed at it might be the hardest blow in a country ruled by the Jantelaw.Egalitarian society – not JantelawSofia Wingren is the CEO of the Swedish teaching and consultancy company, Hyper Island, that focuses on teaching companies how to become digitally fluent. Wingren, unlike Väänänen, is not willing to accredit the Jantelaw to any part of the success her company has achieved. Or any other company for that matter.She, just like me, has never been used to regarding the Jantelaw as any way to achieve success. It’s the people who fight against it that turn out to be successful, Wingren says.In Wingren’s opinion, the media has misunderstood the concept of Jante. The reason for Scandinavian success is not due to Jantelaw, but in the highly developed welfare society that the Scandinavian countries are famous for.“We have an amazing social system backing us up, should something fail or go wrong,” she says, “this makes it less worrisome for people to quit their job to pursue an idea, because they don’t have to put their families’ health or education at stake in order to do so.”Wingren also points out, that the fact that education is free and accessible for everyone it provides society with a much more diverse workforce. And that is also part of the Scandinavian success.One might argue that because the Jantelaw is so much against individual success, that this might be the reason for the Scandinavian societies focusing so much on the community in terms of, for example, healthcare and education.Wingren admits that Scandinavian companies might be different from others because of their flat work hierarchies, but that’s not the result of an arbitrary social rule. “If people think that the Jantelaw has something to do with success, they are mistaken. The Jantelaw justifies jealousy and pettiness, and is not the reason behind our egalitarian society.”De-glorifying the entrepreneurWhile Wingren and Väänänen might be on different standpoints in regard to the beneficial sides of the Jantelaw, Andreas Cleve Lohmann represents a view that’s placed a bit more in the middle.He is the CEO and founder of the Danish company, Corti.AI. Corti uses AI technology for the Danish healthcare sector to understand, analyze, and diagnose acute illnesses in patients, such as cardiac arrest. According to him, the Danish startup culture could use some more of the Jantelaw.“In Denmark, people have started to over-glorify the idea of founding a startup,” Lohmann explains, “it results in many people attempting ideas that are not necessarily very good, and once the going gets tough, most of the people stop, and only a few tough keep on going.”Lohman, just like Wingren, also points to the Scandinavian welfare society. In his opinion, it keeps a lot of people from really investing all their energy in their startup. Because they know that if they fail, there is not so much to lose. In Lohmann’s opinion, the idea of the entrepreneur needs to be de-glorified and instead it needs to be seen as something serious that, if you do it, you do it right, instead of as some side-project.For better or worse, the Jantelaw is ever-presentNone of these three companies agree on how the Jantelaw is affecting business in the Scandinavia. It works in mysterious ways. However, although they all had different standpoints in regard to Jantelaw, they all had this in common: That no matter if they fight against or embrace the Jantelaw, none of them are entirely free of it either. It’s always there.The Jantelaw is a very nuanced concept and cannot be boiled down to have one good or bad effect on startup cultures.The thing that all Scandinavian countries have in common – besides good welfare systems and a seemingly jealous and unsupportive social code – is the fact that they’re small. Sweden, being the biggest of them, only has a population of just below 10 million.So, if you want to start up a company in one of these countries, you know from the very beginning that you have to think beyond your home market – and that you’ll eventually need to ditch the Jantelaw.",Behind every successful Scandinavian startup is a jealous and unsupportive community
9983,3928971,2018-02-28 17:24:28,"About TNWTNW SitesThe Nix Color Sensor matches your color every time…and it’s 30% offIf you’re an interior designer (or know someone who at least imagines they are), then you need to hook them up with the Nix Mini Color Sensor.And just so we’re clear, this isn’t one of those idle, yeah-sure-one-of-these-days kinds of recommendations. If you’re dealing with someone who spends significant time eyeing color, paints, fabrics, etc., then cueing them up to the power of the Nix Mini Color Sensor (now $69, 30 percent off from TNW Deals) will be like showing a carriage driver an internal combustion engine.Can you say game changer?The Nix works like Shazam for the paint and design enthusiast. Just point the Nix at the surface of your choice and within seconds, this eagle-eyed sensor analyzes the pigment and points you to the closest color matches from all the biggest paint brands.When we say surface, we don’t just mean the wall. We mean any surface, whether it’s a painted wall, vinyl, leather, plastic, fabric, dyes — you name it, Nix will find it.And when we say the closest color matches, we mean matches you’ll have trouble telling apart. The Nix checks your choice against more than 28,000 brand name paint colors, as well as the full range of RGB, HEX, CMYK, and LAB hues. Basically, the Nix throws open the catalogues of makers like Benjamin Moore, Dulux, Farrow & Ball, Sherwin Williams and more, and spits out all the specific color options from each manufacturer that’ll work for your project.The Nix is lightweight and can easily attach to a keychain, making it a perfect travel tool. The unit even emits a white LED light so you can get just the right color reading every time. Once you’re synced to the Nix app, you can also store and organize all your chosen colors and keep them on file for next time.The Nix retails for $99, so don’t miss out on the limited time deal to get one for your favorite color freak for only $69.",The Nix Color Sensor matches your color every time...and it’s 30% off
9984,3931753,2018-03-02 15:45:38,"MIT study shows how much driving for Uber or Lyft sucks0Ride-hailing giants Uber and Lyft are delivering pitiful levels of take-home pay to the hundreds of thousands of US independent contractors providing their own vehicles and driving skills to deliver the core service, according to an MIT CEEPR study examining the economics of the two app platforms.The report catalyses the debate about conditions for workers on gig economy platforms, and raises serious questions about the wider societal impacts of tax avoiding, VC-funded tech giants.The study, entitled The Economics of Ride-Hailing: Driver Revenue, Expenses and Taxes, and which was carried out by the MIT Center for Energy and Environmental Policy Research, surveyed more than 1,100 Uber and Lyft ride-hailing drivers combined with detailed vehicle cost information — factoring in costs such as fuel, insurance, maintenance and repairs — to come up with a median profit per hour worked.The upshot? The researchers found profit from ride-hail driving to be “very low”. On an hourly basis, the median profit was $3.37 per hour, with 74% of drivers earning less than the minimum wage in the state where they operate.They also found a median driver generates $0.59 per mile of driving but incurs costs of $0.30 per mile; and almost a third (30 per cent) of drivers were found to incur expenses exceeding their revenue or to be losing money for every mile they drive.The research also looked at how ride-hailing profits are taxed, and suggests that in the US a majority of driver profits are going untaxed owing to how mileage deduction is handled for tax purposes — suggesting Uber and Lyft’s business are denuding the public purse too.From the study:On a monthly basis, mean profit is $661/month (median $310). Drivers are eligible to use a Standard Mileage Deduction for tax purposes ($0.54/mile in 2016) which far exceeds median costs per mile of $0.30/mile. Because of this deduction, most ridehailing drivers are able to declare profits that are substantially lower. Mean drivers who use a Standard Mileage Deduction would declare taxable profit of $175 rather than the $661 earned. These numbers suggest that approximately 74% of driver profit is untaxed.The authors add that if their $661/month mean profit is representative then the US’ Standard Mileage Deduction facilitates “several billion in untaxed income for hundreds of thousands of ride-hailing drivers nationwide”.So what does the study tell us about the ride-hailing business model? “It tells us that it’s a shitty place to work,” says Mark Tluszcz, co-founder and CEO of Mangrove Capital Partners who has described the gig economy model as the modern day sweatshop, and says his VC firm made a conscious decision not to invest in gig economy companies because the model is exploitative.“It tells you that it’s a great place if you’re a company. It’s really a poor place to be an employee or be a worker.”The exploitative asymmetry of ride-hailing platforms comes because workers have a certain amount of fixed costs but the platform intermediary can just hike its commission at will and lower the service cost to the end user whenever it wants to increase competitiveness vs a rival business.“At the end of the day there are a certain amount of fixed costs [for drivers],” says Tluszcz. “You have to buy a car, you have to get insurance, you have to pay for gas… And if you as an intermediary, which those platforms are, are taking an increasing amount of commission — 10%, 15%, now 20 in most of their markets — and then you’re using the price of the trip as a way of beating your competitor… then you as a driver are sitting there with basically all of your fixed costs and your income is going down and frankly the only way to cover your costs is to spend more hours in the car.“Which is frankly what’s clearly illustrated by this study. These people have to spend so much time to cover their costs when you break it down to an hourly revenue, it’s a pitiful amount. And by the way you have no social coverage because you’ve got to take care of that yourself.”At the time of writing neither Uber not Lyft had responded to a request for comment on the MIT study. But an Uber spokesperson told The Guardian the company believes the research methodology and findings are “deeply flawed”, adding: “We’ve reached out to the paper’s authors to share our concerns and suggest ways we might work together to refine their approach.”Tluszcz was quick to dispatch that critique. “MIT is not some second tier organization that did this study,” he points out. “For me that’s a reference moment when MIT says look, there’s an issue here… There’s something wrong in the model and we can tolerate it for a period of time but ultimately we’re creating this lost generation of people.”“These business are built on situations in the market that are not realistic,” he tells TechCrunch. “They took advantage of a hole in legislation… Governments let that happen. And it made all of sudden services cheaper. But people have to eat. People have to live. And ultimately there’s only 100% of a cake.“Cabbies in the UK are not millionaires; they make a decent living. But they make a decent living because there’s a certain price-point to offer the service. And in every industry you have that. There is a certain fair price point to be able to live in that industry… And clearly right now, in the ride-sharing businesses, you don’t have it.”In Europe, where Uber’s business has faced a series of legal challenges, the company has begun offering some subsidized insurance products for platform workers — including one for Uber Eats couriers across Europe and a personal injury and insurance product for drivers in the UK.In January in the UK it also announced a safety cap on the number of consecutive hours drivers on its platform can accept trips, after coming under rising political and legal pressure on safety and working conditions.Last year Uber also lost its first appeal against an employment tribunal that judged a group of Uber drivers to be workers, not self-employed contractors as it had claimed — meaning they are entitled to workers rights such as holiday and sick pay.Uber also had its license to operate in London withdrawn last fall, with the local transport regulator citing concerns about safety and corporate responsibility as key considerations for not renewing the company’s private hire vehicle license.Tluszcz’s view is that such moves prefigure a more major shift incoming in Europe that could cement permanent roadblocks to business models that function via intentional worker exploitation.“The flaw in the [gig economy] model as a worker is so big that it seems to be quite clear that European governments are going to be looking at this and saying this is just not the European ethos. It’s just not,” he argues. “There’s going to be a moment when all these things are clashing. And I think it’s a cultural clash that we have really, between European values of equity and American values of just pure market capitalism.“You can’t expect somebody making $3.37 an hour to take a part of that to contribute to retirement and social coverage. What the hell do you live on?” he adds.“We’re creating the next lost generation of people who simply don’t have enough money to live and those companies are fundamentally enabling it under the premise that they’re offering a cheaper service to consumers… And I just don’t think Europe will put up with this.”Last month the UK government confirmed its intent to act on this area by announcing a package of labor market reforms intended to respond to changes driven by the rise of gig economy platforms. It dubbed the strategy a ‘Good Work Plan’ — billing it as an expansion of workers rights and saying “millions” more workers would get new day-one rights, coupled with a tighter enforcement regime on platforms and companies to ensure they are providing sick and holiday pay rights.“We are proud to have record levels of employment in this country but we must also ensure that workers’ rights are always upheld,” said the UK prime minister, also emphasizing that her goal was to build “an economy that works for everyone”.It’s likely to publish more detail on the employment law reform later this year. But the direction of travel for gig economy platforms in Europe looks clear: Away from being freely able to exploit legal loopholes and towards a much more tightly managed framework of employment and workforce welfare regulations to ensure that underlying support structures (such as the UK’s national minimum wage) aren’t just being circumvented by clever engineering and legal positioning.“This for me is an inherent dilemma one has between capitalism and some level of socialism which we have in Europe,” adds Tluszcz. “This is a clash of two fundamentally different views of the world and ultimately as a company you have to be a company that views your role in society as one of being a contributor — and tech companies can’t hide behind the fact; they must do the same.“And unfortunately all these ride-sharing businesses, and including most of these gig economy companies, are just trying to take advantage of holes and frankly I don’t see them at all looking at their reason to be as at least having a component of ‘I’m good for the society in which I operate’. They don’t. They just simply don’t care.“That’s a dilemma we have as consumers, because on the one hand we like the fact that it’s cheap. But we wish that people could all have a decent living.”Whether US companies will be forced into a less exploitative relationship with their US workers remains to be seen.Tluszcz’s view is that it will need some kind of government intervention for these types of companies to rethink how their models operate and who they are impacting.“Tech companies frankly have an equal amount of responsibility to be great corporate citizens. And right now it feels — particularly because many of these tech companies are born in the US — it almost feels like this Americanism about them says I don’t have to be a good corporate citizen. I’m going to take advantage of the world for me and my shareholders,” he says.“I’m a capitalist but I do think there’s some moral guidance you have to have about the business you’re building. And the US tech companies, around the world — certainly in Europe — are being highly criticized… Where is your moral compass? And unfortunately, today, sitting here, you have to say they lost it.”",MIT study shows how much driving for Uber or Lyft sucks
9985,3931756,2018-03-01 00:00:00,"What does it take to succeed? What are the secrets of the most successful people? Judging by the popularity of magazines such as Success, Forbes, Inc., and Entrepreneur, there is no shortage of interest in these questions. There is a deep underlying assumption, however, that we can learn from them because it's their personal characteristics--such as talent, skill, mental toughness, hard work, tenacity, optimism, growth mindset, and emotional intelligence-- that got them where they are today. This assumption doesn't only underlie success magazines, but also how we distribute resources in society, from work opportunities to fame to government grants to public policy decisions. We tend to give out resources to those who have a past history of success, and tend to ignore those who have been unsuccessful, assuming that the most successful are also the most competent.But is this assumption correct? I have spent my entire career studying the psychological characteristics that predict achievement and creativity. While I have found that a certain number of traits-- including passion, perseverance, imagination, intellectual curiosity, and openness to experience-- do significantly explain differences in success, I am often intrigued by just how much of the variance is often left unexplained.In recent years, a number of studies and books--including those by risk analyst Nassim Taleb, investment strategist Michael Mauboussin, and economist Richard Frank-- have suggested that luck and opportunity may play a far greater role than we ever realized, across a number of fields, including financial trading, business, sports, art, music, literature, and science. Their argument is not that luck is everything; of course talent matters. Instead, the data suggests that we miss out on a really importance piece of the success picture if we only focus on personal characteristics in attempting to understand the determinants of success.The importance of the hidden dimension of luck raises an intriguing question: Are the most successful people mostly just the luckiest people in our society? If this were even a little bit true, then this would have some significant implications for how we distribute limited resources, and for the potential for the rich and successful to actually benefit society (versus benefiting themselves by getting even more rich and successful).The Italian researchers stuck a large number of hypothetical individuals (""agents"") with different degrees of ""talent"" into a square world and let their lives unfold over the course of their entire worklife. They defined talent as whatever set of personal characteristics allow a person to exploit lucky opportunities (I've argued elsewhere that this is a reasonable definition of talent). Talent can include traits such as intelligence, skill, motivation, determination, creative thinking, emotional intelligence, etc. The key is that more talented people are going to be more likely to get the most 'bang for their buck' out of a given opportunity (see here for support of this assumption).All agents began the simulation with the same level of success (10 ""units""). Every 6 months, individuals were exposed to a certain number of lucky events (in green) and a certain amount of unlucky events (in red). Whenever a person encountered an unlucky event, their success was reduced in half, and whenever a person encountered a lucky event, their success doubled proportional to their talent (to reflect the real-world interaction between talent and opportunity).What did they find? Well, first they replicated the well known ""Pareto Principle"", which predicts that a small number of people will end up achieving the success of most of the population (Richard Koch refers to it as the ""80/20 principle""). In the final outcome of the 40-year simulation, while talent was normally distributed, success was not. The 20 most successful individuals held 44% of the total amount of success, while almost half of the population remained under 10 units of success (which was the initial starting condition). This is consistent with real-world data, although there is some suggestion that in the real world, wealth success is even more unevenly distributed, with just eight men owning the same wealth as the poorest half of the world.Although such an unequal distribution may seem unfair, it might be justifiable if it turned out that the most successful people were indeed the most talented/competent. So what did the simulation find? On the one hand, talent wasn't irrelevant to success. In general, those with greater talent had a higher probability of increasing their success by exploiting the possibilities offered by luck. Also, the most successful agents were mostly at least average in talent. So talent mattered.However, talent was definitely not sufficient because the most talented individuals were rarely the most successful. In general, mediocre-but-lucky people were much more successful than more-talented-but-unlucky individuals. The most successful agents tended to be those who were only slightly above average in talent but with a lot of luck in their lives.Consider the evolution of success for the most successful person and the least successful person in one of their simulations:As you can see, the highly successful person in green had a series of very lucky events in their life, whereas the least successful person in red (who was even more talented than the other person) had an unbearable number of unlucky events in their life. As the authors note, ""even a great talent becomes useless against the fury of misfortune.""Talent loss is obviously unfortunate, to both the individual and to society. So what can be done so that those most capable of capitalizing on their opportunities are given the opportunities they most need to thrive? Let's turn to that next.Stimulating SerendipityMany meritocratic strategies used to assign honors, funds, or rewards are often based on the past success of the person. Selecting individuals in this way creates a state of affairs in which the rich get richer and the poor get poorer (often referred to as the ""Matthew effect""). But is this the most effective strategy for maximizing potential? Which is a more effective funding strategy for maximizing impact to the world: giving large grants to a few previously successful applicants, or a number of smaller grants to many average-successful people? This is a fundamental question about distribution of resources, which needs to be informed by actual data.Consider a study conducted by Jean-Michel Fortin and David Currie, who looked at whether larger grants lead to larger discoveries. They found a positive, but only very small relationship between funding and impact (as measured by four indices relating to scientific publications). What's more, those who received a second grant were not more productive than those who only received a first grant, and impact was generally a decelerating function of funding.The authors suggest that funding strategies that focus more on targeting diversity than ""excellence"" are likely to be more productive to society. In a more recent study, researchers looked at the funding provided to 12,720 researchers in Quebec over a fifteen year period. They concluded that ""both in terms of the quantity of papers produced and of their scientific impact, the concentration of research funding in the hands of a so-called 'elite' of researchers generally produces diminishing marginal returns.""Taking these sort of findings seriously, the European Research Council recently gave the biochemist Ohid Yaqub 1.7 million dollars to properly determine the extent of serendipity in science. Coming up with a multidimensional definition of serendipity, Yaqub pinned down some of the mechanisms by which serendipity in science happens, including astute observation, ""controlled sloppiness"" (allowing unexpected events to occur while tracking their origins), and the collaborative action of networks of scientists. This is consistent with Dean Simonton's extensive work on the role of serendipity and chance in the evolution of creative and impactful scientific discoveries.Building on this work, the Italian team who simulated the role of luck in success went one step further in their simulation. Playing God (so to speak), they explored the effectiveness of a number of different funding strategies. They applied different strategies every five years during the 40 year worklife of each agent in the simulation. Without any funding at all, we already saw that the most successful agents were very lucky people with about average levels of talent. What happens once they introduced various funding opportunities into the simulation?This table reveals the most efficient funding strategies over the 40 year period in descending order of efficiency (i.e., requiring the least amount of funding for the greatest return on the investment). Starting at the bottom of the list, you can see that the least effective funding strategies are those that give a certain percentage of the funding to only the already most successful individuals. The ""mixed"" strategies that combine giving a certain percentage to the most successful people and equally distributing the rest is a bit more effective, and distributing funds at random is even more efficient. This last finding is intriguing because it is consistent with other research suggesting that in complex social and economic contexts where chance is likely to play a role, strategies that incorporate randomness can perform better than strategies based on the ""naively meritocratic"" approach.With that said, the best funding strategy of them all was one where an equal number of funding was distributed to everyone. Distributing funds at a rate of 1 unit every five months resulted in 60% of the most talented individuals having a greater than average level of success, and distributing funds at a rate of 5 units every six months resulted in 100% of the most talented individuals having an impact! This suggests that if a funding agency or government has more money available to distribute, they'd be wise to use that extra money to distribute money to everyone, rather than to only a select few. As the researchers conclude,""[I]f the goal is to reward the most talented person (thus increasing their final level of success), it is much more convenient to distribute periodically (even small) equal amounts of capital to all individuals rather than to give a greater capital only to a small percentage of them, selected through their level of success - already reached - at the moment of the distribution.""Stimulating the EnvironmentThis incredible Italian team didn't even stop there! Hey, if you're playing God, why not go all the way. :) They also ran simulations in which they varied the environment of the agents. Using this framework, they simulated either a very stimulating environment, rich of opportunities for everyone (like that of rich and industrialized countries such as the U.S.) as well as a much less stimulating environment, with very few opportunities (like that of Third World countries). Here's what they found:Success of the Most Successful Individuals Living in an Environment with Rich Opportunities (top) or an Environment with Poor Opportunities (bottom)Credit: Pluchino, Biondo, & Rapisarda 2018Look at the difference between the outcome distribution of the environment rich in opportunities for everyone (top) from the outcome distribution of the environment poor in opportunities for everyone (bottom). In the universe simulated at the top, a number of medium to highly talented individuals were able to reach very high levels of success, and the average number of medium-highly talented individuals who reached at least above average levels of success was quite high. In contrast, in the universe simulated at the bottom of the figure, the overall level of success of the society was low, with an average of only 18 individuals able to increase their initial level of success.ConclusionThe results of this elucidating simulation, which dovetail with a growing number of studies based on real-world data, strongly suggest that luck and opportunity play an underappreciated role in determining the final level of individual success. As the researchers point out, since rewards and resources are usually given to those who are already highly rewarded, this often causes a lack of opportunities for those who are most talented (i.e., have the greatest potential to actually benefit from the resources), and it doesn't take into account the important role of luck, which can emerge spontaneously throughout the creative process. The researchers argue that the following factors are all important in giving people more chances of success: a stimulating environment rich in opportunities, a good education, intensive training, and an efficient strategy for the distribution of funds and resources. They argue that at the macro-level of analysis, any policy that can influence these factors will result in greater collective progress and innovation for society (not to mention immense self-actualization of any particular individual).Note: One suggestion I made to the Italian team is for their future simulations to take into account the real-world finding that talent develops over time, and is not a fixed quantity of the individual. They graciously said this was a valid point and would definitely take that into consideration in their future work.The views expressed are those of the author(s) and are not necessarily those of Scientific American.ABOUT THE AUTHOR(S)Scott Barry KaufmanScott Barry Kaufman is an author, researcher, speaker, and public science communicator who is interested in using psychological science to help all kinds of minds live a creative, fulfilling, and meaningful life. He is author of 8 books, including Ungifted: Intelligence Redefined (Basic Books, 2013) and Wired to Create: Unraveling the Mysteries of the Creative Mind (Perigee, 2015, with Carolyn Gregoire).Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.",The Role of Luck in Life Success Is Far Greater Than We Realized
9986,3931917,2018-03-02 15:45:38,"MIT study shows how much driving for Uber or Lyft sucks0Ride-hailing giants Uber and Lyft are delivering pitiful levels of take-home pay to the hundreds of thousands of US independent contractors providing their own vehicles and driving skills to deliver the core service, according to an MIT CEEPR study examining the economics of the two app platforms.The report catalyses the debate about conditions for workers on gig economy platforms, and raises serious questions about the wider societal impacts of tax avoiding, VC-funded tech giants.The study, entitled The Economics of Ride-Hailing: Driver Revenue, Expenses and Taxes, and which was carried out by the MIT Center for Energy and Environmental Policy Research, surveyed more than 1,100 Uber and Lyft ride-hailing drivers combined with detailed vehicle cost information — factoring in costs such as fuel, insurance, maintenance and repairs — to come up with a median profit per hour worked.The upshot? The researchers found profit from ride-hail driving to be “very low”. On an hourly basis, the median profit was $3.37 per hour, with 74% of drivers earning less than the minimum wage in the state where they operate.They also found a median driver generates $0.59 per mile of driving but incurs costs of $0.30 per mile; and almost a third (30 per cent) of drivers were found to incur expenses exceeding their revenue or to be losing money for every mile they drive.The research also looked at how ride-hailing profits are taxed, and suggests that in the US a majority of driver profits are going untaxed owing to how mileage deduction is handled for tax purposes — suggesting Uber and Lyft’s business are denuding the public purse too.From the study:On a monthly basis, mean profit is $661/month (median $310). Drivers are eligible to use a Standard Mileage Deduction for tax purposes ($0.54/mile in 2016) which far exceeds median costs per mile of $0.30/mile. Because of this deduction, most ridehailing drivers are able to declare profits that are substantially lower. Mean drivers who use a Standard Mileage Deduction would declare taxable profit of $175 rather than the $661 earned. These numbers suggest that approximately 74% of driver profit is untaxed.The authors add that if their $661/month mean profit is representative then the US’ Standard Mileage Deduction facilitates “several billion in untaxed income for hundreds of thousands of ride-hailing drivers nationwide”.So what does the study tell us about the ride-hailing business model? “It tells us that it’s a shitty place to work,” says Mark Tluszcz, co-founder and CEO of Mangrove Capital Partners who has described the gig economy model as the modern day sweatshop, and says his VC firm made a conscious decision not to invest in gig economy companies because the model is exploitative.“It tells you that it’s a great place if you’re a company. It’s really a poor place to be an employee or be a worker.”The exploitative asymmetry of ride-hailing platforms comes because workers have a certain amount of fixed costs but the platform intermediary can just hike its commission at will and lower the service cost to the end user whenever it wants to increase competitiveness vs a rival business.“At the end of the day there are a certain amount of fixed costs [for drivers],” says Tluszcz. “You have to buy a car, you have to get insurance, you have to pay for gas… And if you as an intermediary, which those platforms are, are taking an increasing amount of commission — 10%, 15%, now 20 in most of their markets — and then you’re using the price of the trip as a way of beating your competitor… then you as a driver are sitting there with basically all of your fixed costs and your income is going down and frankly the only way to cover your costs is to spend more hours in the car.“Which is frankly what’s clearly illustrated by this study. These people have to spend so much time to cover their costs when you break it down to an hourly revenue, it’s a pitiful amount. And by the way you have no social coverage because you’ve got to take care of that yourself.”At the time of writing neither Uber not Lyft had responded to a request for comment on the MIT study. But an Uber spokesperson told The Guardian the company believes the research methodology and findings are “deeply flawed”, adding: “We’ve reached out to the paper’s authors to share our concerns and suggest ways we might work together to refine their approach.”Tluszcz was quick to dispatch that critique. “MIT is not some second tier organization that did this study,” he points out. “For me that’s a reference moment when MIT says look, there’s an issue here… There’s something wrong in the model and we can tolerate it for a period of time but ultimately we’re creating this lost generation of people.”“These business are built on situations in the market that are not realistic,” he tells TechCrunch. “They took advantage of a hole in legislation… Governments let that happen. And it made all of sudden services cheaper. But people have to eat. People have to live. And ultimately there’s only 100% of a cake.“Cabbies in the UK are not millionaires; they make a decent living. But they make a decent living because there’s a certain price-point to offer the service. And in every industry you have that. There is a certain fair price point to be able to live in that industry… And clearly right now, in the ride-sharing businesses, you don’t have it.”In Europe, where Uber’s business has faced a series of legal challenges, the company has begun offering some subsidized insurance products for platform workers — including one for Uber Eats couriers across Europe and a personal injury and insurance product for drivers in the UK.In January in the UK it also announced a safety cap on the number of consecutive hours drivers on its platform can accept trips, after coming under rising political and legal pressure on safety and working conditions.Last year Uber also lost its first appeal against an employment tribunal that judged a group of Uber drivers to be workers, not self-employed contractors as it had claimed — meaning they are entitled to workers rights such as holiday and sick pay.Uber also had its license to operate in London withdrawn last fall, with the local transport regulator citing concerns about safety and corporate responsibility as key considerations for not renewing the company’s private hire vehicle license.Tluszcz’s view is that such moves prefigure a more major shift incoming in Europe that could cement permanent roadblocks to business models that function via intentional worker exploitation.“The flaw in the [gig economy] model as a worker is so big that it seems to be quite clear that European governments are going to be looking at this and saying this is just not the European ethos. It’s just not,” he argues. “There’s going to be a moment when all these things are clashing. And I think it’s a cultural clash that we have really, between European values of equity and American values of just pure market capitalism.“You can’t expect somebody making $3.37 an hour to take a part of that to contribute to retirement and social coverage. What the hell do you live on?” he adds.“We’re creating the next lost generation of people who simply don’t have enough money to live and those companies are fundamentally enabling it under the premise that they’re offering a cheaper service to consumers… And I just don’t think Europe will put up with this.”Last month the UK government confirmed its intent to act on this area by announcing a package of labor market reforms intended to respond to changes driven by the rise of gig economy platforms. It dubbed the strategy a ‘Good Work Plan’ — billing it as an expansion of workers rights and saying “millions” more workers would get new day-one rights, coupled with a tighter enforcement regime on platforms and companies to ensure they are providing sick and holiday pay rights.“We are proud to have record levels of employment in this country but we must also ensure that workers’ rights are always upheld,” said the UK prime minister, also emphasizing that her goal was to build “an economy that works for everyone”.It’s likely to publish more detail on the employment law reform later this year. But the direction of travel for gig economy platforms in Europe looks clear: Away from being freely able to exploit legal loopholes and towards a much more tightly managed framework of employment and workforce welfare regulations to ensure that underlying support structures (such as the UK’s national minimum wage) aren’t just being circumvented by clever engineering and legal positioning.“This for me is an inherent dilemma one has between capitalism and some level of socialism which we have in Europe,” adds Tluszcz. “This is a clash of two fundamentally different views of the world and ultimately as a company you have to be a company that views your role in society as one of being a contributor — and tech companies can’t hide behind the fact; they must do the same.“And unfortunately all these ride-sharing businesses, and including most of these gig economy companies, are just trying to take advantage of holes and frankly I don’t see them at all looking at their reason to be as at least having a component of ‘I’m good for the society in which I operate’. They don’t. They just simply don’t care.“That’s a dilemma we have as consumers, because on the one hand we like the fact that it’s cheap. But we wish that people could all have a decent living.”Whether US companies will be forced into a less exploitative relationship with their US workers remains to be seen.Tluszcz’s view is that it will need some kind of government intervention for these types of companies to rethink how their models operate and who they are impacting.“Tech companies frankly have an equal amount of responsibility to be great corporate citizens. And right now it feels — particularly because many of these tech companies are born in the US — it almost feels like this Americanism about them says I don’t have to be a good corporate citizen. I’m going to take advantage of the world for me and my shareholders,” he says.“I’m a capitalist but I do think there’s some moral guidance you have to have about the business you’re building. And the US tech companies, around the world — certainly in Europe — are being highly criticized… Where is your moral compass? And unfortunately, today, sitting here, you have to say they lost it.”",MIT study shows how much driving for Uber or Lyft sucks
9987,3931997,2018-03-02 17:09:17,"TNW SitesThis app uses machine learning and AR to teach you how to drawFor some people drawing comes naturally. But, if you’re anything like me, your stick figures look like misshapen horrors for whom death would surely be a relief. Enter Sketch AR: the AI-powered app that aims to make you a better artist regardless of your current skill.The idea is pretty simple: you draw a few plus-signs on a piece of paper, wall, or sleeping friend’s face and then point your smartphone’s camera at it.From there you can cycle through point-by-point lessons that help you learn to associate free space with the next layer of a drawing. It’s quite intuitive and the overlay makes it feel like drawing on tracing paper, but with augmented reality instead.To accomplish this, the developers had to figure out how to overcome various problems. For example, when a person is drawing, often their hand gets in the way of the camera. Furthermore, not every surface or “canvas” is the same.",This app uses AR and AI to teach anyone how to draw
9988,3931998,2018-03-02 17:00:57,"TNW SitesSomeone made a VR experience where you’re George W Bush in a bathtubOh, what a time to be alive. Just when I thought I’d seen the strangest stuff VR had for me, I discover George in the Tub, a satirical dig at former US politicians within a placid VR experience.In the game, players relax in a bathtub while simultaneously painting with a VR controller. As the painting takes shape, it’s revealed by way of the subject matter that they’re inhabiting the body of former US president George W. Bush. The game was picked up by VR funding platform Kaleidoscope last year.The game appears to be inspired by two infamous self-portraits painted by the former US president, one of himself in the tub and the other in a shower. This might have had a bigger impact in 2013, when those portraits were revealed, but who am I to demand topical political references in my VR games?The studio behind the game, Tender Claws, has been working in VR since 2002. Their most recent game is Virtual Virtual Reality, a satirical Daydream VR game in which you play a human working as a companion for AI inside virtual reality worlds (VR inside VR, roll credits). The description of the game on Google Play contains the phrase “An artichoke screams at you,” so I know I’m already on board.The Next Web’s 2018 conference is just a few months away, and it’ll be💥💥. Find out all about our trackshere.",Someone made a game where you play as George Bush in a bathtub
9989,3931999,2018-03-02 16:28:07,"TNW SitesDeepMind’s new robots learned how to teach themselvesThe minute hand on the robot apocalypse clock just inched a little closer to midnight. DeepMind, the Google sister-company responsible for the smartest AI on the planet, just taught machines how to figure things out for themselves.Robots aren’t very good at exploring on their own. AI that only exists to parse data, such as neural networks that decide whether something is a hotdog or not, have relatively little to concentrate on compared to the near-infinite number of things a physical robot has to figure out.To solve this problem DeepMind built a new learning paradigm for AI-powered robots called ‘Scheduled Auxiliary Control (SAC-X).’ This new paradigm gives robots a simple goal like ‘clean up this playground’ and rewards it for completion.The auxiliary tasks we define follow a general principle: they encourage the agent to explore its sensor space. For example, activating a touch sensor in its fingers, sensing a force in its wrist, maximising a joint angle in its proprioceptive sensors or forcing a movement of an object in its visual camera sensors.The researchers don’t tell the robot how to complete the task, they simply equip it with sensors (which are initially turned off) and let it fumble around until it gets things right.By exploring its environment and testing the functionality of its sensors, the robot is able to eventually earn its reward: a point. If it fails, it doesn’t get a point.Watching a robot arm fumble around in a box may not seem impressive at first, especially if you’ve seen similar looking robots build furniture. But the amazing part is that this particular machine isn’t following a program or doing something it was designed for. It’s just a robot trying to figure out how to make a human happy.And this work is important: it’ll change the world if DeepMind or another AI company can perfect it. Right now, there isn’t a robot in existence that could walk/roll into a strange house and tidy up. Making a bed, emptying the trash, or putting on a pot of coffee are all extremely complex tasks for AI. There’s a near infinite amount of ways each task could be performed – more if the robot is allowed to use flamethrowers and make insurance claims.At the end of the day, we’re still a long ways away from “Rosie the Robot” from “The Jetsons.” But, if DeepMind has anything to say about it, we’ll get there. And it all starts with a robot arm learning how to play with blocks by itself.Want to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.",DeepMind taught robots how to learn on their own
9990,3934305,2018-03-02 14:00:00,"Get more infoEngadgetCriticUsersThere's a reason we've heaped praise on Dell's XPSnotebooks over the past few years. They've always been gorgeous and capable machines, with near bezel-less screens that other computer makers quickly copied. Most importantly, they brought an air of refinement to the Windows laptop market -- something you could previously only find from Apple. Dell's latest XPS 13 continues that tradition of excellence, though there are some changes that might irk longtime fans of the lineup.ConsSummaryDell’s latest XPS 13 doesn’t stray from the line’s high standards. It’s fast, slim and sturdy. Sure, the company didn’t change much, but it didn’t really have to.HardwareAs you'd expect, the XPS 13 still feels like a high-end machine, thanks to its sleek aluminum case and rock-solid build quality. This time around, Dell added a white interior to its rose-gold case to make it more fashionable. It might just be a taste issue, but many Engadget editors found the new styling to be a bit too cheap and garish for the XPS line.The woven-glass-fiber palm rest feels comfortable, at least, and Dell claims it's also stain-resistant. Unfortunately, its kryptonite is sunscreen and mustard, so definitely don't bring it to a beach cookout. But, rest assured, you can still snag the XPS 13 with a silver case and black-carbon-fiber interior if you'd prefer to stick with the traditional style.Devindra Hardawar/AOLThis year's XPS 13 comes in a slimmer and lighter package, weighing in at 2.7 pounds and measuring between 7.8 mm and 11.6 mm thick. In comparison, the previous XPS 13 was a bit heavier if you wanted a touchscreen, at 2.9 pounds. It's light, but because the case looks so thin, it feels denser than you'd expect.To make the newer case work, Dell had to drop USB-A ports and a full-size SD card slot. Instead, you've got three USB-C ports, two of which are compatible with Intel's Thunderbolt 3 standard, as well as a micro-SD card slot. And yes, you can charge from any of the USB-C connections. Gamers will also appreciate the ability to hook up an external GPU to the Thunderbolt ports, something that wasn't possible in previous models.Devindra Hardawar/AOLAnd of course, there's a gorgeous 13.3-inch screen available in 1080p or 4K at the high end. You can choose between standard and touchscreen options, and there's HDR support, finally. That'll let you see video that's both brighter and darker than before. The ""InfinityEdge"" bezels are 23 percent thinner than before, according to Dell, though it's not something I noticed at first. (Probably because they were already pretty slim to begin with.)Another minor change: Dell moved the 720p webcam to the center instead of being off to the side. It's still stuck at the bottom of the screen -- which is a casualty of that InfinityEdge display -- but at least it's easier to align your face properly. There are also four far-field microphones in the XPS 13, which will make it easier for you to shout commands at Cortana, or clearly chat with friends over Skype.There aren't really any surprises with the XPS 13: It's simply a solid and refined ultraportable. It's plenty fast, thanks to Intel's eighth-generation processors. It didn't have any trouble keeping up with my daily workflow, which involves juggling dozens of browser tabs, Slack, Spotify, Evernote and a variety of other apps. Still, I was testing a model with an i7 CPU capable of reaching 4GHz with 8GB of RAM -- the entry-level XPS 13s only have 4GB of memory and slower i5 chips.I didn't have a chance to hook up an external GPU to the XPS 13, but honestly, I never had much of a need to. I've never expected ultraportables like this to be capable gaming rigs. So far, Dell has only included integrated graphics in its 13-inch premium laptops. And while that's gotten more capable over the years, it's still never going to let you play high-end games. The larger XPS 15 offers dedicated graphics, but you'll have to give up a bit of portability if you go that route. Of course, lugging around an external GPU box kinda cuts into the portability of the machine as well. But it still offers you a way to game seriously when you're at home or at a LAN party.Devindra Hardawar/AOLThe 4K screen we tested was also impressive: It was bright, sharp and made everything pop. It even fared well in direct sunlight while I was wearing polarized sunglasses (something competitors like the Surface Laptop have trouble with). HDR is a welcome addition. While it's far more essential in your home theater, it's still to be able to enjoy Netflix to its fullest on the go.Thankfully, Dell didn't change the XPS 13's input options much. The keyboard is still a joy to type on, thanks to its generous 1.3mm travel distance and overall responsiveness. It's one of those keyboards where it feels like words just flow out of my fingers. The Precision touchpad is also as smooth and accurate, as we've come to expect.Dell XPS 13 (2018)9:50Surface Book 2 15-inch20:50Surface Book with Performance Base (2016)16:15Surface Laptop14:49Surface Pro13:40ASUS ROG Zephyrus1:50Surface Book with Performance Base (2016)16:15Apple MacBook Pro 2016 (13-inch, no Touch Bar)11:42Apple MacBook Pro with Retina display (13-inch, 2015)11:23Apple MacBook Pro 2016 (15-inch)11:00HP Spectre x360 15t10:17Apple MacBook Pro 2016 (13-inch, Touch Bar)9:55ASUS ZenBook 39:45Apple MacBook (2016)8:45Samsung Notebook 98:16Alienware 137:32HP Spectre 137:07Razer Blade Stealth (Spring 2016)5:48Razer Blade Stealth (Fall 2016)5:36Dell XPS 15 (2016)5:25 (7:40 with the mobile charger)Alienware 154:31Razer Blade Pro (2016)3:48ASUS ROG Strix GL502VS3:03In our battery test, it lasted 9 hours and 50 minutes, which is better than the XPS 2-in-1 and the Zenbook 3, but less than what we've seen from the Surface Laptop and other premium notebooks. The somewhat average performance can be chalked up to the 4K display, which takes more power to drive. The 1080p versions of the XPS 13 should get a few more hours worth of juice.Pricing and the competitionThe XPS 13 starts at $999 with an 8th gen Core i5 CPU, 4GB of RAM and 128GB of storage. That's decent, but we'd recommend bumping up to the $1,200 model at least, which has 8GB of RAM and a faster 256GB SSD. It'll perform much better, and last longer without needing an upgrade, as well. Frankly, it's about time PC makers started making 8GB of RAM standard. You might have been able to survive with 4GB a few years ago, but these days, Chrome alone could gobble that up quickly.As for alternatives, Apple's 13-inch MacBook Pro starts at $1,300 with a seventh-gen chip, while the Surface Laptop starts at $800 (albeit with an underpowered processor). Just as we've seen for the past few years, the XPS 13 is a strong competitor both in terms of price and performance with other premium ultraportables.Wrap-upDevindra Hardawar/AOLOverall, the XPS 13 remains one of the best ultrabooks on the market. It's fast, sturdy and sports a gorgeous screen. Dell didn't make any big changes to the design, but then again, it didn't really have to. When you've got something as good as the XPS 13 lineup, it's better to stick with what works.Devindra has been obsessed with technology for as long as he can remember -- starting with the first time he ever glimpsed an NES. He spent several years fixing other people's computers before he started down the treacherous path of writing about technology. Mission accomplished?",Dell XPS 13 review (2018): Still the best Windows laptop
9991,3934306,2018-03-02 16:00:00,"Add a luxury Volvo to your list of monthly subscriptions$600 per month includes nearly everything but gas.When Volvo introduced the smallest member of its SUV line, the XC40, it wasn't the car that got all the attention. It was how the Swedish automaker planned on leasing it. With Care by Volvo, would-be XC40 drivers wouldn't invest $35,200 to own the all-wheel-drive vehicle but rather pay a monthly subscription of $600 that includes insurance, routine maintenance, wear and tear, roadside assistance, and zero money down. So yeah, it's a bit like a lease. But better.Gallery: Volvo XC40 | 27 Photos""Better"" has changed markets. The iPhone was like a Palm Treo or Blackberry smartphone but better. Netflix rented DVDs like Blockbuster but better. Laptops are just better desktop computers. What Volvo is doing is a better lease -- and it has the potential to change how we look at car ownership.Although we're inching closer to a future in which autonomous cars are commonplace, the way we actually own cars hasn't changed much. You either purchase a car or lease it. Care by Volvo adds a third option: a near frictionless subscription that includes pretty much everything but gas. When the $600-per-month payment is pitted against the cost of leasing other cars in the same price range, it actually makes financial sense for some.Plus, it's a flat rate. You pay $600 per month for 24 months for the Momentum version of the XC40 or $700 for the R-Design version with better wheels, stiffer suspension, more color options, active-bending lights and a few other upgrades.As always, though, there are caveats. To qualify for Care by Volvo, you have to fit within certain insurance and credit parameters, as determined by Liberty Mutual. So if you have good credit but you have a few points on your driving record that put you outside what an underwriter finds acceptable, you're out of luck. There's no $650-per-month option for a bit higher insurance or to cover your bad credit. It's all or nothing.If you don't qualify for Care by Volvo, the all-wheel-drive (AWD) car starts at $35,200 and you can purchase it as you normally would.I asked about families and couples and how that affects the insurance and price of the car. A Volvo spokesperson said that if the drivers listed on the policy are all within Liberty Mutual's acceptable range, the monthly cost of the car is the same as for a single driver.If you are accepted, Volvo sweetens its subscription deal with the ability to upgrade (or downgrade) to another Volvo after 12 months. After 24 months with the same car, you can walk away at zero cost as long as you stay under 15,000 miles per year. After that, as with a regular lease, you'll accrue a per-mile surcharge.The XC40 packs many of the features found in the outstanding XC90 and XC60, except it has a more compact design intended for younger drivers (millennials), first-time Volvo buyers and folks who simply realize they don't need all the space a larger SUV has to offer.During a test drive in Austin, Texas, I found the XC40 to be a worthy addition to the XC line. Like the XC60 and XC90, its refined interior is one of the best in its class, with clean lines and controls placed exactly where you'd expect to find them. All told, it's one of the easiest cars to master: You immediately know exactly where everything is.The Sensus Connect infotainment system continues to be a favorite of mine. A 12.3-inch touchscreen is available in both the Momentum and R-Design versions of the vehicle I drove. The vertical layout with additional tabs makes for quick navigation while the plethora of options available at the top level of the three main screens means less time delving into submenus that could distract drivers from, well, driving.Both versions of the XC40 are all-wheel drive and have a T5 turbo-charged four-cylinder engine with 248 horsepower and 258 foot-pounds of torque. A T4 front-wheel-drive version with 187 horsepower and 221 pounds of torque is expected in the United States this summer. (This car will not be available through the Care by Volvo program.) The T5 engine doesn't get the pulse racing, but it does make dashing through the city and around corners at least somewhat entertaining. When the accelerator is floored, though, the engine makes a bit of a racket in the cabin.The upcoming version of the XC40 with a smaller engine with front-wheel drive will be cheaper, starting at $33,200. Without driving it, I feel like saving $2,000 on the purchase price for a less capable vehicle seems like a mistake.On the road, the XC40 suspension glides effortlessly on highway and city streets, showing off its luxury pedigree. Volvo also sent me down some rough back roads during my drive. The car performed admirably, mostly smoothing out ruts and potholes. Cornering is tight for a vehicle of its size with body roll kept to a minimum, making some of the more challenging corners during the drive a bit more spirited than expected.While the XC40 is the smallest SUV in the Volvo lineup, it does offer some intriguing storage solutions. The center console is large enough for a small box of tissues with room to spare for a few phones. But just in front of it, the automaker has put a tiny trash bin with a self-closing lid. You know, for all those tissues. The driver's seat has a pull-out drawer with enough room for a small iPad. Meanwhile, the speakers have been relocated from the doors to open up even more space. The compartment is wide enough for a dictionary. I stuck my laptop in there, and there was enough space such that on sharp turns, it actually moved back and forth.The trunk also gets an upgrade with the floor doubling as a fold-up barrier that keeps packages from sliding around the storage area. It's great for keeping your grocery bags from falling over and spilling their contents everywhere.Combine these storage touches with a beautiful interior, striking design, a power train capable of making the commute at least somewhat exciting and suspension that reminds you that you're riding in a luxury vehicle and it would be a mistake for anyone looking for a high-end compact to dismiss the XC40. All of that also makes it the perfect car for Volvo to introduce its Car by Volvo subscription service.The crossover/SUV category continues to grow, and if you're going to experiment with a new form of car ownership, this is the way to do it. The XC40 is at a price point that makes a subscription service financially viable for not only the automaker's loyal customers but also folks new to the brand and the luxury market in general. Plus, some of these potential customers are used to a subscription model for other kinds of goods and services.Beyond cars, the idea of ownership has changed dramatically over the past few decades. We've gone from owning CDs and MP3s to paying for monthly access to music via Spotify and Apple Music. DVDs and Blu-rays have given way to streaming services. We barely own our phones, with many of us swapping them out as soon as something shinier lands on the market.With the speed of car-tech iteration accelerating almost as fast as our phones', having a five-year-old vehicle will soon seem as ridiculous to some as having a five-year-old smartphone. The introduction of mass-produced, self-driving vehicles will only exacerbate the need for subscription-based vehicles. They'll be too expensive to own outright.Right now Volvo is pushing for a future in which ownership is fluid, and it seems to be resonating with people. According to the automaker, it's already racked 20,000 global pre-sales for the XC40 through Care by Volvo. Ninety-one percent of those buyers are new to the Volvo brand. What's more, the company's upcoming V60 wagon will also be available via Car by Volvo, opening the door to bigger sales numbers.Expect other automakers to keep an eye on Care by Volvo. Cadillac and Porsche already have subscriptions services for their top-of-the-line vehicles, but those start at $1,800 and don't exactly speak to the average car buyer. The smartphone generation wants something without hassle that can be swapped fairly quickly; Care by Volvo offers that.Volvo might not have the same luxury cachet of BMW and Mercedes, but if Care by Volvo takes off, expect that to change as people decide that swapping a car as often as a smartphone sounds like a better idea than the usual lease.",Add a luxury Volvo to your list of monthly subscriptions
9992,3934307,2018-03-02 15:00:00,"'Far Cry 5' is deeper than you thinkFar Cry 5 is the most socially relevant game in the series yet. This is the first time Ubisoft has brought the open world franchise to America -- specifically, to the wilderness of Montana. But it's more than just another free for all shooter. The game is really a statement on the increasing influence of violent religious extremism in the US, and the need for good people to take a stand against them. While we got a brief glimpse at the game's partner mechanics at E3, we recently a had chance to play it for a few hours, and came away mostly impressed.Gallery: Far Cry 5 screenshots | 5 PhotosIt puts you in the shoes of a rookie cop, who's sent to arrest Joseph Seed, the head of a dangerous cult called The Project at Eden's Gate. They're heavily armed, fanatical about their devotion to their leader, also known as The Father, and are quickly taking over more territory. As you'd expect, your plans to arrest Seed quickly go off the rails. You end up becoming part of a resistance movement, whose goal is to take down the cult by going after Joseph Seed and the rest of his family.""The thing that's truly the genesis for [Far Cry 5] is just this feeling like we're on the edge of something,"" said Dan Hay, creative director for Far Cry 5, in an interview with Engadget. ""There are moments in history where you can kind of look back and say, 'Hey this is the moment where everything is changing.' It feels like three years ago, we were heading into one of those. The game is about that. If you're on the edge of something and it was that one time it was all going to go to shit, would you know it?""Like every Far Cry game, you've got a huge world to explore, and you're free to go about your objectives however you like. This time, though, there's a ""Resistance Meter,"" which tracks your progress throughout the game's several regions. You can build up the influence of the resistance by saving captured citizens, destroying Eden's Gate structures, completing missions and liberating outposts from cultists.UbisoftYou can also team up with allies throughout the game, each of which bring different skills to every fight. There's even a dog named Boomer that can retrieve animals you hunt and attack enemies for you. (And yes, you can pet him on-demand.)As is typical for a Far Cry title, the game looks gorgeous, with vast environments that capture Montana's natural beauty. Thankfully, the developers at Ubisoft also made the gameplay more organic. Instead of climbing towers to find things to do, you actually have to seek out and talk to people. You can read into that what you will. Like the entire game, it seems like a metaphor for something all of America needs to do to move forward as a society.During my playtime with the game, I got to explore the outdoor environment, and take down some stray enemies. With the help of an ally, I also cleared a huge cultist base. Doing so took several tries, but the beauty of Far Cry is that you've got multiple ways to solve every problem. For me, going stealthy with a bow and arrow was best.Ubisoft also worked with Mia Donovan, an expert on cults and fringe groups, who helped add a layer of authenticity to Eden's Gate. The game explores why seemingly normal people would be seduced by dangerous organizations, and at points you'll get a first-hand look at how that process works.""I think games have come to a point now where it's okay to explore certain topics,"" Hay said. ""We also understand that at first blush, people are going to look at the game and say you're making a game about this .... And we say hold on, this [other topic] is what we're actually doing. We take a global perspective about things.""While Far Cry 5 is ultimately just a piece of pop culture entertainment, I'm fascinated by how much Ubisoft Montreal is leaning into such a socially relevant story. Of course, you could just play it like any other game in the series, but it's hard to ignore the subtext. It's a game about the resistance, fighting to save America.Devindra has been obsessed with technology for as long as he can remember -- starting with the first time he ever glimpsed an NES. He spent several years fixing other people's computers before he started down the treacherous path of writing about technology. Mission accomplished?",'Far Cry 5' is deeper than you think
9993,3934405,2018-02-28 14:00:35,"Study: When CEOs’ Equity Is About to Vest, They Cut Investment to Boost the Stock PriceExecutive SummaryDoes short-termism exist? And, if it does, what’s the underlying cause? A new study finds that the more equity CEOs have vesting in a given quarter, the more they cut investment. Vesting equity encourages CEOs to take actions with destructive long-term consequences. The research suggests that the horizonof pay is more important than the total value. It affects the CEO’s incentives to invest, with major implications for the company’s long-run success and contribution to society.miragec/Getty ImagesAre today’s businesses plagued by short-termism? The narrative is compelling. Executives cut investment to hit short-term earnings targets and trigger bonus payouts, the argument goes. They are egged on by short-term shareholders, who care only about making a quick buck, rather than growing the company for the long term. Moreover, long-term investments — such as reducing carbon emissions, developing blockbuster drugs, or training workers — benefit more than just shareholders. So, the charge that businesses are deprioritizing them leads to concerns that business no longer serves society.But despite how common the charge of short-termism has become, rigorous evidence of short-termism is surprisingly difficult to find. Eighty percent of CFOs say that they’d cut investment to meet earnings targets — but what they say isn’t necessarily what they do. A recent McKinsey study found that a “long-termism” index (including how much a firm invests) is correlated with future long-term stock returns, suggesting that long-termism pays off. But causality could easily run the other way. When a firm’s long-term outlook is worse, it should invest less. This is taught in any Finance 101 class, and is presumably what McKinsey advises its clients.So does short-termism exist? And if it does, what’s the underlying cause?This is what Vivian Fang, Katharina Lewellen, and I set out to study. We wanted to see whether CEO pay packets encouraged short-termism — whether CEOs cut investment to pump up their company’s short-term stock price and then cash out their shares.But it’s extremely hard to document causation rather than simply correlation. Say we found that a CEO cuts investment and then sells their shares. One interpretation is that they cut investment to cash out. But another is that their firm’s long-term outlook is poor. This causes the CEO (correctly) to cut investment and also (rationally) to sell their shares. So, there’s correlation between investment and sales, but no causation.We studied not how many shares the CEO sells, but how many shares are scheduled to vest. Here’s the idea: CEOs’ shares come with a vesting or lock-up period. Only once this period is over can they sell — and they typically do, to diversify their personal investments. So, when the lock-up expires, the CEO is concerned about the stock price. But a lock-up expiring today is driven by the board’s decision several years ago to grant the shares. It’s unlikely that at the time the board would have known the firm’s present outlook. So the decision to grant shares back then, and thus the fact that shares are vesting today, is unlikely to be driven by factors that also determine investment. By focusing on when shares vest rather than when CEOs sell shares, we’re able to show causation — not just correlation — from CEOs’ personal wealth concerns to cuts in investment.We find that the more equity CEOs have vesting in a given quarter, the more they cut investment. This result holds for five different measures of investment and for both vesting stock and vesting options. In addition, vesting equity is positively correlated with the CEO just barely meeting analyst earnings forecasts — suggesting that it causes the CEO to focus on short-term earnings rather than long-term investment.That result is consistent with short-term behavior, but we considered an alternative interpretation. Could the investment cuts actually be efficient? Perhaps stock price concerns are motivating, because they induce the CEO to make tough decisions, such as cutting wasteful investment. If so, we would expect the CEO to improve efficiency in other ways as well, such as increasing sales growth or cutting other expenses. And we find no evidence of that.Moreover, in a new paper with Vivian and Allen Huang, we aim to more precisely document the long-term consequences of short-term incentives. To do so, we study share buybacks and M&A, since — unlike with investment cuts — we can measure the long-term returns to these actions as they have a clear announcement date. We find that the more vesting equity the CEO has in a given quarter, the higher the likelihood is of both buybacks and M&A. Like investment cuts, buybacks and M&A could be good rather than bad — but we find that vesting equity leads to the bad type. It is associated with higher short-term returns to both decisions, but significantly lower long-term returns. In short, vesting equity encourages CEOs to take actions with destructive long-term consequences.What does this all mean for CEO pay? There is widespread belief that CEO pay needs to be reformed, but proposed reforms typically focus on the level of pay. Our research suggests that the horizon of pay is more important — it affects the CEO’s incentives to invest, with major implications for the company’s long-run success and contribution to society. Cutting pay in half will win more headlines than extending the vesting horizon from, say, three to five years, but the latter is likely to be much more important. Indeed, the UK government’s green paper on corporate governance has proposed this extension, and the Norwegian sovereign wealth fund’s remuneration principles advocates making the CEO a long-term shareholder.Cutting the level of CEO pay aims to redistribute the CEO’s share of the pie to other stakeholders. But since the CEO’s slice is small to begin with — in large U.S. firms, it accounts for just 0.05% of firm value — the result would be negligible. Increasing the horizon of pay will encourage the CEO to invest for the firm’s long-term success, growing the pie to the benefit of both shareholders and society.Alex Edmans is a Professor of Finance at London Business School, where he specializes in corporate finance, behavioral finance, and corporate social responsibility. He earned his BA from Oxford University and his Ph.D. from MIT, where he was a Fulbright scholar.","Study: When CEOs’ Equity Is About to Vest, They Cut Investment to Boost the Stock Price"
9994,3934565,2018-03-02 01:04:55,"Instagram code reveals unreleased voice and video calling0Instagram wants to be your phone, not just your camera. And it wants to be better at it than Snapchat. Files buried in Instagram and the Instagram Direct standalone app’s Android Application Packages (APKs) are files and icons for “Call” and “Video Call”. APKs often show files for unreleased features that are lying dormant in an app waiting to be surfaced when the company is ready to launch them. So it appears Instagram is preparing to launch audio and video calling within its Direct messaging system.You can see the files in this image shared with TechCrunch by reader Ishan Agarwal:This reinforces our report from January about an image of an a video call button spotted by the WAbetainfo blog. At the time this appeared to be only testing internally and Instagram gave us a “no comment”. Similarly, today when presented with this new evidence, an Instagram spokesperson told me “I’m afraid we can’t comment on this one”.But now that the button icons are in APK, which is publicly available but takes work to decompile and comb through, it’s hard for the company to deny the coming launch. Last month it gave me a no comment when TechCrunch spotted a new Giphy GIF sharing feature…which then launched a week later.The addition of voice and video calling would make Instagram a better chat alternative to Snapchat, which first launched video calling in 2014 and enhanced the feature with easier access plus audio and video notes in 2016. It’s actually surprising Instagram hasn’t launched these features already. Beyond Stories, Instagram has been working hard to leapfrog Snapchat with new features like replay privacy controls that Snapchat lacks.Instagram’s competition has contributed to stagnation in the growth of Snapchat Stories sharing, but messaging remains beloved by Snap’s younger users. While Instagram combined ephemeral and permanent image sharing in Direct in April 2017, and launched the standalone Direct app last year, messaging still feels bolted on to Messenger. It was only added in 2013, three years after Instagram started as just a feed.Instagram Direct combines ephemeral and permanent messagingBut if Instagram can become the best way to communicate visually, no matter the medium, it could steal users from Snapchat and make it’s app more appealing to people around the world. Instagram doesn’t monetize messaging itself, but becoming a stand-in for people’s phones could lead to spillover usage of its feed and Stories that show ads.Instagram is meant to be a window into the lives of your friends. Now you get their weekly highlights in the feed, daily adventures on Stories, monologues on Live, asynchronous memes and messages in Direct. Calling could turn Instagram’s window into a doorway.",Instagram code reveals unreleased voice and video calling
9995,3936944,2018-03-02 15:00:00,"The Rylo 360-degree video camera now works with AndroidRylo is one of a handful of 360-degree cameras that only requires an editing suite as big as your smartphone. Much like the cheaper Insta360 One and more expensive GoPro Fusion, Rylo's companion mobile app lets you play editor and director. Users can set specific focal points or objects for the frame to follow within a 360-video after the fact, before sharing their masterpiece to Instagram or Facebook. With two 208-degree lenses, 4K video resolution and auto-stabilization tech, it's a pretty neat little camera. Since launching last fall, though, it's only worked with iPhones and iPads, but today, as promised, Android support has arrived.It's not quite that simple, though. While anyone can download the app, not every Android device is equipped to handle 360-degree video. But fret not, for Rylo has published a list of recommended devices with which the camera is fully compatible -- pretty much just the latest Samsung Galaxys and Google Pixels. Also noted are a few handsets that are supported but can't export 360-degree video, and a number of smartphones that simply don't have the right hardware to play nice with Rylo. The list is relatively short, but you can assume if you have a newish top-tier device running Android 6.0 Marshmallow or later, it's likely to be compatible.Other than a different sync cable to get Rylo talking to your Android phone, you get the same camera and accessories in the box for the same $499 price. Now all an Android user needs is a life adventurous and exciting enough to justify the purchase.",The Rylo 360-degree video camera now works with Android
9996,3936945,2018-03-02 20:28:00,"Snap may release two new pairs of Spectacles soonSnap's camera-adorned Spectacles were never going to be a huge seller; rather, they represented the company's first dip into hardware. Still, they sold less than expected, with reports claiming the company had hundreds of thousands of unsold pairs and a confirmation that Snap took a $40 million bath after completely mis-estimating demand. Despite a rocky launch, it seems that Snap may not be ready to give up on its hardware ambitions just yet. Cheddar is reporting that Snap is planning two new pairs of Spectacles.The second-generation Spectacles are rumored to be ready this fall, about two years after the original. /Cheddar/ claims that the new glasses will focus more on improved performance, but it sounds like they'll be largely similar to the original. Other changes include new colors and waterproofing, something that could make for some fun videos.But Snap is also reportedly going to release a third version that would include two cameras. Given the popularity and functionality that dual cameras have provided in a growing number of smartphones, it makes sense to see Snap give this a shot in its own camera-focused hardware. Finally, Snap is considering partnerships with companies like Warby Parker to put its technology into their own glasses.These more advanced Spectacles would likely not be available until 2019, though the company apparently wanted to have them ready this year. Similarly, although the single-camera model is planned for this fall, /Cheddar/ reports that it might be delayed into next year as well.",Snap may release two new pairs of Spectacles soon
9997,3937036,2018-03-02 17:18:37,"The Supreme Court Case That Could Give Tech Giants More PowerOn Monday the Supreme Court heard Ohio v. American Express.Credit Gabriella Demczuk for The New York TimesBig tech platforms — Amazon, Facebook, Google — control a large and growing share of our commerce and communications, and the scope and degree of their dominance poses real hazards. A bipartisan consensus has formed around this idea. Senator Elizabeth Warren has charged tech giants with using their heft to “snuff out competition,” and even Senator Ted Cruz — usually a foe of government regulation — recently warned of their “unprecedented” size and power. While the potential tools for redressing the harms vary, a growing chorus is calling for the use of antitrust law.But the decision in a case currently before the Supreme Court could block off that path, by effectively shielding big tech platforms from serious antitrust scrutiny. On Monday the Court heard Ohio v. American Express, a case centering on a technical but critical question about how to analyze harmful conduct by firms that serve multiple groups of users. Though the case concerns the credit card industry, it could have sweeping ramifications for the way in which antitrust law gets applied generally, especially with regards to the tech giants.The case was first brought by the Justice Department against American Express, Visa, and Mastercard for imposing anticompetitive restrictions on merchants. The credit card industry is a classic case of oligopoly. Despite involving millions of merchants and hundreds of millions of cardholders, the credit card business is controlled by four firms. Merchants who need payment networks lack any real bargaining power and have been stuck paying high rates to the oligopoly — steeper costs that ultimately get passed on to consumers.PhotoBy imposing conditions on retailers that accept Amex cards, retailers were told they could not ask their own customers to use, other credit cards, over an American Express card.Credit Shaun Egan/The Image Bank, via Getty ImagesAs one might expect, the credit card companies use their power to block competition. American Express did this by imposing conditions on retailers that accept Amex cards. Retailers were told they could not ask their own customers to use, say, a Discover card, over an American Express card, even if Discover would be a cheaper option. Discover could provide a merchant like The Home Depot better terms than American Express — but, because of American Express’s restrictions, The Home Depot had no way of reflecting Discover’s lower rates to cardholders, thereby keeping Discover’s competitive advantage from translating into greater market share. This meant card networks had no reason to compete when serving merchants, keeping prices high.The district court ruled that American Express’s “anti-steering” provisions stifled price competition and violated the antitrust laws (Mastercard and Visa settled with the government before trial). But when American Express appealed, the Second Circuit reversed, relying on a new concept to create a special set of rules. The concept is that players in “two-sided” markets are unique because they serve different sets of customers in distinct but related markets, effectively facilitating transactions. American Express, for example, charges both merchants who accept its cards and consumers who use them. Using this concept, the Second Circuit held that the government would have to show that any price increases for merchants also harmed cardholders, or at least didn’t benefit them. In effect, the court introduced a dramatically new rule, making it much more difficult to win important antitrust cases and to stop anticompetitive behavior.The case is now at the Supreme Court. If affirmed, the Second Circuit decision would create de facto antitrust immunity for the most powerful companies in the economy. Since internet technologies have enabled the growth of platform companies that serve multiple groups of users, firms like Alphabet, Amazon, Apple, Facebook, and Uber are set to be prime beneficiaries of the Second Circuit’s warped analysis. Amazon, for example, could claim status as a two-sided platform because it connects buyers and sellers of goods; Google because it facilitates a market between advertisers and search users. (An industry trade group representing the tech platforms filed an amicus brief in support of American Express.) Indeed, the reason that the tech giants are lining up behind the Second Circuit’s approach is that — if ratified — it would make it vastly more difficult to use antitrust laws against them.The fact that the tech platforms could effectively be shielded from antitrust is troubling because, in several respects, these firms enjoy dominant market positions that makes antitrust scrutiny of their conduct especially important. By virtue of providing increasingly critical services, tech giants wield immense leverage over the sellers and buyers that rely on their platforms. This power is ripe for abuse. If the Supreme Court ratifies the Second Circuit’s approach, platforms will be able to engage in anticompetitive activity with one set of users, so long as they can plausibly claim that harmful conduct enabled them to benefit another group. Say, for example, that Uber prohibited its drivers from also serving rivals like Lyft, suppressing driver income. Under the current approach, these exclusive agreements would likely violate antitrust law. But under the Second Circuit’s analysis, the case would go nowhere unless plaintiffs could show that this practice also harmed riders.An error has occurred. Please try again later.You are already subscribed to this email.This danger is compounded by the slipperiness of the concept underlying this analysis: the idea of a “two-sided” market. While reports paid for by the credit card industry introduce “two-sided” markets as a novel concept that requires special analysis, markets serving different groups of users have been around for centuries. They include, for example, grain futures exchanges (connecting farmers and buyers of farm products), banking (linking depositors and borrowers) and newspapers (serving advertisers and readers). For decades, courts have recognized that anticompetitive conduct imposed by a newspaper on readers, for example, is no less illegal if it can be shown to benefit advertisers.Indeed, almost all markets can be understood as having two sides. Firms ranging from airlines to meatpackers could reasonably argue that they meet the definition of “two-sided,” thereby securing less stringent review.Oral argument on Monday gave little new insight into how the vote breakdown might fall. Justice Neil Gorsuch pushed an aggressively pro-monopoly line of questions, while Justices Sonia Sotomayor, Stephen Breyer and Elena Kagan were skeptical of arguments offered by American Express. While Chief Justice John Roberts Jr. spoke little, his views may prove decisive. He has criticized unwieldy antitrust standards in the past — a problem that would get much worse if the court rules for American Express.Unless the Supreme Court rejects the Second Circuit’s approach, it will risk immunizing dominant platforms from effective antitrust review, at the very moment when they most need it.Lina M. Khan is the director of legal policy at Open Markets Institute, which filed an amicus brief in Ohio V. American Express, and a visiting fellow at Yale Law School.",Opinion | The Supreme Court Case That Could Give Tech Giants More Power
9998,3937043,2018-03-01 15:26:41,"Site Mobile NavigationSpotify Files to Go Public on the New York Stock ExchangeDaniel Ek, the chief executive and a co-founder of Spotify. According to a company filing, Spotify had nearly $5 billion in revenue last year, up 39 percent from the year before.Credit Don Emmert/Agence France-Presse — Getty ImagesSpotify has already won over the music industry, and listeners worldwide, with an online streaming service that makes millions of songs instantly available. Now, it is ready to test its business model on Wall Street.On Wednesday, the company filed a prospectus to sell shares on the New York Stock Exchange, an indication of the imminent arrival of one of the most anticipated technology stocks in years. It was also a signal of the maturation of the streaming market that has already begun to revive the long-struggling music industry.Instead of a traditional initial public offering, Spotify will, as expected, pursue a direct listing of its shares, an unusual process in which no new stock is issued — and therefore no money is raised. However, existing investors and insiders can trade their shares on the open market.But because Spotify is eschewing Wall Street banks to manage the raising of new capital, the listing could face a rockier reception in the markets. Those banks typically spend weeks assessing investor demand for a company’s shares and try to match the amount that hits the market to ensure the shares are supported.According to the prospectus, investors trading Spotify’s shares in private transactions have valued the company as highly as $23 billion. Spotify’s shares will be traded under the ticker symbol SPOT, but there is no indication of when that will begin.The company, showing no shortage of ambition, said that its mission was “to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by these creators.”For the music industry, Spotify — and the streaming model it has championed — has been a powerful engine. After more than a decade of decline, the global music market began to turn around in about 2015, just as streaming began to take hold.In the United States, for example, streaming now accounts for about two-thirds of recorded music revenues, according to the Recording Industry Association of America, and streaming platforms like Spotify, Apple Music, YouTube and SoundCloud have become the new outlets where stars develop and hits are minted.Even so, many artists remain skeptical of the streaming economy, which heavily rewards mainstream hits but has drawn complaints from other songwriters and musicians, particularly those outside the sphere of pop, who feel they are not being adequately compensated for their work.Spotify’s prospectus is the most detailed disclosure the company has given about its business.According to the filing, Spotify, which began its streaming service in Sweden in 2008 and came to the United States three years later, had nearly $5 billion in revenue in 2017, up 38.6 percent from the year before. In 2016, it had grown by 52 percent.By the end of 2017, Spotify, whose full name is Spotify Technology, had 159 million active users, including 71 million who pay for subscriptions; the rest, using the “freemium” model that has become Spotify’s marketing hallmark, get free access to music but are subjected to advertisements.As quickly as Spotify has grown, though, so have its losses. Last year, it had a net loss of $1.5 billion, up from about $650 million the year before. Its largest expense, by far, is the cost of licenses from record companies and music publishers.An error has occurred. Please try again later.You are already subscribed to this email.The prospectus also highlights Spotify’s successes, and its value to the music industry. Through the end of last year, the company had paid more than $10 billion in music royalties since its inception, and its “churn” — a measure of how many paying users cancel each month — has been steadily declining, to 5.5 percent in 2017 from 7.7 percent in 2015.The largest stakes are owned by its two founders: Daniel Ek, the chief executive and the company’s public face, and Martin Lorentzon, who holds no executive position. In addition to their shares, they also have “beneficiary certificates” granting them extra voting rights. In total, Mr. Ek has 37.3 percent voting power over the company, and Mr. Lorentzon 43.1 percent, according to the prospectus.The major record labels also own minority stakes in the company, as a result of licensing deals struck over the years. Sony has the largest, with 5.7 percent; the others were not disclosed.Spotify’s major competitor is Apple, whose co-founder Steve Jobs had long disdained music subscriptions. Apple finally entered the streaming subscription business in 2015 with Apple Music, and recently said that it had 36 million paying users. But according to a recent report in The Wall Street Journal, Apple Music has been outpacing Spotify in its growth in paid subscribers in the United States.The market for public offerings has been robust so far this year. Through late February, 30 companies listed their shares publicly on markets in the United States, raising $11.4 billion from investors. That was the strongest annual start for the market since 2000, according to data from Thomson Reuters.At least some of the highly valued, privately held technology companies, known as unicorns, could move quickly to capitalize on the opportunity and sell shares publicly.Last week, for instance, the online file storage company Dropbox, valued by the private markets at about $10 billion, filed paperwork to raise up to $500 million in an initial public offering.But to convert that paper wealth into actual cash at top dollar, early-stage investors, company founders and employees with stock options need receptive public markets to buy their shares.If the public investors do not think the companies are worth as much as the small number of deep-pocketed investors who dominate the private markets, it could mean the large amounts of paper wealth could be vaporized once the company’s shares start trading.That’s effectively what happened last year, when some prominent public offerings fizzled once the shares were trading publicly. Shares of Snap, the start-up behind the popular Snapchat app, surged when it went public last March, but it has since had difficulty staying above its offering price of $17 a share. And the meal-delivery start-up Blue Apron, which went public at $10 a share last June, is now trading at less than $3 a share.",Spotify Files to Go Public on the New York Stock Exchange
9999,3938453,2018-03-02 18:31:00,"Testing the Galaxy S9+ on a night out in BarcelonaSamsung's new features seem effective, but fall short of being impressive.The Galaxy S9 and S9+ didn't bring revolutionary changes to the table, but promised a ""reimagined"" camera that's supposed to change the way we use our mobile shooters. Most of the biggest changes Samsung implemented revolve around improving low-light photography, so what better way to make full use of our scenic surroundings here in Barcelona than to take the S9+ out for an evening sightseeing tour? Here's what we learned (other than that the Gothic Quarter is really pretty, and it can get pretty cold in Spain).As you follow along below, check out full-resolution versions of the pictures here.First, a quick overview of the S9+'s new tools. It has a Dual Aperture feature which has the primary camera jump from an aperture of f/2.4 to f/1.5 when it senses that the scene isn't bright enough. The wider opening lets in more light, and is currently the biggest on a smartphone, breaking the record previously held by the LG V30 at f/1.6.Then there's multi-frame noise reduction, which was introduced in the S8 but enhanced in the S9 thanks to dedicated RAM built right into the sensor. This way, the camera can more quickly process the 12 images it uses per shot to reduce noise. Together with the dual pixel sensor that debuted in the S7, this is supposed to make for speedy and clear autofocus, even in challenging conditions.Other features, like the S9+'s dual camera and the new super slow-mo video capture are less specific to low-light photography, but we tested them out in a variety of situations anyway.Overall impressionsBefore we dive into the specific features, it's worth noting that in general, the S9+ takes pretty good photos. Samsung has more or less nailed its smartphone cameras, so this comes as no surprise. Both pictures and video have great colors and clarity, especially in daylight. Scenes that are largely shrouded in darkness appear well-lit, too -- to the point where they look even brighter than in real life, especially when shot in full auto mode.Dual ApertureThe Dual Aperture is largely responsible for the brighter images. Since you only have the option of jumping between f/2.4 or f/1.5 (instead of choosing stops in between), there's no way to finesse your settings to get a more realistic shot, unless you go into Pro mode. That's what I'd recommend, though.One of the downsides of shooting at as wide an aperture as f/1.5 is that dynamic range suffers. Parts of the scene that are bright will be blown out, such as the restaurant in the background of my shot of a fountain. Switching over to Pro mode, I was able to use an aperture of f/1.5, push up my shutter speed and decrease ISO to not only reclaim some of that information, but also get a clearer, less noisy picture overall.Another concern with shooting at f/1.5 is that focus might be softer for subjects that aren't close to the camera. But from our testing so far, the difference in clarity really isn't that noticeable. My shots of a cocktail glass in a dark bar appear almost as crisp in f/1.5 as in f/2.4, although you'll notice a bigger difference if you get super close to the subject. The good news is, if you know what you're doing and want to achieve a very shallow focus effect, you can. If not, you won't notice a significant difference in clarity between the two levels.Noise reductionThe S9 and S9+ use 12 images per shot to smooth out noise, and thanks to the built-in DRAM, are able to do so quickly. While we can't observe this in action, the results are evident. There are fewer color specks in general, compared to photos from an S8, and outlines of objects are crisp, which seems to indicate that the MFNR is working. The Dual Pixel autofocus also appears to be effective -- despite my hands shivering in the cold, there is little motion blur in the wide aperture shots.Upon closer inspection, though, I noticed that edges of buildings appeared muddy in many shots. It looks like Samsung's processing software still introduces artifacts that are noticeable when you zoom in, but for your Instagram and Facebook uploads the photos will be more than adequate.We still haven't pushed the S9+ very hard, or spent all that much time with it, so it's difficult to figure out any major issues and trends so far. What I can tell now is that the Dual Aperture feature really does make a difference. You're getting a whole two stops more to play with. All other things constant, images shot at f/1.5 are noticeably brighter than those at f/2.4.Live Focus, super slow-mo and other featuresThanks to the telephoto and wide-angle lens pair on the S9+, you get features like Live Focus and Dual Capture, which snaps two pictures at once -- one up close and one further out. Live Focus helps you get pictures with a nice blurred background while retaining shallow depth of field, but so far appears to struggle in low light. Super slow-mo, which records video at 960 fps, also requires a fair amount of light to work well.My footage of senior mobile editor Chris Velazco trying (very hard) and failing to catch a moving flower with his mouth in a dim restaurant is barely visible. That's not surprising, though -- at such a high frame rate, most cameras need a ton of light to capture anything decent.Overall, the new low-light features seem effective but the results ultimately aren't extremely impressive. The extra large opening will come in handy especially in really dark scenarios, though, so night owls may reap better photo opps with this phone. Still, we'll be putting the S9s through their paces in a full review soon, so stay tuned to see if we learn anything different.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.",Testing the Galaxy S9+ on a night out in Barcelona
10000,3938455,2018-03-02 20:42:00,"Alexa lost its voice... for real this timeHas Alexa been giving you the cold shoulder recently? You're far from the only one. Outages in Amazon's cloud services muted the voice assistant for many people on March 2nd, producing error messages when you spoke commands to Echo speakers and other Alexa-equipped devices. The issue didn't affect everyone (we successfully spoke to Alexa in Canada during the outage, for instance) and could sometimes be overcome by using the Alexa app, but you certainly didn't want to depend on the AI helper for anything important.Amazon's issue also affected us, we'd add — it prevented us from updating the site properly for a few hours. Sorry about that.We've asked Amazon for comment and will let you know when it can share more details. Whatever it says, the incident is a not-so-subtle reminder of the risks involved in making a device heavily dependent on the internet, even for local tasks. It's great when it works, but it can be maddening when everything goes south. And there's a certain irony to this when Amazon just ran a Super Bowl ad where Alexa lost her voice -- it clearly wasn't expecting that to happen in real life.",Alexa lost its voice... for real this time
10001,3938549,2018-03-01 00:00:00,"For a book that is crammed with adulteries, alcoholism, betrayals, broken friendships, deportations, deprivation, drug addiction, executions, humiliation, illicit abortions, imprisonment, murder, Nazi atrocities, starvation, torture chambers (on the avenue Hoche, passers-by could hear the screams coming up from the cellars’ air vents), treason and worse, Agnès Poirier’s Left Bank is a remarkably exhilarating read.Above all, it has a terrific cast, with, as leading players, Jean-Paul Sartre, Simone de Beauvoir, Albert Camus and Maurice Merleau-Ponty. The novelist, jazz musician and pataphysician Boris Vian, Samuel Beckett and the resident aliens Picasso and Giacometti also feature, as do brilliant African-American musicians and writers such as Miles Davis, James Baldwin and Richard Wright, the vehemently anti-communist Hungarian writer and wife-beater Arthur Koestler and, among the occupiers, the sinister but fascinating German Ernst Jünger, aesthete, entomologist and polymath.Left Bank is an enchanting account of how these exceptionally talented and original people not merely endured these harsh years but also found pleasure, and even a kind of joy, in creating small pockets of private utopia. During the occupation, a leading German commander, Sonderführer Gerhard Heller, shared in this paradoxical pleasure and many years later recalled, ‘I lived in a kind of blessed island, in the middle of an ocean of mud and blood.’Over the course of a decade, both during the occupation and then in the postwar years of austerity, the boldest and brightest Parisians took every opportunity to seize the day.If strict rationing – 1,300 calories a day, if you were lucky – meant it was hard for these Parisians to make merry with the traditional eating and drinking, there were other sources of cheer: easy-going sex (which also kept you warm in unheated rooms), popular music, dancing, artistic creation and endless intense conversations, fuelled by as many cigarettes as the black market could supply. Richard Wright said that the first time he attended an editorial meeting of Sartre’s journal Les Temps modernes, the cigarette smoke was so thick that initially he could not even make out the figure of Simone de Beauvoir in her trademark turban.As the historian Tony Judt once pointed out, the consequence of this pressure-cooker atmosphere was that Paris became more important to the rest of the world in the late 1940s and early 1950s than it had been at any time since the Battle of Waterloo. The free and freed nations soon became fascinated by everything Parisian, from Sartre’s existentialism to Dior’s New Look, and young people from the United States were among the keenest fans.Often funded by government grants to ex-servicemen, budding North American writers found Paris as irresistible in the years immediately after the Second World War as their forebears had in the 1920s, when the dollar had ridden high against the franc. Norman Mailer, Saul Bellow, Nelson Algren (one of Simone de Beauvoir’s lovers), Irwin Shaw and Art Buchwald all set up in the City of Light, though not all of them liked it. Bellow, who, among other things, claimed that he hated French promiscuity, lapsed into serious depression: ‘Paris is the seat of a highly developed humanity, and one thus witnesses highly developed forms of suffering there. Witnesses and, sometimes, experiences. Sadness is a daily levy that civilization imposes in Paris. Gay Paris? Gay, my foot!’By French standards, these visitors were astonishingly well fed and well dressed; by American standards, the Parisians were skinny, shabby and shockingly impoverished. The visitors could hardly believe that world-famous artists and intellectuals could be so poor that they lived in small rooms in cheap hotels and worked in cafes, not because they were bohemians (though they were) but because cafes were warm.Few Americans could resist the temptation to go native. As de Beauvoir wrote to Algren, ‘Young existentialist boys now grow a beard; American intellectual tourists grow beards too. All these beards are awfully ugly! But the existentialist caves are a wonderful success. It is funny, just two blocks – that is all Saint-German-des-Prés – but within those two blocks you cannot find a place to sit down, neither in the bars, cafes, night-clubs nor even on the pavement. Then all around it is just darkness and death.’Poirier is acute and witty on the love–hate relationship between Paris and America, which is one of the major themes of her book. Although left-wing thinkers of every political hue from red to pink were sceptical about American capitalism (incidentally, the French Communist Party hated Sartre and his crowd, and did its best to crush them), their attitude towards both high and low American culture was usually one of frank hero worship. As de Beauvoir commented, ‘American literature, jazz and films had nurtured our youth.’ When Camus asked Sartre if he would like to go to America on behalf of his journal, Combat, Sartre almost jumped for joy. ‘I never saw him so happy,’ Camus reported. The later pages of the book sing the praises of the Marshall Plan, which Poirier regards, justly, as one of America’s greatest achievements.Another of her themes is the unprecedented significance of women in this milieu. In many works of cultural history, women appear simply as wives and daughters, mistresses and muses. But here are the bookshop owners Sylvia Beach (who had published Ulysses in 1922) and Adrienne Monnier, the actresses Maria Casarès (who played Death in Cocteau’s Orphée), Arletty, star of Les Enfants du paradis, notorious for sleeping with the enemy, Brigitte Bardot and Delphine Seyrig. The singers include Juliette Gréco, for whom both Sartre and Raymond Queneau wrote lyrics, and many writers: the novelist Marguerite Duras, the poet Anne-Marie Cazalis, the Horizon representative Sonia Brownell (soon to marry the dying George Orwell), who had been forced to abort Koestler’s child during the Blitz, the novelist and biographer Edith Thomas, Janet Flanner, who reported on Paris for the New Yorker,andDominique Aury, who wrote Histoire d’O under the pseudonym Pauline Réage.Above all, there is de Beauvoir, who, now that the dust has settled, should be seen as the most permanently influential of all these remarkable women and, come to that, men. The Second Sex, written during this period, has surely touched the lives of countless millions, which can hardly be said of Being and Nothingness. Poirier credits de Beauvoir with, among other accomplishments, being the woman whose writings, example and spirit created the likes of Françoise Sagan and Bardot, who were adolescents on the brink of fame in the summer of 1949.Poirier has an enviably clear prose style, as well as a gift for making her characters vivid and, where appropriate, sympathetic. Sartre, for instance, comes across as a much more appealing character here than in many biographical studies. She has a lynx’s eye for telling details, from the ghastly ersatz coffee that Parisians had to choke down to the brands of amphetamine freely available in pharmacies – Luminax, Leviton, Tranquidex, Psychotron (!), Lidepran and Sartre’s excitant of choice, Orthédrine.And she is very good on the stories behind stories, such as the bafflement with which the publisher Gallimard reacted when, three weeks after its publication, Sartre’s seven-hundred-page Being and Nothingness became a freak bestseller. Explanation? ‘It turned out that since the book weighed exactly one kilogram, people were simply using it as a weight, since the usual copper weights had disappeared to be sold on the black market or melted down to make ammunition.’ Perhaps Poirier’s most remarkable achievement is to make her cast seem so interesting and their concerns so urgent that, despite all the horror and the squalor, this Parisian decade can be regarded as a dawn in which for some it was bliss to be alive, and to be young was even better.",Kevin Jackson - Plenty of Sex & Nowhere to Sit
10002,3938552,2018-03-01 00:00:00,"Computer Teacher In Ghana Has No Computers So He Draws Microsoft Word On The Blackboard : Goats and SodaOwura Kwadwo Hottish, a middle school teacher in Ghana, has found a way around the problem. He literally draws the computer screen on the blackboard.Owura Kwadwo Hottish illustrates a window of Microsoft Word using colored chalk on a blackboard. He uses it to teach computer skills to students at the Betenase M/A Junior High School in Kumasi, Ghana. Frimpong Innocent hide captiontoggle captionFrimpong InnocentOwura Kwadwo Hottish illustrates a window of Microsoft Word using colored chalk on a blackboard. He uses it to teach computer skills to students at the Betenase M/A Junior High School in Kumasi, Ghana.Frimpong InnocentCould you teach computer class without a computer?For Owura Kwadwo Hottish, 33, an information and communications technology teacher in Ghana, it's his only option. At the middle school where he works, there are no computers. So using colored chalk, he painstakingly draws a version of the computer screen onto the blackboard.In mid-February, he shared a Facebook post showing photos of himself teaching Microsoft Word using this method. His story went viral, making international headlines around the world.People praised his incredible attention to detail. ""How many days did you take to draw that?"" one commenter marveled. Indeed, his drawing of the word processing software included dozens of buttons and features, from the File tab to the horizontal scroll bar.And he was lauded for his commitment to the students. ""God bless you for the effort you are putting into grooming our young people,"" wrote another.For Hottish, who spends about 30 minutes making these drawings before every class, teaching this way is really no big deal. ""Every subject is taught on the blackboard here,"" he says.He has taught computer class for six years and currently works at Betenase M/A Junior High School in Kumasi, a city about 250 miles from Accra, the capital of Ghana. He studied computers at the Kumasi Technical University. He does have a computer at home, ""but the battery is too weak to send it to school,"" he says.Via WhatsApp, we chatted with Hottish about his newfound fame, where he learned how to draw and what he wishes most for his students. This interview has been edited for length and clarity.You've captured the world's attention for using a chalkboard to teach a computer program. What do you make of all the hype?I was really surprised. I wasn't expecting my Facebook post to go that far.Your story has been shared all over the world. Why do you think people are so fascinated by it?It's because of the chalkboard illustration of Microsoft Office. How I detailed it.Why didn't you just teach them on a computer?There is no computer and I had no choice but to draw for them.Where did you learn your drawing skills? You are quite good!I studied art and graphic design in secondary school.What else do you teach besides Microsoft Word?We teach them the basics, like turning on and off the computer, components of the personal computer and creating folders.And you do that all on the chalkboard!Yes.When your students actually see a real computer are they able to take what you've learned from the chalkboard and apply it to real life?Yes, but not with ease. They sometimes fumble behind the real computers. [Teaching with a real computer] would be easier for them.Do your students have computers at home?[We live in] a rural community and the students don't have it at all in their homes.People talk about the ""digital divide,"" which keeps poor people from entering the digital age. Does a chalkboard picture of Microsoft Word help kids get a way in, or is it a cruel reminder that they are lacking in equipment?They are lacking more than just equipment.Did your students laugh at you when you first tried teaching them computers on the chalkboard?No. That's the normal way and they're used to it. They were OK since they don't have an option, not having computers at the school.So you're not the first to teach computers on a blackboard!Yeah, that's normal in the rural community.Does anyone ever erase your drawings?Yes, after the students are done and the lesson is over. To make room for the next class.So you have to draw a new screen every time!Yes.I've read in news reports that you've received an outpouring of help from foreigners who want to donate computers to your school. Is that actually happening?No, they are showing interest but nothing has been brought to the school. We are praying that they are able to organize themselves and present us with computers.Do students ever correct your drawings?No, they only tell me if they can't see some portions very well.Have any of your students graduated and gone on to study or work in computers?I can't tell, because when they graduate they move to urban areas to make a living or continue their education.We're all neighbors on our tiny globe. The poor and the rich and everyone in between. We'll explore the downs and ups of life in this global village. And if you're curious about our name, Goats and Soda, here's the story.",Computer Teacher With No Computers Chalks Up Clever Classroom Plan
10003,3938710,2018-03-02 21:00:39,"Blockchain will work in trucking — but only if these three things happen0Jonathan Salama is chief technology officer at Transfix, a digital trucking marketplace based in NYC.Sometimes a buzzword gets so overhyped that it deserves some light-hearted mockery. That seems to be the case with “blockchain.” While it’s true that not every industry can benefit from a distributed-ledger technology, the trucking industry most certainly can. In fact, a new consortium called the Blockchain in Transport Alliance (BiTA) is working to apply blockchain to solve some of the most intransigent problems in trucking.Trucking is a massive industry that affects virtually every American. Trucks move roughly 70 percent of the nation’s freight by weight, according to the American Trucking Association. The Association also found that in 2015, gross freight revenues from trucking were $726.4 billion, representing 81.5 percent of the nation’s freight bill.Companies hailing from each piece of the trucking supply chain have joined BiTA, including: UPS, Salesforce, McCleod Software, DAT, Don Hummer Trucking and about 1,000 more applicants. [Full disclosure: Our company,Transfix, is also a member.]A private blockchain for truckingBlockchain is a shared, distributed ledger that facilitates the process of recording transactions and tracking assets in a business network. An asset can be tangible like a truck, or intangible like an insurance requirement. The blockchain that supports cryptocurrencies like bitcoin is a public network open to any investor with millions of users around the world. The blockchain we in the U.S. trucking industry foresee is a private one for shippers, carriers and brokers in the BiTA consortium.As a standards organization, BiTA aims to create a common framework to spur the development of blockchain applications for logistics management, asset tracking, transaction processing and more.Why do we need blockchain? Because trucking is an inefficient industry.Why do we need blockchain? Because trucking is an inefficient industry. Manufacturers have a hard time finding trucks to transport their goods. That’s not because there aren’t enough truckers who want the job. In fact, truckers drive more than 29 billion miles with partial or empty truckloads.According to the American Trucking Association, there are roughly 1.5 million trucking companies employing approximately 3.5 million truck drivers. But 90 percent of these companies have six trucks or fewer. This enormously fragmented industry struggles to match shippers (the demand) with carriers (the supply).Blockchain as the Holy GrailMatching shippers with carriers is just one of the problems blockchain could solve. I know because my co-founder at Transfix, Drew McElroy, is a dyed-in-the-wool trucking devotee. Drew was born into the trucking business. His parents, and later Drew, ran a trucking brokerage company with the express aim of matching shippers with carriers. It was brutally inefficient, taking up to three hours of calling and faxing orders to line up a single delivery.If implemented well, blockchain could be the Holy Grail that makes the entire trucking supply chain more efficient. Imagine a cadre of shippers, carriers and brokers collaborating on a secure, frictionless network.What three things need to happen?In my humble opinion as a technologist, I believe three things must happen to make blockchain viable in trucking.Everyone must trust the blockchain as the single source of truthBlockchain is a digital ledger using blocks (or bundles of transactions) that are linked and secured by cryptography. As a result, data entered into the blockchain cannot be modified or corrupted. What’s more, because the ledger is distributed, there is no single central authority that’s in charge of certifying the information. That’s the beauty of the system.But first, we must trust the data being entered into the blockchain. For example, some manufacturers require their carriers to have $250,000 of cargo insurance before they hand-off their goods to be transported. If a carrier enters “yes, we have the insurance” in the blockchain, their customer must trust that to be true. Similarly, carriers must trust the shippers that hire them through the blockchain to pay them for their services.Because the trucking blockchain will be private, all shippers, carriers and brokers will be vetted, and relationships will likely be built through contracts and agreements. I would also suggest a level of interconnectivity of trustable data sources. For example, to ensure that a carrier has satisfied insurance requirements, the blockchain should connect directly with insurance companies.Small carriers and shippers must participate en masseRemember, 90 percent of all trucking companies in the U.S. have six trucks or fewer. These are small businesses. It’s difficult for any small business, not just those in trucking, to have the means to purchase and learn new technology. In order to participate in the blockchain, both carriers and shippers must have access to the software, hardware and knowledge.This task will prove to be difficult. Just look at the electronic logging device (ELD) rule as a recent effort to get truckers to participate in a shared, technology-driven mission. This was a congressionally mandated rule intended to help create a safer work environment for drivers. The Secretary of Transportation required drivers of commercial motor vehicles involved in interstate commerce to log their miles into a device to make sure they do not drive too many consecutive miles (roughly no more than 11 hours under certain conditions). One month before the December 2017 deadline, only 37 percent of 1,600 fleets were ELD-compliant.Imagine a cadre of shippers, carriers and brokers collaborating on a secure, frictionless network.In addition, the costs required to maintain the trucking blockchain system could be substantial (think of the electricity alone that’s needed to power racks and racks of processors).If a critical mass of small carriers do not participate initially in a trucking blockchain, new carriers must join to replace them. That’s because a lack of supply will drive shipping costs up. Finding more trucking businesses is a tall order because there is already a 48,000 driver shortage.The entire industry must embrace data standardizationAll players in the trucking blockchain must agree on how to characterize their data — e.g. what details must every purchase order or invoice contain. I’m heartened that the BiTA is getting in front of this problem, because they are discussing these standardization questions at the very outset.Data standardization is not easy. For example, electronic data interchange (EDI) is a standard in the logistics industry. EDI has been around for more than 30 years, but there is no one overall EDI standard. Different companies use different versions of EDI, which results in very meticulous and time-consuming integration and development work so that companies can collaborate.Blockchain is still in its infancy and holds great promise. But trucking must overcome three challenges for blockchain to take off. Getting one of the nation’s most fragmented and conventional industries to trust a new online network and embrace data standardization will take a lot of time and iteration. If we collectively chart the future with this budding technology and anticipate unintended consequences, I believe blockchain could be more than a buzzword in trucking.",Blockchain will work in trucking — but only if these three things happen
10004,3938711,2018-03-02 20:03:25,"UiPath raising around $120M at $1B+ valuation for its ‘software robots’ for internal business tasks0The initial hype around bots — applications that run partly or entirely using natural language processing, machine learning, computer vision and other AI tech to help consumers ask and answer questions, buy things and get other stuff done — may have waned a bit, but a startup building the equivalent for the enterprise world, in a fast-growing field called robotic process automation, is seeing its star rise.TechCrunch has learned that UiPath, a startup that builds ‘software robots’ for enterprises to help automate legacy and back-office functions, has raised a Series B round of funding that sources tell us is around $120 million — a round that will catapult it to a valuation of over $1 billion.“Catapult” is the operative word here: the company last raised money in April 2017, a $30 million round that valued UiPath, founded in Bucharest, Romania but now headquartered in New York, at around only $109 million, according to data from PitchBook. In other words, this latest funding increases its valuation nearly tenfold.“I’ve never seen an enterprise company grow this fast,” one source close to the company said to us, when we asked how its valuation jumped so high, so quickly.From what we understand, this latest round — which the company will use to continue investing in its product and growth — will include existing investors along with some high profile new backers. A couple of sources have told us that Kleiner Perkins Caulfield Byers is in this round, along with other “household name” VCs (we are still working on figuring out who these might be). Its previous backers include Accel, Credo Ventures, Earlybird and Seedcamp.The company is announcing the funding as soon as next week. It has declined to talk to us for this article. KPCB has not responded to our request for comment, so the final amount and investor names will likely change.Another clue to the final amount and the fact that an announcement is around the corner: just this morning, UiPath filed documents in Delaware that authorized $100 million+ of a Series B & B-1, where the post-money valuation could be as high as $1.15 billion, according to Lagniappe Labs, the creator of the Prime Unicorn Index.RPA as a market is on something of a growth tear at the moment. Grand View Research forecasts that RPA services will be a $8.75 billion market annually by 2024, while Forrester projects revenues of $2.9 billion by 2021. As of the end of 2017, UiPath’s revenues were on a rate of 500 percent growth annually and from what we understand it’s growing even faster now.UiPath, and other companies working in the area of RPA, are filling a gap in the market for large enterprises: while the general trend for businesses is to upgrade their legacy systems with new technology in the longer term, in the short term, many of them are looking for ways of making their existing systems — and staff — more efficient.UiPath uses AI tools like computer vision, machine learning and natural language processing to automate some of the more mundane tasks around administrative systems and processing “paperwork” such as filling out insurance claims, processing invoices, and most recently, running customer contact center operations.The idea is that using UiPath’s bots creates more time for a company’s human workers to focus on using skills that the AI systems can’t (yet?) handle, for example applying advanced judgement skills to help assess the content of an invoice, or the needs of a customer contacting the business.“In the area of finance, for example, a lot of teams spend time creating reports but don’t require them using as much of their judgment skills, leaving the human employees with little time for the analysing part of their jobs,” Accel partner Luciana Lixandru told us at the time of UiPath’s previous funding round. “A software robot that understands what is happening on the screen using computer vision and performs the task as a human would do it becomes a useful tool.”UiPath has fit neatly so far into the fabric of enterprise IT systems and services.In November last year the company said that it counted over 300 large enterprises as customers — customers we’re written about before include Lufthansa, Generali, Telenor and Dong Energy — and from what we understand that number now is around double that, with customers not just in the financial sector, but also healthcare, telecom, public sector, and more.The startup also partners with the wide array of integrators and software providers that these enterprises are already using to help build and run their IT systems, including the likes of Oracle, Accenture, Capgemini, Symphony Ventures and many others.What’s also worth noting is the pedigree of this company. Although now headquartered in New York, UiPath was founded in Romania — a country well known for engineers but not for hatching many huge tech startups (although that is quickly changing). In the wider conversation we’ve been having about how the tech world is still too weighted in favor of Silicon Valley, it’s nice to see this example of how it’s gradually decentralising, and strong startups founded far from the Bay Area are getting the recognition they deserve.Featured Image: Guido Rosa/Ikon Images/Getty Images0CrunchbaseOverviewUiPath’s innovations track record has made it the leading technology provider for the Robotic Process Automation industry. We believe work should be creative and inspiring. Our mission is to eradicate tedious, repetitive tasks and let software robots do the grunt work. We enable businesses and organizations like yours to develop an agile digital workforce by providing a state-of-the-art platform …",UiPath raising around $120M at $1B+ valuation for its ‘software robots’ for internal business tasks
10005,3938712,2018-03-02 17:15:10,"For example, Alexa is giving me replies like “I’m not sure what went wrong”, “sorry, something went wrong”, or a loud chime followed by “sorry, your echo dot lost its connection” and the red ring of sadness. The problem appears to be related to Alexa’s voice recognition servers, as it’s occurring across both native devices like an Echo and 3rd party devices running Alexa like the Sonos One.Is Amazon Alexa down? I've never seen it with a red ring and tell me it can't understand right now.Some Alexa services still work if you access them through the Alexa app. For an ultimate example of first world problems, I couldn’t turn on my lights this morning with Alexa, but I was eventually able to manually toggle them on and off by using the Alexa iPhone app (because there was no was I was actually walking over to them and bending down to the floor switch).",Alexa has literally lost her voice as users report outages and unresponsiveness
10006,3938716,2018-03-01 22:05:00,"The best mouse of 2018: 10 top computer mice comparedPointing to the best mice of 2018SharesAlthough some tablet and laptop manufacturers might argue that the best mouse is a stylus, we beg to differ. There’s nothing quite like the experience of using a mouse to navigate your computer. Whether you’re on a PC or a Mac, a trackpad or touchscreen simply won’t suffice. Because mice aren’t limited to the confines of the space that bridges the gap between your keyboard and the edge of your laptop, they can be as precise or as yielding as you would prefer.However, even if you already have a mouse in-hand, we assure you that our carefully selected recommendations will blow everything else out of the water. Before you make up your mind on which mouse to buy, you should sit down and think about what you actually need out of your mouse. Does your current mouse constantly bug out? Are you thinking about upgrading to a wireless mouse? Or, are you looking for one of the best gaming mice?Don’t worry, whatever you need out of your new mouse, the best mice will give you accurate and comfortable command of your cursor. Not only will the mice on this list be exponentially more reliable than the mice you might’ve used in the past, but each mouse has unique qualities that can only be found in the modern mouse market. Plus, all of the mice on this list have been tested and ranked accordingly by the TechRadar editorial staff.Smaller than Logitech’s flagship, the MX Master, the Logitech MX Anywhere 2 is the ideal travel mouse. It can connect with up to three distinct devices with 2.4GHZ wireless technology or with Bluetooth, making it obvious why this mouse is renowned for having quick setup and surprisingly long battery life. Capable of extremely quick charging, in just 4 minutes the MX Anywhere 2 charges enough for an entire day.Logitech's flagship is a mighty mouse indeed. Hand-sculpted for comfort, the MX Master connects via Bluetooth or USB dongle and it can pair to up to three devices. The rechargeable battery lasts for up to 40 days and goes from flat to a day of power in four minutes. Plus, you can even use it while it's charging.Judging by the mouthful of a name, Anker’s mouse is defined by its vertical orientation. Though it looks and feels somewhat sideways at first, the Anker Vertical Ergonomic Optical Mouse is only off-putting until you start to wonder how you ever lived without it. It’s built a little more cheaply than other ergonomic mice, but it’s ultimately an inexpensive means of preventing RSI.When it isn’t busy making luxury smartphones you can unlock with your face, Apple is hard at work constructing one of the world’s weirdest computer mice. If you were doing a double-take looking at the specs, you saw right: the Magic Mouse 2 has zero buttons and endless fashion appeal. It’s also controlled by gestures, giving it all the allure of a trackpad as well as a mouse.Like the MX Master, the Triathlon M270 can pair with up to three devices using Bluetooth. What’s more, it uses the same free-spinning scroll wheel featured on the MX Master, letting you zip through documents or webpages. Logitech promises up to 24 months of use before the on one AA battery. The only drawback? Bluetooth makes for lower latency than a wired mouse.This product is only available in the US and UK at the time of this writing. Australian readers: check out a fine alternative in the Logitech MX Anywhere 2The third trackball mouse Logitech has put out since 2008, the MX Ergo Wireless is an anomaly in an industry that is otherwise crowded with optical laser mice. As such, it’s certain to charm those who haven’t quite moved on to the gesture-based desktop trackpads of the present. With the option to lay it flat or use it at a 20-degree angle, this mouse is uniquely satisfying.Though it isn’t chock-full of buttons like most gaming mice in its price range, the Asus ROG Gladius II still shines. With only six buttons, this mouse is versatile, discreet and customizable, allowing you to change out its switches and personalize its stylish RGB lighting as you see fit. It’s lacking a few staples, such as swappable weights, but overall, it’s a solid get.The seventh best mouse on our list is seen but not heard. Because it’s unusually compact as well as silent, it’s the kind of mouse that’s ideally used atop an airplane tray next to a snoozing neighbor. Just slip it in your laptop bag when the flight touches down and, weighing less than a quarter of a pound, you won’t have to worry about any additional strain on your back.9. Microsoft Bluetooth Mobile Mouse 3600The new Surface Precision Mouse might be tempting, but one look at that price tag and our most frugal readers will want to turn their heads in the opposite direction, wherein the Bluetooth Mobile Mouse 3600 is hiding. This affordable vermin is one of the cheapest and reliable wireless mice you can buy and, to make matters better, it’s from a name you can trust.Razer products aren’t for everyone. Specifically, they appeal to an audience that enjoys the svelte industrial design of an Apple gadget, but with a gamer-centric slant. That’s especially the case with the DeathAdder Chroma, a mouse that comes with 16.8 million LED-backlit color options out of the box, along with a hyper-accurate 10,000 DPI sensor.",The best mouse of 2018: 10 top computer mice compared
10007,3938717,2018-03-01 16:17:38,"The best phones at MWC 2018From the S9 to the V9 and everything in betweenSharesAnother year, another MWC, and this one didn’t disappoint as loads of new phones were announced, from flagships like the Samsung Galaxy S9 and Sony Xperia XZ2, to super-affordable options like the Nokia 1.In short, there’s something for almost everyone so picking our top phones from the show was hard, but somehow we’ve managed to create this list of the best phones at MWC 2018.Presented in no particular order, here’s a selection of handsets that could easily be candidates for your next pocket buddy (along with one concept phone that we really wish we could buy).Samsung Galaxy S9Image 1 of 2Image 2 of 2The Samsung Galaxy S9 is likely to be the most popular handset from MWC 2018, the latest entry in a range of phones that arguably only has the iPhone for competition.And while this year’s model seems like a fairly small upgrade over the Samsung Galaxy S8 it’s still worthy of attention, thanks to a dual-aperture camera complete with the world’s first f/1.5 aperture on a phone.That should lead to impressive low light shots, and while Samsung has clearly devoted much of its attention to camera upgrades there are lots of other things to like here, such as the S9’s 5.8-inch QHD+ display, stereo speakers, stylish design and top-tier chipset. All of which comes at a cost of $719.99 / £739 / AU$1,199.Samsung Galaxy S9 PlusIt’s like the Samsung Galaxy S9 but with more of almost everything – more screen at 6.2 inches, more cameras as there’s a dual-lens one on the back, more RAM at 6GB, a bigger battery at 3,500mAh and more storage, with 128GB built-in.Everything else is much the same as the regular-sized S9 and that’s no bad thing, as it means you get a crystal-clear 1440 x 2960 Super AMOLED display, a top-end chipset, a clever variable aperture camera, stereo speakers and a sleek and curvy metal and glass design.At $839.99 / £869 / AU$1,349 it’s not cheap, but we might not see many - if any - better phones this year.Sony Xperia XZ2Image 1 of 2Image 2 of 2It’s been a long time coming, but with the Xperia XZ2 Sony has finally redesigned its smartphone range, delivering a handset with a curvier, glass-clad look and a reduction in bezel (though still more than many recent flagships).Aside from the new look, the Sony Xperia XZ2 also has a 5.7-inch 1080 x 2160 display with a trendy 18:9 aspect ratio, a 19MP camera that has the honor of being the first smartphone snapper capable of shooting 4K HDR video, a high-end Snapdragon 845 chipset and a 3,180mAh battery that supports wireless charging.There’s also a new Dynamic Vibration System, which can add vibrations to games, movies and music and is paired with a decent set of stereo speakers.It's going to cost a pretty hefty £699 (around $950 / AU$1350) and in Europe at least you should be able to buy it from early April (the US release date is still to be confirmed).Sony Xperia XZ2 CompactImage 1 of 2Image 2 of 2Phone screens are generally getting bigger, but if you prefer something smaller Sony has you covered there, with the Sony Xperia XZ2 Compact.This phone is a lot like the standard Sony Xperia XZ2, but with a smaller 5.0-inch (though just as high resolution) 1080 x 2160 screen. It also has the same powerful Snapdragon 845 chipset, the same 19MP camera and the same stereo speakers.Really the only differences other than the screen size are the build – with the XZ2 Compact trading glass for plastic on the back, the battery, with the XZ2 Compact sporting a smaller 2,870mAh unit, and the Dynamic Vibration System, which the XZ2 Compact lacks (as well as wireless charging).But in most ways that matter this looks to be a real compact flagship, and it'll cost £549 (around $750 / AU$970).Asus Zenfone 5 (2018) / Asus Zenfone 5ZImage 1 of 2Image 2 of 2The Asus Zenfone 5 (2018) and Zenfone 5Z were among the more surprising phones at MWC 2018, as Asus isn’t typically known for high-end phones; yet these phones combine a great design with strong specs and potentially appealing prices.You can’t escape the fact that the Zenfone 5 looks a lot like an iPhone X, thanks to its bezel-free screen with a notch at the top, but that coupled with a shimmery glass back ensures it’s a good-looking device.Specs include a 6.2-inch Full HD+ display, dual-lens cameras, stereo speakers, a 3,300mAh battery, a mid-range Snapdragon 636 chipset and 4GB of RAM.The Zenfone 5Z meanwhile has the same specs but swaps the chipset for a high-end Snapdragon 845 one and increases the RAM to up to 8GB, as well as packing in more storage, with a maximum of 256GB.Prices for the Zenfone 5 haven’t been confirmed, but the Zenfone 5Z is set to start at £500 (around $700/AU$885), which is a low price for a flagship and the standard Zenfone 5 will presumably cost less.Nokia 8 SiroccoImage 1 of 2Image 2 of 2Nokia’s new flagship isn’t the Nokia 9 - instead we got the Nokia 8 Sirocco - but despite having the same number in its name as the Nokia 8 it looks to be a significant upgrade.It has an almost all-glass body with dust and water resistance, and a 5.5-inch 1440 x 2560 curved OLED screen.There’s also a pair of Zeiss-tuned cameras on the back – a 12MP wide-angle lens and a 13MP telephoto one.Other specs include a 3,260mAh battery and 6GB of RAM, with the only slight disappointments being 2017’s Snapdragon 835 chipset (rather than the new 845) and the lack of a headphone jack.If those aren’t deal-breakers for you the Nokia 8 Sirocco will set you back AU$1,199 (around £660, $900) when it launches in April.Vivo Apex ConceptImage 1 of 2Image 2 of 2The one phone on this list you won’t be able to buy is the Apex Concept, because, as the name suggests, this is just a concept handset for now – though one that we were able to try out for real at MWC.The highlight of the phone is its fingerprint scanner, which is built into the screen, and not just a small section of the screen. Rather, it takes up a large portion of the bottom section, so you can hit it with ease.The scanner also lets you secure the phone – or elements of it – with two fingerprints instead of one, and the screen is almost completely bezel-free, with Vivo opting for a pop-up front-facing camera, rather than a notch, and embedding the earpiece and proximity sensor under the display.LG V30S ThinQImage 1 of 2Image 2 of 2Being so similar to the standard LG V30, the LG V30S ThinQ perhaps isn’t one of the most exciting announcements of MWC 2018, but it certainly looks like a great phone, building as it does on its 4.5-star predecessor.Just like the original V30 it has a 6.0-inch 1440 x 2880 OLED screen, a Snapdragon 835 chipset, a 3,300mAh battery and a dual-lens camera, pairing a 16MP lens with a 13MP one.What’s changed is that the V30S ThinQ has more RAM at 6GB, more storage at up to 256GB, comes in a new Moroccan Blue shade and has a couple of new camera modes, including ‘AI CAM’, which allows the camera to automatically switch between eight different scenes as needed.It’s not going to convince buyers of 2017’s model to upgrade, but if you’re using something older the LG V30S ThinQ could be worth considering.Nokia 7 PlusImage 1 of 2Image 2 of 2We got a whole host of new Nokia phones at MWC 2018, with the Nokia 7 Plus being an upper mid-range model that stands out through a 6.0-inch 1080 x 2160 display with fairly slim bezels.Other highlights of the Nokia 7 Plus include the same dual-camera setup as the Nokia 8 Sirocco, meaning you get a 12MP wide-angle lens and a 13MP telephoto one. And the company claims you’ll get two days of life from the 7 Plus’s 3,800mAh battery.The rest of the specs are a bit more mid-range, with the Nokia 7 Plus being powered by a Snapdragon 660 chipset and 4GB of RAM, but given that it’s priced at AU$749 (around £350, $490) that spec mix seems pretty reasonable.The Zenfone Max Plus M1 combines a 5.7-inch 18:9 display with a massive 4,130mAh battery and a small chassis. The specs are otherwise mid-range, but it comes in at just $229 (about £164, AU$291).Image 2 of 4The Nokia 1As for the Nokia 1, that’s almost as cheap as a smartphone gets at $85 / AU$149 (around £60). The specs are predictably basic for that money, but it has a compact 4.5-inch display, a 5MP camera, a removable 2,150mAh battery, and – based on our hands-on impressions – seemingly functional performance, thanks to its use of Android One Go.Image 3 of 4The Alcatel 5Then there’s the Alcatel 5, which comes in at a fairly affordable €229.99 (about $280, £200, AU$360), for which you get an almost bezel-free 5.7-inch 720 x 1440 display, a metal shell and a dual-lens camera. There’s even facial recognition.Image 4 of 4The ZTE Blade V9Finally, there’s the ZTE Blade V9, which packs a similar price of €269 (around $330, £235, AU$420). This has an ‘Aurora Glass’ back that shimmers in the light, plus a 5.7-inch 1080 x 2160 display with small bezels, up to 4GB of RAM and a dual-lens camera, though only a fairly low-end Snapdragon 450 chipset.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",The best phones at MWC 2018
10008,3938718,2018-03-02 08:00:11,"Confronting real fears in virtual realitySharesI'm not good with heights. Sweaty hands, tunnel vision … when I get more than 10 feet off the ground, I start to lose my head. As someone who loves hiking, mountains and sprawling city panoramas, that’s a problem. So when Lithuanian software company TeleSoftas offered to cure my acrophobia at MWC 2018 using an HTC Vive, I was intrigued.When they told me that my 'inner child' would be taking me to the top of One World Trade Center in New York, I was confused.Premiering in Barcelona, TeleSoftas' Inner Child VR demo has been designed as a tool for psychotherapists. Part of the problem with treating any kind of mental issue is that psychologists have to ask their patients to imagine being in the situation they fear, or put them in that scenario, where they could be in danger. VR is therefore an appealing alternative; no imagining needed, and no one falls off a building.""It's actually not for those with a fear of heights, it’s for those with a more extreme phobia of heights,"" says Audrone Miskinyte, associate professor in clinical psychology at the University of Bergen (UiB) and Vilniaus Universitetas, who will soon be using Inner Child VR in her daily work as a psychotherapist. The first is basically a fear of falling, the other is an illogical aversion to heights that has a physiological trigger. ""It's exposure therapy where we control the environment, where the patient can be exposed to what they're afraid of.""Inner Child VR takes place within a a steel rectangle surrounded by a handrail. After donning a headset and a pair of biometric gloves strapped with trackers, the patient appears to be standing on a platform. New York can be seen all around, and crucially, below through a lattice floor. The goal is to get to the end of the walkway … with the help of a child avatar that is programmed to loosely resemble the patient.""What's new here is that we're using a child avatar,"" says Miskinyte. It's the first-ever project that applies the principles of 'transactional analysis' theory of the 'free inner child ego state'. The child resembles how the patient was before they got older, and developed irrational phobias. ""We have a child, adult and parent position, and we identify with the child,"" explains Miskinyte. ""It's natural, spontaneous, curious, and fun-seeking, and this kind of child can help us overcome our biggest fears – the child can set an example, can encourage, and can distract, and help us overcome our fear."" When running the scenario, a psychotherapist can control everything the child does and says to get the patient to move to the end of the walkway.So how did I do? Really well. ""I am you when you were little,"" says my inner child avatar at the beginning of the experience. ""We're going somewhere fun,"" he says as it becomes obvious that we’re in an elevator. The doors open and … woah! A fabulous-looking lower Manhattan is there all around, and hundreds of feet below. It's a genuinely dizzying feeling, and the first step was hard to take. ""Look at that boat!"" says the child, distracting me when I stop mid-way to my goal. However, I did eventually follow him right to the end. Halfway through the experience a helicopter arrived, and through a megaphone asked us to wave for a photo. ""I have to go now … think of me when you're scared of heights!"" says the kid as he walks off the platform.Bizarrely, the child started singing 'The Wheels On The Bus' near the end of the experience. ""I made him sing to you, and we sent the helicopter in,"" says Vytautas Kemesis, chief technology officer (CTO) at Telesoftas, from behind a screen showing a third-person view. ""We can control what the kid does and says when you need distractions or instructions.""Within seconds there's a readout of my biometrics – including a slightly elevated heart rate – and a heat-map of where I was looking. ""Most people look down,"" says Kemesis. I mainly looked at the view … though always with sweaty hands on a handrail.Inner Child VR was built using 3D game engine Unity, and TeleSoftas has plans for other VR experiences to cure other anxieties and phobias, such as spiders, flying, and public speaking.MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",How my inner child avatar and an HTC Vive helped me face my fear of heights
10009,3938790,2018-03-03 02:14:26,"About TNWTNW SitesFox triggers its own audience with misleading viral videoA recent Fox News video showed just how easy it is to spin the wheels on whatever apparatus it is that turns the outrage machine. Tucker Carlson last week aired a segment deriding “the liberal effort” to ban the word “man” on college campuses.In just under a week, it’s been shared over 486,000 times and viewed by more than 42 million people. The outrage was real. The subject matter? Not so much.Carlson didn’t lie, per se, but things got interesting when the network stripped the bit of context and slapped it on social media with the headline:Liberals have found their new target. They want to remove all ‘MAN’ words.For several minutes, Carlson and his guest Cathy Arey, founder of Catalina Magazine, held what appeared to be a mock debate featuring the smug and patronizing TV host, a man who operated in hypotheticals and condescension, like this gem: “What if you live in Manchester, Vermont?” And on the other, there was Areu, who was less offended liberal, and more SNL parody of one.The segment centered around a Purdue writing guide’s suggestion that students use inclusive and neutral language in their work. Nowhere does it suggest banning words, or avoiding usage of “man,” in anywhere but its most dated usages.“Mailman,” for example, should be “mail carrier,” as carrier’s are both male and female now. The guide is less a burning source of liberal outrage and more of a public manifesto on how not to be an asshole.Take this example:Incorrect: Although she was a blonde, Mary was still intelligent.Correct: Mary was intelligent.Painting this as a liberal issue, or an institutional one, is overlooking similar rules from guides that existed decades before Carlson’s segment. AP Stylebook, the Chicago Manual of Style, and the MLA Style Guide all have similar passages about how to use gender in writing. There isn’t a professional writer on the planet who hasn’t had to make decisions on how best to use gender in prose.The Associated Press Stylebook’s section gender-specific pronounsCarlson and his guest also ignored further context of the guide, which sought to neutralize gender in writing for both sexes, not just men. “Stewardess” and “Steward” for example, should be “flight attendant,” noting changes in the industry since the 60s when females were responsible for most in-flight care. Others seek to avoid additional wording, such as referring to a “male nurse” or a “female doctor” as “nurse” or “doctor,” respectively.Of those that specifically mentioned the word man it offered other man words as substitutions. “Mankind,” for example, could be substituted to “humanity.” “Man’s achievements” might sound better as “human achievements.”None of the real examples mesh well with Carlson’s spin on the story, but it didn’t matter. In mere hours conspiracy theorists like Alex Jones pounced on the opportunity to retell it, as did numerous fly-by-night conservative Facebook Pages — the same type used to create confusion during the 2016 election.One part, however, I couldn’t help but chuckle at. Toward the end, Carlson says:I guess the question for me is who gets to decide what changes and what doesn’t? So, for example, I think I’ve now decided the most offensive word in the language is ‘college professor,’ okay? Because to me that connotes dumbness, and misuse of power, and tenure, and mediocrity.",Fox triggers its own audience with latest misleading viral video
10010,3938791,2018-03-02 23:01:19,"About TNWTNW SitesCryptocurrency News March 2 – better late than neverSo I had a bunch of stuff to do today so this is late. This isn’t my fault. You’re at fault for demanding I take time out of my busy way of tweeting curse words at an account that I believe to be Satoshi Nakamoto.Grand theft crypto600 mining computers were stolen in Iceland according to some guys with some really long names! The 600 computers over the course of 40 years could mine at least half a Bitcoin. It’s a difficulty joke folks! Anyway, genuinely wonder how this affects cloud mining company Hashflare, which is based in Iceland. Not sure.11kBitcoin is over $11k, which is great news for me, the person that sold some around 4 hours before it hit $11k. Either way, it always produces some really good articles, like this one, where someone suggests that there’s such a thing as a “Bitcoin investment strategy,” which I call “bullshit.” There’s no such thing. I guess you can buy low and sell high? That’s sort of a strategy.Coinbase dataThe fees to do with each currency on Coinbase, courtesy of Superfly Insights.What’s interesting is how little Bitcoin Cash makes up of Coinbase’s revenue – and (though it’s kinda hard to see) how much Litecoin does – especially considering the gap between Litecoin and Ethereum on CoinMarketCap. Also take note of December 19 2017 – a date almost impossible to see on there, which is relevant – the date that Bitcoin Cash joined. They didn’t even see a spike! But what does confuse me is there’s any data before that. I don’t know. This is all making me think too much.Selling my dollars for $0.50 a pieceJohn McAfee Update: cars are for closersLuke – our Cryptocurrency long term analyst (holdings > 6 months) has been with us 8 months. He always takes his own advice. Here is a tiny partial result of his recommendations 6 months ago stashed in the garage of one of our staff houses. pic.twitter.com/kWLtjCGA1qThought cornerWhat happened to Ethereum? Remember when ETH was consistently matching about 10% of BTC? Seriously not sure. It’s not been fun! Makes me wonder why, really. It has more use than BTC, and definitely more than Bitcoin Cash. Whatever, I just want my family back.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",Cryptocurrency News March 2 - better late than never
10011,3938792,2018-03-02 22:30:20,"About TNWTNW SitesJack calls in reinforcements to measure Twitter’s toxicityTwitter’s CEO has officially waved the white flag and called for outside help fixing the site’s toxicity. Specifically, he’s asked for help defining a metric of “conversational health” on Twitter.While it’s good to see Twitter attempt to find a system for controlling the behavior of its users, human beings are too complicated to be algorithmically guided into playing nice.It’s no secret that Twitter has, in recent months, been forced to face its epidemic of negativity, from Russian bots to the spread of fake news. Obviously, now’s the time for contrition and public vows to fix its business. CEO Jack Dorsey delivered this week with an unexpected tweetstorm laying out his feelings on the situation and his call to action:We’re committing Twitter to help increase the collective health, openness, and civility of public conversation, and to hold ourselves publicly accountable towards progress.To that end, he’s asked for help coming up with “health metrics” which will gauge the quality of the site’s conversation. He cites as an example (but not a guideline) Cortico’s metrics for “shared attention, shared reality, variety of opinion, and receptivity.” Basically, that measures whether enough people with different opinions are citing the same facts, talking to each other, and listening.I understand why Jack’s looking for measurable data to work with. It’s got to be tricky trying to find a way to get several million people to play nice with each other without censoring any person or group in particular. While it’s productive for Twitter to try and hold itself accountable, it’s asking for statistical, data-driven help with a problem that’s primarily philosophical in nature.What we know is we must commit to a rigorous and independently vetted set of metrics to measure the health of public conversation on Twitter. And we must commit to sharing our results publicly to benefit all who serve the public conversation.Suppose Twitter imposes a social media conversational health metric on a particular account. The metric says the account isn’t healthy because it retweets fake news bot tweets, or something similar. What do you do? Ban that account? Would metrics even be limited to certain accounts? Key words? If a site-wide metric said health was suffering, would Twitter go on a purge to find the offending accounts and punish them? What if someone deliberately set out to game the system — how could Twitter control for that?Human beings sometimes don’t get along. That sounds flip to say when the difficulties of social media cause the arguments to be writ large across the internet. But even if Twitter were to find a metric that told it users were all talking about the same thing and using the right keywords, there’d still be arguments, instigators, and trolls.I think, if Twitter actually buckled down and set to work removing bots and handling reports of abuse and TOS violations in a more timely manner — either through better automation or a more robust workforce — it might start to see the site get a little cleaner without having to compile data sets and metrics that could possibly be manipulated.It’s a messy process, punishing people who might turn around and complain that they’re being treated poorly. But that’s a much more understandable way of diffusing an argument between humans than relying on data that might not translate well into real-world actions.TNW 2018 is almost here, and we’ll be discussing social media and the balance of power in the age of tech. For more info on how you can join the discussion, visit our conference page here.",An unhealthy Twitter calls for the wrong doctors
10012,3938793,2018-03-02 21:56:31,"Everyone immediately then wants to talk about all the movie-inspired death scenarios, and I can confidently predict to you that they are one to two decades away. So let’s worry about them, but let’s worry about them in a while.That’s not exactly comforting.Rapid advances in artificial intelligence and robotics have amplified the discussion in recent years. With an entire generation raised on science fiction movies depicting robot uprisings, it’s a near-certainty that these are exactly the sort of scenarios we imagine when thinking about the future of robots.And it’s certainly plausible, but not likely.Schmidt goes on to say:The other point that I want to remind everyone, these technologies have serious errors in them, and they should not be used with life-critical decisions. So I would not want to be in an airplane where the computer was making all the general intelligence decisions about flying it. The technology is just not reliable enough ― there too many errors in its use. It is advisory, it makes you smarter and so forth, but I wouldn’t put it in charge of command and control.That last sentence is key.Researchers understand, even if most of us don’t, that AI isn’t as suited to replace humans as it is to augment them. The human brain is complex. And while the average 40-year-old can’t memorize Wikipedia or beat the best poker players, the typical robot can’t handle the simple improvisation that humans excel at.In fact, most of what AI and robots are good at is menial task work, simple and repeatable objectives that are both easy to define and measure. Robots aren’t all that good at improvising; they need a defined set of rules and those rules will increasingly need to include failsafe measures to shut the machine down in periods of failure.And while these technologies will continue to improve, sentience isn’t anywhere on the horizon. For a robot to be dangerous, it has to be programmed to be dangerous. So it’s not robots we should fear, but the humans responsible for writing their code.If you’re looking for a more plausible scenario, it’s this: readers hurling themselves into the nearest body of water after another joke about the latest advancement at Boston Dynamics as the one that will ultimately be responsible for our deaths.",Ex-Google CEO is relatively certain robots aren't going to kill us for another decade or two
10013,3938794,2018-03-02 21:32:43,"TNW SitesResearchers just taught robots to predict your every moveIn a few years time the Droids from Star Wars are going to seem like relics. Today’s robots might be better suited for sewing clothes and building cars, but tomorrow’s could be as indispensable and ubiquitous as our smartphones are.A group of researchers in Europe recently published a white paper unveiling their experiments in teaching robots to anticipate human movements. The team’s work, to create “robots that can predict human actions and intent, and understand human non-verbal cues,” could pave the way for innumerable advances in the field.The researchers focused on combining previous research teaching AI to understand human gaze and pose, both of which are crucial for robots to understand in order for them to work with humans.According to the team:Situations where fast cooperation is essential, for example cooperative assembly, require the understanding of subtle non-verbal cues about the human intention and future action. In these scenarios it is not enough to merely recognize the current action. Instead, it is fundamental to predict actions and anticipate the intent in order to guarantee seamless cooperation.Using black box AI in the form of a recurrent neural network (RNN), the machines learn to determine the “intent” of a person based on where they’re looking (gaze) and the pose of their body. Whenever it gets things right, it transfers its newfound “knowledge” through the various layers in its RNN where the data is then normalized.This creepy robot, used in the research, is learning to predict what people will do next.These machines learn to predict what humans will do next by guessing. If given a limited amount of tasks, these machines could become incredibly accurate in a relatively short amount of time.According to the paper:We assume a leader-follower paradigm where the robot chooses his action as a function of the possible next human actions and the humans actions are not influenced by the robots action choice.It would be incredible if we had robots that could seamlessly zip in and out of our lives as we work and play. With finely tuned AI these machines could move among us, without causing the slightest disturbance, able to deal with our mercurial human nature by predicting what we’ll do next instead of reacting.Of course, if you believe Elon Musk, we’re just a few years away from machines that can move faster than the human-eye.This is nothing. In a few years, that bot will move so fast you’ll need a strobe light to see it. Sweet dreams… https://t.co/0MYNixQXMw",Robots can predict what you'll do next now. That's definitely not terrifying at all.
10014,3939281,2018-03-03 12:10:34,"Four days with the Samsung Galaxy S9: great biometrics and enjoyable AR EmojiMore early impressions of the new GalaxySharesIt’s now a few days since we unboxed what could be one of the biggest phones of 2018, and we’re now getting to grips with the new handset and the features it offers.If you’ve not been keeping up so far, let’s have a quick recap of the Samsung Galaxy S9. Design-wise, things are almost identical to the Galaxy S8, but it’s under the hood that things are a little different.There’s far more power than last year, AR Emojis created from a single snap of your face to be used on social media, and an all-powerful camera that’s supposed to be excellent at low-light and slow motion video.We’ve been giving these features a good test each days, and we’re slowly forming a few impressions ahead of our in-depth review, which is coming up soon.AR Emoji - not uselessWe’ve been playing the most with AR Emoji, because it’s the most novel feature on the phone compared to previous models. We set up ours four days ago, and from there we’re able to make videos through the avatar by mapping our face from the front camera, or use the generated GIFs that the phone instantly creates.We’ve found that we’re using the GIFs regularly, and that if you’re using the default Samsung keyboard you’ve instantly got access to said little moving pictures of you (not in the apps themselves, as previously thought).However, we’ve not been bothered to use the video recording feature of the avatar, simply because it’s not as accurate as we’d like, leading to a lot of flickery facial features, and the angle not being quite right.If it was more accurate, then it’d probably be more of a fun feature, but we can see it falling into the realm of ‘oh, I forgot that was on the phone…’.Camera hasn't wowed... yetThe camera on the Samsung Galaxy S9 has just been awarded the top DxOMark on the market, besting the Google Pixel 2, so we’re hoping to get some stunning photos from it - especially in low light.We’ve not sat down and done any of our standard in-depth testing so far with the camera so far - it’s been largely candid snaps - but there’s not been anything mind-blowing so far that’s come from the camera.The issue is not with the quality of the photos - they’re excellent - but as smartphone photography has improved massively in the last few years, we’re getting used to smashing snaps from our handsets.Image 1 of 4The automatic 'depth of field capture' intelligence is impressive.Image 2 of 4This is in extreme low light, and yet the level of noise is so, so lowImage 3 of 4In a brighter room, we were hoping for a more dynamic picture of the fireImage 4 of 4This scene was actually brighter than we could see with our eyes.However, we did start trying the phone on Pro mode, where you can manually flip between f/1.5 and f/2.4 apertures… and if you look at the lens, you’ll even see the sensor changing in size as the mechanical shutter comes down and reduces the size of the aperture.There’s a marked difference between the two modes, and it’s something we’re looking forward to testing in real depth soon.We’ve not tested the battery so far, as we always wait for it to settle a little before doing any in-depth benchmarks, but it’s still nice to see the Galaxy S9 is checking for any apps that are munching too much power and asking if we want to shut them down.SloMo camera isn't obviousThe SloMo camera is proving trickier to use than we anticipated. The thing about introducing new features like this is that they’re not something that we’re all desperate to use - rather, we found we needed to remember to use it more.As a refresher, the default mode is that the camera will automatically start the slow motion mode when it notices something entering a box in the middle of the viewfinder - it can be hard to position this properly if you’re trying to shoot something like an animal, which is often a moving target.In these cases, it would be better to do things manually, or just be able to shoot the whole thing in slow motion and decide after what you wanted to make slower, but that’s not easy.We still like the idea of creating the slow-motion videos, but it’s not something that we’re totally enamored with right now.Improved biometricsWe have to say we’re incredibly impressed with the upgrade Samsung has brought with the biometric unlocking of the Galaxy S9 - the idea to fuse all the options together is a really neat one.We’ve disconnected our Smart Lock accessories (previously, our Android phones are unlocked whenever in range of our smartwatch) and yet still not been annoyed by getting into the phone… a long way from how we felt when reviewing the Galaxy S8 last year.The face unlock / iris recognition combo isn’t the fastest around, but it’s fast enough. We’ve often found the phone will unlock quicker than expected, jumping us in before we’ve read the notifications on the lock screen, and if there are times when we can’t hold the phone at the right angle to unlock, or it’s late at night, the fingerprint scanner is really accessible.Speakers are just fineWe’ve been trying out the dual speaker on the Samsung Galaxy S9 a fair bit, seeing how it fares compared to a standalone Bluetooth speaker that we’d usually be using.The volume is pretty good, although it’s not as loud as we’d like it to be… sometimes we need to hold it a little closer to us to hear a podcast when cooking, for instance. The sound quality is a touch scratchy at the louder volumes too, but it’s not uncomfortable.We’re looking forward to trying the Dolby Atmos setting on the phone too - it could be a really nice addition if it works well.",Four days with the Samsung Galaxy S9: great biometrics and enjoyable AR Emoji
10015,3941438,2018-03-01 17:09:37,"About TNWTNW SitesHow to optimize your Medium as a freelance writerWhether you’ve only been a writer for a few months or several years, you’ve probably heard of Medium. You might even already have an account on the site, but are you using it to its fullest potential?For those not familiar with Medium, it’s a free publishing platform that stands out from some other options because instead of focusing on the connections or clout you have as a writer, it emphasizes what you have to say.Because of this, many freelancers create accounts and start writing on Medium. However, they don’t often see the site as a career-boosting opportunity, and perhaps that’s because they’re not using all of Medium to their advantage.What makes it appealing to write on the site is that it has a gigantic community, which gives you a broad reach. I have found that many industries use this platform as a tool to engage their audiences and boost their brand exposure by providing informative blog entries.Also, the formatting of each post is easy to upload and read while looking professional. For instance, as a blogger, formatting codes — mostly in HTML — is a skill that I have always had to know. Unfortunately, coding your own blogs can take quite some time if you don’t excel at using it.However, with Medium’s professional setup and formatting, it’s easy to quickly add your text, edit, and publish it to your social media accounts within a matter of minutes. This is just one of Medium’s many benefits for freelancers.Below, you’ll find several more ways you can use this go-to writer site to your benefit. In addition to its easy user experience, you might find that it can promote your brand and solidify your status as a writer.Give praise and insights with the notes featureMedium offers a feature that allows you to leave a private note associated with a section of an article by another author. To use it, highlight a portion of text and then look for the lock icon and click it. If you don’t see the lock icon, the author has disabled private notes in the Settings section.A private note is a fantastic way for you to connect with other writers and give feedback that could boost their efforts. Here’s one example of a comment I made to an author:The example above that I wrote to another Medium user will alert them unless they disable their notes. Using this feature allows comments to be made for text requiring edits, such as improved clarity, revision, or expansion.This networking capability has consistently helped me enhance my posts without using time-consuming emails, adding to my productivity as a freelance author. In this way, Medium can also be used as a kind of networking site for writers that allows them to build relationships with one another.Write purposefully to expand your reachBecause Medium is a popular platform for those who appreciate writing and reading well-formed opinions, it offers abundant opportunities to become a thought leader on Medium.You can browse Medium’s themed channels and find articles related to your interests that get high levels of engagement. Then, analyze them to determine how they excel. Components like headlines and paragraph lengths could help you to make significant impacts on click-through statistics.In addition, I’ve found that it’s helpful to come up with a post frequency schedule. You should always aim for quality over quantity, and when picking topics, come up with fresh angles when possible. Don’t be afraid to express minority viewpoints, especially if you can back up opinions with scientific research.Pay attention to the referrers part of the stats sectionThe stats link in the right sidebar of the Medium interface gives information about which of your posts generate the most interest. Most people know about the views metric, which indicates how many individual visitors saw each post. However, the “Referrers” section is one you might not have investigated yet.After clicking the “Referrers” button below a story title, you’ll see a breakdown of how many viewers found your post via a particular method. With this feature, it’s easy to focus on career growth by adjusting your branding tactics for maximum reach. As an example, if it becomes apparent that half your traffic for a given story came through Twitter, that’s a definite sign you should prioritize your engagement on that platform. In contrast, if the majority of those referrals came from email links, this could indicate your newsletter is working well.Build your Medium presence without writing tons of new contentUnlike some publishing platforms, Medium does not require original posts. Instead, you can quickly add articles you wrote that initially appeared on other sites, which lets you demonstrate your capabilities as a writer.This is also a great way to send visitors to your personal blog or other outlets you write for. Just include a link back to the originally published article in your post — I do this with all of my posts for my blogs.To quickly add existing posts to Medium, click your user icon, go to the “Stories” section and choose “Import.” Paste the URL of a story into the provided field. Then, click “See Your Story” to edit it. One time-saving thing about this feature is it automatically assigns a canonical URL to the story that references the source.Want to save even more time? Medium also has a WordPress plugin that automatically syndicates your blog posts to Medium.You’ll receive payment based on the number of claps your posts get and the amount of time viewers spend reading the content.In other words, the people who pay the monthly fee use their engagement levels to influence how much money you receive. Stripe is the payment distributor, and Medium sends revenues each month.All authors can take part in the Medium Partner Program. However, you can still post free content for everyone to see. If you’re ready to try making money by writing, though, Medium’s paid opportunity is a good start for taking your career in that direction.Give some of your Medium stories a creative commons licenseMedium lets content creators license their works under any of the Creative Commons options. Those that require attribution open up the possibility for others writers or publications to share your work and grow your prominence.Some sites rely on Medium when they lack enough original content. So, Creative Commons license is an easy way for writers to increase their chances of getting syndicated.To choose a license for a piece of content, view it in Edit mode. Go to the “Publish” menu, click “Scheduling/Audience/License,” then “License.” As you can see from these tips, there are strategic moves to try as a freelance writer on Medium.By experimenting with them, you may discover career benefits you hadn’t even imagined, increasing your motivation to continue improving.This post is part of our contributor series. The views expressed are the author's own and not necessarily shared by TNW.",How to optimize your Medium as a freelance writer
10016,3944221,2018-01-31 14:54:34,"Developer tipsGDPR – A Practical Guide For DevelopersYou’ve probably heard about GDPR. The new European data protection regulation that applies practically to everyone. Especially if you are working in a big company, it’s most likely that there’s already a process for getting your systems in compliance with the regulation.The regulation is basically a law that must be followed in all European countries (but also applies to non-EU companies that have users in the EU). In this particular case, it applies to companies that are not registered in Europe, but are having European customers. So that’s most companies. I will not go into yet another “12 facts about GDPR” or “7 myths about GDPR” posts/whitepapers, as they are often aimed at managers or legal people. Instead, I’ll focus on what GDPR means for developers.I’ll try to be a bit more comprehensive this time and cover as many aspects of the regulation that concern developers as I can. And while developers will mostly be concerned about how the systems they are working on have to change, it’s not unlikely that a less informed manager storms in in late spring, realizing GDPR is going to be in force tomorrow, asking “what should we do to get our system/website compliant”.The rights of the user/client (referred to as “data subject” in the regulation) that I think are relevant for developers are: the right to erasure (the right to be forgotten/deleted from the system), right to restriction of processing (you still keep the data, but mark it as “restricted” and don’t touch it without further consent by the user), the right to data portability (the ability to export one’s data in a machine-readable format), the right to rectification (the ability to get personal data fixed), the right to be informed (getting human-readable information, rather than long terms and conditions), the right of access (the user should be able to see all the data you have about them).Additionally, the relevant basic principles are: data minimization (one should not collect more data than necessary), integrity and confidentiality (all security measures to protect data that you can think of + measures to guarantee that the data has not been inappropriately modified).Even further, the regulation requires certain processes to be in place within an organization (of more than 250 employees or if a significant amount of data is processed), and those include keeping a record of all types of processing activities carried out, including transfers to processors (3rd parties), which includes cloud service providers. None of the other requirements of the regulation have an exception depending on the organization size, so “I’m small, GDPR does not concern me” is a myth.It is important to know what “personal data” is. Basically, it’s every piece of data that can be used to uniquely identify a person or data that is about an already identified person. It’s data that the user has explicitly provided, but also data that you have collected about them from either 3rd parties or based on their activities on the site (what they’ve been looking at, what they’ve purchased, etc.)Having said that, I’ll list a number of features that will have to be implemented and some hints on how to do that, followed by some do’s and don’t’s.“Forget me” – you should have a method that takes a userId and deletes all personal data about that user (in case they have been collected on the basis of consent or based on the legitimate interests of the controller (see more below), and not due to contract enforcement or legal obligation). It is actually useful for integration tests to have that feature (to cleanup after the test), but it may be hard to implement depending on the data model. In a regular data model, deleting a record may be easy, but some foreign keys may be violated. That means you have two options – either make sure you allow nullable foreign keys (for example an order usually has a reference to the user that made it, but when the user requests his data be deleted, you can set the userId to null), or make sure you delete all related data (e.g. via cascades). This may not be desirable, e.g. if the order is used to track available quantities or for accounting purposes. It’s a bit trickier for event-sourcing data models, or in extreme cases, ones that include some sort of blockchain/hash chain/tamper-evident data structure. With event sourcing you should be able to remove a past event and re-generate intermediate snapshots. For blockchain-like structures – be careful what you put in there and avoid putting personal data of users. There is an option to use a chameleon hash function, but that’s suboptimal. Overall, you must constantly think of how you can delete the personal data. And “our data model doesn’t allow it” isn’t an excuse. What about backups? Ideally, you should keep a separate table of forgotten user IDs, so that each time you restore a backup, you re-forget the forgotten users. This means the table should be in a separate database or have a separate backup/restore process.Notify 3rd parties for erasure – deleting things from your system may be one thing, but you are also obligated to inform all third parties that you have pushed that data to. So if you have sent personal data to, say, Salesforce, Hubspot, twitter, or any cloud service provider, you should call an API of theirs that allows for the deletion of personal data. If you are such a provider, obviously, your “forget me” endpoint should be exposed. Calling the 3rd party APIs to remove data is not the full story, though. You also have to make sure the information does not appear in search results. Now, that’s tricky, as Google doesn’t have an API for removal, only a manual process. Fortunately, it’s only about public profile pages that are crawlable by Google (and other search engines, okay…), but you still have to take measures. Ideally, you should make the personal data page return a 404 HTTP status, so that it can be removed.Restrict processing – in your admin panel where there’s a list of users, there should be a button “restrict processing”. The user settings page should also have that button. When clicked (after reading the appropriate information), it should mark the profile as restricted. That means it should no longer be visible to the backoffice staff, or publicly. You can implement that with a simple “restricted” flag in the users table and a few if-clasues here and there.Export data – there should be another button – “export data”. When clicked, the user should receive all the data that you hold about them. What exactly is that data – depends on the particular usecase. Usually it’s at least the data that you delete with the “forget me” functionality, but may include additional data (e.g. the orders the user has made may not be delete, but should be included in the dump). The structure of the dump is not strictly defined, but my recommendation would be to reuse schema.org definitions as much as possible, for either JSON or XML. If the data is simple enough, a CSV/XLS export would also be fine. Sometimes data export can take a long time, so the button can trigger a background process, which would then notify the user via email when his data is ready (twitter, for example, does that already – you can request all your tweets and you get them after a while). You don’t need to implement an automated export, although it would be nice. It’s sufficient to have a process in place to allow users to request their data, which can be a manual database-querying process.Allow users to edit their profile – this seems an obvious rule, but it isn’t always followed. Users must be able to fix all data about them, including data that you have collected from other sources (e.g. using a “login with facebook” you may have fetched their name and address). Rule of thumb – all the fields in your “users” table should be editable via the UI. Technically, rectification can be done via a manual support process, but that’s normally more expensive for a business than just having the form to do it. There is one other scenario, however, when you’ve obtained the data from other sources (i.e. the user hasn’t provided their details to you directly). In that case there should still be a page where they can identify somehow (via email and/or sms confirmation) and get access to the data about them.Consent checkboxes – “I accept the terms and conditions” would no longer be sufficient to claim that the user has given their consent for processing their data. So, for each particular processing activity there should be a separate checkbox on the registration (or user profile) screen. You should keep these consent checkboxes in separate columns in the database, and let the users withdraw their consent (by unchecking these checkboxes from their profile page – see the previous point). Ideally, these checkboxes should come directly from the register of processing activities (if you keep one). Note that the checkboxes should not be preselected, as this does not count as “consent”. Another important thing here is machine learning/AI. If you are going to use the user’s data to train your ML models, you should get consent for that as well (unless it’s for scientific purposes, which have special treatment in the regulation). Note here the so called “legitimate interest”. It is for the legal team to decide what a legitimate interest is, but direct marketing is included in that category, as well as any common sense processing relating to the business activity – e.g. if you collect addresses for shipping, it’s obviously a legitimate interest. So not all processing activities need consent checkboxes.Re-request consent – if the consent users have given was not clear (e.g. if they simply agreed to terms & conditions), you’d have to re-obtain that consent. So prepare a functionality for mass-emailing your users to ask them to go to their profile page and check all the checkboxes for the personal data processing activities that you have.“See all my data” – this is very similar to the “Export” button, except data should be displayed in the regular UI of the application rather than an XML/JSON format. I wouldn’t say this is mandatory, and you can leave it as a “desirable” feature – for example, Google Maps shows you your location history – all the places that you’ve been to. It is a good implementation of the right to access. (Though Google is very far from perfect when privacy is concerned). This is not all about the right to access – you have to let unregistered users ask whether you have data about them, but that would be a more manual process. The ideal minimum would be to have a feature “check by email”, where you check if you have data about a particular email. You also need to tell the user in what ways you are processing their data. You can simply print all the records in your data process register for which the user has consented to.Age checks – you should ask for the user’s age, and if the user is a child (below 16), you should ask for parent permission. There’s no clear way how to do that, but my suggestion is to introduce a flow, where the child should specify the email of a parent, who can then confirm. Obviously, children will just cheat with their birthdate, or provide a fake parent email, but you will most likely have done your job according to the regulation (this is one of the “wishful thinking” aspects of the regulation).Keeping data for no longer than necessary – if you’ve collected the data for a specific purpose (e.g. shipping a product), you have to delete it/anonymize it as soon as you don’t need it. Many e-commerce sites offer “purchase without registration”, in which case the consent goes only for the particular order. So you need a scheduled job/cron to periodically go through the data and anonymize it (delete names and addresses), but only after a certain condition is met – e.g. the product is confirmed as delivered. You can have a database field for storing the deadline after which the data should be gone, and that deadline can be extended in case of a delivery problem.Now some “do’s”, which are mostly about the technical measures needed to protect personal data (outlined in article 32). They may be more “ops” than “dev”, but often the application also has to be extended to support them. I’ve listed most of what I could think of in a previous post. An important note here is that this is not mandated by the regulation, but it’s a good practice anyway and helps with protecting personal data.Encrypt the data in transit. That means that communication between your application layer and your database (or your message queue, or whatever component you have) should be over TLS. The certificates could be self-signed (and possibly pinned), or you could have an internal CA. Different databases have different configurations, just google “X encrypted connections. Some databases need gossiping among the nodes – that should also be configured to use encryptionEncrypt the data at rest – this again depends on the database (some offer table-level encryption), but can also be done on machine-level. E.g. using LUKS. The private key can be stored in your infrastructure, or in some cloud service like AWS KMS.Encrypt your backups – kind of obviousImplement pseudonymisation – the most obvious use-case is when you want to use production data for the test/staging servers. You should change the personal data to some “pseudonym”, so that the people cannot be identified. When you push data for machine learning purposes (to third parties or not), you can also do that. Technically, that could mean that your User object can have a “pseudonymize” method which applies hash+salt/bcrypt/PBKDF2 for some of the data that can be used to identify a person. Pseudonyms could be reversible or not, depending on the usecase (the definition in the regulation implies reversibility based on a secret information, but in the case of test/staging data it might not be). Some databases have such features built-in, e.g. Orale.Protect data integrity – this is a very broad thing, and could simply mean “have authentication mechanisms for modifying data”. But you can do something more, even as simple as a checksum, or a more complicated solution (like the one I’m working on). It depends on the stakes, on the way data is accessed, on the particular system, etc. The checksum can be in the form of a hash of all the data in a given database record, which should be updated each time the record is updated through the application. It isn’t a strong guarantee, but it is at least something.Have your GDPR register of processing activities in something other than Excel – Article 30 says that you should keep a record of all the types of activities that you use personal data for. That sounds like bureaucracy, but it may be useful – you will be able to link certain aspects of your application with that register (e.g. the consent checkboxes, or your audit trail records). It wouldn’t take much time to implement a simple register, but the business requirements for that should come from whoever is responsible for the GDPR compliance. But you can advise them that having it in Excel won’t make it easy for you as a developer (imagine having to fetch the excel file internally, so that you can parse it and implement a feature). Such a register could be a microservice/small application deployed separately in your infrastructure.Log access to personal data – every read operation on a personal data record should be logged, so that you know who accessed what and for what purpose. This does not follow directly from the provisions of the regulation, but it is kinda implied from the accountability principles. What about search results (or lists) that contain personal data about multiple subjects? My hunch is that simply logging “user X did a search for criteria Y” would suffice. But don’t display too many personal data in lists – for example see how facebook makes you go through some hoops to get a person’s birthday. Note: some have treated article 30 as a requirement to keep an audit log. I don’t think it is saying that – instead it requires 250+ companies (or companies processing data regularly) to keep a register of the types of processing activities (i.e. what you use the data for). There are other articles in the regulation that imply that keeping an audit log is a best practice (for protecting the integrity of the data as well as to make sure it hasn’t been processed without a valid reason)Register all API consumers – you shouldn’t allow anonymous API access to personal data. I’d say you should request the organization name and contact person for each API user upon registration, and add those to the data processing register.Finally, some “don’t’s”.Don’t use data for purposes that the user hasn’t agreed with – that’s supposed to be the spirit of the regulation. If you want to expose a new API to a new type of clients, or you want to use the data for some machine learning, or you decide to add ads to your site based on users’ behaviour, or sell your database to a 3rd party – think twice. I would imagine your register of processing activities could have a button to send notification emails to users to ask them for permission when a new processing activity is added (or if you use a 3rd party register, it should probably give you an API). So upon adding a new processing activity (and adding that to your register), mass email all users from whom you’d like consent. Note here that additional legitimate interests of the controller might be added dynamically.Don’t log personal data – getting rid of the personal data from log files (especially if they are shipped to a 3rd party service) can be tedious or even impossible. So log just identifiers if needed. And make sure old logs files are cleaned up, just in caseDon’t put fields on the registration/profile form that you don’t need – it’s always tempting to just throw as many fields as the usability person/designer agrees on, but unless you absolutely need the data for delivering your service, you shouldn’t collect it. Names you should probably always collect, but unless you are delivering something, a home address or phone is unnecessary.Don’t assume 3rd parties are compliant – you are responsible if there’s a data breach in one of the 3rd parties (e.g. “processors”) to which you send personal data. So before you send data via an API to another service, make sure they have at least a basic level of data protection. If they don’t, raise a flag with management.Don’t assume having ISO XXX makes you compliant – information security standards and even personal data standards are a good start and they will probably 70% of what the regulation requires, but they are not sufficient – most of the things listed above are not covered in any of those standardsOverall, the purpose of the regulation is to make you take conscious decisions when processing personal data. It imposes best practices in a legal way. If you follow the above advice and design your data model, storage, data flow , API calls with data protection in mind, then you shouldn’t worry about the huge fines that the regulation prescribes – they are for extreme cases, like Equifax for example. Regulators (data protection authorities) will most likely have some checklists into which you’d have to somehow fit, but if you follow best practices, that shouldn’t be an issue.I think all of the above features can be implemented in a few weeks by a small team. Be suspicious when a big vendor offers you a generic plug-and-play “GDPR compliance” solution. GDPR is not just about the technical aspects listed above – it does have organizational/process implications. But also be suspicious if a consultant claims GDPR is complicated. It’s not – it relies on a few basic principles that are in fact best practices anyway. Just don’t ignore them.25 thoughts on “GDPR – A Practical Guide For Developers”Hi Bozho, Excellent article. I was wondering what your thought were on how to handle historical backups when implementing “Forget me”. Would every backup containing data on the subject need to be restored in order to delete the relevant data and then subsequently backed up again? This could be a nightmare scenario for a large company with a lot of data and a lot of forget me requests Kind Regards, DarrenI really like your article! I have just one comment which I think is worth to mention, you do not have to implement everything, if you could with high probability assume that for example right to portability will be used very rarely you could define manual process for extracting personal data from database and use it when it will be needed. I think GDPR put a requirement on data Controller to provide possibility to do so, the way is up to the controller.Further to Darren’s comment/question about right-to-erasure and backups: A similar problem also occurs when using the event sourcing architecture, if personal data is stored in an immutable event log. One option for these scenarios is to use cryptographic erasure: encrypt personal data field upfront, with a key specific to the data subject, and deleting the key when needed to enforce deletion of the data. This is something we’ve implemented for Java. More info here: https://axoniq.io/events/2017/11/gdpr-webinar.html@Darren I added a little more about backups. Basically, you keep a list of forgotten user IDs and re-delete them on restore.@Frans yes, that’s a good approach. In some cases events (in event sourcing) can be deleted or modified/anonymized without affecting anything else, so that’s also an option (slightly easier, but potentially breaking)@Albert – that’s right. It better be automated, but it doesn’t have to be. I’ll add a clarification @Dawn – yupBasically you assume, that you already have perfect data quality and have identified all persons with some account id. But the regulation never mentions some id, it requires to identify natural persons, not accounts.Some example from my real live experience with data we have seen at almost every customer companies. You have an contract with an ISP for your internet and another contract with the same ISP for your mobile phone. What we have seen is, that most of the companies create TWO seperate accounts for this and don’t get the data connected. Especially, if there are some company fusions or just different departments. The result is currently, that you might get two ad mails for a new product of the ISP.For the GDRP it would NOT be sufficent, to make some buttons after the account login, if the natural person has two accounts. You have to find ALL data regarding this one natural person. So the buttons are good, but if you don’t control your data quality you could get into trouble.So, you are right, GDPR is not THAT complicated, but it isn’t THAT easy as you say. The basic implementation for some features might only require some weeks, but only if you already have solved some very hard problems. Maybe it is quite easy for small or “new” companies, which only have ONE (at most two) database, but for most companies we talk to, this is not the reality.Just my thoughts (I work at a company with heavy experience with data(-quality)) Greetings MarcelHello Bozho, first of thank you for a comprehensive and an exhaustive article. I have a bit of an obscure question. How about third parties who generate user interaction data which is used for ROI, conversion and such measurements? Especially where they don’t explicitly or implicitly know the user ? do those 3rd parties need to provide data export for the specific user? I am asking because in order to offer an export, they’d need to be able to bind the actual app user to their user agnostic tracking system.If they can’t deduce the user, they cannot do any of the above. However, they should follow the e-privacy directive and the upcoming e-privacy regulation which defines how cookies and other tracking mechanisms are usedI think it covers the most common use cases . There will certainly be edge cases depending on the business needs that are not covered above, though. The other day we got such a question – “what to do in case we get the data of the user and their consent over the phone”. Seems like the proper thing is to just mark the consent in a CRM on behalf of the user, but it is not yet clear – maybe some call archiving will be needed in case of sensitive data? Can’t say at this point without consulting with legal experts.Thanks for sharing your analysis. I’ve spent some days in 2017 to scan official ressources, including the original GDPR text, and for some points, I came to a slightly different conclusion. Basically, almost every time you write “must” (encrypt data base content; provide a data download button; allow direct personal data editing; etc…), on my side, went to the conclusion that this is an option, not a requirement. What is required is to grant each individual access to their personal data; the how (is it automated or manually) is not enforced. Thus, a snail mail process would meet the requirement. Regarding encryption, the text states “shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk”. The notion of “level of security appropriate to the risk” is key here : whether data are usual ecommerce data (postal address) or personal insurance data (history of failures, …) does matter, and measures are to be adapted. By the way, it is not only a technical point, but also a process point : what about the developer who would code the encryption of the data : how do you ensure that he will not be able to access/decypher all data ?I was curious if GDPR only applies to client data or if it also applies to employee/admin user data as well.For example with event sourcing or access logs, would have something like “Employee X changed Customer Y’s address on 01/01/2018”. Can the employee/admin ask for their data to be forgotten? (eg when employee leaves company)What would you recommend for people who are both customers and employees?It applies to employee/admin data as well, yes, BUT it is based on contract, rather than consent. So the employee can’t ask to be forgotten. You just have to define a data retention period for that kind of audit data (it shouldn’t be “forever”)You are correct. It is more fuzzy than “must” vs “must not”. I’ve listed the general good practices that would make you safer, but whether a compliance audit will absolutely require them – depends on many factors.Great article. Helps our developers really grasp the concepts i’ve been trying to get across on our implementation journey. We are now more focused. Our real challenge is in implementing a solution for data at rest that avoids having to encrypt the whole database.Great article, I’m just in the process of ensuring GDPR compliance in our reporting databases, and one issue that we’re having is with our main Datawarehouse, in which we’ve got data from about 5 legacy systems combining, we’re finding that realistically we need to obfuscate the personal details identically for the systems, so that I’ve got the same fake name and postcode in each system (for instance) to be able to match or throw up anomalies in an exception report.Ideally, the best approach would be to start afresh with empty systems and populate each with specific test data but getting the diversity, volume and historical issues would mean the data was hugely unrealistic and at the point of going live we’d hit new unforeseen issues that the clean, sensible test data didn’t expose.Another challenge is when updating the data with live deltas, the obfuscation needs to be similarly consistent – so, for example, we’ve got me, Mr Smith, first appearing in our CRM system as a lead, so that creates a record in the warehouse and after anoymisation, I’m ‘Mr Jones’ (along with obfuscated email, phone, address etc) – then I sign up as a customer in the sales system, we have to go back to the CRM system and find my pseudonym and use that, whereas if I’ve just appeared directly in the Sales system, they’ll need to come up with a new pseudonym, randomly generated, and then later, if I appear in the CRM system (if they did a customer mailout for instance) they’ve got to do the same. Essentially, the first time I appear in any system I’m given my pseudonymous values, and appearances in subsequent systems must tie back to that first appearance. Also this needs to be done on a field by field basis, as not all systems have all the same fields – (one might have email, another not).Hi Bozho, I am not developer but a “manager” 🙂 However I would like to ask a developer type question. Is it possible that access to data within a database is granted via an API that ensures you should have access. That way developers cannot use PHP encoded into the webpage to see all data without logging their access. Thanks JamesFor me there is a contradiction between the “forget me” functionality and when you were saying you can restore the database with a backup and then erasing those users’ personal data. In my understanding this should not be enough to do compile with the regulations. Same goes for encrypting your backup, I just fail to see how is that compatible with GDPR. My understanding is that you have to delete EVERY personal information you storing about the specific person in your system. Doesnt matter if its in a log file or database or happen to be in a backup file.“Yes, but that just shifts the responsibility to the developers of the API. Ultimately someone will have to write queries”Well, yes and no. The company can get audited, so it’s not really the developer responsibility of the API. It’s the whole company, and the developer has to make the requirement efforts to compile with the regulations. And again, I don’t think leaving the user personal data in backups is compatible with GDPR. Although I don’t know a better solution neither, since every company has incremental backups and it just makes it close to impossible to do such a thing – removing personal data from those backups too -.About the API – from organizational point of view it is of course better to limit the number of people (and applications) that have direct access to the database. No doubt about it.As for backups – since eventually old backups are discarded (even in the case of incremental backups, full backups are performed), then I think you are fine with having an encrypted backup + a separate table with forgotten users. Apart from that, I agree, you can’t delete personal data from backups. It’s sufficient to acknowledge that, to protect the backups (encrypt, limit access to them), and have them expire. I guess..",GDPR - A Practical Guide For Developers - Bozho's tech blog
10017,3944222,2018-03-01 12:12:08,"We X-Rayed Some MLB Baseballs. Here’s What We Found.On 6,105 occasions last season, a major leaguer walked to the plate and hammered a baseball over the outfield wall. The 2017 season broke the home run record that was set in 2000 — the peak of the steroid era — when players hit 5,693 homers, and it built upon the remarkable 5,610 that were hit in 2016. It was a stunning display of power that played out in every MLB park almost every night. And with spring training underway in Florida and Arizona, MLB’s power surge is showing no sign of letting up.But while we now know what caused the spike in home runs at the turn of the century — even if we didn’t at the time — the reason for the most recent flurry of long balls remains an unsolved mystery. Any number of factors might have contributed to the home run surge, including bigger, stronger players or a new emphasis on hitting fly balls. But none of those possibilities looms larger than the ball itself.MLB and its commissioner, Rob Manfred, have repeatedly denied rumors that the ball has been altered in any way — or “juiced” — to generate more homers. But a large and growing body of research shows that, beginning in the middle of the 2015 season, the MLB baseball began to fly further. And new research commissioned by “ESPN Sport Science,” a show that breaks down the science of sports,1 suggests that MLB baseballs used after the 2015 All-Star Game were subtly but consistently different than older baseballs. The research, performed by the Keck School of Medicine at the University of Southern California and Kent State University’s Department of Chemistry and Biochemistry, reveals changes in the density and chemical composition of the baseball’s core — and provides our first glimpse inside the newer baseballs.Looking inside the balls and testing their chemical composition revealed that the cores of recent balls were somewhat less dense than the cores of balls used before the 2015 All-Star Game. The newer cores weigh about a half a gram less than the older ones, which might be enough to cause baseballs hit on a typical home run trajectory to fly about 6 inches farther. That alone is hardly enough to explain the home run surge of recent seasons, but when combined with previousresearchfinding that baseballs began to change in other small ways starting around the same time, it suggests that a number of minor differences may have combined to contribute to the remarkable upswing in home run power we’ve witnessed since 2015.Asked about these findings, MLB noted that it had commissioned a group of scientists and statisticians to investigate any changes to the ball, and that the committee would issue a report on its research soon. According to Alan Nathan, one of the physicists on the commission, the task force found that all the characteristics that MLB regularly measures, including the weight, circumference, seam height and bounciness of the ball, were within ranges that meant variations in the baseballs were unlikely to significantly affect home run rates. MLB declined to provide the data supporting these assertions.Independent investigations byFiveThirtyEight, publications like The Ringer, and Nathan himself have shown differences in the characteristics of the ball and the way it performs. Research has shown that balls used in games after the 2015 All-Star Game were bouncier and less air resistant compared with baseballs from the 2014 season, when players hit a relatively modest 4,186 homers, the fewest since 1995. (Nathan noted that MLB does not regularly measure air resistance.) Taken together, these changes would result in a ball that would come off the bat at a higher speed and carry farther. While investigations have been able to show that the baseball behaves differently in recent years, no one had looked inside the ball to see if there was evidence of changes to the way the baseball is constructed.So far, these investigations have primarily looked at the exterior of the baseball.Broadly, MLB baseballs — which are produced by Rawlings in Costa Rica — are made of three components: an exterior shell of cowhide, a winding of several layers of yarn, and a core of rubber-coated cork, also known as a “pill.”To analyze possible changes to the inside of the ball, particularly the core, “ESPN Sport Science” purchased one new ball from Rawlings and seven game-used baseballs from eBay, confirming their authenticity through MLB’s authenticator program.2The eight baseballs we tested were split into two groups: an “old group” of four balls used in games played between August 2014 and May 2015, and a “new group” of three balls used in games played between August 2016 and July 2017, plus the brand-new ball. The aim was to see if the internal composition of the baseballs had changed in ways that would affect the ball’s performance.3The balls were first analyzed by Dr. Meng Law, Dr. Jay Acharya and Darryl Hwang at the Keck School of Medicine at USC using a computerized tomography, or CT, scan. This test is typically used to look inside a human head or body, but in this case, it allowed Dr. Law’s team to examine the interior of the baseballs without cracking them open and destroying them.Initial CT imaging showed that baseballs in the same group had a negligible variation in internal properties.When comparing the new and old groups, however, there was a clear difference in the density of the core.In an MLB baseball, the core consists of four parts: a cork pellet at the center, surrounded by a layer of black rubber held together by a rubber ring where the halves meet, all of which is then molded together in a layer of pink rubber.Dr. Law’s team isolated the density difference to the outer (pink) layer of the core, which was, on average, about 40 percent less dense in the new group of balls.While other parts of the ball showed slight differences in density and volume, none were as noteworthy as the changes to the core.It’s not just that the inside of the ball looks different — the chemical composition of the cores appears to have changed as well. After being tested at the Keck School, the same set of balls were sent to Kent State University. There, researchers at Soumitra Basu’s lab in the Chemistry and Biochemistry department cut open the balls to examine the cores using a thermogravimetric analysis (TGA). This test essentially cooks a material to see which parts parts of it vaporize at which temperatures. Using that information, researchers can create a molecular profile of a given material.This test showed that the pink layer of the core in baseballs from the new group was, on average, composed of about 7 percent more polymer than the same area in baseballs from the old group. Additionally, an analysis with a scanning electron microscope showed that the same layer in the new balls contained, on average, 10 percent less silicon, relative to the amount of other ingredients in the pill. According to the Kent State researchers, these chemical changes produced a more porous, less dense layer of rubber — which explains the results seen in the CT scan at the Keck School.It may not seem obvious, but these slight changes in the chemical composition of the core could have an impact on how the balls played once they were sewn up and shipped to major league teams. Less dense cores could mean lighter baseballs. The cores of the new balls weighed, on average, about 0.5 grams less than the cores from the old group. This difference was statistically significant, which means it’s highly unlikely that it was due to sampling error. The overall weight of the balls also dropped by an average of about a 0.5 grams between groups, but, unlike with the cores, this difference was not statistically significant.4Half a gram isn’t much — it’s only about the weight of a paperclip. A tiny change like this might add only about 6 inches to flight of a baseball hit on a typical home run trajectory, according to Nathan’s calculations. But the timing of these changes to the weight and density of the core coincides with a much larger boost to the bounciness of the baseball. According to a previous analysis performed by The Ringer, that increase in bounciness alone would add around 0.6 mph to the speed of the ball as it leaves the bat and add roughly 3 feet to the travel distance of a fly ball — enough to make the difference between the warning track and the stands.On top of the fact that the balls became bouncier as the core itself changed, previous research at FiveThirtyEight showed that they also became less air resistant. The decrease in drag is probably a result of a smaller, slicker baseball with lower seams. The change in air resistance could add an additional 5 feet to the travel distance of a fly ball. Combine all these factors together — a lighter, more compact baseball with tighter seams and more bounce — and the ball could fly as much as 8.6 feet farther. According to Nathan’s calculations, this would lead to a more than 25 percent increase in the number of home runs. Asked whether these changes in combination could have significantly affected the home run rate, MLB declined to comment.In actuality, home runs spiked by about 46 percent between 2014 and 2017, which means that the changes to the ball could account for more than half of the increase. The remainder could be reasonably chalked up to a philosophical shift among MLB hitters, who are likely swinging upward to maximize the number of balls they hit in the air and are not shy about the increase in strikeouts that may come with that approach.MLB Commissioner Rob Manfred has repeatedlydenied that the baseball is juiced. On numerous occasions, he has said league testing found that baseballs continue to fall within the range that MLB designates as acceptable, and he recently said that MLB testing showed the balls to be fundamentally the same. But even if the baseballs still meet the league’s manufacturing guidelines, their performance could change enough to double (or, theoretically, halve) the number of home runs hit in a year.In fact, in January of 2015, Rawlings filed a patent application for a manufacturing process that would allow it to produce softballs and non-MLB baseballs5 that were as bouncy as possible while still falling within the manufacturing specifications set by the league. This type of ball is constructed quite differently from MLB baseballs, so there’s no indication that this patent means Rawlings is deliberately manipulating major league baseballs in this way, but it demonstrates that it’s at least theoretically possible for balls to be “fundamentally the same” while also performing differently than they have in the past.Kathy Smith-Stephens, senior director of quality and compliance at Rawlings, said that no change had been made to the baseballs but that “natural variation” occurs in the manufacturing process. She noted that they “continuously tweak” — though later in the interview she asked that we say “continuously refine” — the manufacturing process in an effort to reduce variations, but said that Rawlings’ internal testing had shown no difference in the ball’s weight or bounciness.Evidence that the baseball is at least partially responsible for the last few years’ spike in the home run rate mounted throughout the summer of 2017 and reached a peak during October’s World Series. In those seven games, the Houston Astros and Los Angeles Dodgers smashed 24 homers, including eight in one game. In the wake of this power display, Manfred asked all 30 teams to start storing baseballs in a climate-controlled room and commissioned a task force of scientists and statisticians to investigate whether the ball was juiced in 2017. Our own research, combined with controlled tests from three separate academic laboratories, strongly suggests that the physical properties of the ball have changed. Taken together, all these studies give us a lot of evidence to suggest that today’s baseballs differ in meaningful ways from those of a few years ago. In other words, there are many questions for Manfred’s committee to address.Special thanks to Sean O’Rourke, Dr. Cynthia Bir and Nathan Beals for additional research assistance.FootnotesESPN owns FiveThirtyEight.Contractors working for MLB affix a tamper-resistant hologram sticker to balls that they personally witness being used in the game. The sticker includes a serial number that can be entered into the MLB authenticator program to confirm that the ball is real and to find out which game it was used in.This sample is admittedly small, but according to the Kent State scientists we worked with, it’s sufficient to determine statistical significance in the two groups.The ball as a whole weighs much more than the core alone, and there was more variation in the weight of the full baseballs than in the weight of the cores, both of which meant that the bar for statistically significant variations in weight was higher for the whole baseball than for just the core.The patent applies to balls with foam cores, which might be used in softball or youth-league baseball, for example, but does not apply to the type of baseballs used in MLB, with layers of yarn around a cork-and-rubber core.Rob Arthur is FiveThirtyEight’s baseball columnist and also writes about crime. @No_Little_PlansTim Dix is a writer based in Los Angeles, where he mostly produces television programming involving sports or science or both.",We X-Rayed Some MLB Baseballs. Here’s What We Found.
10018,3944467,2018-03-02 14:49:44,"TNW SitesVoice assistants aren’t built for kids — we need to fix thatVoice assistants have recently made the leap from your personal smart device into the home via home devices, such as Amazon Alexa or Google Home. It’s been estimated that 39 million Americans now own a voice-activated smart home speaker. These assistants are now being integrated into other smart devices in the home such as TVs, lights, fridges, headphones, etc.It’s clear that voice as an interface has suddenly gone mainstream and we are regularly seeing advertisements of families enjoying and interacting with their home devices. But as voice interactions become ubiquitous in the home, I think there’s an important question we need to ask ourselves: “Are these voice interfaces appropriate for children?”Like you might expect, there’s no simple answer to that question. However, we can get closer to an answer by looking at a few key aspects of this issue that must be addressed.Systems must engage with kids appropriatelyFirstly, the system must respond appropriately to children. White House press secretary Sarah Huckabee Sanders recently called out Amazon after she claimed that her two-year-old child was able to use the Echo to purchase an $80 Batman toy. It‘s worth noting that simply calling out the word ‘batman,’ even repeatedly, is not going to purchase batman for you on Amazon; the UI simply doesn’t work that way. However, it does highlight an interesting issue; that home devices should not allow children to access the same environments as adults.Alexa is not alone in her indiscriminate behavior. My eight-year-old was recently using Siri on my phone and the word ‘bitch’ appeared. Note that she had not actually said the word, which speaks more to its poor accuracy for kids voices. Shocked, she showed me the phone. While Siri did reply with an amusing ‘there’s no need for that,’ Siri should have identified that this was a child speaking and responded appropriately, choosing to not print the obscenity to the screen.For voice assistants to work accurately and appropriately, they must be able to identify children’s voices and create, implement, and adhere to proper protocols in regards to how they handle requests. Otherwise we’ll just get more of incidents like this:Privacy and data protection for kids’ voice dataSecondly, children’s voice data must be processed and stored in accordance with data privacy and protection laws globally. This throws up many more challenges which so far, seem to have been mostly pushed aside. We need to understand more about how our children’s data is being used and stored.The FTC’s Children’s Online Privacy Protection Act (COPPA) dictates how operators must handle personally identifiable information, such as voice, collected from children under the age of 13. To comply with COPPA, explicit permission must be sought from parents or guardians for operators collecting, processing, and storing kids voice data. In Europe, the forthcoming General Data Protection Regulation (GDPR) legislation of May 2018, is set to equal COPPA in these respects.A mainstream example can be found in an episode of HBO’s comedy series, Silicon Valley. The show highlighted the problem of collecting children’s data without permission when the character Dinesh unknowingly violated the rules, resulting in billions of dollars worth of fines.Home devices such as Google Home and Amazon’s Alexa work pretty much out of the box, allowing you to immediately use voice commands to do simple things like set timers, tell jokes, read the news, and check weather forecasts. So while both provide the option of creating user profiles for kids where parents can give their consent, you are not required to do so prior to using the device.So, what happens if a user has not yet given permission for their child’s data to be collected, but they are still using the device? How about when a user has given permission for their child’s data to be collected but their friend is visiting the house?To address such scenarios, the FTC recently relaxed the rule, just enough that common tasks like voice searches can be done for kids without risk to the company. Specifically, the FTC will not take an enforcement action against an operator for not obtaining parental consent before collecting the audio file from a child’s when it is collected solely as a replacement of written words, such as to perform a search or to fulfill a verbal instruction or request — as long as it is held for a brief time and only for that purpose. So, we’re all good right?Well, yes, but just so we’re clear — the FTC now require operators, without parental consent, to immediately delete children’s voice data. This includes not storing data or extracting information from the data to improve the voice service. It’s worth noting that the EU GDPR has not provided any such leniency or guidance on this matter. So, is this actually being done? Can all operators in this space hold their hands up and confirm that they are immediately deleting voice data for all kids they don’t have consent for? It’s not likely, but we certainly should hope so.Companies must state how they will use the dataLastly, all operators need to clearly articulate how they intend to use legally acquired kids voice data. Will the voice data simply be used to improve the speech recognition services, with data remaining in the company? Or will the child’s voice data be sold to third parties for data mining and marketing purposes?We must acknowledge and expect that children will use these voice assistants and smart devices when they are so easily accessible in the home. Voice interactions have many positive benefits allowing kids to naturally interact with technology without the need for screens. As we watch the rise of the voice interface as it becomes more integrated into our appliances, our cars, our personal devices, and what is needed is for child-specific interfaces with appropriate responses. This means not allowing children open access to web browsing, purchasing, inappropriate language, or content — or allow video calling to your contacts, etc.While full compliance with US COPPA and EU GDPR is required by law, grey areas remain in this fast changing technology space. Companies leading the voice assistant space need to provide more transparency with respect to children’s voice data: Do operators immediately detect kids voices as distinguished from adults and if so, is all kids voice data immediately deleted where permission has not been obtained? If permission was granted, how is the data used and who has access to it?",Voice assistants aren’t built for kids — we need to fix that
10019,3947920,2018-03-03 18:00:00,"These are a few of our favorite things (from MWC 2018)Mobile World Congress 2018 has finally come to a close, and while this year felt more low-key than usual, there was still plenty to see and appreciate on the show floor. Samsung was here in full-force, Sony outed a pair of surprisingly compelling flagship phones, Vivo showed up with yet another kooky concept -- the list goes on. As always, though, Team Engadget happened upon some other developments that captivated us more than we expected. That's MWC for you -- it'll always catch you by surprise.As we board our planes and begin our long treks home, join us for a final look at some of our favorite things -- not just gadgets -- from MWC. (Oh, and take a gander at our unnerving AR emoji while you're at it.)Edgar Alvarez, Senior EditorNot only does it have the potential to help cops or firefighters during emergencies with features like a removable heart rate monitor and body camera -- it looks like it came straight out of a menswear runway show, too. Its fashionable streetwear looks are no accident either, since Nokia worked with Kolon (a South Korean fashion brand) to design the jacket. I'll probably never own it, but at least I got to try it on for a few minutes here in Barcelona.Cherlynn Low, Reviews EditorGalaxy S9/S9+ AR emojisOf all the new features of the Galaxy S9 and S9+, the most interesting is AR Emoji. Sure, it's easy to dismiss this feature as an imitation of Apple's Animoji, and the emoji themselves creep some people out. But the fact that they were controversial and invite participation makes this arguably the standout addition to the Galaxy S series. Technologically, the feature isn't the most impressive -- its depiction of faces varies wildly in accuracy, there are limited styles to choose from when customizing your avatar and the front camera struggles to pick up your facial movements and reproduce them.Still, almost everyone on my team here made their own AR Emoji and some of them even clamored to try it out. We had a ball of a time doing impressions of each other, using the person's avatar. Yes, the novelty of AR Emojis will eventually wear out, but for now it provided a hell of a lot of fun, and certainly cheered me up this cold, bleak MWC.Mat Smith, UK Bureau ChiefThe exaggerated death of the headphone jackAny company that makes wireless headphones and buds might try to convince you that the future is set: get rid of those silly (cheaper, universal) headphones, and get some Bluetooth things instead.However, MWC 2018's graduating class of smartphones had only three devices that nixed the 3.5mm socket: Sony's duo of Xperia XZ2 phones and Nokia's new flagship redesign, the Sirocco 8. Options are good. Holding onto my reliable, wired, no-recharge needed earbuds is even better.Nick Summers, ReporterHuawei MateBook X ProI'm still searching for my dream Windows laptop. I like the Dell XPS 13, but the design is dull and the bezel between the hinge and the display is a little large for my tastes. I like the Surface Laptop too, but it's expensive and the alcantara material worries me. That's why I'm so intrigued by the Huawei MateBook X Pro that was announced at Mobile World Congress.It has a simple, Macbook-esque look and the screen-to-body ratio is phenomenal. The pop-up webcam? It's a nice idea, but I think the up-the-nose viewing angle is awful. Thankfully, I rarely take video calls. Otherwise, the MateBook X Pro seems like a solid ultrabook with plenty of power and an adequate selection of ports. Everything I want, basically, from a Macbook Pro replacement.Chris Velazco, Senior Mobile EditorThe first wave of Android Go Edition phonesAndroid Go was first announced at last year's Google I/O developer conference, but the first Go Edition phones just showed up here in Spain. In terms of hardware, none of them are particularly special, save for maybe the cutesy, familiar Nokia 1. Even so, I find Google's commitment to building a more consistent kind of Android experience for people, no matter how much they paid for their smartphone, terribly admirable.Google's suite of Go-enabled apps are a pleasure to use too, and I'm pleased that vital features like Google Assistant are now available to just about everyone. It'll take some time to really understand Android Go's impact, but since Google has confirmed it will make Go builds for future Android versions, I think we just might be entering the golden age of cheap phones.James Trew, Managing EditorChris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.",These are a few of our favorite things (from MWC 2018)
10020,3947924,2018-03-03 15:00:00,"For this iPhone clone maker, it's all about survivalA Chinese company called Leagoo made headlines at MWC for showing off its S9, a cheap Android phone that looks like an iPhone X and swiped Samsung's flagship name. These kinds of clones have been a part of the industry forever, clogging up eBay and disappointing whoever buys them. The S9 is indeed the latest in a long line of clones, but as I discovered, the forces that brought it to market are more interesting than the facsimile itself.After all, it seems odd that a company with the resources to build a portfolio of products and lock down a sponsorship deal with Tottenham Hotspur FC would resort to blatant copying. And these copies aren't much to write home about either. The S9s at Leagoo's booth ran a very buggy version of Android, and the hardware felt chintzy and insubstantial. Those software issues should get worked out before launch, but honestly, this is exactly the kind of cut-rate experience you'd expect from a $150 iPhone clone. That said, the bits of the iPhone X Leagoo attempted to copy at least look correct.The notch is roughly the right size, and the 13+2MP dual camera around back looks more or less spot-on, even if it doesn't take particularly great pictures. The Leagoo S9 I played with seemed far from finished, but even at its best, it seems unlikely to thrill.Chris Velazco/EngadgetAnd the iPhone isn't the only flagship Leagoo has attempted to ape, either. Off to the side of Leagoo's booth at MWC was a plasticky phone with a dual camera and stripes etched horizontally high on its back. That was the company aiming at a domestic rival: Huawei's high-powered Mate 10 Pro. In fairness, Leagoo designs and produces original smartphones as well -- quite a few, in fact.After a conversation with Leagoo Senior Vice President Zhijie Xie, things became more clear. For Leagoo, cranking out the occasional clone phone isn't just a way for the company to capitalize on new trends in the industry. It's about survival.""For right now, the situation for the mobile phone business,"" Xie said, his voice trailing off a little. ""Especially [when you're] like us -- you just have to stay alive.""The S9 then is pure marketing given physical form, a device meant to drive buzz in hopes of boosting the business as a whole. If this sounds familiar, well, it should: that's the idea being the new Nokia's string of retro phone releases.""This one just makes some noise,"" conceded Xie while gesturing to the S9.The weariness in his voice was apparent, and he's earned it. Leagoo is still a small company, with only a few hundred employees and two factories: one in Shenzhen and one in Sichuan province. And given the sort of global competition small companies like Leagoo face, carving out a space in the market is no easy feat. On a large scale, Apple and Samsung tend to dominate both smartphone conversations and market share to the point where even highly capable competitors are struggling to make an impact.""Even for LG and Sony sales are going down every year,"" Xie said. ""We just [have to] stay alive and try to find some core-selling products."" Leagoo's search for those core products has been a tricky one so far. Xie pointed out that the company's M-series smartphones were doing well, but a recent release fizzled out because of performance issues related to its Spreadtrum chipset. These kinds of issues seem even more painful when you consider Leagoo's limited reach. Despite its firmly Chinese origins, Leagoo doesn't sell phones there at all. All of the companies efforts are concentrated in southeast Asia and eastern Europe, partially because standing up to China's internal competition can be nearly impossible for upstarts.""We never did China,"" Xie added. ""The competition is too strong. Huawei, Xiaomi, Meizu, they're all there."" And beyond just being there, those companies have much more experience cutting deals with Chinese carriers and retailers, leading to a situation where smaller can be boxed out of their own home.""At the beginning, we created this company and decided to only focus on overseas markets,"" said Xie. ""In China, you sometimes need relationships; we do not have the background, and so we chose fair markets. If you work hard, if your products are good, you can sell.""I'll admit to being surprised after playing with a few other Leagoo smartphones at the booth. They were perfectly passable devices. The low price tags involved meant they couldn't stack up to many of the devices I regularly test, but the phones themselves seem sound. When it isn't churning out clones, Leagoo seems to do decent work. Whether that's enough to help Leagoo find its place in the world is a different story.Chris is Engadget's senior mobile editor and moonlights as a professional moment ruiner. His early years were spent taking apart Sega consoles and writing awful fan fiction. That passion for electronics and words would eventually lead him to covering startups of all stripes at TechCrunch. The first phone he ever swooned over was the Nokia 7610, because man, those curves.","For this iPhone clone maker, it's all about survival"
10021,3948177,2018-03-03 19:00:50,"For months now, much of the media attention on the crypto space has been directed at ebbs and flows in the price of bitcoin on one side, and whiz-bang ICOs on the other.The price of the most valuable cryptocurrency, Bitcoin (specifically the BTC chain), has backpedaled significantly from highs set in December 2017. The chart below shows pricing data from the CoinDesk Bitcoin Price Index (BPI) over the last 365 days.Those dramatic price swings write headlines. And the media, Crunchbase News included, has not been shy in covering bitcoin’s ups and downs.But the comparatively quiet and glacially paced world of traditional venture capital deserves no short shrift from reporters, market analysts and enthusiasts alike. At the time of writing, 2018’s venture fundraising totals alone are more than 40 percent of the way to 2017’s high water mark, according to Crunchbase data.And it’s been only around two months since the start of the year.But like all emerging technologies, and most nascent companies working on them, there’s no telling whether these bets will generate significant returns. Like with the very cryptocurrency mining computers hashing away at these blockchains, venture investment in this ecosystem may prove to be a waste of energy and a lot of hot air. But venture investors seem alright with buying equity during the dip.Here we’ll see how much venture money is being invested, by whom and where these venture-backed crypto companies call home.(Data) mining for insights into blockchain and blockchain-adjacent companiesTo avoid any complaints from so-called maximalist supporters of any one cryptocurrency or blockchain ecosystem, we’re going to base the following analysis on a fairly wide basket of companies.To learn more about the data set of companies we used for this article, skip to the bottom for notes on methodology. What follows is an analysis of the data that shakes out of our bundle of crypto companies.Despite all the market hype around ICOs, some of which have raised hundreds of millions of dollars, venture investment in blockchain and related companies has kept pace, as well.What’s captured here are just good ol’ fashioned venture rounds — convertible notes, seed and angel rounds, Series As and on through the alphabet — not the Wild West world of ICOs. The chart above excludes rounds labeled as ICOs, even if they had participation from VCs.The chart makes an important point: Despite price volatility in crypto-land’s most valued blockchain asset, bitcoin (specifically the BTC chain) venture investment — in terms of sheer dollar volume — is on pace to eclipse even the banner year of 2017.So we’ve seen how much is being invested, but which countries are leading the way?Listed headquarters of recently funded companies reveals legal trendsIn the chart below, we chart the location of the blockchain companies that raised venture funding in 2017 and 2018 so far.Two main features stand out from the chart above: venture fundraising activity in blockchain and blockchain-adjacent companies is highly concentrated in just a handful of countries, with the U.S. leading the way, and a small but growing percentage of companies are choosing to locate themselves in countries with friendly attitudes toward blockchain and cryptocurrency innovation.The two that stand out here are Singapore and Switzerland, each of which are home to (at least) four percent of the startups that raised venture funding over the last 14 months. Over the course of reporting on other stories, Crunchbase News has learned from investors and entrepreneurs that many Asia-focused blockchain companies and investors in Singapore and Hong Kong are increasingly attractive domiciles for Chinese firms leaving that country in the wake of regulatory crackdown. Japan and Malaysia are also popular locales in Asia for blockchain companies and funds, in part thanks to permissive regulatory environments.In Europe, Switzerland has been particularly progressive when it comes to clarifying policies around cryptocurrencies and blockchain technology. At CryptoCon in Chicago earlier this month, Brent Traidman, chief revenue officer for Zurich-based mobile wallet-maker Bread, referred to the country as “crypto valley.” Switzerland’s financial authority issued specific guidance to companies looking to raise capital in ICOs last week.As long as the regulatory environment for cryptocurrencies and other blockchain assets remains somewhat cryptic in the U.S., American crypto-entrepreneurs may opt to leave the country for clearer legal frameworks abroad.Notes on methodologyHere’s how we found the data we worked with.We first created a list of companies in Crunchbase’s bitcoin, ethereum, blockchain, cryptocurrency and virtual currency categories. Then we took the list of companies in Crunchbase’s data that have raised capital via an initial coin offering (a funding method better known by its initialism ICO). Finally, we created another list of companies that use those keywords, in addition to “digital currency” and “utility token” in their company descriptions.We then combined and de-duplicated the list to produce a data set of just over 2,900 blockchain and blockchain-adjacent organizations that we’ll use in our analysis. And, at least for the purposes of this article, we’re going to refer to these companies using some variation of that inelegant if quite inclusive phrasing: “blockchain and blockchain-adjacent.”",2018 VC investment into crypto startups set to surpass 2017 tally
10022,3948183,2018-03-03 10:01:05,"Life through Lens: inside Google’s plan to re-skin realityWhy ARCore could be the centre of search going forwardShares“It’s the ultimate try before you buy,” says Amit Singh, vice president of AR and VR at Google.He’s referring to the Porsche Mission E Concept that’s being driven around in front of us. But it’s not really there, it’s part of an application developed using ARCore, Google’s powerful software kit that allows developers to create augmented reality experiences for Android phones.Originally locked to the Google Pixel range, Google has opened up the SDK called ARCore 1.0. It’s now out of preview and comes complete with improvements and learnings Google has amassed since first announcing ARCore in August 2017 and before that with its Project Tango platform.“We have scaled it so that it is across 100 million phones and growing,” says Singh.At MWC 2018, it was included in the Samsung Galaxy S9 reveal and it’s available for other Samsung handsets, as well as some LG phones and the OnePlus 5. You can also expect it to come to Asus, Nokia, Huawei and ZTE phones in the near future.“We want AR to be a core feature on Android, at least for the high end phones and then over time for many more,” explains Singh.Scaling is all very well, but if the apps aren’t there it’s moot. Google knows this and that’s why it is looking to tap into different markets with Lens, in particular China.AR for everyone“We’re partnering with Huawei, Xiaomi and Samsung to bring ARCore to this market,” says Singh. Google in its blog post emphasises this, noting, ""Everyone should get to experience augmented reality.""From the demos TechRadar had, everyone should. If driving a car around wasn’t impressive enough, we ducked and got into the interior, analysed the thermodynamics of the vehicle, then color picked from the shirt Singh was wearing and gave the car a coat of paint in that shade.“The ability to re-skin reality is compelling here,” says Aparna Chennapragada, Product Lead of Lens. “But to even create that space you need to know what the semantic meaning is of what you are looking at, and that is why we started Lens.“Yes, we’re putting digital into the real world but we’re attaching semantic meaning to it. If you want to know something about an object, you tap into the rest of Google. The knowledge graph, maps, the whole shebang.""Lens is a powerful visual intelligence layer.”One of the most impressive showcases of this is the cutting-and-pasting of real-life text. Look around you, wherever you are there is some text. But, for obvious reasons, the text isn’t searchable.Google Lens changes this. We were shown this working on a menu - Lens allowed you to take that text, highlight it and explore certain words. It even worked, to some extent, with handwriting.Deceptively simple, incredibly powerful“All those years of understanding different text on web pages comes in really handy,” says Chennapragada. “On the web, you know something in a bigger font is important, something that starts a paragraph is another thing. We use all of that experience and apply it to being able to browse the real world.”“It sounds deceptively simple but it’s incredibly powerful,” continues Chennapragada. “Say, for something like Coachella - you have a lineup on a poster and you want to see what each of these bands are like. You can use Lens, immediately find them on YouTube, and instantly get a sense of who is playing there.”Other demos we were shown included a Snapchat Lens that allowed you to tour Barcelona’s stadium Camp Nou and an app that allowed you to furnish a room.It’s here where ARCore’s technology shows most promise - it doesn’t just have the ability to ‘add’ furniture to a room, but if there is real furniture around, it also has the ability know what that furniture is and may even know the model and make.Search has expanded from typing into a search box to speaking and it’s now clear that the next medium for Google is the camera.“We think about it in two parts: there’s ‘camera in,'” says Chennapragada. That’s what we think about Google Lens when you point the camera at something, make some sense of it, put some meaning to it - is it a building, a business card etc?Then there’s ‘camera out’ which is all of the AR stuff. So, we can put that sticker over there, place a virtual furniture couch in the real world. That is how we are thinking about both and how they come together.“For us that is the evolution of search and information discovery.”MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",Life through Lens: inside Google’s plan to re-skin reality
10023,3948185,2018-03-03 13:01:40,"Putting the 'real' into virtual reality: the teams building virtual worlds on HTC ViveFantastic HTC Vive experiences and the people who make themSharesHeading to the HTC Vive booth at MWC 2018 was a headset-wearing eye opener for anyone that’s followed the company and its journey into virtual reality.There were scores of developers showing their wares, each focused on delivering an entirely different VR experience that caters for their audience. Walking through it all was a bit like when James Bond enters Q’s lab.The one thread running through was the use of the HTC Vive and, in particular, its the Vive Tracker, which can turn almost anything into a VR controller.“We launched the tracker at last year’s MWC and had one or two examples of what it can do. But this year we brought many more examples,” says Graham Wheeler, Snr Director of VR at HTC.“We want to explain to people that, if you use your imaginations, what you can do with the Vive is basically limitless.”The key to this limitless imagination is that a tracker can be put on pretty much anything.We saw them strapped to arcade-style guns for a demo of zombie shooter Arizona Sunshine, on table tennis bats for some virtual sports fun and, our favorite, shin pads and football boots for the fantastic Mi Hiepa Sports, who have developed a coaching tool that helps footballers train while in rehabilitation.Ball skills“Mental health is a big thing for footballers that can’t train,” says Adam Dickinson, Development Director at the company. “We can help with that with our non-contact training drills and the HTC Vive.”The demo we tried used the second generation Vive trackers, and it was a fantastic experience. The responsiveness of the trackers was very impressive, with the feeling when you actually hit a ball appearing more real than virtual.The demo we tried was a simple passing exercise to hit the balls that came at us into the corresponding goals. The sensors meant that you could add quite a bit of complexity to your moves, including steadying the ball, stopping it with your feet then kicking it into the net.“Don’t forget to look at your feet when you’re playing,” instructed Dickinson before we even kicked a ball.“The sensors are so detailed, they can even respond to the length of the grass which changes how the ball moves.”Looking at our feet, we checked our studs to make sure all was okay - and the level of detail was stunning.It’s not just us saying that, the Manchester-based studio has the likes of Paul Scholes interested in the technology, and it’s currently installed in Manchester United’s training academy.Height clubSwapping boots for balloons, the Grand Canyon hot air balloon VR experience showed off the HTC Vive Pro with the wireless add-on, one of the first times the technology has been available to try out.""We have a child, adult and parent position, and we identify with the child,"" explains Audrone Miskinyte, a professor who is to start using this experience in her studies.""It's natural, spontaneous, curious, and fun-seeking, and this kind of child can help us overcome our biggest fears – the child can set an example, can encourage, and can distract, and help us overcome our fear.""Our untethered experience came in the form of the HTC Vive Focus - the headset that's currently only available in China at the moment. Regardless of its unavailability in the western market, Wheeler believes it shows off the best of what Vive has to offer.“People know that they will get the highest possible experience on each of the elements, when they use a Vive,” says Wheeler.“The Vive Focus offers six degree of freedom ‘world-scale’ tracking in a mobile setting so is the most premium experience on mobile.”After rolling around dodging shots and cowering behind some crates, we can attest to this.Coming into FocusThe HTC Vive Focus is a powerful device. It uses the same lenses (1,600 x 1,440 OLED display) as the HTC Vive Pro and is powered by a Snapdragon 835 processor. It’s mobile but doesn’t use a phone, which is a key differentiator when looking at it compared to something like the Samsung Gear VR. Launching first in China is a strategic move, explains Wheeler.“The Vive Focus is in a very different market. In China they don’t have Google or Facebook, so that market has different requirements. What we are trying to do is take the lead role in that environment,” he notes.“The Wave SDK, which is the open source SDK that the Focus is built on, isn’t just in HTC but in multiple manufacturers. So they can come and use our Vive Port and have different products and tiers and this allows us to get different developers on board which means we end up having great content.“That’s why we have launched it in China and we want to grow that. We are keeping a close eye on it for other markets but we are not announcing anything yet.”While HTC is keeping a close eye on China and the Vive Focus, everyone else is keeping a close eye on HTC. With mounting losses and persistent rumors of its Vive division being spun off, it is showcases like the one at MWC 2018 that may make the difference between success and failure for the headset.Wheeler is upbeat about what the future holds and he puts it all down to the tech innovations HTC is at the forefront at the moment.“Looking to the future, if you take AR, VR, 5G and AI - we are an expert in all of those fields and have a heritage in smartphones, the VR segment.""Tie all of those together and you have convergence. We see it as the Vive reality, where all of those experiences comes together.“It’s testament to how difficult it is to create the Vive experience [that there aren’t only a few trying to compete in the VR space].“We are setting a benchmark and people are trying to strive to that benchmark and that’s a brilliant scenario for us.”MWC (Mobile World Congress) is the world's largest exhibition for the mobile industry, stuffed full of the newest phones, tablets, wearables and more. TechRadar is reporting live from Barcelona all week to bring you the very latest from the show floor. Head to our dedicatedMWC 2018 hubto see all the new releases, along with TechRadar's world-class analysis and buying advice about your next phone.",Putting the 'real' into virtual reality: the teams building virtual worlds on HTC Vive
10024,3948257,2018-03-02 14:47:56,"TNW SitesToot toot: 6 top tips to tremendous traffic in 2018I’ve found in my entrepreneurial efforts, and when I’ve consulted and mentored other companies, that a great product or service will only take you so far. Without the right marketing tools in place to back it up, that great idea will likely go overlooked and forgotten, no matter how fresh or exciting an idea it might be.Unsurprisingly, one of the best ways to get great results for your startup is taking steps to improve your web traffic. But how?While quickly growing your web traffic may seem like a challenging task, it’s essential for entrepreneurs who are serious about increasing their sales and leads. So how can you take your website traffic to new heights in the coming year?Here are a few key (and simple) “hacks” that have made a difference for me and will help you get started:1) Fine-tune your siteThe time for New Year’s resolutions may be past, but anytime is an ideal time to evaluate your website and its strengths and weaknesses. After all, the quality of your site doesn’t just impact the user experience once someone arrives at your site — it can also affect where you end up in SEO rankings.Focus on the factors that search engines take into consideration when determining your ranking, particularly site speed, mobile optimization, and long-tail keyword implementation.To ensure that your pages load quickly, be on the lookout for large files (particularly photos and videos) that could be compressed or embedded.Look for any other areas that might impact your site’s usability or keep it from living up to Google’squality guidelines. Among others, tools like Google’s very own PageSpeed Insights make it easy to determine exactly where you need to improve.2) Become a guest bloggerNo — it’s no longer 2005 — but if you really wish to spread the word about your company, guest blogging is still one of the best ways to get started. This is one way I continue to spread the word for my own businesses.Even the most authoritative of third-party publishing sites typically allow you to link to your company’s website in your author bio — this presents a great opportunity to generate quality backlinks and inform new potential customers about your business.Of course, you shouldn’t try to write for just any website. Look for high-ranking sites that target your particular niche, as these are more likely to reach a wide readership.In general, you should avoid sites that seem spammy or have an overabundance of ads. Instead, look for sites with high levels of social interaction and a strong domain authority, as these will generate quality traffic. Use the “veteran test”: If an industry veteran has heard of a publication you already know you have a built-in audience.3) Let advertising automation tools do the ‘heavy lifting’If you can make room in your budget, turning over some of your more web-traffic related work to a third-party service (or freelancer) will deliver far greater results than you could achieve on your own.There are many cool tools and resources available in today’s market to help your company master the main channels that drive web traffic — you just need to know where to look.For example, one tool that I’ve found to be highly effective is StoreYa’sTraffic Booster, which creates PPC campaigns on Google and Facebook and automatically optimizes bids and keywords to drive quality customers to the relevant pages that will convince them to buy.Additionally, high-end resources likeSE Ranking help facilitate the SEO side of things by automating keyword research, backlink monitoring and other factors that have a direct influence on your search ranking.4) Update your listingsWhile your goal should be for your website to appear first when a potential customer searches for your brand, there’s no denying that third-party listings can also play a huge role in whether someone discovers your business in the first place.There are a lot of online directories out there, and you should be sure that individuals who find these listings are provided with accurate URLs and company info.Take the time to go through your listings on sites like Google, Yelp, Yahoo, FourSquare, Yellow Pages, and Bing. If you haven’t already “claimed” a listing for your business, now is the time to do so.This way, you can ensure that customers will receive accurate information (and website links), no matter how they come across your business. Better yet, it also allows you to monitor and respond to reviews.5) Step up your email marketingTargeting new customers is great, but your pre-existing client base is generally more likely to come back to your website and make additional purchases. They sometimes just need an extra reminder — and the best way to do that is through email marketing.Use email addresses you’ve collected from landing pages, social media, and paying customers to form tightly targeted lists. Focus the content of your emails based on where a particular user might be in your conversion funnel, providing them with useful information, engaging headlines, and of course, links to where they can learn more on your site.A properly targeted email can ultimately lead to more sales — emails reminding users of an abandoned shopping cart have animpressive 40.5 percent open rate, helping businesses recapture lost web traffic and sales.6) Provide free contentRegularly posting new blog content to your website can be a great way to attract site visitors — in fact, consistent blogging cantriple your inbound traffic. But blogging alone is just a first step.To really drive traffic to your website, you often need to provide some sort of additional incentive.Providing a free ebook or access to a special seminar will require more writing than a typical blog post. You’ll not only need to write the content; you’ll also need to make it visually appealing and spread the word via Facebook and other channels so people can discover it.However, when done right, these engaging forms of free content will help you build up your email lists, generate leads and create opportunities for new and recurring traffic.A fresh startEven if you haven’t achieved the web traffic results you were hoping for in the past, there’s no reason why 2018 can’t become a rousing success.As you use the above tips and tools to bring more potential customers to your website, you’ll soon be able to achieve the growth and profitability you’ve been working so hard to achieve.",Toot toot: 6 top tips to tremendous traffic in 2018
10025,3948258,2018-02-28 17:24:50,"Dive into this 16-book collection and you’ll be carried through the entire web development lifecycle. You’ll start with making sure you’ve got the basic understanding to know and use a variety of programming languages and tools like HTML5, CSS3, PHP, Git, and more.These books will also take you through the coding process, whether you decide to build with JavaScript, Rails, Python, or other options.Meanwhile, you’ll also tackle some of the hardware and integration issues that always exist for back-end web engineering specialists. Your training will cover integrating databases, optimizing performance, and learning the tools and methods used for networking security, including debugging and testing.Once you’re finished, these 16 books will have carried you end-to-end in the web creation process, a ridiculously valuable skill set for anyone seeking top-notch web talent.These eBooks would cost you $29 individually, which is why you shouldn’t sleep on this chance to get 16 books for the price of 1 — just $29 while the deal lasts.",Get a complete web development education for less than $5 per e-book
10026,3950646,2018-02-20 16:05:38,"The Tyranny of ConvenienceConvenience is the most underestimated and least understood force in the world today. As a driver of human decisions, it may not offer the illicit thrill of Freud’s unconscious sexual desires or the mathematical elegance of the economist’s incentives. Convenience is boring. But boring is not the same thing as trivial.In the developed nations of the 21st century, convenience — that is, more efficient and easier ways of doing personal tasks — has emerged as perhaps the most powerful force shaping our individual lives and our economies. This is particularly true in America, where, despite all the paeans to freedom and individuality, one sometimes wonders whether convenience is in fact the supreme value.As Evan Williams, a co-founder of Twitter, recently put it, “Convenience decides everything.” Convenience seems to make our decisions for us, trumping what we like to imagine are our true preferences. (I prefer to brew my coffee, but Starbucks instant is so convenient I hardly ever do what I “prefer.”) Easy is better, easiest is best.Convenience has the ability to make other options unthinkable. Once you have used a washing machine, laundering clothes by hand seems irrational, even if it might be cheaper. After you have experienced streaming television, waiting to see a show at a prescribed hour seems silly, even a little undignified. To resist convenience — not to own a cellphone, not to use Google — has come to require a special kind of dedication that is often taken for eccentricity, if not fanaticism.For all its influence as a shaper of individual decisions, the greater power of convenience may arise from decisions made in aggregate, where it is doing so much to structure the modern economy. Particularly in tech-related industries, the battle for convenience is the battle for industry dominance.Americans say they prize competition, a proliferation of choices, the little guy. Yet our taste for convenience begets more convenience, through a combination of the economics of scale and the power of habit. The easier it is to use Amazon, the more powerful Amazon becomes — and thus the easier it becomes to use Amazon. Convenience and monopoly seem to be natural bedfellows.Given the growth of convenience — as an ideal, as a value, as a way of life — it is worth asking what our fixation with it is doing to us and to our country. I don’t want to suggest that convenience is a force for evil. Making things easier isn’t wicked. On the contrary, it often opens up possibilities that once seemed too onerous to contemplate, and it typically makes life less arduous, especially for those most vulnerable to life’s drudgeries.But we err in presuming convenience is always good, for it has a complex relationship with other ideals that we hold dear. Though understood and promoted as an instrument of liberation, convenience has a dark side. With its promise of smooth, effortless efficiency, it threatens to erase the sort of struggles and challenges that help give meaning to life. Created to free us, it can become a constraint on what we are willing to do, and thus in a subtle way it can enslave us.It would be perverse to embrace inconvenience as a general rule. But when we let convenience decide everything, we surrender too much.Convenience as we now know it is a product of the late 19th and early 20th centuries, when labor-saving devices for the home were invented and marketed. Milestones include the invention of the first “convenience foods,” such as canned pork and beans and Quaker Quick Oats; the first electric clothes-washing machines; cleaning products like Old Dutch scouring powder; and other marvels including the electric vacuum cleaner, instant cake mix and the microwave oven.Convenience was the household version of another late-19th-century idea, industrial efficiency, and its accompanying “scientific management.” It represented the adaptation of the ethos of the factory to domestic life.However mundane it seems now, convenience, the great liberator of humankind from labor, was a utopian ideal. By saving time and eliminating drudgery, it would create the possibility of leisure. And with leisure would come the possibility of devoting time to learning, hobbies or whatever else might really matter to us. Convenience would make available to the general population the kind of freedom for self-cultivation once available only to the aristocracy. In this way convenience would also be the great leveler.This idea — convenience as liberation — could be intoxicating. Its headiest depictions are in the science fiction and futurist imaginings of the mid-20th century. From serious magazines like Popular Mechanics and from goofy entertainments like “The Jetsons” we learned that life in the future would be perfectly convenient. Food would be prepared with the push of a button. Moving sidewalks would do away with the annoyance of walking. Clothes would clean themselves or perhaps self-destruct after a day’s wearing. The end of the struggle for existence could at last be contemplated.The dream of convenience is premised on the nightmare of physical work. But is physical work always a nightmare? Do we really want to be emancipated from all of it? Perhaps our humanity is sometimes expressed in inconvenient actions and time-consuming pursuits. Perhaps this is why, with every advance of convenience, there have always been those who resist it. They resist out of stubbornness, yes (and because they have the luxury to do so), but also because they see a threat to their sense of who they are, to their feeling of control over things that matter to them.By the late 1960s, the first convenience revolution had begun to sputter. The prospect of total convenience no longer seemed like society’s greatest aspiration. Convenience meant conformity. The counterculture was about people’s need to express themselves, to fulfill their individual potential, to live in harmony with nature rather than constantly seeking to overcome its nuisances. Playing the guitar was not convenient. Neither was growing one’s own vegetables or fixing one’s own motorcycle. But such things were seen to have value nevertheless — or rather, as a result. People were looking for individuality again.Perhaps it was inevitable, then, that the second wave of convenience technologies — the period we are living in — would co-opt this ideal. It would conveniencize individuality.You might date the beginning of this period to the advent of the Sony Walkman in 1979. With the Walkman we can see a subtle but fundamental shift in the ideology of convenience. If the first convenience revolution promised to make life and work easier for you, the second promised to make it easier to be you. The new technologies were catalysts of selfhood. They conferred efficiency on self-expression.Consider the man of the early 1980s, strolling down the street with his Walkman and earphones. He is enclosed in an acoustic environment of his choosing. He is enjoying, out in public, the kind of self-expression he once could experience only in his private den. A new technology is making it easier for him to show who he is, if only to himself. He struts around the world, the star of his own movie.So alluring is this vision that it has come to dominate our existence. Most of the powerful and important technologies created over the past few decades deliver convenience in the service of personalization and individuality. Think of the VCR, the playlist, the Facebook page, the Instagram account. This kind of convenience is no longer about saving physical labor — many of us don’t do much of that anyway. It is about minimizing the mental resources, the mental exertion, required to choose among the options that express ourselves. Convenience is one-click, one-stop shopping, the seamless experience of “plug and play.” The ideal is personal preference with no effort.We are willing to pay a premium for convenience, of course — more than we often realize we are willing to pay. During the late 1990s, for example, technologies of music distribution like Napster made it possible to get music online at no cost, and lots of people availed themselves of the option. But though it remains easy to get music free, no one really does it anymore. Why? Because the introduction of the iTunes store in 2003 made buying music even more convenient than illegally downloading it. Convenient beat out free.As task after task becomes easier, the growing expectation of convenience exerts a pressure on everything else to be easy or get left behind. We are spoiled by immediacy and become annoyed by tasks that remain at the old level of effort and time. When you can skip the line and buy concert tickets on your phone, waiting in line to vote in an election is irritating. This is especially true for those who have never had to wait in lines (which may help explain the low rate at which young people vote).The paradoxical truth I’m driving at is that today’s technologies of individualization are technologies of mass individualization. Customization can be surprisingly homogenizing. Everyone, or nearly everyone, is on Facebook: It is the most convenient way to keep track of your friends and family, who in theory should represent what is unique about you and your life. Yet Facebook seems to make us all the same. Its format and conventions strip us of all but the most superficial expressions of individuality, such as which particular photo of a beach or mountain range we select as our background image.I do not want to deny that making things easier can serve us in important ways, giving us many choices (of restaurants, taxi services, open-source encyclopedias) where we used to have only a few or none. But being a person is only partly about having and exercising choices. It is also about how we face up to situations that are thrust upon us, about overcoming worthy challenges and finishing difficult tasks — the struggles that help make us who we are. What happens to human experience when so many obstacles and impediments and requirements and preparations have been removed?Today’s cult of convenience fails to acknowledge that difficulty is a constitutive feature of human experience. Convenience is all destination and no journey. But climbing a mountain is different from taking the tram to the top, even if you end up at the same place. We are becoming people who care mainly or only about outcomes. We are at risk of making most of our life experiences a series of trolley rides.Convenience has to serve something greater than itself, lest it lead only to more convenience. In her 1963 classic, “The Feminine Mystique,” Betty Friedan looked at what household technologies had done for women and concluded that they had just created more demands. “Even with all the new labor-saving appliances,” she wrote, “the modern American housewife probably spends more time on housework than her grandmother.” When things become easier, we can seek to fill our time with more “easy” tasks. At some point, life’s defining struggle becomes the tyranny of tiny chores and petty decisions.An unwelcome consequence of living in a world where everything is “easy” is that the only skill that matters is the ability to multitask. At the extreme, we don’t actually do anything; we only arrange what will be done, which is a flimsy basis for a life.We need to consciously embrace the inconvenient — not always, but more of the time. Nowadays individuality has come to reside in making at least some inconvenient choices. You need not churn your own butter or hunt your own meat, but if you want to be someone, you cannot allow convenience to be the value that transcends all others. Struggle is not always a problem. Sometimes struggle is a solution. It can be the solution to the question of who you are.Embracing inconvenience may sound odd, but we already do it without thinking of it as such. As if to mask the issue, we give other names to our inconvenient choices: We call them hobbies, avocations, callings, passions. These are the noninstrumental activities that help to define us. They reward us with character because they involve an encounter with meaningful resistance — with nature’s laws, with the limits of our own bodies — as in carving wood, melding raw ingredients, fixing a broken appliance, writing code, timing waves or facing the point when the runner’s legs and lungs begin to rebel against him.Such activities take time, but they also give us time back. They expose us to the risk of frustration and failure, but they also can teach us something about the world and our place in it.So let’s reflect on the tyranny of convenience, try more often to resist its stupefying power, and see what happens. We must never forget the joy of doing something slow and something difficult, the satisfaction of not doing what is easiest. The constellation of inconvenient choices may be all that stands between us and a life of total, efficient conformity.Tim Wu is a law professor at Columbia, the author of “The Attention Merchants: The Epic Struggle to Get Inside Our Heads” and a contributing opinion writer.",Opinion | The Tyranny of Convenience
10027,3950889,2018-03-02 16:21:55,"TNW SitesHow to restore brand credibility after having fucked up publiclySocial media is a place for brands to amplify their stories, humanize themselves, and connect with the people that make them what they are. More recently, it’s also become a stage for consumers to take brands to task for their missteps. As a social strategist and creative on the agency side for the better part of a decade, I’ve seen my share of digital crises and the aftermath.Social has ignited the rise of a culture where bad actors are called out and negative experiences are no longer discussed behind closed doors. Given the nature of social as such an accessible, public channel for consumers to demand accountability and resolution, a single call-out can snowball into a reputation crisis for a brand that isn’t prepared.If anything is crystal clear to me, it’s that call-outs don’t happen in a vacuum. The impact of call-out culture echoes to every aspect of a business, from consumer sentiment to revenue. A single negative post has the potential to reach thousands of people in a matter of seconds, on a truly global scale. If your brand isn’t prepared to react quickly in the face of today’s viral culture, you’re effectively risking your reputation and opening up your bottom line to long-term damage.It starts with accountabilityIn the midst of a brand crisis it’s easy to see the merit in defending your position. But 55 percent of consumers aren’t looking for loaded explanations or defensiveness, they’re looking for an apology and a solution. Reactively explaining, rather than actively fixing, keeps your brand in the hot seat and can lead to lasting negative impressions of your business. A slew of brands are making concerted efforts to move toward humanizing their corporate and social entities, so it makes sense that pushing policy-focused responses and stock language rather than addressing and solving a public problem can rub consumers the wrong way. In most cases, a company needs to respond immediately, be transparent and take action.When Pepsi experienced the PR crisis heard ‘round the world — treading into political waters with a commercial that attempted to speak to its millennial audience — the social reaction was swift. After an immediatebarrage of tweets calling the company out for a seemingly tone-deaf message, the ad was pulled within 24 hours of being released.Pepsi was careful to respond not with defensiveness, but with a plain apology. “Clearly we missed the mark, and we apologize,” Pepsi wrote the following day. A simple media statement that was honest, remorseful, and human.In my experience, the answer to moving forward from a brand nightmare starts with business operations rather than PR damage control. Instead of leaning on a communications team to spin the issue, work toward correcting the issue from within — and immediately follow with an honest, authentic public statement about the importance and execution of those efforts. When the media calamity subsides, the public will remember your response and your effort (or lack thereof) to rectify.Good social care is good businessWe all saw what happened to United last year. It wasn’t pretty and the numbers reflected that. It was estimated that United Airlines’ stock dropped by 6.3 percent within a day following video footage of a passenger being forcibly removed from a flight going viral across social media. That’s a $1.4 billion loss in the company’s value. That’s chilling, but devastating results like this can be mitigated when brands leverage social to put a proactive customer care strategy into effect.Data from Twitter shows that customer care on their platform is the key ingredient to boosting consumers’ willingness to spend. The data shows how a simple response when a customer reaches out to your business increases that consumer’s odds of spending three to 20 percent more with you in the future.Responding not only boosts overall satisfaction, but a consumer’s willingness to spread the word. Customers are 44 percent more likely to share their positive experiences and 30 percent more likely to recommend your brand after receiving a response on social.Rebuilding reputationSocial has never been a more influential tool in consumer decision-making. McKinsey surveyed 20,000 consumers across Europe and found that social recommendations are behind more than a quarter of all purchases. Momentum on social has a ripple effect. Most consumers say they’ll share others’ complaints or join in with their own experiences, setting the stage for the shift in consumers’ buying decisions.So use a crisis as an opportunity to reevaluate everything, from product and advertising to brand strategy. In the wake of Samsung’s Galaxy Note 7 recall, the company launched a series of apologetic ads discussing the safety and quality testing of their latest product release, as well as releasing investigation details into the faulty devices. This proactive commitment to transparency seems to have paid off, as the company was able to set record profits that quarter, despite the recall.An interesting learning opportunityMissteps happen in business. A social call-out can be the canary in a coal mine that illuminates a larger issue within your organization. For United, we saw it spur a revision of corporate policy and employee training around the rebooking process; for Samsung, we saw a renewed commitment to enhanced quality testing and the public accountability around that process.A PR blowout on social isn’t the end of your reputation. A leader is always willing to seek out areas to improve and digital crises like these offer us exactly that opportunity. Leverage what you learn about your customers’ experiences through the lens of the call-out, acknowledge and right the issue, and leave the public with a better impression of your brand.",How to restore brand credibility after having fucked up publicly
10028,3950890,2018-03-02 15:42:02,"TNW SitesCryptocurrencies can boost India’s digital ambitions — here’s howThe humble beginning of cryptocurrencies in India was back in 2013, when a vintage pizzeria in Mumbai started accepting Bitcoin. Since then, Indians’ interest in crypto has only increased, leading to the emergence of many Indian crypto exchanges and a number of businesses that accept cryptocurrencies. Considering there are more than 600 million young Indians, it’s palpable that adoption of technology and global trends (including crypto investment) happens at a fairly healthy rate.However, the Indian economy runs on top of complex financial regulations which has far-reaching impact on the crypto investor sentiment in the country. In the midst of complex regulations and inherently volatile market, the crypto scene thrives and comes with additional India-specific issues.In this piece I’ll show you the nuances of the Indian market and why virtual currencies can actually accelerate the growth of digital India by working seamlessly with something known as ‘India Stack.’Cloudy with a chance of legitimacyAccording to a recent IT survey, the Indian crypto market is adding 200,000 tech-savvy traders every month and in the last 17 months, $3.5 billion worth of transactions have been conducted. Looking at the massive growth, it’s imperative that the stakeholders are eagerly waiting for a clear stance from the government.While different countries have taken both positive and negative decisions on cryptocurrencies, in India the apex financial regulator, i.e., the Reserve Bank of India has been issuing warnings to investors since December 2013 — the year the Mumbai pizzeria started accepting Bitcoin.The underlying theme has remained the same until today, there can be severe security and financial risk associated with cryptocurrency investment and trading. Therefor the government might not be able to protect the consumers who are exposed to this volatile market.The finance minister, Mr. Arun Jaitley has confirmed that the virtual currencies are not legal tender in India — this means that right now cryptocurrencies cannot be used as an alternative to INR. Hence, although there’s no ban on trading, the crypto market remains unregulated without any legal framework.But recently there was new development in cryptocurrency legislation in India, the Union Budget of 2018 presented on February 1has the following key takeaways :The government will proactively take steps to curb illegal activities that are driven by cryptocurrenciesThe government will also explore use of distributed ledger technology as part of Digital India initiativeThis is in line with the recent news that India is planning to regulate trading of digital currencies — an important step to stop unlawful acts. It’s quite crucial since India is already fighting issues like cross-border terrorism and the government is wary of the possibility that black money is being channeled into the crypto market which further fuels such type of danger. Therefore, the new law will have two-pronged agenda, i.e., establish guidelines so that the source of money can be tracked and common investors can be protected.It’d be worthy to note that in India all the crypto exchanges are already KYC compliant and capital transfer happens via digital banking. This is in quite contrast to South Korea which recently implemented the regulations to use the same name in both the bank and the exchange.Virtual currency are great for India StackIndia Stack is an ambitious project for 1.3 billion Indians which has been described as the following according to the official website:“IndiaStack is a set of APIs that allows governments, businesses, startups, and developers to utilize an unique digital infrastructure to solve India’s hard problems towards presence-less, paperless, and cashless service delivery.”They key for us is the “cashless service delivery” part and the unique ID system on top which India Stack operates, i.e., Aadhaar — the world’s largest biometric ID system which has enrolled more than 1.2 billion Indians. Since its inception, it’s become absolutely seamless for any Indian to open a digital bank account and link the existing account to the ID as well.Although this is the platform on which the digital banking system and payment network is getting built, in order to become truly cashless what we’ll need is a blockchain-powered cryptocurrency that has approval of the Government.This will bring about a drastic change in the way financial transactions are conducted and lead to complete eradication of corruption — including money laundering and tax evasion. Apart from that, the large consumer base which is moving towards digital economy will get the benefit of improved online transactions. This would also remove the surcharges levied on card payments resulting in benefits for both small and medium businesses and consumers.Perhaps this is not far from us — back in November 2017, there were news around introduction of Laxmi (Goddess of wealth) coin which would be legally approved by the Indian government. The official website of the coin doesn’t give any date of launch, other than the statement that it will be available soon. Nothing has been confirmed by the government as well.Moreover, we already have examples of countries adopting crypto coins — Arizona senate has passed bill to accept income tax payment via cryptocoins and Russia is about to issue CryptoRuble. Apart from that, Venezuela has already launched state-sponsored coin called Petro to allow people to pay for taxes and public services. Although it’s argued that the petrol-backed coin has emerged as a means to counter the looming economic crisis, its utility in a government setup is a great example.The futureThe very nature (peer-to-peer) of these coins make them somewhat insulated from the purview of the government. So instead of remaining silent, the government needs to clearly define the asset class of cryptocurrency — a commodity, an asset, or a security apart from devising regulatory framework around trading and investment. This will help the government gain additional capital from tax and embrace something that has potential to boost the GDP as well.Considering that the blockchain-based technologies are rapidly growing and they are well poised to disrupt several traditional industries, the government needs to craft a conducive environment that can foster economic decentralization, social parity, and better access to global financial market.",Cryptocurrencies can boost India’s digital ambitions — here’s how
10029,3954013,2018-03-04 01:05:20,"Tina Sharkey has something to sell you (300 things, actually)0Brandless is an usual company. A direct-to-consumer purveyor of food, beauty, and personal care products, it says that every item it makes is non-genetically modified, kosher, fair-trade, gluten-free, often organic and, in the case of cleaning supplies, EPA “Safer Choice” certified. They are also priced at $3 across the board. The idea, says cofounder and CEO Tina Sharkey, is to “democratize better.” She believes that Brandless — which is very much a brand — is selling items to people, often with dietary restrictions, who “couldn’t shop their values” before Brandless.That’s no small thing to Sharkey, who cares very much about Brandless’s customers, as anyone who has seen her speak publicly can attest. In fact, Sharkey, appearing at a StrictlyVC event earlier this week, spoke about the importance of shared principles in sweeping language that elicited fervor in many of the gathered listeners — and some fatigue in others.She talked of Brandless users who “didn’t have access before” to affordable gluten-free and organic products or “who had to drive 100 miles round trip” or who “didn’t know things existed like tree-free toilet paper, made with sugar cane and bamboo grasses.” (This last product was news to us, too.)Sharkey — who has led a number of consumer-facing companies in her career, including cofounding iVillage and later serving as president and CEO of BabyCenter — said she sees in Brandless users “all of America,” not just those who “live in such a frickin’ bubble on the coasts.”Elites in East and West Coast cities are “not our country” alone, she continued. “Our country is filled with extraordinary people, and we have bifurcated and sliced and diced and segmented people to such a degree that we’ve forgotten that we’re all awesome Americans, and American deserve better, no matter your politics.”If it was hard to remember at times that she was talking about a company that sells nearly 300 household items, from maple syrup to fluoride-free toothpaste, the crowd didn’t seem to notice, nodding along in agreement.Sharkey doesn’t reveal much publicly about revenue or user or growth numbers, though in fairness, it’s early days. She prefers talking instead about the roughly 70 percent savings that Brandless says it provides customers compared with more established brands of similar quality, whose goods are usually purchased on retail shelves. Brandless calls this mark-up a “brand tax” and has trademarked the term.Sharkey is also quick to note what else Brandless does for its customers. For example, in addition to selling affordable products that it says are better for users, Brandless has partnered with the charitable organization Feeding America, a nationwide network of 200 food banks that’s trying to fight hunger in the United States. When customers check out, they are informed that they’ve just purchased a meal for someone, which, according to a footnote on Brandless’s website, is the equivalent of just 9 cents per order.Sharkey is finding other ways to shape Brandless’s positive narrative, too, including Brandless Life, a content-rich initiative that’s currently in beta and designed to both keep shoppers engaged and sell them more products.As Sharkey explained it, the company already has at its fingertips a lot of data to put to further use, including about what attracts visitors, how often they return to shop, and what drives them to either try new things or replenish items they’ve purchased in the past. Why not use it to boost sales?By way of illustration, snacks falls into the “expandable consumption” category, Sharkey said, adding, “If I send you a huge box of snacks, you’re likely to eat them or share them at the office or soccer game.” On the other hand, she’d said, “If I send you a huge box of our peppermint mouthwash” — which Sharkey observed is “alcohol- and sulfate-free and just $3” — a shopper “is not likely to gargle more.”Largely, it’s such “one-and-done” products that Brandless believes it can sell more of, including by proposing new uses for them via breezy articles and videos. As it pertains to that mouthwash, said Sharkey, “Did you know you can clean your washing machine with it? Did you know you could soak paper towels with it and put it at the bottom of your garbage?” Users will soon, she suggested.Of course, whether Brandless succeeds or fails will ultimately depend on the quality of its products and how many people it persuades to try them. For all of Sharkey’s talk about community and content and Brandless’s mindfulness about those in need, the company’s Brandless products better taste and perform better than everyone else’s on the market in a similar price band.Roughly $50 million in funding from investors, which Brandless quietly closed before hitting the market last summer, should help get them there.But Sharkey is a powerful force, too. Though we would have preferred learning more about the company’s inner workings — and its challenges — Sharkey’s marketing approach, her avowed belief that sales come from “understanding people, first and foremost” and her emphasis on “connecting people around their affinities and passions,” may well be a strategy that pays.More than an hour after wrapping up her talk, numerous attendees could still be overheard singing Sharkey’s praises. “Ugh, I just loved Tina’s talk,” one of them told us on her way out the door. “I hadn’t even heard of Brandless until tonight. I’m definitely buying something from that woman.”","Tina Sharkey has something to sell you (300 things, actually)"
10030,3954015,2018-03-03 18:00:02,"Most of us tend to think people trying to open the door when we’re away want to steal stuff. Amazon.com would like to change that assumption.The e-commerce giant shelled out a reported $1.1 billion this week to acquire Ring, a developer of Wi-Fi-connected door bells. The move follows the rollout last year of Amazon Key, a smart lock and camera system for in-home deliveries to customers who don’t want packages sitting outside.But Amazon’s not the only one placing high value on front-door tech. In the age of on-demand delivery, service providers have long seen the typical low-tech door as an impediment to expansion. Smart-lock developers and home security companies also have been putting resources into the space, alongside a number of venture investors.As we reported a few months ago in an overview of smart-lock and building access investments, the way people open the front door hasn’t changed much in a century. Most of us still get in by turning a flat metal key into a lock. Visitors ring old-fashioned door bells. And there’s commonly no digital record of who came or why.There’s a lot of money going into changing that status quo. Since last year, venture investors have poured more than $200 million into an assortment of companies with businesses and technologies tied to keys, locks and building access. To date, those companies have more than $500 million, according to an analysis of Crunchbase funding data.Ring, which raised $200 million in venture funding, is one of two heavily funded companies to be acquired in recent months. In October, August Home, a smart-lock developer that had raised more than $70 million in venture funding, sold to ASSA ABLOY Group, the largest global supplier of door opening products, for an undisclosed sum.There’s good reason to think the up cycle for smart locks and entry systems has further to go. For one, to a large degree, lock and building access-related investments are an extension of the connected home space, and growing adoption of connected home systems provides a major entry point for key and door tech.Many legitimate businesses also want an easier way to get through the door. After all, delivering things to customers’ doors, either inside or out, constitutes an industry with a collective valuation in the trillions of dollars. Amazon.com alone is valued at more than $700 billion.Additionally, anecdotally, it does appear urban humans over recent decades are trending toward greater laziness. Increasingly, people want their laundry, meal kits, produce, shaving supplies and pet treats delivered to their doors. Just as we used to think of going to the store as a pain, we will soon consider stepping outside to pick up a package as the height of inconvenience.Those trends might not be great for the state of humanity. But they are bullish indicators for the smart lock industry.","Front-door tech is hot, and it’s not just Amazon who wants in"
10031,3954093,2018-03-02 15:40:08,"TNW SitesBuild and run a Cisco network — and have all the credentials you’ll need to get hiredCisco Systems is on a roll…and for a company that already dominates its market sector, that’s saying something. Following a stellar earnings report, analyst Nomura Instinet encouraged stock buyers to get in on Cisco earlier this month. Emboldened by Cisco’s ethernet switch market share growth from 49 to 53 percent late last year, the analyst predicts Cisco’s surge should continue well into next year.So, if Cisco already supplies the equipment and infrastructure running a vast percentage of the world’s networked systems AND is poised to grow even larger, now would be great time for an IT expert to get very familiar with Cisco and their offerings.Over these nine courses, you’ll delve deep into everything it takes to build, install and manage a network driven by Cisco devices. That knowledge comes with nine Cisco-approved CCNA and CCNP certifications to prove your abilities within that tech environment.Whether it’s hardware compatibility, system mapping, or quick troubleshooting, this training will put you in a position to handle a Cisco-driven network…and make yourself a shoo-in for any job requiring Cisco mastery.A nearly $3,300 value, you can lock in this training at a rock-bottom rate with this limited time deal — only $59.",Build and run a Cisco network — and have all the credentials you’ll need to get hired
10032,3956497,2018-03-03 23:37:36,"How Not To Be A CrankIf you ever wanted to see me dressed as someone whose name is probably Jimmy Stab-Pants or Joey “Crime-Nose” Faffalone, now’s your chance.The above hideous depiction is from an article in Science Magazine, introducing my work with Nick. We investigate errors in science, usually social science. Not a lot of people do this work, and we’re quite good at it.We have different styles, but it works. Nick is horrified of making mistakes, because he is English and decent. I am horrified of making mistakes because I am a massive egotist from a faraway planet, high on drugs that have not been invented yet.Anyway.The following is how we avoid being flung, headless and twitching, into the wicker crank basket with the rest of the online monstrosities.Nick, Introductory Note:I’m glad that James asked me to help write this piece.Ever since I got started in the debunking business, my concern has been how not to be lumped in with the ‘wall of gammon’, a.k.a. all the other shouty 50-something white men on the Internet complaining about facts.I haven’t always done as well as I could have at avoiding snarky public comments and over-hasty judgement, but hopefully I’m getting better. My very strong fear of making mistakes (and my inability to feel anything other than extremely bad when I do, even if it doesn’t impact other people very much) probably helps with this.P.S. My head actually does look like that.IntroductionIf you want to be a scientific critic, the primary problem you must overcome is that criticism is hard. When you criticize science in public, you are taking a complicated argument (what you’ve discovered), to people who don’t care very much (the journal where the offending article was published), about the work of someone who wishes you’d shut up (the authors).This can be difficult to navigate. Although it’s often ‘a complete pain in the taint’ more than just ‘difficult’.As much as we like to tell ourselves that science is self-correcting, this phrase really means science contains mechanisms that allow correction, rather than scientific problems will eventually fix themselves via some kind of intellectual osmosis.However, we think we’ve done reasonably well at managing this process so far.The most telling paragraph in Science’s description of our work was:Despite the charged nature of their work — after all, careers can be on the line — Brown and Heathers have attracted surprisingly little criticism from their peers in science. In part, that’s likely because of their strategy of gently but methodically ratcheting up the pressure on authors and journals.We have made arguments that people have listened to. More than occasionally, these arguments have resulted in a correction of the scientific record, and that’s something that allows people to take future criticisms more seriously. It’s generative — be useful and serious, and people will give you license to continue to be so.In other words, people do not think that we are cranks.More importantly, the people who need to be convinced before the formal scientific record can be corrected do not think we are cranks.You can imagine that might help a smidge.“Crank” is a pejorative term used for a person who holds an unshakable belief that most of his or her contemporaries consider to be false.[1] A crank belief is so wildly at variance with those commonly held as to be considered ludicrous. Cranks characteristically dismiss all evidence or arguments which contradict their own unconventional beliefs, making any rational debate a futile task and rendering them impervious to facts, evidence, and rational inferenceThat is, we are not fault-finding where no faults exist, we are not misrepresenting things, we are not dismissing evidence, we are not rude (or, at least, not so rude that the police have been involved), and we are certainly convince-able. We convince each other all the time.A: “Hey, XYZ has a bunch of problems”*five minutes pass*B: “You have missed something very obvious, and I don’t know how your wife puts up with you.”We’ve both been both A and B.Here’s something interesting though: we’ve seen quite a lot of scientific criticism from other people by now, from a variety of angles. And quite often it doesn’t really have the same … cut-through.Specifically, plenty of criticism exists that one or both of us think is potentially or probably valid, and it ends up being totally ignored. The reason for that is generally the delivery — it is where crankery meets dickery.So, while this article is called How Not To Be A Crank, it might be better titled How Not To Be Seen As A Crank, or simply How Not To Be A Dick.Basically, if you’re right and everyone hates you, you’re ineffective.Oh, you can hear the retorts now:“BUT I’M RIGHT! “Yeah, and no-one cares. Do you want to be right at home by yourself, or would you prefer your opinions result in outcomes outside of your apartment?“BUT I AM A HARSH AND UNCOMPROMISING CRITIC”No-one wants to tell you how to feel, or how to be, this is simply a series of points that might help other people care. Maybe even as much as you do.So.Let’s say you think you’ve found a serious problem with a scientific paper or argument, and you’d like to tell people. Here’s how to not be a crank about it:1. Calm. Down.No-one else thinks your problems are as important as you do. Bellowing them, as much as it might be tempting, will shut down everything.It’s amazing this needs to be said, but everything you do will be seen through an emotional prism if you bring a skinful of raw emotion to the table. If you’re highly emotional, people will just assume you’re irrational.Now we’ve got that out of the way…2. Don’t be a dick.Scientific criticism does not exist in a social vacuum. Rather, people exist at every layer of the critical process. Criticism of ideas they publish, represent, or promote will hurt their feelings and their bottom line. Period.This is why you should try not to make it worse by criticizing the people involved too much. It is very hard to avoid calling someone dishonest if they display a continual pattern of totally demonstrable dishonesty, because they probably are. It’s just usually not helpful.So, think what you think, but don’t do it in public. Keep the trash talk for Skype, Twitter, or Facebook DM group where you hang out with your fellow destructo-critics. Let off the steam when you have to. You may deal with venal, terrible, dissembling people who drop dirty underpants in the soup of human progress, but put on your best Pan Am smile, and write the story up for the public with as much professionalism as you can muster.What is infinitely worse than the above, though, is when remarks are personal and gratuitous. Dickery is unnecessary — whisper campaigns, rumour mongering, ‘anonymous notes’ and blackballing aren’t illegal but they are still dickish.And death threats (and trolling, and weird online gender-flavoured violence) are not just deeply unpleasant, they’re CRIMES. If you do this, you are breaking the law AND being a shit human being, and you should either stop or walk yourself into the sun.On a selfish level for us, at least, this sort of behaviour is also counter-productive. Your behaviour is an immediate invitation for the targets of criticism to paint us all with the same brush. Of course this painting is wretchedly dishonest, but you’re practically INVITING them to throw every objection they receive overboard because a small proportion of them contain dickishness. Basically, you’re defanging real criticism because you’re behaving like a child.Frankly, those of us working in error detection / meta-science topics would like to round up anyone giving ammunition to the “all critics are mad dogs” argument, and have them pushed into a giant fan.Now, after these two, a lot of the heavy lifting is done. The rest is just candy, but important candy nonetheless.3. Pick one font. Make it black.What is it with frothing anger and three fonts with red highlighting? Did you transcribe this from your initial notes in crayon? Pick a normal format for communication, one which doesn’t look like you spend the rest of your time fingerpainting, and stick to it.4. Write complete sentences. Not too long.What it is with crankery and the inability to write anything but run-on sentences? If you look like you can’t write, you won’t be read. If you know how to write academic language, why would you abandon that now — at the precise time that you need someone to take you seriously?5. Make friends“I am a warrior from the frozen north, and I bow to no master!”Great. How very masculines of you. When you’re quite finished, admit your weaknesses and make some friends.If you are not in academia, this is even more important. Having a PhD conveys gravitas, as does a .edu e-mail address. Is that fair? Maybe. Either way, if you don’t have either, many people will assume by default that you are a crank, especially in fields that attract controversy.But: if you can legitimately say that Dr. Marvin Bigshotte of The University For Special Clever Chaps has read your work, the editor may give your complaint a hearing.However, if you can’t find anyone in academia who’ll take you seriously, ask yourself why.Hint: It almost certainly isn’t a conspiracy against your bold and trenchant political stance. Very few academics are so corrupt that they won’t take half-decent scientific arguments into consideration.This especially helps when you have to…6. Ask someone you trust if you’re making senseThe midnight hour makes us all a bit weird, and if you’re up late hounding some problem you’ve dug out of a manuscript you’ve found, well, you might have lost the plot completely. The best indication that you’re making sense comes from other people. Ask them before you send things to the wider world.We’ve both made some severe errors in public by pointing out “obvious” errors in cases where we had simply missed that the authors had, in fact, taken everything into account. You can get away with this once or twice, but you need to keep your hit rate WAY above 50% to be taken seriously.The irony is: most of you have forgotten about these. But we haven’t. And you can’t police yourself. Enlist the help of people who’ll tell you you’re wrong.7. Don’t CC the entire worldIf you get a reputation as an iconoclast, even if it’s a sub-Z-grade reputation, people will start to treat you as a banner carrier. This means, apparently, that in any given dust-up it’s OK to suddenly cc you into random email chains which are the length of a Tolstoy novel and about as much fun.This is rude, and odd. The response to you doing this will be “I don’t know the first thing about the topic you’re roping me into, I don’t know you, and I don’t owe you anything.”Suddenly inserting members of the public into methodological criticism to be a passive audience doesn’t give you any cachet, it just grots up inboxes.8. Accept it when no-one cares, and move on.So you have a problem with something someone published, and you tried to get someone else to care, and they don’t. What should you do now? If your answer is SCREAM LOUDLY FOR YEARS then you fail. Minus ten points from Yellingdor.Basically, if you start by prosecuting your original arguments ineffectively, no-one will listen when you send them your 27th email.There’s a certain arrogance to this: you alienate people, and then having done that, you insist that they listen to you. There’s no engagement in these emails, they are just screaming I AM RIGHT over and over again.The moment someone thinks ‘oh, THIS again’ about something you’ve written you’ve already lost. You’ve lost the right for someone to take you seriously.This still applies if you “know” you are right, and indeed even if multiple independent experts in the field all confirm that they think you’re right too. Talk about it, shout about it, get people onside, but don’t be surprised when no-one cares.We have a heaving drawer full of serious, actual, real problems with scientific papers. They aren’t going anywhere because either the editors or one (or both) of us think that life is too short.Is this fair? No. So be it. Teaspoon of concrete time.9. If you are without doubt, you are wrong.Cranks don’t doubt. Their frame is inappropriate — it is I know and you don’t, and never I think I have a better idea.If you know you’re right, you’re probably wrong.We feel this all the time, even dealing with terrible work from terrible researchers. Even when we’re often right. Even finding the 120,000th mistake, the first thought should always be “Maybe I’m wrong this time.”10. Words matter. Stay away from using the harsh ones.Don’t say fraud, don’t say fabrication. Don’t say reprehensible, irresponsible, stupid, reckless. Don’t say cowardice, malice, idiocy. Do not use the adjectives massive, colossal, wondrous, or monstrous. Do not call the righteous fury of the heavens to blast your enemies asunder. You are not Gandhi, or Caligula, or Immortal Technique. You are an old man(*) with a Yahoo email address.You’re allowed a ‘problematic’ on a good day. Describe problems in functional terms. DON’T attempt to sum up the character of the authors, journal editors, or anyone else for that matter.Yes, even if they ARE stupid, malicious, irresponsible cowards responsible for massive fabrication.ConclusionRemember, you get no leverage from being right. You get leverage from other people accepting that you’re right. Does that frustrate you? Then take up gardening.",How Not To Be A Crank
10033,3956733,2018-03-02 16:54:41,"About TNWTNW SitesThe ecommerce industry could be a lot better — here’s howEvery single day hundreds, if not millions of websites get successfully launched. A good number of them are ecommerce stores that try to sell everything from pins to planes. But, only a handful of them taste the success of the notoriety of the famed ones like Amazon, eBay, Etsy, and the likes.Almost every ecommerce entrepreneur begins his or her venture with the dazzling dream of becoming the next big giant. While the giants and their success stories have already given us much feed for coffee conversation, there is something that most of us, including the aspiring ecommerce entrepreneurs, fail to take notice of.Ecommerce has a lot of problems.Yup. You read that right. The much glorified and trillion-dollar market size boasting ecommerce industry does have its shortcomings. And, these shortcomings are becoming critical with the growth of automation led by AI and machine learning.I know that everybody’s a critic and just pointing out problems doesn’t get us anywhere. That’s why I’ve decided to outline the four major problems facing ecommerce and include what I think could help us solve them. These solutions should go a long way in making shopping not just a casual affair, but a relationship that goes beyond business transactions.Problem 1: It’s not ‘human’ enoughAdmit it. That joy of being personally attended and being helped by a sales assistant in a real store is missing in ecommerce. Even with chatbots for online shopping and AI-powered product suggestions, online shopping is not ‘human’ enough. It lacks a personal touch to it. Poorly designed websites with a slow loading speed and cluttered navigation only make things worse.Solution: Create engaging and personalized copyWell-thought and personalized website copy can help overcome the lack of a human connection. The copy should preferably be in a conversational tone, without being over salesy. Reading ‘Buy now, buy now, buy now!!” often would turn off your customers. Not just your website copy, even product descriptions must have a personal touch to make customers want them more. AO.com plays it neat with copywriting that shines with brilliance not in landing pages but also in product descriptions.Problem 2: Marketing is not persuasive enoughOn an average day, a working professional receives about 121 emails. Only a small portion of this staggering number is actually opened and read. Industry estimates cite a 18 percent open rate for marketing emails.The problem is that an opened email may not necessarily convert. If the marketing email is too salesy, or if it’s not aligned to the interests of the customer, or if it does not retarget a previously shown interest, then the marketing campaign is bound to fail. Personalization holds the key to marketing success.Solution: Create personalized campaignsYou need to care for customers like any other people you have a relationship with. Of course your relationship with a customer is based on his or her desire to pay you for your product or service offering. But, customers are people and they need to be treated as such, so the tendency of a large cross-section of ecommerce stores to looks at customers as mere numbers is unacceptable.The good news is that you can do better than your competition by treating customers like people, like in a true relationship. Personalized campaigns that are tailor-made to the customer’s needs and aspirations are the need of the hour.In fact, global brands like Starbucks have gone a step ahead to create everlasting customer relationships. The coffee brand sends their customer birthday wishes as well as issues them special offers or discount for the day. Truly a thoughtful way any business can treat its customers.Credit: Starbucks MelodyProblem 3: It’s facing cybersecurity threatsCybersecurity is more of a threat than a challenge to ecommerce. Even big players like Target, eBay, and Home Depot have been affected by cyber-attacks in the past. Despite the rise in mobile commerce by 20 percent and in-app payments by 55 percent, customers still have a fear lurking in their minds when it comes to making ecommerce payments. Online payment statistics report that at least 10 percent of customers still prefer a prepaid or gift card as the safest means.Imagine a 10 percent annual loss of business for an online store that does not accept prepaid or gift cards!?Of course, there are tons of online material that show how to safeguard your store from being hacked. Still, there is a need to assure customers that you are a secure website that they can share their private and payment information with.Solution: Display trust indicatorsCustomers want explicit proof that your website is a safe place to carry out online payment transactions. Trust indicators like HTTPS bar, authorized dealer badges, etc. on checkout pages will help customers feel at peace.Installing a SSL certificate will give the HTTPS prefix and a green address bar for your website. The HTTPS address bar is great since it signals your visitors that your website is safe to entrust information with. You can choose from Organizational Validation, Domain Validation, or Extended Validation certificate that will secure your website as well as boost customer confidence.There is a growing need to make ecommerce a digital fortress. Things are already looking up, thanks to Google, WordPress, and several other internet organizations that are driving the web towards a safer environment. HTTPS everywhere and similar movements are bound to make ecommerce more secure for customers.Problem 4: Brand engagement ends with the store websiteEcommerce is actually an incredibly broad term, despite the fact that we’ve come to accept it as the synonym for online shopping. There isn’t any major harm in this common misconception, but it has led retailers to limit themselves to selling on their websites — which is a mistake you should avoid.In today’s omnichannel world, this website-online business model can backfire. We live in a hyperconnected world where customers use more apps, social media, and instant messages more than anything else. Ecommerce has become closely tied with social networking and instant messaging. Unfortunately, not many retailers are leveraging social networking the way it should be. Despite a 133 percent higher conversion rate, social commerce in the US amounted to only five percent of the entire ecommerce market.Solution: Meet customers where they areYour ecommerce venture does not end with having a good website along. To thrive digitally it also needs to build a social media presence across multiple digital channels where customers frequent. Social media, has given birth to a whole new breed of ecommerce called social commerce. Image-sharing platforms Instagram, Pinterest, Tumblr, are turning into virtual POS outlets where customers can buy the products that they see and like.Zappos has a rather funky way of capturing the social crowd’s attention with its social media campaigns. You can get plenty of inspiration from their social media handles like Hootsuite, Oreo, and Gatorade. Credit: SocialatedEcommerce is great, but it could be awesomeEcommerce has come a long way from the classified looking home pages. On demand ordering and same day deliveries are shaping the next frontiers of ecommerce. Drones are all set to take over the delivery process and Amazon Go promises cashless checkout. All this and much more to feel great about online shopping.Nevertheless, it’s not foolproof and is riddled with some shortcomings like what I have tried to express in this blog. The industry as a whole needs an upheaval of sorts which customers and their shopping experience would be put right at the center.To sum it up, an ecommerce store that is humane and takes care of the wants and needs of its customers will leave any competitors in the dust.",The ecommerce industry could be a lot better — here’s how
10034,3959418,2018-03-04 14:30:00,"Here's what you missed at MWC 2018The Galaxy S9 and... Not a whole lot else.This year's Mobile World Congress has been strangely quiet. Despite Samsung's return to the event to launch its latest flagship phones and Google unveiling new Android Go devices, the convention has been almost uneventful. In fact, the most interesting thing to have happened this show has been snow falling in Barcelona, with temperatures dipping close to 38 degrees Fahrenheit (almost 0 degrees Celsius). Although Nokia's parent company tried to drum up interest by reviving an old favorite like it did before, people just didn't care as much the second time around.There was plenty of news around 5G developments, since an early version of the spec was approved late last year. Every major company had its own demos of achieving 5G speeds on all varieties of communications -- from phones and detachables to cars, drones and VR headsets. FCC Chairman Ajit Pai himself was here to share plans to open up 28GHz and 24GHz spectrum for auction late this year, as well as to discuss his belief that the US needs light regulation to ""enable the 5G applications of the future.""Despite it being an underwhelming show this year, there were still a few surprising reveals from the likes of LG, ASUS and Huawei. To catch up on all that you might have missed, check out the video above.Cherlynn is reviews editor of Engadget. She led a mostly unexciting life in Singapore, her home country, until she came to New York in 2012. Since then, she's earned her master's in journalism from Columbia University's Graduate School of Journalism and covered smartphones and wearables for Laptop Mag and Tom's Guide. Life is now like a Hollywood movie, with almost as many lights and much more Instagram. And also more selfies.",Here's what you missed at MWC 2018
10035,3959512,2018-02-14 21:17:17,"Nick Brown does not look like your average student. He's 53 for a start and at 6ft 4in with a bushy moustache and an expression that jackknifes between sceptical and alarmed, he is reminiscent of a mid-period John Cleese. He can even sound a bit like the great comedian when he embarks on an extended sardonic riff, which he is prone to do if the subject rouses his intellectual suspicion.A couple of years ago that suspicion began to grow while he sat in a lecture at the University of East London, where he was taking a postgraduate course in applied positive psychology. There was a slide showing a butterfly graph – the branch of mathematical modelling most often associated with chaos theory. On the graph was a tipping point that claimed to identify the precise emotional co-ordinates that divide those people who ""flourish"" from those who ""languish"".According to the graph, it all came down to a specific ratio of positive emotions to negative emotions. If your ratio was greater than 2.9013 positive emotions to 1 negative emotion you were flourishing in life. If your ratio was less than that number you were languishing.It was as simple as that. The mysteries of love, happiness, fulfilment, success, disappointment, heartache, failure, experience, random luck, environment, culture, gender, genes, and all the other myriad ingredients that make up a human life could be reduced to the figure of 2.9013.It seemed incredible to Brown, as though it had been made up. But the number was no invention. Instead it was the product of research that had been published, after peer review, in no less authoritative a journal than American Psychologist – the pre-eminent publication in the world of psychology that is delivered to every member of the American Psychological Association. Co-authored by Barbara Fredrickson and Marcial Losada and entitled Positive Affect and the Complex Dynamics of Human Flourishing, the paper was subsequently cited more than 350 times in other academic journals. And aside from one partially critical paper, no one had seriously questioned its validity.Masters student Nick Brown: 'If you want to be a whistleblower you have to be prepared to lose your job. I’m able to do what I’m doing here because I’m nobody.'Fredrickson is a distinguished psychologist, a professor at the University of North Carolina, a winner of several notable psychology awards and bestselling author of a number of psychology books, including Positivity, which took her and Losada's academic research and recast it for a mass audience – the subtitle ran ""Top-Notch Research Reveals the 3-to-1 Ratio That Will Change Your Life"".""Just as zero degrees celsius is a special number in thermodynamics,"" wrote Fredrickson in Positivity, ""the 3-to-1 positivity ratio may well be a magic number in human psychology.""Fredrickson is the object of widespread admiration in the field of psychology. Martin Seligman, former president of the American Psychological Association and a bestselling author in his own right, went so far as to call her ""the genius of the positive psychology movement"". On top of which she is also an associate editor at American Psychologist.By contrast, Brown was a first-term, first-year, part-time masters student who was about to take early retirement from what he calls a ""large international organisation"" in Strasbourg, where he had been head of IT network operations. Who was he to doubt the work of a leading professional which had been accepted by the psychological elite? What gave him the right to suggest that the emperor had gone naturist?""The answer,"" says Brown when I meet him in a north London cafe, ""is because that's how it always happens. Look at whistleblower culture. If you want to be a whistleblower you have to be prepared to lose your job. I'm able to do what I'm doing here because I'm nobody. I don't have to keep any academics happy. I don't have to think about the possible consequences of my actions for people I might admire personally who may have based their work on this and they end up looking silly. There are 160,000 psychologists in America and they've got mortgages. I've got the necessary degree of total independence.""Armed with that independence, he went away and looked at the maths that underpinned Fredrickson and Losada's ratio. Complex or non-linear dynamics are not easy for an untrained mathematician to understand, much less work out. Losada, who claimed expertise in non-linear dynamics, was working as a business consultant and making mathematical models of business team behaviour when he first met Fredrickson.In Positivity, Fredrickson describes the moment when Losada explained how he could apply complex dynamics to her theories of positive psychology. ""Hours into our lively discussion, he made a bold claim: based on his mathematical work, he could locate the exact positivity ratio that would distinguish those who flourished from those who didn't.""So impressed was she by this boast that Fredrickson arranged a sabbatical from her teaching duties ""so I could immerse myself in the science of dynamic systems that Marcial had introduced me to"".There were several psychologists, versed in non-linear dynamics, who smelt something fishy about the maths in the published paper. Stephen Guastello, from Marquette University, wrote a note of mild complaint to American Psychologist, which it chose not to publish because ""there wasn't enough interest in the article"". Guastello feels now that he should have been more forceful in his opinions. ""In retrospect,"" he says, ""I see how I could have been more clearly negative and less supportive of what looked like an article that could move the field forward if someone would follow up with some strong empirical work.""John Gottman, a leading authority in the psychology of successful relationships, wrote to Losada because he couldn't follow the equations. ""I thought it was something I didn't know about, because he's a smart guy, Losada. He never answered my email,"" he says. Gottman also wrote to Fredrickson. ""She said she didn't understand the math either.""""Not many psychologists are very good at maths,"" says Brown. ""Not many psychologists are even good at the maths and statistics you have to do as a psychologist. Typically you'll have a couple of people in the department who understand it. Most psychologists are not capable of organising a quantitative study. A lot of people can get a PhD in psychology without having those things at their fingertips. And that's the stuff you're meant to know. Losada's maths were of the kind you're not meant to encounter in psychology. The maths you need to understand the Losada system is hard but the maths you need to understand that this cannot possibly be true is relatively straightforward.""Brown had studied maths to A-level and then took a degree in engineering and computer science at Cambridge. ""But I actually gave up the engineering because the maths was too hard,"" he says, laughing at the irony. ""So I'm really not that good at maths. I can read simple calculus but I can't solve differential equations. But then neither could Losada!""He went back over Losada's equations and he noticed that if he put in the numbers Fredrickson and Losada had then you could arrive at the appropriate figures. But he realised that it only worked on its own terms. ""When you look at the equation, it doesn't contain any data. It's completely self-referential.""Unfortunately, while his grasp of maths was strong enough to see the problem, it wasn't sufficiently firm to be able to mount an academic takedown of Fredrickson's and Losada's work. Yet that was what he wanted to do. Once he knew to his own satisfaction that their research was fundamentally flawed, he was not going to be content to let things pass. So he decided to seek the help of an academic mathematician. Not just any academic mathematician either, but one who had made a name for himself by puncturing the bogus use of maths and science in another discipline.Back in 1996, Alan Sokal wrote a paper called Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity and submitted it to an academic cultural studies journal called Social Text, which promptly published the article. As the title suggested, the paper was dense with impenetrable theory. Among other things, it disparaged the scientific method and western intellectual hegemony and claimed that quantum gravity could only be understood through its political context.The paper, as Sokal quickly admitted, was a hoax, a deliberate pastiche of the sorts of nonsensical postmodern appropriations of maths and physics at which French critical theorists particularly excelled – among them Jacques Derrida, Jacques Lacan, Gilles Deleuze and Julia Kristeva. A major intellectual controversy ensued in which postmodernists stood accused of pseudo-science, absurd cultural relativism and the concealing of ignorance and innumeracy behind obscurantist prose. In response Sokal was derided as a pedant, a literalist and a cultural imperialist.Despite the counterattacks, Sokal gained a reputation as a formidable enemy of bad science. As such he was regularly approached by people who believed they had uncovered an intellectual imposture, be it in architecture, history or musicology.""I don't think I'm a crank,"" Brown had said in his email to Sokal. ""I am just this grad student with no qualifications or credentials, starting out in the field. I don't know how to express this kind of idea especially coherently in academic written form, and I suspect that even if I did, it would be unlikely to be published.""But like many such requests, it began to disappear beneath a pile of other emails. It was only several weeks later that Sokal came across it again and realised that on this occasion he could help because it was in a field he knew something about: mathematics and physics.""The Lorenz equation Losada used was from fluid dynamics,"" says Sokal, ""which is not the field that I'm specialised in, but it's elementary enough that any mathematician or physicist knows enough. In 10 seconds I could see it was total bullshit. Nick had written a very long critique and basically it was absolutely right. There were some points where he didn't quite get the math right but essentially Nick had seen everything that was wrong with the Losada and Fredrickson paper.""Sokal did a little research and was amazed at the standing the Fredrickson and Losada paper enjoyed. ""I don't know what the figures are in psychology but I know that in physics having 350 citations is a big deal,"" he says. ""Look on Google you get something like 27,000 hits. This theory is not just big in academia, there's a whole industry of coaching and it intersects with business and business schools. There's a lot of money in it.""The concept of positive thinking dates back at least as far as the ancient Greeks. Throughout written history, metaphysicians have grappled with questions of happiness and free will. The second-century Stoic sage Epictetus argued that ""Your will needn't be affected by an incident unless you let it"". In other words, we can be masters and not victims of fate because what we believe our capability to be determines the strength of that capability.In one way or another, positive thinking has always been concerned with optimising human potential, which is a key component of psychology. But in the 20th century, confronting the great traumas of two annihilating wars, the psychology profession became increasingly focused on the dysfunctional and pathological aspects of the human mind. The emphasis was on healing the ill rather than improving the well.So it was left to popular or amateur psychology, and in particular that sector specialising in business success, to accentuate the positive. Books such as Norman Vincent Peale's The Power of Positive Thinking, published in 1952, became huge bestsellers. By the 1970s and 1980s, self-help had mushroomed into a vast literary genre that encompassed everything from the secrets of material achievement to the new age promises of chakras, reiki and self-realisation.On becoming president of the American Psychological Association in 1998, Martin Seligman set out to bring scientific rigour to the issue of self-improvement. In his inaugural speech, he announced a shift in psychology towards a ""new science of human strengths"".Barbara Frederickson, associate editor of American Psychologist, accepts the errors in the maths that Nick Brown pointed out, but still stands by her theory of positivity.""It's my belief,"" said Seligman, ""that since the end of the second world war, psychology has moved too far away from its original roots, which were to make the lives of all people more fulfilling and productive, and too much toward the important, but not all-important, area of curing mental illness.""He called for ""a reoriented science that emphasises the understanding and building of the most positive qualities of an individual"". It was an optimistic period in American history. The economy was buoyant, US geopolitical power was unchallenged and no major conflicts were raging. As a result, there was almost a messianic note of global ambition in Seligman's address. ""We can show the world what actions lead to wellbeing, to positive individuals, to flourishing communities, and to a just society,"" he declared.Suddenly a plethora of positive psychology books began to appear, written by eminent psychologists. There was Flow: The Psychology of Happiness by Mihaly Csikszentmihalyi, who with Seligman is seen as the co-founder of the modern positive psychology movement; Authentic Happiness: Using the New Positive Psychology to Realise Your Potential for Lasting Fulfilment by Seligman himself. And of course Fredrickson's Positivity, approved by both Seligman and Csikszentmihalyi. Each of them appeared to quote and promote one another, creating a virtuous circle of recommendation.And these books were not only marketed like a previous generation of self-help manuals, they often shared the same style of cod-sagacious prose. ""Positivity opens your mind naturally, like the water lily that opens with sunlight,"" writes Fredrickson in Positivity.Then there was the lucrative lecture circuit. Both Seligman and Fredrickson are hired speakers. One website lists Seligman's booking fee at between $30,000 and $50,000 an engagement. In this new science of happiness, it seemed that all the leading proponents were happy.But then Nick Brown started to ask questions.Around the time Brown first came across Fredrickson's work, a case came to light in Holland in which a psychologist called Diederik Stapel, who was dean of faculty at Tilburg University, was caught by his graduate students making up data. It turned out he'd been falsifying his research for the previous 15 years. Brown, who is currently translating Stapel's autobiography, got in touch with him and asked him why he did it.""The way he describes it,"" says Brown, ""is that the environment was conducive to it. He said, 'I could either do the hard work or put my hand in the jar and take out a biscuit'."" It does a massive amount of harm to science when this sort of thing happens. Nobody's accusing Fredrickson of making anything up. She just basically invented her own method. Is that worse than inventing your own data?""After he had established contact with Sokal, Brown sent him a 15,000-word draft, which was much too long for publication. At first the professor agreed to give Brown advice on cleaning up the draft. He also told him that he should go to American Psychologist, and he contributed a pedagogic section, explaining the maths.""I still wasn't thinking that I was going to be a co-author but Nick sent me drafts and I just liked his writing style,"" recalls Sokal. ""It made me laugh. He had this gift for English understatement.""Getting their critique of Fredrickson into the publication of which she was an associate editor was a tall order. To help him get across the line, Brown had already recruited Harris Friedman, a sympathetic psychologist who had doubts about Fredrickson's claims but was not sufficiently versed in maths to make a case on his own.Sending revised versions back and forth among themselves, the three men gradually composed what they considered to be a watertight argument. The initial title they submitted to American Psychologist was The Complex Dynamics of an Intellectual Imposture – an ironic play on Fredrickson and Losada's original piece. That was rejected by the editor because he argued that the word ""imposture"" implied a deliberate fraud on the part of Fredrickson and Losada.Sokal insists that this was never their intention. As Brown puts it in characteristic manner. ""This particular paper wasn't an act of fraud and it wasn't about statistics. It's that someone had a brain-fart one day.""Following much negotiation, Brown, Sokal and Friedman had their paper accepted by American Psychologist and it was published online last July under the only slightly less provocative title of The Complex Dynamics of Wishful Thinking. Referring to the bizarrely precise tipping point ratio of 2.9013 that Fredrickson and Losada trumpeted applied to all humans regardless of age, gender, race or culture, the authors – in fact Brown, in this sentence – wrote: ""The idea that any aspect of human behaviour or experience should be universally and reproducibly constant to five significant digits would, if proven, constitute a unique moment in the history of the social sciences.""The paper mounted a devastating case against the maths employed by Fredrickson and Losada, who were offered the chance to respond in the same online issue of American Psychologist. Losada declined and has thus far failed to defend his input in any public forum. But Fredrickson did write a reply, which, putting a positive spin on things, she titled Updated Thinking on Positivity Ratios.She effectively accepted that Losada's maths was wrong and admitted that she never really understood it anyway. But she refused to accept that the rest of the research was flawed. Indeed she claimed that, if anything, the empirical evidence was even stronger in support of her case. Fredrickson subsequently removed the critical chapter that outlines Losada's input from further editions of Positivity. She has avoided speaking to much of the press but in an email exchange with me, she maintained that ""on empirical grounds, yes, tipping points are highly probable"" in relation to positive emotions and flourishing.""She's kind of hoping the Cheshire cat has disappeared but the grin is still there,"" says Brown, who is dismissive of Fredrickson's efforts at damage limitation. ""She's trying to throw Losada over the side without admitting that she got conned. All she can really show is that higher numbers are better than lower ones. What you do in science is you make a statement of what you think will happen and then run the experiment and see if it matches it. What you don't do is pick up a bunch of data and start reading tea leaves. Because you can always find something. If you don't have much data you shouldn't go round theorising. Something orange is going to happen to you today, says the astrology chart. Sure enough, you'll notice if an orange bicycle goes by you.""But social psychology is full of theorising and much of it goes unquestioned. This is particularly the case when the research involves, as it does with Fredrickson, self-report, where the subjects assess themselves.As John Gottman says: ""Self-report data is easier to obtain, so a lot of social psychologists have formed an implicit society where they won't challenge one another. It's a collusion that makes it easier to publish research and not look at observational data or more objective data.""In general, says Gottman, the results of self-report have been quite reliable in the area of wellbeing. The problem is that when it comes down to distinguishing, say, those who ""languish"" from those who ""flourish"", there may be all manner of cultural and personal reasons why an individual or group might wish to deny negative feelings or even downplay positive ones.""It's a lot more complicated than Fredrickson is suggesting,"" says Gottman.After initially being turned down, Brown, Sokal and Friedman went through American Psychologist's lengthy appeals procedure and won the right to reply to Fredrickson's reply. They are currently working on what is certain to be a very carefully considered response. But it doesn't take a psychologist to work out that, given the nature of human behaviour, it's unlikely to be the last word.",The British amateur who debunked the mathematics of happiness
10036,3959518,2018-03-01 13:00:00,"Researchers say bad engineering, not a deliberate attack, may be to blamePhoto: Alexandre Meneghini/Reuters Sonic Mystery: At least 24 employees of the U.S. embassy in Cuba heard high-pitched sounds between December 2016 and August 2017, and suffered injuries thought to be related to the noise.Last August, reports emerged that U.S. and Canadian diplomats in Cuba had suffered a host of mysterious ailments. Speculation soon arose that a high-frequency sonic weapon was to blame. Acoustics experts, however, were quick to point out the unlikeliness of such an attack. Among other things, ultrasonic frequencies—from 20 to 200 kilohertz—don’t propagate well in air and don’t cause the ear pain, headache, dizziness, and other symptoms reported in Cuba. Also, some victims recalled hearing high-pitched sounds, whereas ultrasound is inaudible to humans.The mystery deepened in October, when the Associated Press (AP) released a 6-second audio clip, reportedly a recording of what U.S. embassy staff heard. The chirping tones, centered around 7 kHz, were indeed audible, but they didn’t suggest any kind of weapon.“Given all the possible explanations, this definitely seems the most plausible and the most technically feasible” —Fadel Adib, MITLooking at a spectral plot of the clip on YouTube, Kevin Fu, a computer scientist at the University of Michigan, noted some unusual ripples. He thought he might know what they meant.Fu’s lab specializes in analyzing the cybersecurity of devices connected to the Internet of Things, such as sensors, pacemakers, RFIDs, and autonomous vehicles. That work has taught him that modern electronics often behave in unpredictable ways and that such devices can be manipulated—intentionally or inadvertently—using carefully crafted acoustic or radio interference. To Fu, the ripples in the spectral readout suggested some kind of interference.He discussed the AP clip with his frequent collaborator, Wenyuan Xu, a professor at Zhejiang University, in Hangzhou, China, and her Ph.D. student Chen Yan. “We saw it as an interesting puzzle,” says Xu, whose lab works on embedded security, including the use of ultrasound and radio waves to fool voice-recognition systems and self-driving cars. “It was a lot of fun to try to solve it.”“I thought it might be subharmonics,” Fu recalls. “But a week later, Chen said, ‘No, Kevin, you’re wrong, and I just did an experiment to prove it.’ ”Yan and Xu started with a fast Fourier transform of the AP audio, which revealed the signal’s exact frequencies and amplitudes. Then, through a series of simulations, Yan showed that an effect known as intermodulation distortion could have produced the AP sound. Intermodulation distortion occurs when two signals having different frequencies combine to produce synthetic signals at the difference, sum, or multiples of the original frequencies.When signal processing equipment behaves in a nonlinear way, it can cause this type of distortion. For example, Fu says, microphone circuitry can exhibit nonlinear behavior, and waves propagating through air can also behave in a nonlinear fashion. “As acoustic waves containing multiple frequencies travel through a nonlinear system, you can get these bizarre ripples in the spectrum of the signal,” he explains. “At the same time, intermodulation distortion can produce lower-frequency signals than the original signals. In other words, inaudible ultrasonic waves going through air can produce audible by-⁠products.”Yan followed up the simulations with lab experiments, in which he used two ultrasonic speakers, one emitting a signal at 25 kHz and the other at 32 kHz. When he crossed the two signals, it produced the telltale high-pitched sound at 7 kHz, which was equal to the difference between the two speakers’ frequencies—and the same frequency as in the AP audio. In a nod to the Internet meme “rickrolling,” Yan was even able to embed an ultrasonic version of the Rick Astley song “Never Gonna Give You Up,” which became audible at the point where the two signals crossed.Having reverse engineered the AP audio, Fu, Xu, and Yan then considered what combination of things might have caused the sound at the U.S. embassy in Cuba. “If ultrasound is to blame, then a likely cause was two ultrasonic signals that accidentally interfered with each other, creating an audible side effect,” Fu says. There are existing sources of ultrasound in office environments, such as room-occupancy sensors [see, for example, “How an Ultrasonic Sensor Nearly Derailed a Ph.D. Thesis”]. “Maybe there was also an ultrasonic jammer in the room and an ultrasonic transmitter,” he suggests. “Each device might have been placed there by a different party, completely unaware of the other.”One thing the investigation didn’t explore was whether the AP audio could have produced the wide range of symptoms, including brain damage, that afflicted embassy workers. “We know that audible signals can cause pain, but we didn’t look at the physiological effects beyond that,” Fu says. At press time, the FBI had yet to announce the results of its investigation. A panel of Cuban scientists and medical doctors, meanwhile, concluded that a “collective psychogenic disorder” brought on by stress may have been at work.Fadel Adib, a professor at MIT who specializes in wireless technology for sensing and communications, calls the study by Fu and his colleagues “a creative take on what might have happened.” Adib, who wasn’t involved in the research but reviewed the results, adds that wireless signals can and do interact with one another. “And if that happens, you’ll hear signals you wouldn’t expect to hear,” he says. “Given all the possible explanations, this definitely seems the most plausible and the most technically feasible.”Fu is careful to offer a caveat: “Of course, we don’t know for certain this was the cause. But bad engineering just seems much more likely than a sonic weapon.”This article appears in the March 2018 print issue as “Reverse Engineering the ‘Sonic Weapon.’ ”Editor’s note: An article by Kevin Fu, Wenyuan Xu, and Chen Yan about their research will be published by IEEE Spectrum in March. Their technical report, “On Cuba, Diplomats, Ultrasound, and Intermodulation Distortion,” is available on the Security and Privacy Group’s website [PDF].","Finally, a Likely Explanation for the “Sonic Weapon” Used at the U.S. Embassy in Cuba"
10037,3962321,2018-03-04 18:16:36,"Snapchat is stuck in the uncanny valley of AR glasses0“Timing”, Snapchat CEO Evan Spiegel said cryptically when asked what the greatest threat was for Snap Inc. “I think the big risks are always the really big product ideas that we’re investing in that are just hard to get right” he told the Goldman Sachs conference two weeks ago.The statements got lost amongst flashier quotes. He defended the Snapchat redesign saying “Even the complaints we’re seeing reinforce the philosophy”, and described Snap’s office atmosphere as “just below the boil…. Like when you heat water, and it’s really fucking hot, but it’s just below the boil.”Yet it’s his thoughts on ‘timing’ that give us the deepest insight into Snapchat’s toughest problem: overcoming reality.“I think if we look at the future, there’s a lot of different components to what we’re trying to accomplish and timing is an important one of those — especially in the technology business, both in terms of consumers’ willingness to try new products, but also in terms of technological development and what that can power in a set amount of time. As I’m looking at the next decade, big products that we’re trying to develop, I think timing is a funny thing. That’s probably what I’ll have my eye on as we just continue to try to develop products, and we’re willing to wait to get ’em right but that’s probably the risk.”For Snap, no idea is bigger than building augmented reality glasses that pack world-morphing features into a stylish form factor. I believe this is what Spiegel was hinting at. He knows Snap is mired in an uncanny valley between the lackluster truth of today’s augmented reality hardware, and the desireable AR gadgets that are years beyond our current engineering prowess.The fact is that no one has been able to build this. Google Glass flopped with consumers and pivoted to the enterprise. Magic Leap’s bulky headset requires a backpack or beltpack to power it, and is still just a rendering. Facebook has nothing to show and Apple has kept anything it’s tinkering with secret. Intel’s Vaunt glasses are perhaps the closest — small and stylish, but merely delivering text notifications and directions projected onto your retina.We’re still a long way from compact AR glasses that can overlay virtual objects onto the real world the way Snapchat’s smartphone app does. Perhaps years. And that’s the “timing” risk Spiegel described. Snap might be willing to wait, or more accurately, forced to wait. But the public markets might not be so patient, and bigger, better funded hardware-first companies are racing for the prize.Just how slow Snapchat is inching towards the goal was revealed yesterday by Alex Heath of Cheddar’s stellar scoop on the future of Snap Spectacles. He reports that Snap is preparing to launch v2 of Spectacles later this year and v3 in 2019. But they won’t deliver on the fever dream of AR glasses that seamlessly alter our lives.v1 of Spectacles emerged in Fall 2016 with a way to record circular first-person video from a camera built into frame. But getting the videos off the glasses and into your phone in high resolution proved a buggy hassle. Despite a hyped up launch with lines outside its Snap Bot vending machines in surprise locations, their utility was limited.Word of a new version of Spectacles might surprise some. Leaked data showed less than half of owners kept using them after a month with many ditching Spectacles after just a week. The best camera is the one you have with you. But between recharging via proprietary cable, their fragility, and their bulky triangular case, people rarely had their Spectacles when they needed them. Snap only sold about 150,000 pairs, with hundreds of thousands lying unsold in warehouses, and it registered a $40 million write-off on the hardware business.After 2 years of improvements, Spectacles v2 will reportedly be…water resistant, available in new colors, and have fewer bugs. That’s little progress in a long time. And the launch has already missed internal deadlines.v3 is due in 2019 and is supposed to be a little more ambitious with two cameras to add 3D depth effects to the videos they record. But that’s still a far cry from us digitally hallucinating a dancing hot dog through Spectacles themselves.This all brings the “timing” problem into focus. Snapchat claims to be a “camera company” but doesn’t make smartphones, which everyone uses as cameras. It’s done plenty with just software, bringing augmented reality to the masses through puppy ears and rainbow puke. Still, the world is waiting for AR glasses, and Snap can’t make them yet.What it has built, and what’s on its now-revealed roadmap for the next two years, doesn’t cut it. The concept known as the “uncanny valley” explains our revulsion to humanoid objects such as androids, life-like dolls, and 3D animation that don’t convincingly appear human. You’d expect that the more human-esque something looks, the more warmly we’d receive it. But in fact our positive perception drops into a valley until something reaches a high threshold of accuracy in mimicking humans.I believe Snapchat is stuck in the uncanny valley of AR glasses. At the start of the spectrum are our smartphones. They can produce somewhat convincing augmented reality…as long as you suspend your disbelief triggered by holding your phone in front of your face. At the far end are the AR glasses we imagine, which look just like normal glasses or even contact lenses, but that can overlay convincing AR objects on our view such that we can’t tell they’re not real.In the middle are Snapchat Spectacles, with all the stigma attached. Beyond shooting circular video from the first-person perspective, they’re arguably much worse cameras than our phones. Poor resolution, limited battery life, video-only, tough to export from, can’t take selfies, and they lack the auto-stabilized swiveling gimbals that are our hands.And from a form factor standpoint, they’re obtrusive, literally putting a barrier between us and everyone else, while scaring them that we might be recording. Unless they elevate you to having technological super-powers, which they don’t, they inhibit your ability to interact with humanity. If you wear Spectacles, you might get called a glasshole. Meanwhile your phone is always with you, but resides humbly in your pocket or purse.As described, Spectacles v2 and v3 won’t change this equation. That’s why I’d be surprised if they sell well enough to become a serious hardware business for Snap, or deliver enough unique content to boost Snapchat usage.The company’s irreverent brand and roguish design aesthetic let it get away with unpolished products that would never meet Apple’s standards. Still, Snap will likely have to spend years and billions of dollars to get to a v5 or v6 of Spectacles that are able to claw their way out of the uncanny valley. It may be running short on both time and money.Snap lost $350 million last quarter and $3.5 billion in 2017. It’s got just $2 billion in cash and securities in the bank. Facebook is waging total war on Snapchat, cloning its features, blocking its user growth, and jockeying for its vertical video ad dollars. By the time Snap gets to viable AR Spectacles, the company founded in 2011 could be ten years old and starting to lose its luster with the next generation of teens. And each day, Apple, Google, or another hardware giant gets closer to launching their own market-defining glasses.Snapchat is and has always been in Spiegel’s hands, between his voting rights and product instincts. If he’s compelled to bet Snap’s future on AR glasses, no one can dissuade him. But the timeline of pure technological progress is one thing beyond the CEO’s control. As I wrote when assessing the potential of Snap’s IPO, it all comes down to “do you believe in Evan Spiegel?” When he says “timing” is the company’s biggest risk, expect that Snap could spend an excruciatingly long sentence locked in the valley.",Snapchat is stuck in the uncanny valley of AR glasses
